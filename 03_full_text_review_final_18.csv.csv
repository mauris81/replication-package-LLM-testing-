"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"8J24MLCH","conferencePaper","2024","Qi, Fei; Hou, Yingnan; Lin, Ning; Bao, Shanshan; Xu, Nuo","A Survey of Testing Techniques Based on Large Language Models","Proceedings of the 2024 International Conference on Computer and Multimedia Technology","979-8-4007-1826-7","","10.1145/3675249.3675298","https://doi.org/10.1145/3675249.3675298","With the development of software testing technology, Large Language Model (LLM) driven testing method have gradually become an emerging trend in the field of software testing. This paper presents a comprehensive review of LLM-based testing techniques. The results of 19 studies using LLM to optimize testing techniques are analyzed from the perspective of software testing. This paper discusses in detail how to use LLM to optimize test techniques for generating automated test code and generating diverse input in software test tasks. It also summarizes the challenges and opportunities faced by this field. The above conclusions can identify the shortcomings of LLM-based software testing technology and the direction of future research.","2024","2025-11-25 22:29:34","2025-11-25 22:29:34","","280–284","","","","","","","ICCMT '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sanming, China","","","","LLM; Pre-trained Large Language Model; Software Testing Techniques","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"65SJKKLB","conferencePaper","2025","Fang, Zihan; Li, Jiliang; Liang, Anda; Bai, Gina R.; Huang, Yu","A Comparative Study on ChatGPT and Checklist as Support Tools for Unit Testing Education","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","10.1145/3696630.3727244","https://doi.org/10.1145/3696630.3727244","Testing is widely practiced in software engineering, and many tools have been developed to support students in learning testing. Prior research suggests that a lightweight testing checklist improves learning outcomes but doesn't address students' challenges in writing test code that matches their intentions or design. Meanwhile, generative AI tools (e.g., ChatGPT) bring new promise as another form of software assistance tool. In this study, we examined the impact of various support tools (checklist, ChatGPT, or both) on unit testing among 42 students. Our results indicated that using these tools individually or in combination produced a comparable effect on student performance in unit testing. Students preferred using the checklist but acknowledged ChatGPT's effectiveness in accelerating task completion and addressing programming language challenges. While ChatGPT demonstrated potential benefits for testing education, it did not overcome the implementation challenges identified in the previous study. Moreover, reliance on ChatGPT may hinder students' deeper engagement with new concepts, which is crucial for comprehensive learning, as they often interact superficially with AI-generated responses without employing the critical thinking necessary to evaluate the information provided. Therefore, we proposed recommendations for both students and instructors on adapting to learning and teaching in the AI era and offer insights into the evolving role of AI in education.","2025","2025-11-25 22:29:35","2025-11-25 22:29:35","","871–882","","","","","","","FSE Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","ChatGPT; unit testing; checklist; testing education","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UQLDFJKI","conferencePaper","2025","Yu, Junji; Shu, Honglin; Fu, Michael; Wang, Dong; Tantithamthavorn, Chakkrit; Kamei, Yasutaka; Chen, Junjie","A Preliminary Study of Large Language Models for Multilingual Vulnerability Detection","Proceedings of the 34th ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-1474-0","","10.1145/3713081.3731746","https://doi.org/10.1145/3713081.3731746","Deep learning-based approaches, particularly those leveraging pre-trained language models (PLMs), have shown promise in automated software vulnerability detection. However, existing methods are predominantly limited to specific programming languages, restricting their applicability in multilingual settings. Recent advancements in large language models (LLMs) offer language-agnostic capabilities and enhanced semantic understanding, presenting a potential solution to this limitation. While existing studies have explored LLMs for vulnerability detection, their detection performance remains unknown for multilingual vulnerabilities. To address this gap, we conducted a preliminary study to evaluate the effectiveness of PLMs and state-of-the-art LLMs across seven popular programming languages. Our findings reveal that the PLM CodeT5P achieves the best performance in multilingual vulnerability detection, particularly in identifying the most critical vulnerabilities. Based on these results, we further discuss the potential of LLMs in advancing real-world multilingual vulnerability detection. This work represents an initial step toward exploring PLMs and LLMs for cross-language vulnerability detection, offering key insights for future research and practical deployment.","2025","2025-11-25 22:29:38","2025-11-25 22:29:38","","161–168","","","","","","","ISSTA Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","large language model; multilingual vulnerability; vulnerability detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TYEMY7NT","conferencePaper","2024","Yan, Dapeng; Gao, Zhipeng; Liu, Zhiming","A Closer Look at Different Difficulty Levels Code Generation Abilities of ChatGPT","Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering","979-8-3503-2996-4","","10.1109/ASE56229.2023.00096","https://doi.org/10.1109/ASE56229.2023.00096","","2024","2025-11-25 22:29:38","2025-11-25 23:11:13","","1887–1898","","","","","","","ASE '23","","","","IEEE Press","Echternach, Luxembourg","","","","","","","","","","","","large language model; code generation; Chat-GPT; clean code; program competition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RUB9Y26K","journalArticle","2024","Hou, Xinyi; Zhao, Yanjie; Liu, Yue; Yang, Zhou; Wang, Kailong; Li, Li; Luo, Xiapu; Lo, David; Grundy, John; Wang, Haoyu","Large Language Models for Software Engineering: A Systematic Literature Review","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3695988","https://doi.org/10.1145/3695988","Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a Systematic Literature Review (SLR) on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We selected and analyzed 395 research articles from January 2017 to January 2024 to answer four key Research Questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, pre-processing, and application, highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and highlighting promising areas for future study. Our artifacts are publicly available at .","2024-12","2025-11-25 22:29:39","2025-11-25 22:29:39","","","","8","33","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Software Engineering; Survey","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6XQYBAEN","conferencePaper","2025","Kim, Myeongsoo; Stennett, Tyler; Sinha, Saurabh; Orso, Alessandro","A Multi-Agent Approach for REST API Testing with Semantic Graphs and LLM-Driven Inputs","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00179","https://doi.org/10.1109/ICSE55347.2025.00179","As modern web services increasingly rely on REST APIs, their thorough testing has become crucial. Furthermore, the advent of REST API documentation languages, such as the OpenAPI Specification, has led to the emergence of many black-box REST API testing tools. However, these tools often focus on individual test elements in isolation (e.g., APIs, parameters, values), resulting in lower coverage and less effectiveness in fault detection. To address these limitations, we present AutoRestTest, the first black-box tool to adopt a dependency-embedded multi-agent approach for REST API testing that integrates multi-agent reinforcement learning (MARL) with a semantic property dependency graph (SPDG) and Large Language Models (LLMs). Our approach treats REST API testing as a separable problem, where four agents—API, dependency, parameter, and value agents—collaborate to optimize API exploration. LLMs handle domain-specific value generation, the SPDG model simplifies the search space for dependencies using a similarity score between API operations, and MARL dynamically optimizes the agents' behavior. Our evaluation of AutoRestTest on 12 real-world REST services shows that it outperforms the four leading black-box REST API testing tools, including those assisted by RESTGPT (which generates realistic test inputs using LLMs), in terms of code coverage, operation coverage, and fault detection. Notably, AutoRestTest is the only tool able to trigger an internal server error in the Spotify service. Our ablation study illustrates that each component of AutoRestTest—the SPDG, the LLM, and the agent-learning mechanism—contributes to its overall effectiveness.","2025","2025-11-25 22:29:43","2025-11-25 22:46:39","","1409–1421","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","Large language models; Semantics; Software engineering; Testing; Fault detection; automated REST API testing; multi-agent reinforcement learning for testing; Automated REST API Testing; Reinforcement learning; Documentation; Closed box; Web services; Servers; Multi-Agent Reinforcement Learning for Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DFLGD5XK","conferencePaper","2024","Groce, Alex; Dong, Liming; Lu, Qinghua; Zhu, Liming","A Pilot Study in Surveying Data Challenges of Automatic Software Engineering Tasks","Proceedings of the 4th International Workshop on Software Engineering and AI for Data Quality in Cyber-Physical Systems/Internet of Things","979-8-4007-0672-1","","10.1145/3663530.3665020","https://doi.org/10.1145/3663530.3665020","The surge in automatic SE research aims to boost development efficiency and quality while reducing costs. However, challenges such as limited real-world project data and inadequate data conditions constrain the effectiveness of these methods. To systematically understand these challenges, our pilot study reviews prevalent data challenges across various SE tasks. Despite these challenges, thanks to the advances of large language model offers promising performance on SE tasks. Overall, this pilot survey focused on provide a quick retrospective review on SE data challenges and introduce practical LLM solutions from the SE community to mitigate these challenges.","2024","2025-11-25 22:29:49","2025-11-25 22:29:49","","6–11","","","","","","","SEA4DQ 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","LLM; Automatic Software Engineering; Data Challenge; Pilot Survey","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SYIDMU33","conferencePaper","2024","Chen, Tianyi; Jiang, Yanjie; Fan, Fu; Liu, Bo; Liu, Hui","A Position-Aware Approach to Decomposing God Classes","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3694992","https://doi.org/10.1145/3691620.3694992","God classes are widely recognized as code smells, significantly impairing the maintainability and readability of source code. However, resolving the identified God classes remains a formidable challenge, and we still lack automated and accurate tools to resolve God classes automatically. To this end, in this paper, we propose a novel approach (called ClassSplitter) to decompose God classes. The key observation behind the proposed approach is that software entities (i.e., methods and fields) that are physically adjacent often have strong semantic correlations and thus have a great chance of being classified into the same class during God class deposition. We validate this hypothesis by analyzing 54 God class decomposition refactorings actually conducted in the wild. According to the observation, we measure the similarity between software entities by exploiting not only traditional code metrics but also their relative physical positions. Based on the similarity, we customize a clustering algorithm to classify the methods within a given God class, and each of the resulting clusters is taken as a new class. Finally, ClassSplitter allocates the fields of the God class to the new classes according to the field-access-based coupling between fields and classes. We evaluate ClassSplitter using 133 real-world God classes from open-source applications. Our evaluation results suggest that ClassSplitter could substantially improve the state of the art in God class decomposition, improving the average MoJoFM by 47%. Manual evaluation also confirmed that in most cases (77%) the solutions suggested by ClassSplitter were preferred by developers to alternatives suggested by the state-of-the-art baseline approach.","2024","2025-11-25 22:29:50","2025-11-25 22:29:50","","129–140","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","large language model; code smells; god class; software refactoring","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RGG92Q2Z","conferencePaper","2024","Kulsum, Ummay; Zhu, Haotian; Xu, Bowen; d'Amorim, Marcelo","A Case Study of LLM for Automated Vulnerability Repair: Assessing Impact of Reasoning and Patch Validation Feedback","Proceedings of the 1st ACM International Conference on AI-Powered Software","979-8-4007-0685-1","","10.1145/3664646.3664770","https://doi.org/10.1145/3664646.3664770","Recent work in automated program repair (APR) proposes the use of reasoning and patch validation feedback to reduce the semantic gap between the LLMs and the code under analysis. The idea has been shown to perform well for general APR, but its effectiveness in other particular contexts remains underexplored. In this work, we assess the impact of reasoning and patch validation feedback to LLMs in the context of vulnerability repair, an important and challenging task in security. To support the evaluation, we present VRpilot, an LLM-based vulnerability repair technique based on reasoning and patch validation feedback. VRpilot (1) uses a chain-of-thought prompt to reason about a vulnerability prior to generating patch candidates and (2) iteratively refines prompts according to the output of external tools (e.g., compiler, code sanitizers, test suite, etc.) on previously generated patches. To evaluate performance, we compare VRpilot against the state-of-the-art vulnerability repair techniques for C and Java using public datasets from the literature. Our results show that VRpilot generates, on average, 14% and 7.6% more correct patches than the baseline techniques on C and Java, respectively. We show, through an ablation study, that reasoning and patch validation feedback are critical. We report several lessons from this study and potential directions for advancing LLM-empowered vulnerability repair.","2024","2025-11-25 22:29:53","2025-11-25 22:29:53","","103–111","","","","","","","AIware 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","Large Language Models; Automated Vulnerability Repair","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZJ2TNNFX","conferencePaper","2023","Jain, Ridhi; Gervasoni, Nicole; Ndhlovu, Mthandazo; Rawat, Sanjay","A Code Centric Evaluation of C/C++ Vulnerability Datasets for Deep Learning Based Vulnerability Detection Techniques","Proceedings of the 16th Innovations in Software Engineering Conference","979-8-4007-0064-4","","10.1145/3578527.3578530","https://doi.org/10.1145/3578527.3578530","Recent years have witnessed tremendous progress in NLP-based code comprehension via deep neural networks (DNN) learning, especially Large Language Models (LLMs). While the original application of LLMs is focused on code generation, there have been attempts to extend the application to more specialized tasks, like code similarity, author attribution, code repairs, and so on. As data plays an important role in the success of any machine learning approach, researchers have also proposed several benchmarks which are coupled with a specific task at hand. It is well known in the machine learning (ML) community that the presence of biases in the dataset affects the quality of the ML algorithm in a real-world scenario. This paper evaluates several existing datasets from DNN’s application perspective. We specifically focus on training datasets of C/C++ language code. Our choice of language stems from the fact that while LLM-based techniques have been applied and evaluated on programming languages like Python, JavaScript, and Ruby, there is not much LLM research for C/C++. As a result, datasets generated synthetically or from real-world codes are in individual research work. Consequently, in the absence of a uniform dataset, such works are hard to compare with each other. In this work, we aim to achieve two main objectives– 1. propose code-centric features that are relevant to security program analysis tasks like vulnerability detection; 2. a thorough (qualitative and quantitative) examination of the existing code datasets that demonstrate the main characteristics of the individual datasets to have a clear comparison. Our evaluation finds exciting facts about existing datasets highlighting gaps that need to be addressed.","2023","2025-11-25 22:30:03","2025-11-25 22:30:03","","","","","","","","","ISEC '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Allahabad, India","","","","datasets; program graphs; software metrics; software vulnerability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ASAM4PXE","conferencePaper","2025","Wei, Wei","Static Analysis and LLM for Comprehensive Java Unit Test Generation","2025 8th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)","","","10.1109/AEMCSE65292.2025.11042526","","Software testing is crucial in ensuring the reliability and correctness of software applications. However, generating comprehensive test cases manually can be time-consuming and error-prone. This paper introduces SAGEN, a tool designed to automate Java unit test generation by leveraging static analysis of Syntax Trees (AST) and large language models (LLMs). SAGEN identifies literal values and their ranges, generating test cases that improve coverage and quality. In our experiments, SAGEN outperforms traditional test case generation tools such as EvoSuite and Randoop. It demonstrates a 10 % improvement in code coverage and a 13 % enhancement in test case quality. Furthermore, SAGEN achieves a compile pass rate of 89.7 %, proving its effectiveness in producing both high-quality and reliable test cases.","2025-05","2025-11-25 22:33:23","2025-11-25 22:33:23","","87-92","","","","","","","","","","","","","","","","","","","","","","","","Large language models; software testing; Software testing; test case generation; Test pattern generators; Java; Java unit testing; LLM; Manuals; Software; Software engineering; Software reliability; static analysis; Static analysis; Syntactics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZNJAW8A4","conferencePaper","2025","Nan, Zifan; Guo, Zhaoqiang; Liu, Kui; Xia, Xin","Test Intention Guided LLM-Based Unit Test Generation","2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)","","","10.1109/ICSE55347.2025.00243","","The emergence of Large Language Models (LLMs) has accelerated the progress of intelligent software engineering technologies, which brings promising possibilities for unit test generation. However, existing approaches for unit tests directly generated from Large Language Models (LLMs) often prove impractical due to their low coverage and insufficient mocking capabilities. This paper proposes IntUT, a novel approach that utilizes explicit test intentions (e.g., test inputs, mock behaviors, and expected results) to effectively guide the LLM to generate high-quality test cases. Our experimental results on three industry Java projects and live study demonstrate that prompting LLM with test intention can generate high-quality test cases for developers. Specifically, it achieves the improvements on branch coverage by 94 % and line coverage by 49 %. Finally, we obtain developers' feedback on using IntUT to generate cases for three new Java projects, achieving over 80 % line coverage and 30 % efficiency improvement on writing unit test cases.","2025-04","2025-11-25 22:37:02","2025-11-25 22:37:02","","1026-1038","","","","","","","","","","","","","","","","","","","","ISSN: 1558-1225","","","","Large language models; Test pattern generators; Java; LLM; Software engineering; Industries; Life estimation; unit test generation; program analysis; mocking; test intention","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2HGQRQXH","conferencePaper","2025","Rawson, Jessica; Reddivari, Sandeep","A ChatGPT-Powered Tool for Automating Context-Aware Acceptance Criteria Generation for User Stories","2025 IEEE International Conference on Information Reuse and Integration and Data Science (IRI)","","","10.1109/IRI66576.2025.00067","","There has been a growing interest in using Natural Language Processing (NLP), such as OpenAI's ChatGPT for software engineering tasks, including requirements engineering (RE), software design, and software testing. This paper covers a practical implementation of a ChatGPT-powered prompt engineering framework designed to generate context-specific acceptance criteria for user stories. The tool automates each stage of the framework—preprocessing contextual information and generating the final tailored acceptance criteria. We outline the design and implementation of the tool in this paper and its effectiveness through two repositories. This work demonstrates the potential of large language models (LLMs) to reduce the manual effort involved in RE, streamline development workflows, and minimize rework-related costs in agile software projects.","2025-08","2025-11-25 22:37:03","2025-11-25 22:37:03","","325-330","","","","","","","","","","","","","","","","","","","","ISSN: 2835-5776","","","","Large language models; Software testing; Manuals; Software engineering; Chatbots; ChatGPT; Prompt engineering; Requirements Engineering; Artificial Intelligence; Prompt Engineering; Acceptance Criteria; Data science; Requirements engineering; Rough surfaces; Software design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KG4H66JQ","conferencePaper","2025","Treviğno-Villalobos, Marlen; Quesada-López, Christian; Jiménez-Delgado, Efrén; Quirós-Oviedo, Rocío; Díaz-Oreiro, Ignacio","A Comparative Evaluation of ChatGPT, DeepSeek and Gemini in Automatic Unit Test Generation: a Success Rate Analysis","2025 IEEE VIII Congreso Internacional en Inteligencia Ambiental, Ingenieria de Software y Salud Electronica y Movil (AmITIC)","","","10.1109/AmITIC68284.2025.11214621","","The advancement of large-scale language models (LLMs) has opened up new possibilities for automating unit test generation, a traditionally manual and expensive task. This quantitative study evaluates the performance of three LLMs-ChatGPT 4o mini, DeepSeek v3, and Gemini 2.5 Flash Pro-in generating test cases for methods in C# developed in Unity. The execution success rate of the generated tests was measured using real and synthetic data. The synthetic data was intentionally created to represent common structures, while the real data came from existing project functions. The experimental design was controlled and included the factors LLM and data type and the blocks cyclomatic complexity and contextual memory with four replicates per combination, for a total of 96 experimental treatments. The results show that LLMs have a high potential to support the automatic generation of unit tests. Furthermore, it was evidenced that the choice of model has a significant effect on the success rate of the generated tests. These findings provide useful initial evidence to guide the selection and use of LLMs in test automation processes within software development environments","2025-09","2025-11-25 22:37:23","2025-11-25 22:37:23","","1-8","","","","","","","","","","","","","","","","","","","","","","","","Test pattern generators; LLM; Manuals; Software; Chatbots; Software development management; unit testing; automatic testing; Computational modeling; C# languages; Indexes; Monitoring; prompt; Synthetic data; Unity","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"94A5X5TE","conferencePaper","2025","Margabandu, Logan; Chen, Zizhao; Wong, W. Eric; Hsu, Chih-Wei","A Design of Experiments Oracle LLM for Research-Grounded Combinatorial Testing","2025 25th International Conference on Software Quality, Reliability, and Security Companion (QRS-C)","","","10.1109/QRS-C65679.2025.00021","","As artificial intelligence moves across all scientific and engineering disciplines, a big challenge remains: expert domain knowledge, especially in specialized areas like Design-of-Experiments (DoE), is hard to interpret or inaccessible to non-experts and thus hinders broader application and innovation. As large language models (LLMS) continue to improve, they offer a practical way to help close the knowledge gap and make expert-level information more accessible to a wider audience. To address this need, we designed and developed a new solution: a Design-of-Experiments Oracle (DOE), a conversational LLM. This paper presents this AI-powered system that changes how complex DoE principles are understood and used. DOE uses state-of-the-art techniques including a Llama 3.2 model inside a Retrieval-Augmented Fine-Tuned Transformer (RAFT) architecture. Its knowledge base is informed by and enables semantic retrieval from combinatorial testing research. This is a conversational expert system. With this system, we want to enable engineers and researchers to apply complex DoE principles, give them interactive, research-based guidance. Ultimately, this is to contribute to the efforts to speed up innovation, improve system reliability and make advanced engineering methods more accessible in the digital age.","2025-07","2025-11-25 22:38:39","2025-11-25 22:38:39","","82-90","","","","","","","","","","","","","","","","","","","","ISSN: 2693-9371","","","","Large language models; Semantics; Software reliability; Software quality; Large Language Models; Technological innovation; Transformers; Security; Combinatorial testing; Combinatorial Testing; Retrieval augmented generation; Reliability engineering; Conversational AI; Design-of-Experiments (DoE); Oracle LLM; Retrieval Augmented Generation (RAG)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V5JW55F2","conferencePaper","2025","Ma, Lezhi; Liu, Shangqing; Li, Yi; Xie, Xiaofei; Bu, Lei","SpecGen: Automated Generation of Formal Program Specifications via Large Language Models","2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)","","","10.1109/ICSE55347.2025.00129","","In the software development process, formal program specifications play a crucial role in various stages, including requirement analysis, software testing, and verification. However, manually crafting formal program specifications is rather difficult, making the job time-consuming and labor-intensive. Moreover, it is even more challenging to write specifications that correctly and comprehensively describe the semantics of complex programs. To reduce the burden on software developers, automated specification generation methods have emerged. However, existing methods usually rely on predefined templates or grammar, making them struggle to accurately describe the behavior and functionality of complex real-world programs. To tackle this challenge, we introduce SpecGen, a novel technique for formal program specification generation based on Large Language Models (LLMs). Our key insight is to overcome the limitations of existing methods by leveraging the code comprehension capability of LLMs. The process of SpecGen consists of two phases. The first phase employs a conversational approach that guides the LLM in generating appropriate specifications for a given program, aiming to utilize the ability of LLM to generate high-quality specifications. The second phase, designed for where the LLM fails to generate correct specifications, applies four mutation operators to the model-generated specifications and selects verifiable specifications from the mutated ones through a novel heuristic selection strategy by assigning different weights of variants in an efficient manner. We evaluate SpecGen on two datasets, including the SV-COMP Java category benchmark and a manually constructed dataset containing 120 programs. Experimental results demonstrate that SpecGen succeeds in generating verifiable specifications for 279 out of 385 programs, outperforming the existing LLM-based approaches and conventional specification generation tools like Houdini and Daikon. Further investigations on the quality of generated specifications indicate that SpecGen can comprehensively articulate the behaviors of the input program.","2025-04","2025-11-25 22:38:39","2025-11-25 22:38:39","","16-28","","","","","","","","","","","","","","","","","","","","ISSN: 1558-1225","","","","large language model; Large language models; Semantics; Software testing; Java; Software; Software engineering; Software development management; Codes; Benchmark testing; Grammar; program verification; specification inference","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8TUNTQDM","conferencePaper","2025","Zhou, Shiyao; Wang, Jincheng; Ye, He; Zhou, Hao; Goues, Claire Le; Luo, Xiapu","LWDIFF: an LLM-Assisted Differential Testing Framework for Webassembly Runtimes","2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)","","","10.1109/ICSE55347.2025.00233","","WebAssembly (Wasm) runtimes execute Wasm programs, a popular low-level language for efficiently executing high-level languages in browsers, with broad applications across diverse domains. The correctness of those runtimes is critical for both functionality and security of Wasm execution, motivating testing approaches that target Wasm runtimes specifically. However, existing Wasm testing frameworks fail to generate test cases that effectively test all three phases of runtime, i.e., decoding, validation, and execution. To address this research gap, we propose a new differential testing framework for Wasm runtimes, which leverages knowledge from the Wasm language specification that prior techniques overlooked, enhancing comprehensive testing of runtime functionality. Specifically, we first use a large language model to extract that knowledge from the specification. We use that knowledge in the context of multiple novel mutation operators that generate test cases with diverse features to test all three runtime phases. We evaluate LWDIFF by applying it to eight Wasm runtimes. Compared with the state-of-the-art Wasm testers, LWDIFF achieves the highest branch coverage and identifies the largest number of bugs. In total, LWDIFF discovers 31 bugs across eight runtimes, all of which are confirmed, with 25 of them previously undiscovered.","2025-04","2025-11-25 22:38:57","2025-11-25 22:38:57","","153-164","","","","","","","","","","","","","","","","","","","","ISSN: 1558-1225","","","","Large language models; Codes; Testing; Computer bugs; Security; Runtime; Feature extraction; Browsers; Decoding; High level languages","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"24FPK6IL","conferencePaper","2025","Li, Jiageng; Dong, Zhen; Wang, Chong; You, Haozhen; Zhang, Cen; Liu, Yang; Peng, Xin","LLM Based Input Space Partitioning Testing for Library APIs","2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)","","","10.1109/ICSE55347.2025.00153","","Automated library APIs testing is difficult as it requires exploring a vast space of parameter inputs that may involve objects with complex data types. Existing search based approaches, with limited knowledge of relations between object states and program branches, often suffer from the low efficiency issue, i.e., tending to generate invalid inputs. Symbolic execution based approaches can effectively identify such relations, but fail to scale to large programs. In this work, we present an LLM-based input space partitioning testing approach, LISP, for library APIs. The approach lever-ages LLMs to understand the code of a library API under test and perform input space partitioning based on its understanding and rich common knowledge. Specifically, we provide the signature and code of the API under test to LLMs, with the expectation of obtaining a text description of each input space partition of the API under test. Then, we generate inputs through employing the generated text description to sample inputs from each partition, ultimately resulting in test suites that systematically explore the program behavior of the API. We evaluate LISP on more than 2,205 library API meth-ods taken from 10 popular open-source Java libraries (e.g., apache/commons-lang with 2.6k stars, guava with 48.8k stars on GitHub). Our experiment results show that LISP is effective in library API testing. It significantly outperforms state-of-the-art tool EvoSuite in terms of edge coverage. On average, LISP achieves 67.82 % branch coverage, surpassing EvoSuite by 1.21 times. In total, LISP triggers 404 exceptions or errors in the experiments, and discovers 13 previously unknown vulnerabilities during evaluation, which have been assigned CVE IDs.","2025-04","2025-11-25 22:39:09","2025-11-25 22:39:09","","1436-1448","","","","","","","","","","","","","","","","","","","","ISSN: 1558-1225","","","","Large language models; Java; Software engineering; Software development management; Codes; Testing; Large Language Models; Symbolic Execution; API testing; Libraries; Search problems; Space exploration; Input Space Partitioning Testing; Stars","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""