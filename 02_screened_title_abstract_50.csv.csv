"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"J3TR5CG9","conferencePaper","2025","Dong, Chunhao; Jiang, Yanjie; Zhang, Yuxia; Zhang, Yang; Liu, Hui","ChatGPT-Based Test Generation for Refactoring Engines Enhanced by Feature Analysis on Examples","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00210","https://doi.org/10.1109/ICSE55347.2025.00210","Software refactoring is widely employed to improve software quality. However, conducting refactorings manually is tedious, time-consuming, and error-prone. Consequently, automated and semi-automated tool support is highly desirable for software refactoring in the industry, and most of the main-stream IDEs provide powerful tool support for refactoring. However, complex refactoring engines are prone to errors, which in turn may result in imperfect and incorrect refactorings. To this end, in this paper, we propose a ChatGPT-based approach to testing refactoring engines. We first manually analyze bug reports and test cases associated with refactoring engines, and construct a feature library containing fine-grained features that may trigger defects in refactoring engines. The approach automatically generates prompts according to both predefined prompt templates and features randomly selected from the feature library, requesting ChatGPT to generate test programs with the requested features. Test programs generated by ChatGPT are then forwarded to multiple refactoring engines for differential testing. To the best of our knowledge, it is the first approach in testing refactoring engines that guides test program generation with features derived from existing bugs. It is also the first approach in this line that exploits LLMs in the generation of test programs. Our initial evaluation of four main-stream refactoring engines suggests that the proposed approach is effective. It identified a total of 115 previously unknown bugs besides 28 inconsistent refactoring behaviors among different engines. Among the 115 bugs, 78 have been manually confirmed by the original developers of the tested engines, i.e., IntelliJ IDEA, Eclipse, VScode-Java, and NetBeans.","2025","2025-11-25 22:29:30","2025-11-25 22:47:05","","2714–2725","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","Chatbots; ChatGPT; Computer bugs; differential testing; Engines; Industries; Libraries; refactoring; refactoring engines; Software engineering; Software quality; Test pattern generators; Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8J24MLCH","conferencePaper","2024","Qi, Fei; Hou, Yingnan; Lin, Ning; Bao, Shanshan; Xu, Nuo","A Survey of Testing Techniques Based on Large Language Models","Proceedings of the 2024 International Conference on Computer and Multimedia Technology","979-8-4007-1826-7","","10.1145/3675249.3675298","https://doi.org/10.1145/3675249.3675298","With the development of software testing technology, Large Language Model (LLM) driven testing method have gradually become an emerging trend in the field of software testing. This paper presents a comprehensive review of LLM-based testing techniques. The results of 19 studies using LLM to optimize testing techniques are analyzed from the perspective of software testing. This paper discusses in detail how to use LLM to optimize test techniques for generating automated test code and generating diverse input in software test tasks. It also summarizes the challenges and opportunities faced by this field. The above conclusions can identify the shortcomings of LLM-based software testing technology and the direction of future research.","2024","2025-11-25 22:29:34","2025-11-25 22:29:34","","280–284","","","","","","","ICCMT '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sanming, China","","","","LLM; Pre-trained Large Language Model; Software Testing Techniques","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"65SJKKLB","conferencePaper","2025","Fang, Zihan; Li, Jiliang; Liang, Anda; Bai, Gina R.; Huang, Yu","A Comparative Study on ChatGPT and Checklist as Support Tools for Unit Testing Education","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","10.1145/3696630.3727244","https://doi.org/10.1145/3696630.3727244","Testing is widely practiced in software engineering, and many tools have been developed to support students in learning testing. Prior research suggests that a lightweight testing checklist improves learning outcomes but doesn't address students' challenges in writing test code that matches their intentions or design. Meanwhile, generative AI tools (e.g., ChatGPT) bring new promise as another form of software assistance tool. In this study, we examined the impact of various support tools (checklist, ChatGPT, or both) on unit testing among 42 students. Our results indicated that using these tools individually or in combination produced a comparable effect on student performance in unit testing. Students preferred using the checklist but acknowledged ChatGPT's effectiveness in accelerating task completion and addressing programming language challenges. While ChatGPT demonstrated potential benefits for testing education, it did not overcome the implementation challenges identified in the previous study. Moreover, reliance on ChatGPT may hinder students' deeper engagement with new concepts, which is crucial for comprehensive learning, as they often interact superficially with AI-generated responses without employing the critical thinking necessary to evaluate the information provided. Therefore, we proposed recommendations for both students and instructors on adapting to learning and teaching in the AI era and offer insights into the evolving role of AI in education.","2025","2025-11-25 22:29:35","2025-11-25 22:29:35","","871–882","","","","","","","FSE Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","ChatGPT; unit testing; checklist; testing education","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WUHN6LRM","conferencePaper","2024","Chen, Yang; Jabbarvand, Reyhaneh","Can ChatGPT Repair Non-Order-Dependent Flaky Tests?","Proceedings of the 1st International Workshop on Flaky Tests","979-8-4007-0558-8","","10.1145/3643656.3643900","https://doi.org/10.1145/3643656.3643900","Regression testing helps developers check whether the latest code changes break software functionality. Flaky tests, which can non-deterministically pass or fail on the same code version, may mislead developers' concerns, resulting in missing some bugs or spending time pinpointing bugs that do not exist. Existing flakiness detection and mitigation techniques have primarily focused on general order-dependent (OD) and implementation-dependent (ID) flaky tests. There is also a dearth of research on repairing test flakiness, out of which, mostly have focused on repairing OD flaky tests, and a few have explored repairing a subcategory of non-order-dependent (NOD) flaky tests that are caused by asynchronous waits. As a result, there is a demand for devising techniques to reproduce, detect, and repair NOD flaky tests. Large language models (LLMs) have shown great effectiveness in several programming tasks. To explore the potential of LLMs in addressing NOD flakiness, this paper investigates the possibility of using ChatGPT to repair different categories of NOD flaky tests. Our comprehensive study on 118 from the IDoFT dataset shows that ChatGPT, despite as a leading LLM with notable success in multiple code generation tasks, is ineffective in repairing NOD test flakiness, even by following the best practices for prompt crafting. We investigated the reasons behind the failure of using ChatGPT in repairing NOD tests, which provided us valuable insights about the next step to advance the field of NOD test flakiness repair.","2024","2025-11-25 22:29:35","2025-11-25 22:47:05","","22–29","","","","","","","FTW '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","Codes; Computer bugs; Debugging; large language models; Large language models; Large Language Models; Maintenance engineering; Prevention and mitigation; software testing; Software testing; Software Testing; test flakiness; Test Flakiness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UQLDFJKI","conferencePaper","2025","Yu, Junji; Shu, Honglin; Fu, Michael; Wang, Dong; Tantithamthavorn, Chakkrit; Kamei, Yasutaka; Chen, Junjie","A Preliminary Study of Large Language Models for Multilingual Vulnerability Detection","Proceedings of the 34th ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-1474-0","","10.1145/3713081.3731746","https://doi.org/10.1145/3713081.3731746","Deep learning-based approaches, particularly those leveraging pre-trained language models (PLMs), have shown promise in automated software vulnerability detection. However, existing methods are predominantly limited to specific programming languages, restricting their applicability in multilingual settings. Recent advancements in large language models (LLMs) offer language-agnostic capabilities and enhanced semantic understanding, presenting a potential solution to this limitation. While existing studies have explored LLMs for vulnerability detection, their detection performance remains unknown for multilingual vulnerabilities. To address this gap, we conducted a preliminary study to evaluate the effectiveness of PLMs and state-of-the-art LLMs across seven popular programming languages. Our findings reveal that the PLM CodeT5P achieves the best performance in multilingual vulnerability detection, particularly in identifying the most critical vulnerabilities. Based on these results, we further discuss the potential of LLMs in advancing real-world multilingual vulnerability detection. This work represents an initial step toward exploring PLMs and LLMs for cross-language vulnerability detection, offering key insights for future research and practical deployment.","2025","2025-11-25 22:29:38","2025-11-25 22:29:38","","161–168","","","","","","","ISSTA Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","large language model; multilingual vulnerability; vulnerability detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TYEMY7NT","conferencePaper","2024","Yan, Dapeng; Gao, Zhipeng; Liu, Zhiming","A Closer Look at Different Difficulty Levels Code Generation Abilities of ChatGPT","Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering","979-8-3503-2996-4","","10.1109/ASE56229.2023.00096","https://doi.org/10.1109/ASE56229.2023.00096","","2024","2025-11-25 22:29:38","2025-11-25 23:11:13","","1887–1898","","","","","","","ASE '23","","","","IEEE Press","Echternach, Luxembourg","","","","","","","","","","","","large language model; code generation; Chat-GPT; clean code; program competition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RUB9Y26K","journalArticle","2024","Hou, Xinyi; Zhao, Yanjie; Liu, Yue; Yang, Zhou; Wang, Kailong; Li, Li; Luo, Xiapu; Lo, David; Grundy, John; Wang, Haoyu","Large Language Models for Software Engineering: A Systematic Literature Review","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3695988","https://doi.org/10.1145/3695988","Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a Systematic Literature Review (SLR) on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We selected and analyzed 395 research articles from January 2017 to January 2024 to answer four key Research Questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, pre-processing, and application, highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and highlighting promising areas for future study. Our artifacts are publicly available at .","2024-12","2025-11-25 22:29:39","2025-11-25 22:29:39","","","","8","33","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Software Engineering; Survey","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6XQYBAEN","conferencePaper","2025","Kim, Myeongsoo; Stennett, Tyler; Sinha, Saurabh; Orso, Alessandro","A Multi-Agent Approach for REST API Testing with Semantic Graphs and LLM-Driven Inputs","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00179","https://doi.org/10.1109/ICSE55347.2025.00179","As modern web services increasingly rely on REST APIs, their thorough testing has become crucial. Furthermore, the advent of REST API documentation languages, such as the OpenAPI Specification, has led to the emergence of many black-box REST API testing tools. However, these tools often focus on individual test elements in isolation (e.g., APIs, parameters, values), resulting in lower coverage and less effectiveness in fault detection. To address these limitations, we present AutoRestTest, the first black-box tool to adopt a dependency-embedded multi-agent approach for REST API testing that integrates multi-agent reinforcement learning (MARL) with a semantic property dependency graph (SPDG) and Large Language Models (LLMs). Our approach treats REST API testing as a separable problem, where four agents—API, dependency, parameter, and value agents—collaborate to optimize API exploration. LLMs handle domain-specific value generation, the SPDG model simplifies the search space for dependencies using a similarity score between API operations, and MARL dynamically optimizes the agents' behavior. Our evaluation of AutoRestTest on 12 real-world REST services shows that it outperforms the four leading black-box REST API testing tools, including those assisted by RESTGPT (which generates realistic test inputs using LLMs), in terms of code coverage, operation coverage, and fault detection. Notably, AutoRestTest is the only tool able to trigger an internal server error in the Spotify service. Our ablation study illustrates that each component of AutoRestTest—the SPDG, the LLM, and the agent-learning mechanism—contributes to its overall effectiveness.","2025","2025-11-25 22:29:43","2025-11-25 22:46:39","","1409–1421","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","Large language models; Semantics; Software engineering; Testing; Fault detection; automated REST API testing; multi-agent reinforcement learning for testing; Automated REST API Testing; Reinforcement learning; Documentation; Closed box; Web services; Servers; Multi-Agent Reinforcement Learning for Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3V58PYED","conferencePaper","2024","Sallou, June; Durieux, Thomas; Panichella, Annibale","Breaking the Silence: the Threats of Using LLMs in Software Engineering","Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results","979-8-4007-0500-7","","10.1145/3639476.3639764","https://doi.org/10.1145/3639476.3639764","Large Language Models (LLMs) have gained considerable traction within the Software Engineering (SE) community, impacting various SE tasks from code completion to test generation, from program repair to code summarization. Despite their promise, researchers must still be careful as numerous intricate factors can influence the outcomes of experiments involving LLMs. This paper initiates an open discussion on potential threats to the validity of LLM-based research including issues such as closed-source models, possible data leakage between LLM training data and research evaluation, and the reproducibility of LLM-based findings. In response, this paper proposes a set of guidelines tailored for SE researchers and Language Model (LM) providers to mitigate these concerns. The implications of the guidelines are illustrated using existing good practices followed by LLM providers and a practical example for SE researchers in the context of test case generation.","2024","2025-11-25 22:29:47","2025-11-25 22:46:52","","102–106","","","","","","","ICSE-NIER'24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","Codes; Data models; Evaluation; Guidelines; Large language models; Large Language Models; Maintenance engineering; Reproducibility of results; Software engineering; Software Engineering; Test pattern generators; Training data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DFLGD5XK","conferencePaper","2024","Groce, Alex; Dong, Liming; Lu, Qinghua; Zhu, Liming","A Pilot Study in Surveying Data Challenges of Automatic Software Engineering Tasks","Proceedings of the 4th International Workshop on Software Engineering and AI for Data Quality in Cyber-Physical Systems/Internet of Things","979-8-4007-0672-1","","10.1145/3663530.3665020","https://doi.org/10.1145/3663530.3665020","The surge in automatic SE research aims to boost development efficiency and quality while reducing costs. However, challenges such as limited real-world project data and inadequate data conditions constrain the effectiveness of these methods. To systematically understand these challenges, our pilot study reviews prevalent data challenges across various SE tasks. Despite these challenges, thanks to the advances of large language model offers promising performance on SE tasks. Overall, this pilot survey focused on provide a quick retrospective review on SE data challenges and introduce practical LLM solutions from the SE community to mitigate these challenges.","2024","2025-11-25 22:29:49","2025-11-25 22:29:49","","6–11","","","","","","","SEA4DQ 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","LLM; Automatic Software Engineering; Data Challenge; Pilot Survey","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SYIDMU33","conferencePaper","2024","Chen, Tianyi; Jiang, Yanjie; Fan, Fu; Liu, Bo; Liu, Hui","A Position-Aware Approach to Decomposing God Classes","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3694992","https://doi.org/10.1145/3691620.3694992","God classes are widely recognized as code smells, significantly impairing the maintainability and readability of source code. However, resolving the identified God classes remains a formidable challenge, and we still lack automated and accurate tools to resolve God classes automatically. To this end, in this paper, we propose a novel approach (called ClassSplitter) to decompose God classes. The key observation behind the proposed approach is that software entities (i.e., methods and fields) that are physically adjacent often have strong semantic correlations and thus have a great chance of being classified into the same class during God class deposition. We validate this hypothesis by analyzing 54 God class decomposition refactorings actually conducted in the wild. According to the observation, we measure the similarity between software entities by exploiting not only traditional code metrics but also their relative physical positions. Based on the similarity, we customize a clustering algorithm to classify the methods within a given God class, and each of the resulting clusters is taken as a new class. Finally, ClassSplitter allocates the fields of the God class to the new classes according to the field-access-based coupling between fields and classes. We evaluate ClassSplitter using 133 real-world God classes from open-source applications. Our evaluation results suggest that ClassSplitter could substantially improve the state of the art in God class decomposition, improving the average MoJoFM by 47%. Manual evaluation also confirmed that in most cases (77%) the solutions suggested by ClassSplitter were preferred by developers to alternatives suggested by the state-of-the-art baseline approach.","2024","2025-11-25 22:29:50","2025-11-25 22:29:50","","129–140","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","large language model; code smells; god class; software refactoring","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RGG92Q2Z","conferencePaper","2024","Kulsum, Ummay; Zhu, Haotian; Xu, Bowen; d'Amorim, Marcelo","A Case Study of LLM for Automated Vulnerability Repair: Assessing Impact of Reasoning and Patch Validation Feedback","Proceedings of the 1st ACM International Conference on AI-Powered Software","979-8-4007-0685-1","","10.1145/3664646.3664770","https://doi.org/10.1145/3664646.3664770","Recent work in automated program repair (APR) proposes the use of reasoning and patch validation feedback to reduce the semantic gap between the LLMs and the code under analysis. The idea has been shown to perform well for general APR, but its effectiveness in other particular contexts remains underexplored. In this work, we assess the impact of reasoning and patch validation feedback to LLMs in the context of vulnerability repair, an important and challenging task in security. To support the evaluation, we present VRpilot, an LLM-based vulnerability repair technique based on reasoning and patch validation feedback. VRpilot (1) uses a chain-of-thought prompt to reason about a vulnerability prior to generating patch candidates and (2) iteratively refines prompts according to the output of external tools (e.g., compiler, code sanitizers, test suite, etc.) on previously generated patches. To evaluate performance, we compare VRpilot against the state-of-the-art vulnerability repair techniques for C and Java using public datasets from the literature. Our results show that VRpilot generates, on average, 14% and 7.6% more correct patches than the baseline techniques on C and Java, respectively. We show, through an ablation study, that reasoning and patch validation feedback are critical. We report several lessons from this study and potential directions for advancing LLM-empowered vulnerability repair.","2024","2025-11-25 22:29:53","2025-11-25 22:29:53","","103–111","","","","","","","AIware 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","Large Language Models; Automated Vulnerability Repair","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EKQL9EWF","conferencePaper","2024","Khajezade, Mohamad; Wu, Jie JW; Fard, Fatemeh Hendijani; Rodriguez-Perez, Gema; Shehata, Mohamed Sami","Investigating the Efficacy of Large Language Models for Code Clone Detection","Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension","979-8-4007-0586-1","","10.1145/3643916.3645030","https://doi.org/10.1145/3643916.3645030","Large Language Models (LLMs) have demonstrated remarkable success in various natural language processing and software engineering tasks, such as code generation. The LLMs are mainly utilized in the prompt-based zero/few-shot paradigm to guide the model in accomplishing the task. GPT-based models are one of the popular ones studied for tasks such as code comment generation or test generation. These tasks are 'generative' tasks. However, there is limited research on the usage of LLMs for 'non-generative' tasks such as classification using the prompt-based paradigm. In this preliminary exploratory study, we investigated the applicability of LLMs for Code Clone Detection (CCD), a non-generative task. By building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we first investigated two different prompts using ChatGPT to detect Type-4 code clones in Java-Java and Java-Ruby pairs in a zero-shot setting. We then conducted an analysis to understand the strengths and weaknesses of ChatGPT in CCD. ChatGPT surpasses the baselines in cross-language CCD attaining an F1-score of 0.877 and achieves comparable performance to fully fine-tuned models for mono-lingual CCD, with an F1-score of 0.878. Also, the prompt and the difficulty level of the problems has an impact on the performance of ChatGPT. Finally, we provide insights and future directions based on our initial analysis1.","2024","2025-11-25 22:30:01","2025-11-25 22:47:28","","161–165","","","","","","","ICPC '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","Buildings; Charge coupled devices; Chatbots; Cloning; code clone detection; Code Clone Detection; Codes; Data models; few-shot learning; Few-shot Learning; large language models; Large Language Models; Test pattern generators; zero-shot learning; Zero-shot Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZJ2TNNFX","conferencePaper","2023","Jain, Ridhi; Gervasoni, Nicole; Ndhlovu, Mthandazo; Rawat, Sanjay","A Code Centric Evaluation of C/C++ Vulnerability Datasets for Deep Learning Based Vulnerability Detection Techniques","Proceedings of the 16th Innovations in Software Engineering Conference","979-8-4007-0064-4","","10.1145/3578527.3578530","https://doi.org/10.1145/3578527.3578530","Recent years have witnessed tremendous progress in NLP-based code comprehension via deep neural networks (DNN) learning, especially Large Language Models (LLMs). While the original application of LLMs is focused on code generation, there have been attempts to extend the application to more specialized tasks, like code similarity, author attribution, code repairs, and so on. As data plays an important role in the success of any machine learning approach, researchers have also proposed several benchmarks which are coupled with a specific task at hand. It is well known in the machine learning (ML) community that the presence of biases in the dataset affects the quality of the ML algorithm in a real-world scenario. This paper evaluates several existing datasets from DNN’s application perspective. We specifically focus on training datasets of C/C++ language code. Our choice of language stems from the fact that while LLM-based techniques have been applied and evaluated on programming languages like Python, JavaScript, and Ruby, there is not much LLM research for C/C++. As a result, datasets generated synthetically or from real-world codes are in individual research work. Consequently, in the absence of a uniform dataset, such works are hard to compare with each other. In this work, we aim to achieve two main objectives– 1. propose code-centric features that are relevant to security program analysis tasks like vulnerability detection; 2. a thorough (qualitative and quantitative) examination of the existing code datasets that demonstrate the main characteristics of the individual datasets to have a clear comparison. Our evaluation finds exciting facts about existing datasets highlighting gaps that need to be addressed.","2023","2025-11-25 22:30:03","2025-11-25 22:30:03","","","","","","","","","ISEC '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Allahabad, India","","","","datasets; program graphs; software metrics; software vulnerability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ASAM4PXE","conferencePaper","2025","Wei, Wei","Static Analysis and LLM for Comprehensive Java Unit Test Generation","2025 8th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)","","","10.1109/AEMCSE65292.2025.11042526","","Software testing is crucial in ensuring the reliability and correctness of software applications. However, generating comprehensive test cases manually can be time-consuming and error-prone. This paper introduces SAGEN, a tool designed to automate Java unit test generation by leveraging static analysis of Syntax Trees (AST) and large language models (LLMs). SAGEN identifies literal values and their ranges, generating test cases that improve coverage and quality. In our experiments, SAGEN outperforms traditional test case generation tools such as EvoSuite and Randoop. It demonstrates a 10 % improvement in code coverage and a 13 % enhancement in test case quality. Furthermore, SAGEN achieves a compile pass rate of 89.7 %, proving its effectiveness in producing both high-quality and reliable test cases.","2025-05","2025-11-25 22:33:23","2025-11-25 22:33:23","","87-92","","","","","","","","","","","","","","","","","","","","","","","","Large language models; software testing; Software testing; test case generation; Test pattern generators; Java; Java unit testing; LLM; Manuals; Software; Software engineering; Software reliability; static analysis; Static analysis; Syntactics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZNJAW8A4","conferencePaper","2025","Nan, Zifan; Guo, Zhaoqiang; Liu, Kui; Xia, Xin","Test Intention Guided LLM-Based Unit Test Generation","2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)","","","10.1109/ICSE55347.2025.00243","","The emergence of Large Language Models (LLMs) has accelerated the progress of intelligent software engineering technologies, which brings promising possibilities for unit test generation. However, existing approaches for unit tests directly generated from Large Language Models (LLMs) often prove impractical due to their low coverage and insufficient mocking capabilities. This paper proposes IntUT, a novel approach that utilizes explicit test intentions (e.g., test inputs, mock behaviors, and expected results) to effectively guide the LLM to generate high-quality test cases. Our experimental results on three industry Java projects and live study demonstrate that prompting LLM with test intention can generate high-quality test cases for developers. Specifically, it achieves the improvements on branch coverage by 94 % and line coverage by 49 %. Finally, we obtain developers' feedback on using IntUT to generate cases for three new Java projects, achieving over 80 % line coverage and 30 % efficiency improvement on writing unit test cases.","2025-04","2025-11-25 22:37:02","2025-11-25 22:37:02","","1026-1038","","","","","","","","","","","","","","","","","","","","ISSN: 1558-1225","","","","Large language models; Test pattern generators; Java; LLM; Software engineering; Industries; Life estimation; unit test generation; program analysis; mocking; test intention","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2HGQRQXH","conferencePaper","2025","Rawson, Jessica; Reddivari, Sandeep","A ChatGPT-Powered Tool for Automating Context-Aware Acceptance Criteria Generation for User Stories","2025 IEEE International Conference on Information Reuse and Integration and Data Science (IRI)","","","10.1109/IRI66576.2025.00067","","There has been a growing interest in using Natural Language Processing (NLP), such as OpenAI's ChatGPT for software engineering tasks, including requirements engineering (RE), software design, and software testing. This paper covers a practical implementation of a ChatGPT-powered prompt engineering framework designed to generate context-specific acceptance criteria for user stories. The tool automates each stage of the framework—preprocessing contextual information and generating the final tailored acceptance criteria. We outline the design and implementation of the tool in this paper and its effectiveness through two repositories. This work demonstrates the potential of large language models (LLMs) to reduce the manual effort involved in RE, streamline development workflows, and minimize rework-related costs in agile software projects.","2025-08","2025-11-25 22:37:03","2025-11-25 22:37:03","","325-330","","","","","","","","","","","","","","","","","","","","ISSN: 2835-5776","","","","Large language models; Software testing; Manuals; Software engineering; Chatbots; ChatGPT; Prompt engineering; Requirements Engineering; Artificial Intelligence; Prompt Engineering; Acceptance Criteria; Data science; Requirements engineering; Rough surfaces; Software design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KG4H66JQ","conferencePaper","2025","Treviğno-Villalobos, Marlen; Quesada-López, Christian; Jiménez-Delgado, Efrén; Quirós-Oviedo, Rocío; Díaz-Oreiro, Ignacio","A Comparative Evaluation of ChatGPT, DeepSeek and Gemini in Automatic Unit Test Generation: a Success Rate Analysis","2025 IEEE VIII Congreso Internacional en Inteligencia Ambiental, Ingenieria de Software y Salud Electronica y Movil (AmITIC)","","","10.1109/AmITIC68284.2025.11214621","","The advancement of large-scale language models (LLMs) has opened up new possibilities for automating unit test generation, a traditionally manual and expensive task. This quantitative study evaluates the performance of three LLMs-ChatGPT 4o mini, DeepSeek v3, and Gemini 2.5 Flash Pro-in generating test cases for methods in C# developed in Unity. The execution success rate of the generated tests was measured using real and synthetic data. The synthetic data was intentionally created to represent common structures, while the real data came from existing project functions. The experimental design was controlled and included the factors LLM and data type and the blocks cyclomatic complexity and contextual memory with four replicates per combination, for a total of 96 experimental treatments. The results show that LLMs have a high potential to support the automatic generation of unit tests. Furthermore, it was evidenced that the choice of model has a significant effect on the success rate of the generated tests. These findings provide useful initial evidence to guide the selection and use of LLMs in test automation processes within software development environments","2025-09","2025-11-25 22:37:23","2025-11-25 22:37:23","","1-8","","","","","","","","","","","","","","","","","","","","","","","","Test pattern generators; LLM; Manuals; Software; Chatbots; Software development management; unit testing; automatic testing; Computational modeling; C# languages; Indexes; Monitoring; prompt; Synthetic data; Unity","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UT86JM3D","conferencePaper","2025","Olianas, Dario; Leotta, Maurizio; Ricca, Filippo","Leveraging Large Language Models for Explicit Wait Management in End-to-End Web Testing","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST62969.2025.10988981","","End-to-end (E2E) testing is an approach in which an application is automatically tested through scripts that simulate the actions a user would perform. Properly managing asynchronous interactions is crucial in this approach to avoid test failures and flakiness. In the Selenium WebDriver framework, this is typically addressed by using thread sleeps (which pause the test for a fixed time) or explicit waits (function calls that pause the test execution until a specified condition is met). Explicit waits require the selection of both a condition to wait for (e.g., element visibility, element clickability) and an element on which that condition applies. Since thread sleeps are unreliable and replacing them with appropriate explicit waits is a time consuming task, in this work, we leverage a Large Language Model (LLM) to assist testers in selecting the most appropriate explicit waits. We defined a structured procedure (a series of prompts) for engaging with the LLM and validated this approach empirically on three test suites affected by asynchronous waiting issues, as well as on 12 synthetic examples. Additionally, we compared our approach with SleepReplacer, the current state-of-the-art tool for replacing thread sleeps with explicit waits in E2E web test suites. The results show that the LLM-based approach can automatically replace the majority of thread sleeps in a test suite on the first attempt, outperforming SleepReplacer.","2025-03","2025-11-25 22:37:39","2025-11-25 22:37:39","","577-581","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Artificial Intelligence; End-to-end; Flak-iness; Large language models; LLM; Selenium; Selenium WebDriver; Software testing; Testing; Waiting Strategies","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RBL44SZV","conferencePaper","2025","Xie, Wei; Qian, Jian; Li, Yangdi; Huang, Jianye; Zheng, Yanrong; Liu, Yanan","A Method for Intelligent Optimization Algorithms to Automatically Generate LLM Test Data","2025 5th Asia-Pacific Conference on Communications Technology and Computer Science (ACCTCS)","","","10.1109/ACCTCS66275.2025.00047","","As AI products become more and more complex, traditional methods of manual test data generation are facing problems such as high cost, low efficiency, and insufficient test coverage, which affect the quality and stability of products. In order to solve the above problems, this paper proposes a method for automatic generation of AI (LLM) test data by intelligent optimization algorithm, and applies it to practical cases to carry out more detailed research. Experiments show that after a company introduces an intelligent optimization algorithm to automatically generate AI test data, based on this method, the test data generation time of its software has been reduced from 80.16 hours to 20.25 hours. And at the same time, the test coverage has increased from 60.32 % to 95.10 %, and the defect discovery rate has also increased from 65.51 % to 90.17 %. It can be seen that this method can significantly improve the test efficiency, optimize the test cost, and enhance the stability and quality of the software. It has been proved that intelligent optimization algorithms can be reasonably applied to automatically generate software test data, and then form a reliable method to provide enterprises with more accurate and efficient software testing solutions, and provide other companies with valuable optimization experience.","2025-04","2025-11-25 22:37:39","2025-11-25 22:37:39","","230-234","","","","","","","","","","","","","","","","","","","","","","","","Artificial intelligence; Automatically generated; Companies; Costs; Data collection; intelligent optimization algorithm; Methods for automatic generation of AI test data; Optimization; Software; Software algorithms; Software quality; Software reliability; Software testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GFSXV3HK","conferencePaper","2025","Biagiola, Matteo; Ghislotti, Gianluca; Tonella, Paolo","Improving the Readability of Automatically Generated Tests Using Large Language Models","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST62969.2025.10989020","","Search-based test generators are effective at producing unit tests with high coverage. However, such automatically generated tests have no meaningful test and variable names, making them hard to understand and interpret by developers. On the other hand, large language models (LLMs) can generate highly readable test cases, but they are not able to match the effectiveness of search-based generators, in terms of achieved code coverage. In this paper, we propose to combine the effectiveness of search-based generators with the readability of LLM generated tests. Our approach focuses on improving test and variable names produced by search-based tools, while keeping their semantics (i.e., their coverage) unchanged. Our evaluation on nine industrial and open source LLMs show that our readability improvement transformations are overall semantically-preserving and stable across multiple repetitions. Moreover, a human study with ten professional developers, show that our LLM-improved tests are as readable as developer-written tests, regardless of the LLM employed.","2025-03","2025-11-25 22:37:39","2025-11-25 22:37:39","","162-173","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Codes; Generators; Hands; Large language models; Large Language Models; Readability; Semantics; Software testing; Software Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S9WQB9HZ","journalArticle","2025","Deng, Yao; Tu, Zhi; Yao, Jiaohong; Zhang, Mengshi; Zhang, Tianyi; Zheng, Xi","TARGET: Traffic Rule-Based Test Generation for Autonomous Driving via Validated LLM-Guided Knowledge Extraction","IEEE Transactions on Software Engineering","","1939-3520","10.1109/TSE.2025.3569086","","Recent incidents with autonomous vehicles highlight the need for rigorous testing to ensure safety and robustness. Constructing test scenarios for autonomous driving systems (ADSs), however, is labor-intensive. We propose TARGET, an end-to-end framework that automatically generates test scenarios from traffic rules. To address complexity, we leverage a Large Language Model (LLM) to extract knowledge from traffic rules. To mitigate hallucinations caused by large context during input processing, we introduce a domain-specific language (DSL) designed to be syntactically simple and compositional. This design allows the LLM to learn and generate test scenarios in a modular manner while enabling syntactic and semantic validation for each component. Based on these validated representations, TARGET synthesizes executable scripts to render scenarios in simulation. Evaluated seven ADSs with 284 scenarios derived from 54 traffic rules, TARGET uncovered 610 rule violations, collisions, and other issues. For each violation, TARGET generates scenario recordings and detailed logs, aiding root cause analysis. Two identified issues were confirmed by ADS developers: one linked to an existing bug report and the other to limited ADS functionality.","2025-07","2025-11-25 22:37:39","2025-11-25 22:37:39","","1950-1968","","7","51","","","","","","","","","","","","","","","","","","","","","autonomous driving system testing; Autonomous vehicles; Data mining; DSL; LLM; Meteorology; Roads; Scenario generation; scenario-based testing; Semantics; Syntactics; Test generation; Test pattern generators; Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WVQA7J8H","conferencePaper","2025","Maybury, Mark; Forrest, Greg; O'Donnell, Donna","Lockheed Martin AI Factory: Generative AI and MLOps for Engineering, Enterprise and Edge","2025 IEEE International Conference on AI and Data Analytics (ICAD)","","","10.1109/ICAD65464.2025.11114065","","This article reports on the rapid creation and deployment at scale of hundreds of Large Language Model (LLM) applications, highlighting several across enterprise, engineering and edge use cases. This outcome was accelerated by the creation of an open architecture, secure and scalable generative AI Factory. An extensible platform, AI Factory empowers thousands of developers and over 50,000 end users across a diverse set of data types and use cases throughout our global enterprise. This evolvable approach reveals how to affordably deploy generative AI to create value securely at scale.","2025-06","2025-11-25 22:37:39","2025-11-25 22:37:39","","1-7","","","","","","","","","","","","","","","","","","","","","","","","AI Factory; Data analysis; Generative AI; Large language models; LLMs; Production facilities; scale; security; Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TJG567VW","conferencePaper","2025","Isaku, Erblin; Laaber, Christoph; Sartaj, Hassan; Ali, Shaukat; Schwitalla, Thomas; Nygård, Jan F.","LLMs in the Heart of Differential Testing: A Case Study on a Medical Rule Engine","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST62969.2025.10989025","","The Cancer Registry of Norway (CRN) uses an automated cancer registration support system (CaReSS) to support core cancer registry activities, i.e., data capture, data curation, and producing data products and statistics for various stakeholders. GURI is a core component of CaReSS, which is responsible for validating incoming data with medical rules. Such medical rules are manually implemented by medical experts based on medical standards, regulations, and research. Since large language models (LLMs) have been trained on a large amount of public information, including these documents, they can be employed to generate tests for GURI. Thus, we propose an LLM-based test generation and differential testing approach (LLMeDiff) to test GURI. We experimented with four different LLMs, two medical rule engine implementations, and 58 real medical rules to investigate the hallucination, success, time efficiency, and robustness of the LLMs to generate tests, and these tests' ability to find potential issues in GURI. Our results showed that GPT-3.5 hallucinates the least, is the most successful, and is generally the most robust; however, it has the worst time efficiency. Our differential testing revealed 22 medical rules where implementation inconsistencies were discovered (e.g., regarding handling rule versions). Finally, we provide insights for practitioners and researchers based on the results.","2025-03","2025-11-25 22:37:40","2025-11-25 22:37:40","","429-440","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Automated Software Testing; Cancer; Differential Testing; Electronic Health Records; Engines; Large language models; Large Language Models; Medical Rules; Regulation; Robustness; Software testing; Stakeholders; Standards; Test Generation; Test pattern generators; Web services","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QESEKL63","conferencePaper","2024","Couder, Juan Ortiz; Gomez, Dawson; Ochoa, Omar","Requirements Verification Through the Analysis of Source Code by Large Language Models","SoutheastCon 2024","","","10.1109/SoutheastCon52093.2024.10500073","","In the most recent years, Large Language Models (LLMs) have gained popularity and have been accepted and used in different domains due to their ability to understand and generate written language. LLMs allow us to analyze large amounts of data in a few moments, yet they are also extremely simple to use, making them a very powerful assistive tool that can aid in a wide range of tasks; from planning a family trip, to aid during the development process of a huge system. For software developers, LLMs have been mostly used for code generation, explanation, or optimization. Software verification is a crucial part of software development as it is the process of ensuring that a system meets specific requirements. Requirements specifications play a pivotal role in software verification as they define what a system should do. In this paper we propose the use of LLMs for code verification through the analysis of requirements specifications. We prove that LLMs, such as GPT-3.5, can verify a list of requirements through a given code and evaluate why the requirements have or have not been met.","2024-03","2025-11-25 22:37:40","2025-11-25 22:37:40","","75-80","","","","","","","","","","","","","","","","","","","","ISSN: 1558-058X","","","","ChatGPT; Codes; GPT-3.5; Large Language Model; Mathematical models; Planning; Robustness; Software; Software design; Software Engineering; Software Requirements; Source coding; Verification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7TRZUINF","conferencePaper","2025","Yoon, Juyeon; Kim, Seah; Kim, Somin; Jung, Sukchul; Yoo, Shin","Integrating LLM-Based Text Generation with Dynamic Context Retrieval for GUI Testing","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST62969.2025.10989041","","Automated GUI testing plays a crucial role for smartphone vendors who have to ensure that the widely used mobile apps-that are not essentially developed by the vendors-are compatible with new devices and system updates. While existing testing techniques can automatically generate event sequences to reach different GUI views, inputs such as strings and numbers remain difficult to generate, as their generation often involves semantic understanding of the app functionality. Recently, Large Language Models (LLMs) have been successfully adopted to generate string inputs that are semantically relevant to the test case. This paper evaluates the LLM-based input generation in the industrial context of vendor testing of both in-house and 3rd party mobile apps. We present DROIDFILLER, an LLM based input generation technique that builds upon existing work with more sophisticated prompt engineering and customisable context retrieval. DROIDFILLER is empirically evaluated using a total of 120 textfields collected from a total of 45 apps, including both in-house and 3rd party ones. The results show that DROIDFILLER can outperform both vanilla LLM based input generation as well as the existing resource pool approach. We integrate DROIDFILLER into the existing GUI testing framework used at Samsung, evaluate its performance, and discuss the challenges and considerations for practical adoption of LLM-based input generation in the industry.","2025-03","2025-11-25 22:37:40","2025-11-25 22:37:40","","394-405","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Accuracy; Automation; Graphical user interfaces; GUI Testing; Industries; Large language models; Large Language Models; Mobile applications; Prompt engineering; Semantics; Software testing; Test Automation; Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"26IICWM4","conferencePaper","2025","Barboni, Morena; Lampa, Filippo; Morichetta, Andrea; Polini, Andrea; Zulkoski, Edward","Mutant-Driven Test Generation for Ethereum Smart Contracts via LLMs","2025 IEEE International Conference on Artificial Intelligence Testing (AITest)","","","10.1109/AITest66680.2025.00033","","Bugs in Solidity smart contracts caused millions of dollars in losses in the past year alone. Mutation testing can expose weaknesses in test suites that simpler coverage metrics often miss, but the effort required to generate test cases for live mutants remains a major barrier to adoption. To address this, we present Alchemist, a framework that generates test cases from Solidity mutants via LLMs. Alchemist incorporates the scientific method into its test generation process, enabling the systematic refinement of test cases through hypotheses. Evaluation on Solidity projects demonstrates that this method improves test quality over direct prompting while reducing developer effort.","2025-07","2025-11-25 22:37:40","2025-11-25 22:37:40","","209-216","","","","","","","","","","","","","","","","","","","","ISSN: 2835-3560","","","","Blockchains; Computer bugs; Ethereum; Large Language Model; Large language models; Measurement; Mutation Testing; Smart Contract; Smart contracts; Solidity; Systematics; Test Generation; Test pattern generators; Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6682HFJG","conferencePaper","2024","Molina, Facundo; Copia, Juan Manuel; Gorla, Alessandra","Improving Patch Correctness Analysis via Random Testing and Large Language Models","2024 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST60714.2024.00036","","Patch correctness assessment represents a crucial step in the patch validation process, with the potential to enhance the practical adoption of automated program repair (APR) techniques and substantially reduce validation costs. While some automated techniques have been proposed for assessing patch correctness, they primarily focus on either ranking patches based on their likelihood of being correct or classifying them as correct or incorrect without offering any further explanatory information. In this paper, we introduce FIXCHECK, a novel approach that combines random testing and large language models to automatically generate fault-revealing tests for potentially incorrect patches. To achieve this, FIXCHECK employs a two-fold process: Firstly, a random testing procedure generates a comprehensive set of test cases. Secondly, a large language model is utilized to derive meaningful assertions for each test case. Additionally, FIXCHECK incorporates a selection and prioritization mechanism, which evaluates the generated tests executed on the patched program and discards or ranks them based on their likelihood of revealing faults in the patch. To assess the effectiveness of our approach, we conducted evaluations on a benchmark comprising 160 patches, encompassing both patches created by developers and patches generated by APR tools. The results demonstrate that FIXCHECK effectively generates fault-revealing tests for 62 % of incorrect patches written by developers, with a high level of confidence. Furthermore, it complements existing patch correctness assessment techniques by providing fault-revealing tests for up to 50% of the incorrect patches identified by state-of-the-art techniques.","2024-05","2025-11-25 22:37:40","2025-11-25 22:37:40","","317-328","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Analytical models; Benchmark testing; Costs; Fault diagnosis; Large language models; Large Language Models; Maintenance engineering; Patch Correctness Assessment; Random Testing; Software testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E5QAPYIW","conferencePaper","2024","Hyun, Sangwon; Guo, Mingyu; Babar, M. Ali","METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities","2024 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST60714.2024.00019","","Large-Language Models (LLMs) have shifted the paradigm of natural language data processing. However, their black-boxed and probabilistic characteristics can lead to potential risks in the quality of outputs in diverse LLM applications. Recent studies have tested Quality Attributes (QAs), such as robustness or fairness, of LLMs by generating adversarial input texts. However, existing studies have limited their coverage of QAs and tasks in LLMs and are difficult to extend. Additionally, these studies have only used one evaluation metric, Attack Success Rate (ASR), to assess the effectiveness of their approaches. We propose a MEtamorphic Testing for Analyzing LLMs (METAL) framework to address these issues by applying Metamorphic Testing (MT) techniques. This approach facilitates the systematic testing of LLM qualities by defining Metamorphic Relations (MRs), which serve as modularized evaluation metrics. The METAL framework can automatically generate hundreds of MRs from templates that cover various QAs and tasks. In addition, we introduced novel metrics to assess the effectiveness of MRs accurately by integrating the ASR method into the semantic qualities of text. Through the experiments conducted with three prominent LLMs, we have confirmed that the METAL framework effectively evaluates essential QAs on primary LLM tasks and reveals the quality risks in LLMs. Moreover, the newly proposed metrics can guide the optimal MRs for testing each task and suggest the most effective method for generating MRs.","2024-05","2025-11-25 22:37:40","2025-11-25 22:37:40","","117-128","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large-language models; Measurement; Metals; Metamorphic testing; Perturbation methods; Probabilistic logic; Quality attributes; Semantics; Software testing; Systematics; Text perturbations","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8TFNBH8U","conferencePaper","2025","Kaplan, Hasan","Systematic Testing of Security-Related Vulnerabilities in LLM-Based Applications","2025 IEEE/ACM 4th International Conference on AI Engineering – Software Engineering for AI (CAIN)","","","10.1109/CAIN66642.2025.00043","","Large Language Models (LLMs) have emerged as transformative tools in natural language understanding and generation. They possess billions of parameters, which enable them to generate coherent and contextually rich text [1]. These capabilities have made LLMs vital in domains such as customer service, content creation, and programming assistance [2], [3]. However, these advances come with significant risks. For example, a recent study showed that LLM responses contain private or sensitive information accidentally exposed during training [4], [5]. Furthermore, adversarial attacks have been shown to reduce system accuracy by as much as under controlled conditions [6]. A high-profile example is the misuse of LLMs to generate biased or harmful text when manipulated through adversarial prompts [7].","2025-04","2025-11-25 22:37:40","2025-11-25 22:37:40","","264-266","","","","","","","","","","","","","","","","","","","","","","","","Accuracy; adversarial attacks; Application software; Customer services; data leakage; devsecops; Large language models; large language models (llms); Natural language processing; Programming; prompts; security risks; Systematics; Testing; Training","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9B5AP2BM","conferencePaper","2025","Yoon, Juyeon; Feldt, Robert; Yoo, Shin","Adaptive Testing for LLM-Based Applications: A Diversity-Based Approach","2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","","","10.1109/ICSTW64639.2025.10962467","","The recent surge of building software systems powered by Large Language Models (LLMs) has led to the development of various testing frameworks, primarily focused on treating prompt templates as the unit of testing. Despite the significant costs associated with test input execution and output assessment, the curation of optimized test suites is yet overlooked in these tools, which calls for tailored test selection or prioritization strategies. In this paper, we show that diversity-based testing techniques, such as Adaptive Random Testing (ART) with appropriate string distance metrics, can be effectively applied to the testing of prompt templates. Our proposed adaptive testing approach adjusts the conventional ART process to this context by selecting new test inputs based on scores derived from existing test suite and their labelling results. Our results, obtained using various implementations that explore several string-based distances, confirm that our approach enables the discovery of failures with reduced testing budgets and promotes the generation of more varied outputs.","2025-03","2025-11-25 22:37:40","2025-11-25 22:37:40","","375-382","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Adaptive Random Testing; Large language models; Life estimation; LLM Applications; LLM Testing; Manuals; Measurement; Reviews; Software systems; Software testing; Subspace constraints; Surges; Test Prioritization; Test Selection; Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CZ5458K2","conferencePaper","2025","Xin, Zhidong; Huo, Haitao; Yang, Ka; Xue, Zishan","Application of Large Language Model Text Analysis in the Study of Urban Image","2025 5th International Conference on Artificial Intelligence, Big Data and Algorithms (CAIBDA)","","","10.1109/CAIBDA65784.2025.11183480","","To improve the accuracy of emotion discrimination in massive text analysis and analyze urban international image in detail, so as to provide a reference for urban construction and capital image building. In this paper, more than 80,000 social media comments were collected with the key words “Beijing” and “Peking”. Large language models (LLMs) was used to mine multi-level information of social media comments related to the evaluation of the city of Beijing, introduce in detail the relevant technical methods for large-scale text analysis using the LLMs, and comprehensively evaluate its performance. Our comparison results with the manual review results showed that the LLMs could accurately identify themes and discriminate emotions, which is expected to significantly improve the efficiency of text analysis and help researchers quickly grasp the themes and intentions of massive text information. Large Language Models (LLMs) significantly enhance text analysis efficiency. When combined with modular Python programming and iterative testing, they optimize performance and streamline debugging. Compared to traditional software, LLMs excel in interpreting complex semantics like irony and puns, and they adeptly handle multilingual social media comments. Their profound understanding of internet language and popular expressions further underscores their superiority in modern text analysis. Our text analysis on Beijing's image as an international city showed that Beijing's overall urban image on social platforms was positive and favorable, with higher attention paid to its economic, scientific, political and cultural dimensions, and higher subjective ratings in traditional culture, urban landscape, economic and technological fields, municipal facilities and public services. The international city image of Beijing in social media is dynamic, with multi-level and multi-theme relative independence and evolutionary stability.","2025-06","2025-11-25 22:37:40","2025-11-25 22:37:40","","371-378","","","","","","","","","","","","","","","","","","","","","","","","Beijing city image; Economics; Large language model; Large language models; Semantics; social media; Social networking (online); Software; Stability analysis; Streaming media; Testing; text analysis; Text analysis; Urban areas","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BPLU7HMT","conferencePaper","2024","Rahman, Tajmilur; Zhu, Yuecai; Maha, Lamyea; Roy, Chanchal; Roy, Banani; Schneider, Kevin","Take Loads Off Your Developers: Automated User Story Generation using Large Language Model","2024 IEEE International Conference on Software Maintenance and Evolution (ICSME)","","","10.1109/ICSME58944.2024.00082","","Software Maintenance and Evolution (SME) is moving fast with the assistance of artificial intelligence (AI), especially Large Language Models (LLM). Researchers have already started automating various activities of the SME workflow. Un-derstanding the requirements for maintenance and development work i.e. Requirements Engineering (RE) is a crucial phase that kicks off the SME workflow through multiple discussions on a proposed scope of work documented in different forms. The RE phase ends with a list of user stories for each unit task and usually created and tracked on a project management tool such as GitHub, Jira, AzurDev, etc. In this research, we collaborated with Bell Mobility to develop a tool “Geneus” (Generate UserSory) using GPT-4-turbo to automatically create user stories from software requirements documents. Requirements documents are usually long and contain complex information. Since LLMs typically suffer from hallucination when the input is too complex, this paper proposes a new prompting strategy, “Refine and Thought” (RaT), to mitigate that issue and improve the performance of the LLM in prompts with large and noisy contexts. Along with manual evaluation using RUST (Readability, Understandability, Specificity, Technical-aspects) survey questionnaire, automatic evaluation with BERTScore, and AlignScore evaluation metrics are used to evaluate the results of the “Geneus” tool. Results show that our method with RaT performs consistently better in most of the cases of interactions compared to the single-shot baseline method. However, the BERTScore and AlignScore test results are not consistent. In the median case, Geneus performs significantly better in all three interactions (requirements specifi-cation, user story details, and test case specifications) according to AlignScorebut it shows slightly low performance in requirements specifications according to BERTScore. Distilling RE documents requires significant time & effort from the senior members of the team through multiple meetings with stakeholders. We believe automating this process will certainly reduce additional loads off the software engineers and increase the ultimate productivity allowing them to utilize their time on other prioritized tasks.","2024-10","2025-11-25 22:37:40","2025-11-25 22:37:40","","791-801","","","","","","","","","","","","","","","","","","","","ISSN: 2576-3148","","","","Auto Generate; Large language models; LLM; Maintenance; Planning; Productivity; Project management; Prompt Engineering; Refine and Thought; Requirements engineering; Software maintenance; Software Maintenance Tasks; Stakeholders; Surveys; Testing; User Story","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T3SDJHRW","journalArticle","2024","Karpurapu, Shanthi; Myneni, Sravanthy; Nettur, Unnati; Gajja, Likhit Sagar; Burke, Dave; Stiehm, Tom; Payne, Jeffery","Comprehensive Evaluation and Insights Into the Use of Large Language Models in the Automation of Behavior-Driven Development Acceptance Test Formulation","IEEE Access","","2169-3536","10.1109/ACCESS.2024.3391815","","Behavior-driven development (BDD) is an Agile testing methodology fostering collaboration among developers, QA analysts, and stakeholders. In this manuscript, we propose a novel approach to enhance BDD practices using large language models (LLMs) to automate acceptance test generation. Our study uses zero and few-shot prompts to evaluate LLMs such as GPT-3.5, GPT-4, Llama-2-13B, and PaLM-2. The paper presents a detailed methodology that includes the dataset, prompt techniques, LLMs, and the evaluation process. The results demonstrate that GPT-3.5 and GPT-4 generate error-free BDD acceptance tests with better performance. The few-shot prompt technique highlights its ability to provide higher accuracy by incorporating examples for in-context learning. Furthermore, the study examines syntax errors, validation accuracy, and comparative analysis of LLMs, revealing their effectiveness in enhancing BDD practices. However, our study acknowledges that there are limitations to the proposed approach. We emphasize that this approach can support collaborative BDD processes and create opportunities for future research into automated BDD acceptance test generation using LLMs.","2024","2025-11-25 22:37:40","2025-11-25 22:37:40","","58715-58721","","","12","","","","","","","","","","","","","","","","","","","","","Agile software development; automated acceptance testing; Automation; Behavioral sciences; Best practices; Calculators; cucumber; generative AI; GPT-35 and GPT-4; Large language models; Llama-13B; natural language processing; PaLM-2; prompt engineering; software testing; Software testing; Syntactics; Task analysis; test case automation; Test pattern generators; Testing; zero-shot and few-shot","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9E2XZN2T","conferencePaper","2025","Mitani, Shohei; Moona, Salonee; Matsuo, Shin'ichiro; Burger, Eric","LLM-AQuA-DiVeR: LLM-Assisted Quality Assurance Through Dialogues on Verifiable Specification with Requirement Owners","2025 IEEE/ACM International Workshop on Responsible AI Engineering (RAIE)","","","10.1109/RAIE66699.2025.00008","","Quality Assurance (QA) is important for verifying software compliance with stakeholder requirements. QA faces a fundamental challenge of requirement interpretation ambiguity, which can result in insufficient software verification and failure in achieving the stakeholders' intended quality. The interpre-tation challenge intensifies in software development driven by Large Language Models (LLMs), where over-reliance can lead to missed quality-critical alternatives. However, existing works have paid limited attention to stakeholder involvement. We propose an LLM-assisted QA framework extending conventional LLM-driven development to enable stakeholder engagement in software verification. Our framework employs formal methods and rigorous testing to meet diverse quality demands, though this comprehensive verification introduces technical complexity affecting stakeholder engagement and verification costs. Our framework addresses these challenges through two key LLM roles: 1) an explanation assistant for stakeholder understanding, 2) a refinement assistant for incorporating stakeholder feedback while maintaining feasible verification costs. Our initial evaluation empirically demonstrates the framework's effectiveness through participant assessment scores, showing improved quality risk comprehension and efficient feedback incorporation in the verification process.","2025-04","2025-11-25 22:37:40","2025-11-25 22:37:40","","21-28","","","","","","","","","","","","","","","","","","","","","","","","Costs; Faces; Formal Method; Large language models; Large Language Models; LLM-Driven Development; Measurement; Quality assurance; Software development management; Software quality; Software Quality Assurance; Stakeholders; Standards; Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TJN5EVMC","conferencePaper","2025","Emektar, Miraç; Harmancı, Fatih M.; Güran, Aysun; Karadavut, Berat; Kirmizi, Günay; Öztürk, Berkay; Karamuk, Ata Emir; Güven, Mert","AI-Driven Synthetic Test Data and Scenario Generation via GAN-LLM Integration: A Modular Approach for Web Application Testing","2025 10th International Conference on Computer Science and Engineering (UBMK)","","","10.1109/UBMK67458.2025.11206764","","The growing complexity of web applications has heightened the need for automated and adaptive testing solutions. Manual test case authoring and static data generation often fall short in handling dynamic user interfaces and form-driven workflows. In this paper, we present a modular test automation architecture that integrates Generative Adversarial Networks (GANs) and Large Language Models (LLMs) to enable end-to-end generation and execution of test scenarios. The system utilizes a LLaMA4-based LLM to semantically extract input fields from web forms and guide both data and scenario generation. CTGAN (Conditional Tabular Generative Adversarial Network) is used to synthesize realistic, context-aware tabular test data, while SeqGAN (Sequence Generative Adversarial Network) produces logically ordered user interaction steps. These outputs are executed in a browser using Selenium WebDriver to simulate real-time user behavior. The architecture supports both contextual (URL-based) and non-contextual (prompt-based) modes and allows individual modules to be used independently or as a unified pipeline. The system also includes a graphical user interface to facilitate test configuration and execution. Experimental results across five public web applications show high format compliance in synthetic data (95.05 percent) and strong scenario validity, with an average automation success rate of 84.6 percent. The proposed approach enhances test coverage while significantly reducing manual effort, offering a scalable and privacy-aware solution for modern QA workflows.","2025-09","2025-11-25 22:37:40","2025-11-25 22:37:40","","1217-1222","","","","","","","","","","","","","","","","","","","","ISSN: 2521-1641","","","","Automation; Computer architecture; Generative adversarial networks; Large language models; Large Language Models; Manuals; Real-time systems; Scenario generation; Scenario-based Testing; Selenium; Synthetic data; Synthetic Data Generation; Test Automation; Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KKILYQCW","conferencePaper","2025","Oliveira, Flávia; Tiago, Leonardo; Nascimento, Alay; Castro, Renata; Chaves, Lennon","Employing Prompt Engineering for Generation of Bug Report: An Experience Report in Industry","2025 International Conference on Computer Technology Applications (ICCTA)","","","10.1109/ICCTA65425.2025.11166286","","The anticipated identification and correction of bugs during software testing processes reduce costs and ensure product quality, preventing these bugs from reaching end users. Consequently, communication between testers and developers regarding existing problems in software occurs through bug reports, which necessitates clear and precise documentation to enable developers to implement corrections effectively. In this context, Large Language Models (LLMs) have been utilized to facilitate bug report composition. This paper presents the results of an empirical study on the application of LLMs to assist in writing bug reports within a real software test team from the industry. To conduct this study, prompts were developed with the objective of employing them to report the most common defects according to the test scope managed by the test team. To evaluate the created prompts, a questionnaire was designed to measure the perception of testers regarding the use of LLMs in generating bug reports. Of the 7 participants from the test team, selected through non-probability convenience sampling, who participated in the study, 71% strongly agreed that utilizing an LLM to report bugs is straightforward, and 86% strongly agreed that it is feasible to use the LLM’s output to create bug reports. Moreover, the participants provided observations regarding the study, such as the occurrence of hallucinations and the necessity to modify the prompt when LLM did not produce the desired response. Thus, this experience report evaluates the implementation of prompts for reporting issues in a test team from the software industry.","2025-05","2025-11-25 22:37:40","2025-11-25 22:37:40","","40-46","","","","","","","","","","","","","","","","","","","","","","","","Browsers; Bug Report; Computer bugs; Industries; Large language models; LLM; Product design; Prompt engineering; Prompt Engineering; Quality assessment; Servers; Software; Software Test; Software testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HR2QH7XW","journalArticle","2025","Li, Ying; Zhong, Ye; Yang, Lijuan; Wang, Yanbo; Zhu, Penghua","LLM-Guided Crowdsourced Test Report Clustering","IEEE Access","","2169-3536","10.1109/ACCESS.2025.3530960","","This paper proposes a clustering method for crowdsourced test reports based on a large language model to solve the limitations of existing methods in processing repeated reports and utilizing multi-modal information. Existing crowdsourced test report clustering methods have significant shortcomings in handling duplicate reports, ignoring the semantic information of screenshots, and underutilizing the relationship between text and images. The emergence of LLM provides a new way to solve these problems. By integrating the semantic understanding ability of LLM, key information can be extracted from the test report more accurately, and the semantic relationship between screenshots and text descriptions can be used to guide the clustering process, thus improving the accuracy and effectiveness of clustering. The method in this paper uses a pre-trained LLM (such as GPT-4) to encode the text in the test report, and uses a visual model such as CLIP to encode the application screenshots, converting the text descriptions and images into high-dimensional semantic vectors. The cosine similarity is then used to calculate the similarity between the vectors, and semantic binding rules are constructed to guide the clustering process, ensuring that semantically related reports are assigned to the same cluster and semantically different reports are assigned to different clusters. Through experimental verification, this method is significantly superior to traditional methods in several evaluation indicators, demonstrating its great potential in improving the efficiency and quality of crowdsourced test report processing. In the future, this method is expected to be widely used in the process of software testing and maintenance, and further promote technological progress.","2025","2025-11-25 22:37:40","2025-11-25 22:37:40","","24894-24904","","","13","","","","","","","","","","","","","","","","","","","","","Clustering methods; crowdsourced testing; Data mining; Feature extraction; Large language model; Large language models; Natural language processing; Security; Semantics; Software; test report clustering; Testing; Vectors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PUWFSS3H","conferencePaper","2024","Li, Youwei; Li, Yangyang; Yang, Yangzhao","Test-Agent: A Multimodal App Automation Testing Framework Based on the Large Language Model","2024 IEEE 4th International Conference on Digital Twins and Parallel Intelligence (DTPI)","","","10.1109/DTPI61353.2024.10778901","","This paper introduces a multimodal agent-based app automation testing framework, named Test-Agent, built on the Large Language Model (LLM), designed to address the growing challenges in mobile application automation testing. As mobile applications become more prevalent and emerging systems like Harmony OS Next and mini-programs emerge, traditional automated testing methods, which depend on manually crafting test cases and scripts, are no longer sufficient for cross-platform compatibility and complex interaction logic. The Test-Agent framework employs artificial intelligence technologies to analyze application interface screenshots and user natural language instructions. Combined with deep learning models, it automatically generates and executes test actions on mobile devices. This innovative approach eliminates the need for pre-written test scripts or backend system access, relying solely on screenshots and UI structure information. It achieves cross-platform and cross-application universality, significantly reducing the workload of test case writing, enhancing test execution efficiency, and strengthening cross-platform adaptability. Test-Agent offers an innovative and efficient solution for automated testing of mobile applications.","2024-10","2025-11-25 22:37:40","2025-11-25 22:37:40","","609-614","","","","","","","","","","","","","","","","","","","","","","","","Adaptation models; Agent; App Automation Testing; Automation; Deep learning; Digital twins; Large Language Model; Large language models; Logic; Mobile applications; Mobile handsets; Natural languages; Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UGDH5QTN","conferencePaper","2025","Tao, Yingjie; Wang, Weiwei; Guo, Junxia","Intent-driven Web UI Tests Repair with LLM","2025 8th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)","","","10.1109/AEMCSE65292.2025.11042761","","As web applications are frequently updated, changes may introduce to web elements in new versions, causing test cases to fail. Consequently, automatic web test repair techniques are proposed to reduce the cost of regression testing. Most existing methods focus on finding the correct candidate elements or related attributes to fix the broken test case. However, when test case failures are caused by test flow changes or propagated breakages, those methods that focus solely on matching the failing element in the new version cannot work well. Through empirical analysis, we found that the test intent and the reasons that caused the test failure are useful in test case reparation. This paper proposes a novel intent-driven web test repair approach named LetTe, which first parses the test intent and failure reasons of failed web UI tests, and then guides the Large Language Model (LLM) to fix them via prompt design and fine-tuning. LetTe’s repair logic is to simulate that of a human expert. According to the test intent and reason for failure, “think about” the possible repair plan and then generate repair candidates through the corresponding chain-of-thought. We evaluate LetTe on 7 web applications collected from open-source websites as well as publicly available datasets. The experimental results show that our approach has a 75% correct repair rate, which is higher than all baseline methods.","2025-05","2025-11-25 22:37:40","2025-11-25 22:37:40","","306-314","","","","","","","","","","","","","","","","","","","","","","","","Computers; Costs; Data mining; Large Language Model; Large language models; Logic; Maintenance engineering; Manuals; Prompt engineering; Software engineering; Test Case Repair; Test Intent; Testing; Web Application Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C8LWJ9LI","conferencePaper","2024","Yabaku, Mounika; Pombo, Nuno; Ouhbi, Sofia","Exploring the Potential Use of Generative AI in Software Engineering Education","2024 IEEE 18th International Conference on Application of Information and Communication Technologies (AICT)","","","10.1109/AICT61888.2024.10740416","","The integration of Generative AI into software engineering education marks a transformative shift in teaching methodologies. This paper explores its potential, highlighting the benefits of enhancing student engagement, creativity, and efficiency while preparing them for industry challenges. Through a comprehensive analysis of 13 popular generative AI tools, we examine their roles in various software engineering tasks such as requirements analysis, design, coding, debugging, and testing. This paper contributes to the broader discourse on the future of software engineering education by offering evidence-based recommendations for leveraging generative AI to create adaptive and forward-thinking instructional strategies.","2024-09","2025-11-25 22:37:40","2025-11-25 22:37:40","","1-7","","","","","","","","","","","","","","","","","","","","ISSN: 2472-8586","","","","AIDriven Educational Tools; Debugging; Education; Generative AI; Large Language Models (LLMs); Pedagogical Innovation; Requirements engineering; Software; Software development management; Software engineering; Software Engineering Education; Software measurement; Testing; Usability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"94A5X5TE","conferencePaper","2025","Margabandu, Logan; Chen, Zizhao; Wong, W. Eric; Hsu, Chih-Wei","A Design of Experiments Oracle LLM for Research-Grounded Combinatorial Testing","2025 25th International Conference on Software Quality, Reliability, and Security Companion (QRS-C)","","","10.1109/QRS-C65679.2025.00021","","As artificial intelligence moves across all scientific and engineering disciplines, a big challenge remains: expert domain knowledge, especially in specialized areas like Design-of-Experiments (DoE), is hard to interpret or inaccessible to non-experts and thus hinders broader application and innovation. As large language models (LLMS) continue to improve, they offer a practical way to help close the knowledge gap and make expert-level information more accessible to a wider audience. To address this need, we designed and developed a new solution: a Design-of-Experiments Oracle (DOE), a conversational LLM. This paper presents this AI-powered system that changes how complex DoE principles are understood and used. DOE uses state-of-the-art techniques including a Llama 3.2 model inside a Retrieval-Augmented Fine-Tuned Transformer (RAFT) architecture. Its knowledge base is informed by and enables semantic retrieval from combinatorial testing research. This is a conversational expert system. With this system, we want to enable engineers and researchers to apply complex DoE principles, give them interactive, research-based guidance. Ultimately, this is to contribute to the efforts to speed up innovation, improve system reliability and make advanced engineering methods more accessible in the digital age.","2025-07","2025-11-25 22:38:39","2025-11-25 22:38:39","","82-90","","","","","","","","","","","","","","","","","","","","ISSN: 2693-9371","","","","Large language models; Semantics; Software reliability; Software quality; Large Language Models; Technological innovation; Transformers; Security; Combinatorial testing; Combinatorial Testing; Retrieval augmented generation; Reliability engineering; Conversational AI; Design-of-Experiments (DoE); Oracle LLM; Retrieval Augmented Generation (RAG)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V5JW55F2","conferencePaper","2025","Ma, Lezhi; Liu, Shangqing; Li, Yi; Xie, Xiaofei; Bu, Lei","SpecGen: Automated Generation of Formal Program Specifications via Large Language Models","2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)","","","10.1109/ICSE55347.2025.00129","","In the software development process, formal program specifications play a crucial role in various stages, including requirement analysis, software testing, and verification. However, manually crafting formal program specifications is rather difficult, making the job time-consuming and labor-intensive. Moreover, it is even more challenging to write specifications that correctly and comprehensively describe the semantics of complex programs. To reduce the burden on software developers, automated specification generation methods have emerged. However, existing methods usually rely on predefined templates or grammar, making them struggle to accurately describe the behavior and functionality of complex real-world programs. To tackle this challenge, we introduce SpecGen, a novel technique for formal program specification generation based on Large Language Models (LLMs). Our key insight is to overcome the limitations of existing methods by leveraging the code comprehension capability of LLMs. The process of SpecGen consists of two phases. The first phase employs a conversational approach that guides the LLM in generating appropriate specifications for a given program, aiming to utilize the ability of LLM to generate high-quality specifications. The second phase, designed for where the LLM fails to generate correct specifications, applies four mutation operators to the model-generated specifications and selects verifiable specifications from the mutated ones through a novel heuristic selection strategy by assigning different weights of variants in an efficient manner. We evaluate SpecGen on two datasets, including the SV-COMP Java category benchmark and a manually constructed dataset containing 120 programs. Experimental results demonstrate that SpecGen succeeds in generating verifiable specifications for 279 out of 385 programs, outperforming the existing LLM-based approaches and conventional specification generation tools like Houdini and Daikon. Further investigations on the quality of generated specifications indicate that SpecGen can comprehensively articulate the behaviors of the input program.","2025-04","2025-11-25 22:38:39","2025-11-25 22:38:39","","16-28","","","","","","","","","","","","","","","","","","","","ISSN: 1558-1225","","","","large language model; Large language models; Semantics; Software testing; Java; Software; Software engineering; Software development management; Codes; Benchmark testing; Grammar; program verification; specification inference","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VVGRDCJZ","conferencePaper","2025","Cunha, Felipe; Perkusich, Mirko; Albuquerque, Danyllo; Gorgônio, Kyller; Perkusich, Angelo","Llm-Codeval: a Framework for Verifying Implementations of Mathematical Functions Using Language Models","2025 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)","","","10.23919/SoftCOM66362.2025.11197356","","Mathematical functions are essential to software systems in areas like probabilistic modeling, simulation, and scientific computing. However, verifying their correctness becomes challenging when definitions come from proprietary tools or academic literature without reference code, leaving no clear ground truth. This paper proposes LLMCODEVAL, a lightweight and systematic framework for validating such implementations using Large Language Models (LLMs). The framework comprises three stages: (i) specification and implementation based on formal contracts; (ii) semantic validation via LLM-generated correctness proofs, followed by expert review; and (iii) empirical evaluation using statistical metrics. We applied LLM-CODEVAL to five ranked-node aggregation functions in Bayesian Networks (BNs) across 48 scenarios with varying weights, variances, and parent states. All implementations achieved Brier Scores below 0.0001, with LLM proofs aligning to specifications after at most two review iterations. The results demonstrate that LLM-CODEVAL enables explainable, auditable, and reproducible verification of mathematical functions. Its stage-based design makes it adaptable to domains where formal definitions exist but verified implementations are unavailable. These findings underscore both the potential and the current limitations of LLM-assisted verification: while LLMs excel at structuring logical arguments, they still require expert oversight to ensure precise mathematical semantics.","2025-09","2025-11-25 22:38:56","2025-11-25 22:38:56","","1-6","","","","","","","","","","","","","","","","","","","","ISSN: 1847-358X","","","","Bayes methods; Bayesian Networks; Code Validation; Codes; Computational modeling; Large language models; Large Language Models; Mathematical models; Reviews; Semantics; Software Engineering; Software systems; Systematics; Telecommunications; Test Automation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8TUNTQDM","conferencePaper","2025","Zhou, Shiyao; Wang, Jincheng; Ye, He; Zhou, Hao; Goues, Claire Le; Luo, Xiapu","LWDIFF: an LLM-Assisted Differential Testing Framework for Webassembly Runtimes","2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)","","","10.1109/ICSE55347.2025.00233","","WebAssembly (Wasm) runtimes execute Wasm programs, a popular low-level language for efficiently executing high-level languages in browsers, with broad applications across diverse domains. The correctness of those runtimes is critical for both functionality and security of Wasm execution, motivating testing approaches that target Wasm runtimes specifically. However, existing Wasm testing frameworks fail to generate test cases that effectively test all three phases of runtime, i.e., decoding, validation, and execution. To address this research gap, we propose a new differential testing framework for Wasm runtimes, which leverages knowledge from the Wasm language specification that prior techniques overlooked, enhancing comprehensive testing of runtime functionality. Specifically, we first use a large language model to extract that knowledge from the specification. We use that knowledge in the context of multiple novel mutation operators that generate test cases with diverse features to test all three runtime phases. We evaluate LWDIFF by applying it to eight Wasm runtimes. Compared with the state-of-the-art Wasm testers, LWDIFF achieves the highest branch coverage and identifies the largest number of bugs. In total, LWDIFF discovers 31 bugs across eight runtimes, all of which are confirmed, with 25 of them previously undiscovered.","2025-04","2025-11-25 22:38:57","2025-11-25 22:38:57","","153-164","","","","","","","","","","","","","","","","","","","","ISSN: 1558-1225","","","","Large language models; Codes; Testing; Computer bugs; Security; Runtime; Feature extraction; Browsers; Decoding; High level languages","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"24FPK6IL","conferencePaper","2025","Li, Jiageng; Dong, Zhen; Wang, Chong; You, Haozhen; Zhang, Cen; Liu, Yang; Peng, Xin","LLM Based Input Space Partitioning Testing for Library APIs","2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)","","","10.1109/ICSE55347.2025.00153","","Automated library APIs testing is difficult as it requires exploring a vast space of parameter inputs that may involve objects with complex data types. Existing search based approaches, with limited knowledge of relations between object states and program branches, often suffer from the low efficiency issue, i.e., tending to generate invalid inputs. Symbolic execution based approaches can effectively identify such relations, but fail to scale to large programs. In this work, we present an LLM-based input space partitioning testing approach, LISP, for library APIs. The approach lever-ages LLMs to understand the code of a library API under test and perform input space partitioning based on its understanding and rich common knowledge. Specifically, we provide the signature and code of the API under test to LLMs, with the expectation of obtaining a text description of each input space partition of the API under test. Then, we generate inputs through employing the generated text description to sample inputs from each partition, ultimately resulting in test suites that systematically explore the program behavior of the API. We evaluate LISP on more than 2,205 library API meth-ods taken from 10 popular open-source Java libraries (e.g., apache/commons-lang with 2.6k stars, guava with 48.8k stars on GitHub). Our experiment results show that LISP is effective in library API testing. It significantly outperforms state-of-the-art tool EvoSuite in terms of edge coverage. On average, LISP achieves 67.82 % branch coverage, surpassing EvoSuite by 1.21 times. In total, LISP triggers 404 exceptions or errors in the experiments, and discovers 13 previously unknown vulnerabilities during evaluation, which have been assigned CVE IDs.","2025-04","2025-11-25 22:39:09","2025-11-25 22:39:09","","1436-1448","","","","","","","","","","","","","","","","","","","","ISSN: 1558-1225","","","","Large language models; Java; Software engineering; Software development management; Codes; Testing; Large Language Models; Symbolic Execution; API testing; Libraries; Search problems; Space exploration; Input Space Partitioning Testing; Stars","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KTJR3DJN","conferencePaper","2025","Mani, Nariman; Attaranasl, Salma","Enhancing Adaptive Test Healing with Graph Neural Networks for Dependency-Aware Decision Making","2025 IEEE International Conference on Artificial Intelligence Testing (AITest)","","","10.1109/AITest66680.2025.00023","","Flaky tests are a major obstacle in modern CI/CD pipelines, leading to unreliable feedback, increased reruns, and developer frustration. Our previously published adaptive healing framework combined Large Language Models (LLMs) and Reinforcement Learning (RL) to automate flaky test recovery, but it assumed test independence and failed to account for structural dependencies between tests. In this paper, we introduce a significant extension to that baseline: a Graph Neural Network (GNN)-based Test Dependency Mapping layer that models intertest relationships. By integrating GNN embeddings with LLM-classified failures, the RL agent becomes dependency-aware, enabling more precise and efficient healing decisions. We evaluate the enhanced framework on a real-world industrial platform, a social lifestyle application actively used by thousands of users for health, nutrition, and coaching. Results show a 90% reduction in flaky test-related costs and faster, autonomous resolution of dependency-induced failures.","2025-07","2025-11-25 22:40:41","2025-11-25 22:40:41","","126-133","","","","","","","","","","","","","","","","","","","","ISSN: 2835-3560","","","","Adaptive systems; Adaptive Test Healing; Continuous Integration (CI); Costs; Decision making; Flaky Tests; GPT; Graph neural networks; Graph Neural Networks (GNN); Large language models; Large Language Models (LLMs); Pipelines; Real-time systems; Reinforcement learning; Reinforcement Learning (RL); Self-Healing Test Automation; Testing; Transformers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TYE68T7Z","journalArticle","2025","Khojah, Ranim; de Oliveira Neto, Francisco Gomes; Mohamad, Mazen; Leitner, Philipp","The Impact of Prompt Programming on Function-Level Code Generation","IEEE Transactions on Software Engineering","","1939-3520","10.1109/TSE.2025.3587794","","Large Language Models (LLMs) are increasingly used by software engineers for code generation. However, limitations of LLMs such as irrelevant or incorrect code have highlighted the need for prompt programming (or prompt engineering) where engineers apply specific prompt techniques (e.g., chain-of-thought or input-output examples) to improve the generated code. While some prompt techniques have been studied, the impact of different techniques — and their interactions — on code generation is still not fully understood. In this study, we introduce CodePromptEval, a dataset of 7072 prompts designed to evaluate five prompt techniques (few-shot, persona, chain-of-thought, function signature, list of packages) and their effect on the correctness, similarity, and quality of complete functions generated by three LLMs (GPT-4o, Llama3, and Mistral). Our findings show that while certain prompt techniques significantly influence the generated code, combining multiple techniques does not necessarily improve the outcome. Additionally, we observed a trade-off between correctness and quality when using prompt techniques. Our dataset and replication package enable future research on improving LLM-generated code and evaluating new prompt techniques.","2025-08","2025-11-25 22:40:42","2025-11-25 22:40:42","","2381-2395","","8","51","","","","","","","","","","","","","","","","","","","","","Accuracy; Benchmark testing; code generation; Codes; Encoding; Few shot learning; Large language models; Programming; Prompt engineering; prompt programming; Software; Software engineering; Training","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7NYN3XC2","conferencePaper","2024","Ramasamy, Vijayalakshmi; Ramamoorthy, Suganya; Walia, Gursimran Singh; Kulpinski, Eli; Antreassian, Aaron","Enhancing User Story Generation in Agile Software Development Through Open AI and Prompt Engineering","2024 IEEE Frontiers in Education Conference (FIE)","","","10.1109/FIE61694.2024.10893343","","This innovative practice full paper explores the use of AI technologies in user story generation. With the emergence of agile software development, generating comprehensive user stories that capture all necessary functionalities and perspectives has become crucial for software development. Every computing program in the United States requires a semester-or year-long senior capstone project, which requires student teams to gather and document technical requirements. Effective user story generation is crucial for successfully implementing software projects. However, user stories written in natural language can be prone to inherent defects such as incompleteness and incorrectness, which may creep in during the downstream development activities like software designs, construction, and testing. One of the challenges faced by software engineering educators is to teach students how to elicit and document requirements, which serve as a blueprint for software development. Advanced AI technologies have increased the popularity of large language models (LLMs) trained on large multimodal datasets. Therefore, utilizing LLM-based techniques can assist educators in helping students discover aspects of user stories that may have been overlooked or missed during the manual analysis of requirements from various stakeholders. The main goal of this research study is to investigate the potential application of OpenAI techniques in software development courses at two academic institutions to enhance software design and development processes, aiming to improve innovation and efficiency in team project-based educational settings. The data used for the study constitute student teams generating user stories by traditional methods (control) vs. student teams using OpenAI agents (treatment) such as gpt-4-turbo for generating user stories. The overarching research questions include: RQ-l) What aspects of user stories generated using OpenAI prompt engineering differ significantly from those generated using the traditional method? RQ-2) Can the prompt engineering data provide insights into the efficacy of the questions/prompts that affect the quality and comprehensiveness of user stories created by software development teams? Industry experts evaluated the user stories created and analyzed how prompt engineering affects the overall effectiveness and innovation of user story creation, which provided guidelines for incorporating AI-driven approaches into software development practices. Overall, this research seeks to contribute to the growing body of knowledge on the application of AI in software engineering education, specifically in user story generation. Investigating the use of AI technologies in user story generation could further enhance the usability of prompt engineering in agile software development environments. We plan to expand the study to investigate the long-term effects of prompt engineering on all phases of software development.","2024-10","2025-11-25 22:40:42","2025-11-25 22:40:42","","1-8","","","","","","","","","","","","","","","","","","","","ISSN: 2377-634X","","","","Agile software development; Collaboration; Collaboration network; complex network analysis; Prompt engineering; Software; Software engineering; Stakeholders; structured collaboration network; Technical requirements; Technological innovation; Testing; Usability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IG4SD6F4","conferencePaper","2025","Domanski, Peter; Faridi, Mukarram Ali; Kaunang, Gabriel; Pradeep, Wilson; Singh, Adit; Amrizal, Muhammad Alfian; Li, Yanjing; Firouzi, Farshad; Chakrabarty, Krishnendu","Silent Data Corruption: Advancing Detection, Diagnosis, and Mitigation Strategies","2025 IEEE 43rd VLSI Test Symposium (VTS)","","","10.1109/VTS65138.2025.11022867","","Silent Data Corruptions (SDCs) pose a critical challenge to computer system reliability, arising from vulnerabilities across different layers of the computing stack. This paper addresses this challenge through three complementary contributions that systematically target SDCs from hardware manufacturing to application-level resilience. First, we analyze timing failures caused by random process variations in advanced technology nodes, revealing that extreme slow paths at lower voltages are dominated by single weak transistors—insights crucial for manufacturing and in-field testing. Second, we introduce an LLM-driven framework that generates targeted functional test programs to induce SDCs, demonstrating its effectiveness in stressing hardware, uncovering latent vulnerabilities, and increasing energy consumption in a given device under test (DUT), making it a valuable tool for in-field testing. Third, as machine learning continues to drive advancements across critical domains such as healthcare, finance, and autonomous systems, ensuring its reliability is paramount. However, the susceptibility of these applications to SDCs threatens their reliability and robustness. To address this, we propose Fidelity-Q, a novel fault injection methodology to evaluate the impact of SDCs on Quantized Neural Networks (QNNs), showing that lower-bit quantization increases error susceptibility. Collectively, these contributions provide a comprehensive approach to identifying, analyzing, and mitigating SDCs across the computing stack, from hardware testing to machine learning applications.","2025-04","2025-11-25 22:40:42","2025-11-25 22:40:42","","1-11","","","","","","","","","","","","","","","","","","","","ISSN: 2375-1053","","","","Computer network reliability; Error Detection; Fault Injection; Hardware; Hardware Reliability; Machine learning; Manufacturing; Prevention and mitigation; Reliability; Resilience; Silent Data Corruption; Test Generation; Test pattern generators; Testing; Very large scale integration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""