"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"KKNHA5X5","conferencePaper","2024","Bhatia, Shreya; Gandhi, Tarushi; Kumar, Dhruv; Jalote, Pankaj","Unit Test Generation using Generative AI : A Comparative Performance Analysis of Autogeneration Tools","Proceedings of the 1st International Workshop on Large Language Models for Code","979-8-4007-0579-3","","10.1145/3643795.3648396","https://doi.org/10.1145/3643795.3648396","Generating unit tests is a crucial task in software development, demanding substantial time and effort from programmers. The advent of Large Language Models (LLMs) introduces a novel avenue for unit test script generation. This research aims to experimentally investigate the effectiveness of LLMs, specifically exemplified by ChatGPT, for generating unit test scripts for Python programs, and how the generated test cases compare with those generated by an existing unit test generator (Pynguin). For experiments, we consider three types of code units: 1) Procedural scripts, 2) Function-based modular code, and 3) Class-based code. The generated test cases are evaluated based on criteria such as coverage, correctness, and readability. Our results show that ChatGPT's performance is comparable with Pynguin in terms of coverage, though for some cases its performance is superior to Pynguin. We also find that about a third of assertions generated by ChatGPT for some categories were incorrect. Our results also show that there is minimal overlap in missed statements between ChatGPT and Pynguin, thus, suggesting that a combination of both tools may enhance unit test generation performance. Finally, in our experiments, prompt engineering improved ChatGPT's performance, achieving a much higher coverage.*These authors contributed equally.","2024","2025-11-25 22:29:27","2025-11-25 22:48:11","","54–61","","","","","","","LLM4Code '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","Large language models; Semantics; Software testing; Test pattern generators; Software; Accuracy; Chatbots; ChatGPT; generative AI; Generative AI; Software development management; Codes; Unit Test Generation; large language models; Large Language Models; Scalability; unit test generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3W4ZYMMQ","journalArticle","2024","Crandall, Johannah L.; Crandall, Aaron S.","Large Language Model-Supported Software Testing with the CS Matrix Taxonomy","J. Comput. Sci. Coll.","","1937-4771","","","New breakthroughs in code synthesis from Generative Pre-Trained Transformers (GPT) and Large Language Model (LLM) algorithms are driving significant changes to software engineering education. Having algorithms able to generate components of a software project means that software developers will need stronger skills in requirements specification to guide code generation as well as stronger skills in code review, testing, and integration to incorporate AI-generated code into projects. Shifts in industry and classroom practices are already occurring with the availability of inline code generation tools like GitHub's Copilot, which makes discussion of pedagogical strategies in this area a timely topic. Of immediate concern in computer science education is the potential for LLM-generated code and code help to undermine the learning of CS students. In order to avoid such undermining in even intentional uses of LLM-enhanced learning supports, it is necessary to clarify the roles such supports need to play in the pedagogical process. The Computer Science Matrix Taxonomy provides a strong framework for organizing software testing learning outcomes as well as delineating the operational space in which LLM-based feedback tools should operate to support those learning outcomes. In this paper, the authors operationalize the CS Matrix Taxonomy for software testing learning outcomes and illustrate the integration of LLM-generated test strategy suggestions as an extension of the peer coding/testing model. The work includes examples of AI-generated code testing suggestions that students would use to help guide their own code synthesis for assignments or projects.","2024-10","2025-11-25 22:29:27","2025-11-25 22:29:27","","49–58","","1","40","","","","","","","","","","","","","","","","","Place: Evansville, IN, USA Publisher: Consortium for Computing Sciences in Colleges","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8XML2G8S","conferencePaper","2025","Wynn-Williams, Stephen; Tyrrell, Ryan; Pantelic, Vera; Lawford, Mark; Menghi, Claudio; Nalla, Phaneendra; Artail, Hassan","Can Generative AI Produce Test Cases? An Experience from the Automotive Domain","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","10.1145/3696630.3728568","https://doi.org/10.1145/3696630.3728568","Engineers need automated support for software testing. Generative AI is a novel technology for generating new content; however, its applicability for test case generation is still unclear. This work considers the following question: Can generative AI produce test cases in industrial software applications? We framed our question in the automotive domain. We performed our evaluation in collaboration with a large automotive manufacturer to assess to what extent generative AI can produce test cases (a.k.a. test scripts) from informal test case specifications. We considered 1) informal test case specifications defined in Rational Quality Manager, an industrial test management tool from IBM, and 2) executable test scripts specified as ecu.test packages supported by the ecu.test tool from Tracetronic. We used generative AI to produce the test scripts from the informal test case descriptions. Our results show that generative AI can produce correct or near-correct test scripts in a reasonable number of cases. We also analyzed the effects of prompt design, choice of generative AI model, and context accuracy on the effectiveness of our solution and reflected on our results.","2025","2025-11-25 22:29:27","2025-11-25 22:29:27","","456–467","","","","","","","FSE Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","software testing; LLM; generative AI; automotive software","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q4GCSYJZ","conferencePaper","2025","Zhang, Jiashuo; Chen, Jiachi; Grundy, John; Gao, Jianbo; Wang, Yanlin; Chen, Ting; Guan, Zhi; Chen, Zhong","Automated Test Generation for Smart Contracts via On-Chain Test Case Augmentation and Migration","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00096","https://doi.org/10.1109/ICSE55347.2025.00096","Pre-deployment testing has become essential to ensure the functional correctness of smart contracts. However, since smart contracts are stateful programs integrating many different functionalities, manually writing test cases to cover all potential usages requires significant effort from developers, leading to insufficient testing and increasing risks in practice. Although several testing techniques for smart contracts have been proposed, they primarily focus on detecting common low-level vulnerabilities such as re-entrancy, rather than generating expressive and function-relevant test cases that can reduce manual testing efforts. To bridge the gap, we propose SolMigrator, an automated technique designed to generate expressive and representative test cases for smart contracts. To our knowledge, SolMigrator is the first migration-based test generation technique for smart contracts, which extracts test cases from real-world usages of on-chain contracts and migrates them to test newly developed smart contracts with similar functionalities. Given a target smart contract to be tested and an on-chain similar source smart contract, SolMigrator first transforms the on-chain usage of the source contract into off-chain executable test cases based on on-chain transaction replay and dependency analysis. It then employs fine-grained static analysis to migrate the augmented test cases from the source to the target smart contract. We built a prototype of SolMigrator and have evaluated it on real-world smart contracts within the two most popular categories, ERC20 and ERC721. Our evaluation results demonstrate that SolMigrator effectively extracts test cases from existing on-chain smart contracts and accurately migrates them across different smart contracts, achieving an average precision of 96.3% and accuracy of 93.6%. Furthermore, the results indicate that these migrated test cases effectively cover common key functionalities of the target smart contracts. This provides promising evidence that real-world usages of existing smart contracts can be transformed into effective test cases for other newly developed smart contracts.","2025","2025-11-25 22:29:27","2025-11-25 22:29:27","","1947–1959","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","ethereum; smart contracts; test generation; test migration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"35PIQM5R","conferencePaper","2024","Jorgensen, Steven; Nadizar, Giorgia; Pietropolli, Gloria; Manzoni, Luca; Medvet, Eric; O'Reilly, Una-May; Hemberg, Erik","Large Language Model-based Test Case Generation for GP Agents","Proceedings of the Genetic and Evolutionary Computation Conference","979-8-4007-0494-9","","10.1145/3638529.3654056","https://doi.org/10.1145/3638529.3654056","Genetic programming (GP) is a popular problem-solving and optimization technique. However, generating effective test cases for training and evaluating GP programs requires strong domain knowledge. Furthermore, GP programs often prematurely converge on local optima when given excessively difficult problems early in their training. Curriculum learning (CL) has been effective in addressing similar issues across different reinforcement learning (RL) domains, but it requires the manual generation of progressively difficult test cases as well as their careful scheduling. In this work, we leverage the domain knowledge and the strong generative abilities of large language models (LLMs) to generate effective test cases of increasing difficulties and schedule them according to various curricula. We show that by integrating a curriculum scheduler with LLM-generated test cases we can effectively train a GP agent player with environments-based curricula for a single-player game and opponent-based curricula for a multi-player game. Finally, we discuss the benefits and challenges of implementing this method for other problem domains.","2024","2025-11-25 22:29:27","2025-11-25 22:29:27","","914–923","","","","","","","GECCO '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Melbourne, VIC, Australia","","","","large language models; curriculum learning; linear GP","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FTVDCB8W","journalArticle","2025","Yang, Zhenzhen; Huang, Rubing; Cui, Chenhui; Niu, Nan; Towey, Dave","Requirements-Based Test Generation: A Comprehensive Survey","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3771727","https://doi.org/10.1145/3771727","As an important way of assuring software quality, software testing generates and executes test cases to identify software failures. Many strategies have been proposed to guide test-case generation, such as source-code-based approaches and methods based on bug reports. Requirements-based test generation (RBTG) constructs test cases based on specified requirements, aligning with user needs and expectations, without requiring access to the source code. Since its introduction in 1994, there have been many contributions to the development of RBTG, including various approaches, implementations, tools, assessment and evaluation methods, and applications. This paper provides a comprehensive survey on RBTG, categorizing requirements types, classifying approaches, investigating types of test cases, summarizing available tools, and analyzing experimental evaluations. This paper also summarizes the domains and industrial applications of RBTG, and discusses some open research challenges and potential future work.","2025-10","2025-11-25 22:29:28","2025-11-25 22:29:28","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Software testing; test generation; software requirements; survey","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QXEM2SSA","conferencePaper","2025","Mahmoud, Yara; Azim, Akramul; Liscano, Ramiro; Smith, Kevin; Chang, Yee-Kang; Tauseef, Qasim; Seferi, Gkerta","Leveraging LLM Enhanced Commit Messages to Improve Machine Learning Based Test Case Prioritization","Proceedings of the 21st International Conference on Predictive Models and Data Analytics in Software Engineering","979-8-4007-1594-5","","10.1145/3727582.3728681","https://doi.org/10.1145/3727582.3728681","In the rapidly evolving landscape of software development, software testing is critical for maintaining code quality and reducing defects. Effective test case prioritization employs techniques to identify defects early and ensure software quality. New avenues of research have explored using machine learning (ML) to automate the process, most current applications leverage a machine learning model using numerical features to prioritize the test cases. This study investigates the enhancement of this process by incorporating text-based features derived from git commit messages, which often include valuable information about code changes. Given that commit messages are often poorly written and inconsistent, we employ a large language model (LLM) to rewrite these messages based on code diffs, with the aim of improving the quality of their format and the information they contain. We then assess whether these refined commit messages, as an additional feature, contribute to better performance of the test case prioritization model. Our preliminary results indicate that the inclusion of LLM-enhanced commit messages leads to a noticeable improvement in prioritization effectiveness, suggesting a promising avenue for integrating natural language processing techniques in software testing workflows.","2025","2025-11-25 22:29:28","2025-11-25 22:29:28","","45–54","","","","","","","PROMISE '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Trondheim, Norway","","","","large language model; software testing; commit messages; natural language processing; test case prioritization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HIXAM5EP","journalArticle","2024","Yuan, Zhiqiang; Liu, Mingwei; Ding, Shiji; Wang, Kaixin; Chen, Yixuan; Peng, Xin; Lou, Yiling","Evaluating and Improving ChatGPT for Unit Test Generation","Proc. ACM Softw. Eng.","","","10.1145/3660783","https://doi.org/10.1145/3660783","Unit testing plays an essential role in detecting bugs in functionally-discrete program units (e.g., methods). Manually writing high-quality unit tests is time-consuming and laborious. Although the traditional techniques are able to generate tests with reasonable coverage, they are shown to exhibit low readability and still cannot be directly adopted by developers in practice. Recent work has shown the large potential of large language models (LLMs) in unit test generation. By being pre-trained on a massive developer-written code corpus, the models are capable of generating more human-like and meaningful test code. In this work, we perform the first empirical study to evaluate the capability of ChatGPT (i.e., one of the most representative LLMs with outstanding performance in code generation and comprehension) in unit test generation. In particular, we conduct both a quantitative analysis and a user study to systematically investigate the quality of its generated tests in terms of correctness, sufficiency, readability, and usability. We find that the tests generated by ChatGPT still suffer from correctness issues, including diverse compilation errors and execution failures (mostly caused by incorrect assertions); but the passing tests generated by ChatGPT almost resemble manually-written tests by achieving comparable coverage, readability, and even sometimes developers' preference. Our findings indicate that generating unit tests with ChatGPT could be very promising if the correctness of its generated tests could be further improved. Inspired by our findings above, we further propose ChatTester, a novel ChatGPT-based unit test generation approach, which leverages ChatGPT itself to improve the quality of its generated tests. ChatTester incorporates an initial test generator and an iterative test refiner. Our evaluation demonstrates the effectiveness of ChatTester by generating 34.3% more compilable tests and 18.7% more tests with correct assertions than the default ChatGPT. In addition to ChatGPT, we further investigate the generalization capabilities of ChatTester by applying it to two recent open-source LLMs (i.e., CodeLLama-Instruct and CodeFuse) and our results show that ChatTester can also improve the quality of tests generated by these LLMs.","2024-07","2025-11-25 22:29:28","2025-11-25 22:29:28","","","","FSE","1","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large language model; Test generation; Unit testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"29BBSLAB","conferencePaper","2025","Hu, Qiang; Xie, Xiaofei; Chen, Sen; Quan, Lili; Ma, Lei","Large Language Model Supply Chain: Open Problems From the Security Perspective","Proceedings of the 34th ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-1474-0","","10.1145/3713081.3731747","https://doi.org/10.1145/3713081.3731747","Large Language Model (LLM) is changing the software development paradigm and has gained huge attention from both academia and industry. Researchers and developers collaboratively explore how to leverage the powerful problem-solving ability of LLMs for specific domain tasks. Due to the wide usage of LLM-based applications, e.g., ChatGPT, multiple works have been proposed to ensure the security of LLM systems. A comprehensive understanding of the entire process of LLM system construction (the LLM supply chain) is crucial, but relevant works are limited. More importantly, the security issues hidden in the LLM SC that could greatly impact the reliable usage of LLMs lack exploration. Most existing works mainly focus on assuring the quality of LLM from the model level; however, security assurance for the entire LLM SC is ignored. In this work, we take the first step to discuss the potential security risks in each component and the integration between components of LLM SC. We summarize 12 security-related risks and provide promising guidance to help build safer LLM systems. We hope our work can facilitate the evolution of artificial general intelligence with secure LLM ecosystems.","2025","2025-11-25 22:29:28","2025-11-25 22:29:28","","169–173","","","","","","","ISSTA Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","large language model; security assessment; software supply chain","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MA2ZLEH2","bookSection","2025","Harman, Mark; Ritchey, Jillian; Harper, Inna; Sengupta, Shubho; Mao, Ke; Gulati, Abhishek; Foster, Christopher; Robert, Hervé","Mutation-Guided LLM-based Test Generation at Meta","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","","https://doi.org/10.1145/3696630.3728544","This paper1 describes Meta's Automated Compliance Hardening (ACH) system for mutation-guided LLM-based test generation. ACH generates relatively few mutants (aka simulated faults), compared to traditional mutation testing. Instead, it focuses on generating currently undetected faults that are specific to an issue of concern. From these currently uncaught faults, ACH generates tests that can catch them, thereby 'killing' the mutants and consequently hardening the platform against regressions. We use privacy concerns to illustrate our approach, but ACH can harden code against any type of regression. In total, ACH was applied to 10,795 Android Kotlin classes in 7 software platforms deployed by Meta, from which it generated 9,095 mutants and 571 privacy-hardening test cases. ACH also deploys an LLM-based equivalent mutant detection agent that achieves a precision of 0.79 and a recall of 0.47 (rising to 0.95 and 0.96 with simple pre-processing). ACH was used in Messenger and WhatsApp test-a-thons where engineers accepted 73% of its tests, judging 36% to privacy relevant. We conclude that ACH hardens code against specific concerns and that, even when its tests do not directly tackle the specific concern, engineers find them useful for their other benefits.","2025","2025-11-25 22:29:28","2025-11-25 22:29:28","","180–191","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3BHFWC3Q","conferencePaper","2025","Cheng, Xiang; Sang, Fan; Zhai, Yizhuo; Zhang, Xiaokuan; Kim, Taesoo","Rug: Turbo LLM for Rust Unit Test Generation","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00097","https://doi.org/10.1109/ICSE55347.2025.00097","Unit testing improves software quality by evaluating isolated sections of the program. This approach alleviates the need for comprehensive program-wide testing and confines the potential error scope within the software. However, unit test development is time-consuming, requiring developers to create appropriate test contexts and determine input values to cover different code regions. This problem is particularly pronounced in Rust due to its intricate type system, making traditional unit test generation tools ineffective in Rust projects. Recently, large language models (LLMs) have demonstrated their proficiency in understanding programming language and completing software engineering tasks. However, merely prompting LLMs with a basic prompt like ""generate unit test for the following source code"" often results in code with compilation errors. In addition, LLM-generated unit tests often have limited test coverage.To bridge this gap and harness the capabilities of LLM, we design and implement RUG, an end-to-end solution to automatically generate the unit test for Rust projects. To help LLM's generated test pass Rust strict compilation checks, Rug designs a semantic-aware bottom-up approach to divide the context construction problem into dependent sub-problems. It solves these sub-problems sequentially using an LLM and merges them to a complete context. To increase test coverage, Rug integrates coverage-guided fuzzing with LLM to prepare fuzzing harnesses. Applying Rug on 17 real-world Rust programs (average 24,937 LoC), we show that Rug can achieve a high code coverage, up to 71.37%, closely comparable to human effort (73.18%). We submitted 113 unit tests generated by Rug covering the new code: 53 of them have been accepted, 17 rejected, and 43 are pending for review.","2025","2025-11-25 22:29:28","2025-11-25 22:47:49","","2983–2995","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","large language model; Large language models; Test pattern generators; Software engineering; Software quality; Codes; Testing; Source coding; Scalability; Large language model; Unit testing; rust; unit testing; Fuzzing; Rust; Reviews","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A67I9HUX","journalArticle","2025","Martignano, M.; Damiani, A.; Nucciarelli, L.; Gui, D.; Magalini, S.","Software Verification and Generative AI Some Practical Examples and Considerations","Ada Lett.","","1094-3641","10.1145/3742939.3742946","https://doi.org/10.1145/3742939.3742946","Software verification, that is requirements baseline analysis, technical specification analysis, design analysis and code and testing analysis [1], is a crucial aspect of software development, ensuring that the products of each development phase satisfy the conditions imposed at the start of that phase [2]. Traditional software verification techniques often rely on manual effort, which can be time-consuming and error prone. However, with recent advancements in Generative Artificial Intelligence (AI) and Large Language Models (LLMs), there is a growing opportunity to automate and improve software verification activities. This paper describes how Generative AI, particularly LLMs, can facilitate software verification activities, including understanding of documentation, code analysis, bug detection and testing. Benefits are presented together with the associated challenges and limitations, especially the potential risk of exposing sensitive and proprietary information.","2025-06","2025-11-25 22:29:28","2025-11-25 22:29:28","","51–55","","2","44","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","llm; generative ai; slm; software; verification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5Z8ZL92A","bookSection","2025","Nan, Zifan; Guo, Zhaoqiang; Liu, Kui; Xia, Xin","Test Intention Guided LLM-Based Unit Test Generation","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00243","The emergence of Large Language Models (LLMs) has accelerated the progress of intelligent software engineering technologies, which brings promising possibilities for unit test generation. However, existing approaches for unit tests directly generated from Large Language Models (LLMs) often prove impractical due to their low coverage and insufficient mocking capabilities. This paper proposes IntUT, a novel approach that utilizes explicit test intentions (e.g., test inputs, mock behaviors, and expected results) to effectively guide the LLM to generate high-quality test cases. Our experimental results on three industry Java projects and live study demonstrate that prompting LLM with test intention can generate high-quality test cases for developers. Specifically, it achieves the improvements on branch coverage by 94% and line coverage by 49%. Finally, we obtain developers' feedback on using IntUT to generate cases for three new Java projects, achieving over 80% line coverage and 30% efficiency improvement on writing unit test cases.","2025","2025-11-25 22:29:28","2025-11-25 22:29:28","","1026–1038","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6ALPHNXC","journalArticle","2025","Li, Yue; Liu, Bohan; Zhang, Ting; Wang, Zhiqi; Lo, David; Yang, Lanxin; Lyu, Jun; Zhang, He","A Knowledge Enhanced Large Language Model for Bug Localization","Proc. ACM Softw. Eng.","","","10.1145/3729356","https://doi.org/10.1145/3729356","A significant number of bug reports are generated every day as software systems continue to develop. Large Language Models (LLMs) have been used to correlate bug reports with source code to locate bugs automatically. The existing research has shown that LLMs are effective for bug localization and can increase software development efficiency. However, these studies still have two limitations. First, these models fail to capture context information about bug reports and source code. Second, these models are unable to understand the domain-specific expertise inherent to particular projects, such as version information in projects that are composed of alphanumeric characters without any semantic meaning. To address these challenges, we propose a Knowledge Enhanced Pre-Trained model using project documents and historical code, called KEPT, for bug localization. Project documents record, revise, and restate project information that provides rich semantic information about those projects. Historical code contains rich code semantic information that can enhance the reasoning ability of LLMs. Specifically, we construct knowledge graphs from project documents and source code. Then, we introduce knowledge graphs to the LLM through soft-position embedding and visible matrices, enhancing its contextual and professional reasoning ability. To validate our model, we conducted a series of experiments on seven open-source software projects with over 6,000 bug reports. Compared with the traditional model (Locus), KEPT performs better by 33.2% to 59.5% in terms of mean reciprocal rank, mean average precision, and Top@N. Compared with the best-performing non-commercial LLM (CodeT5), KEPT achieves an improvement of 36.6% to 63.7%. Compared to the state-of-the-art commercial LLM developed by OpenAI, called text-embedding-ada-002, KEPT achieves an average improvement of 7.8% to 17.4%. The results indicate that introducing knowledge graphs contributes to enhance the effectiveness of the LLM in bug localization.","2025-06","2025-11-25 22:29:28","2025-11-25 22:29:28","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","large language model; bug localization; information retrieval; knowledge enhancement","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3WMU4EJT","journalArticle","2025","He, Ziyao; Huq, Syed Fatiul; Malek, Sam","Enhancing Web Accessibility: Automated Detection of Issues with Generative AI","Proc. ACM Softw. Eng.","","","10.1145/3729371","https://doi.org/10.1145/3729371","Websites are integral to people’s daily lives, with billions in use today. However, due to limited awareness of accessibility and its guidelines, developers often release web apps that are inaccessible to people with disabilities, who make up around 16% of the global population. To ensure a baseline of accessibility, software engineers rely on automated checkers that assess a webpage’s compliance based on predefined rules. Unfortunately, these tools typically cover only a small subset of accessibility guidelines and often overlook violations that require a semantic understanding of the webpage. The advent of generative AI, known for its ability to comprehend textual and visual content, has created new possibilities for detecting accessibility violations. We began by studying the most widely used guideline, WCAG, to determine the testable success criteria that generative AI could address. This led to the development of an automated tool called GenA11y, which extracts elements from a page related to each success criterion and inputs them into an LLM prompted to detect accessibility issues on the web. Evaluations of GenA11y showed its effectiveness, with a precision of 94.5% and a recall of 87.61%. Additionally, when tested on real websites, GenA11y identified an average of eight more types of accessibility violations than the combination of existing tools.","2025-06","2025-11-25 22:29:28","2025-11-25 22:29:28","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","LLM; Generative AI; Accessibility; WCAG","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EZXAU6WR","conferencePaper","2024","Xue, Zhiyi; Li, Liangguo; Tian, Senyue; Chen, Xiaohong; Li, Pingping; Chen, Liangyu; Jiang, Tingting; Zhang, Min","LLM4Fin: Fully Automating LLM-Powered Test Case Generation for FinTech Software Acceptance Testing","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680388","https://doi.org/10.1145/3650212.3680388","FinTech software, crucial for both safety and timely market deployment, presents a compelling case for automated acceptance testing against regulatory business rules. However, the inherent challenges of comprehending unstructured natural language descriptions of these rules and crafting comprehensive test cases demand human intelligence. The emergence of Large Language Models (LLMs) holds promise for automated test case generation, leveraging their natural language processing capabilities. Yet, their dependence on human intervention for effective prompting hampers efficiency. In response, we introduce a groundbreaking, fully automated approach for generating high-coverage test cases from natural language business rules. Our methodology seamlessly integrates the versatility of LLMs with the predictability of algorithmic methods. We fine-tune pre-trained LLMs for improved information extraction accuracy and algorithmically generate comprehensive testable scenarios for the extracted business rules. Our prototype, LLM4Fin, is designed for testing real-world stock-trading software. Experimental results demonstrate LLM4Fin’s superiority over both state-of-the-art LLM, such as ChatGPT, and skilled testing engineers. We achieve remarkable performance, with up to 98.18% and an average of 20%−110% improvement on business scenario coverage, and up to 93.72% on code coverage, while reducing the time cost from 20 minutes to a mere 7 seconds. These results provide robust evidence of the framework’s practical applicability and efficiency, marking a significant advancement in FinTech software testing.","2024","2025-11-25 22:29:28","2025-11-25 22:29:28","","1643–1655","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","large language model; test case generation; fintech software; Software acceptance testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L66LI4L7","journalArticle","2025","Zhang, Yuwei; Lu, Qingyuan; Liu, Kai; Dou, Wensheng; Zhu, Jiaxin; Qian, Li; Zhang, Chunxi; Lin, Zheng; Wei, Jun","CITYWALK: Enhancing LLM-Based C++ Unit Test Generation via Project-Dependency Awareness and Language-Specific Knowledge","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3763791","https://doi.org/10.1145/3763791","Unit testing plays a pivotal role in the software development lifecycle, as it ensures code quality. However, writing high-quality unit tests remains a time-consuming task for developers in practice. More recently, the application of large language models (LLMs) in automated unit test generation has demonstrated promising results. Existing approaches primarily focus on interpreted programming languages (e.g., Java), while mature solutions tailored to compiled programming languages like C++ are yet to be explored. The intricate language features of C++, such as pointers, templates, and virtual functions, pose particular challenges for LLMs in generating both executable and high-coverage unit tests. To tackle the aforementioned problems, this paper introduces CITYWALK, a novel LLM-based framework for C++ unit test generation. CITYWALK enhances LLMs by providing a comprehensive understanding of the dependency relationships within the project under test via program analysis. Furthermore, CITYWALK incorporates language-specific knowledge about C++ derived from project documentation and empirical observations, significantly improving the correctness of the LLM-generated unit tests. We implement CITYWALK by employing the widely popular LLM GPT-4o. The experimental results show that CITYWALK outperforms current state-of-the-art approaches on a collection of ten popular C++ projects. Our findings demonstrate the effectiveness of CITYWALK in generating high-quality C++ unit tests.","2025-08","2025-11-25 22:29:28","2025-11-25 22:29:28","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Unit Test Generation; Language-Specific Knowledge; Program Dependence Analysis; Retrieval-Augmented Generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WXAGX79K","conferencePaper","2025","Takayanagi, Takehiro; Izumi, Kiyoshi; Sanz-Cruzado, Javier; McCreadie, Richard; Ounis, Iadh","Are Generative AI Agents Effective Personalized Financial Advisors?","Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval","979-8-4007-1592-1","","10.1145/3726302.3729897","https://doi.org/10.1145/3726302.3729897","Large language model-based agents are becoming increasingly popular as a low-cost mechanism to provide personalized, conversational advice, and have demonstrated impressive capabilities in relatively simple scenarios, such as movie recommendations. But how do these agents perform in complex high-stakes domains, where domain expertise is essential and mistakes carry substantial risk? This paper investigates the effectiveness of LLM-advisors in the finance domain, focusing on three distinct challenges: (1) eliciting user preferences when users themselves may be unsure of their needs (2) providing personalized guidance for diverse investment preferences, and (3) leveraging advisor personality to build relationships and foster trust. Via a lab-based user study with 64 participants, we show that LLM-advisors often match human advisor performance when eliciting preferences, although they can struggle to resolve conflicting user needs. When providing personalized advice, the LLM was able to positively influence user behavior, but demonstrated clear failure modes. Our results show that accurate preference elicitation is key, otherwise, the LLM-advisor has little impact, or can even direct the investor toward unsuitable assets. More worryingly, users appear insensitive to the quality of advice being given, or worse these can have an inverse relationship. Indeed, users reported a preference for and increased satisfaction as well as emotional trust with LLMs adopting an extroverted persona, even though those agents provided worse advice.","2025","2025-11-25 22:29:28","2025-11-25 22:29:28","","286–295","","","","","","","SIGIR '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Padua, Italy","","","","large language models; generative ai; financial advisor; user study","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TCPLE5LT","journalArticle","2025","Altmayer Pizzorno, Juan; Berger, Emery D.","CoverUp: Effective High Coverage Test Generation for Python","Proc. ACM Softw. Eng.","","","10.1145/3729398","https://doi.org/10.1145/3729398","Testing is an essential part of software development. Test generation tools attempt to automate the otherwise labor-intensive task of test creation, but generating high-coverage tests remains challenging. This paper proposes CoverUp, a novel approach to driving the generation of high-coverage Python regression tests. CoverUp combines coverage analysis, code context, and feedback in prompts that iteratively guide the LLM to generate tests that improve line and branch coverage. We evaluate our prototype CoverUp implementation across a benchmark of challenging code derived from open-source Python projects and show that CoverUp substantially improves on the state of the art. Compared to CodaMosa, a hybrid search/LLM-based test generator, CoverUp achieves a per-module median line+branch coverage of 80% (vs. 47%). Compared to MuTAP, a mutation- and LLM-based test generator, CoverUp achieves an overall line+branch coverage of 89% (vs. 77%). We also demonstrate that CoverUp’s performance stems not only from the LLM used but from the combined effectiveness of its components.","2025-06","2025-11-25 22:29:28","2025-11-25 22:29:28","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large language models; Artificial intelligence; Test generation; Code coverage; Regression testing; Software testing and debugging","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5NQ3DSPD","conferencePaper","2024","Yu, Xiao; Liu, Lei; Hu, Xing; Keung, Jacky; Xia, Xin; Lo, David","Practitioners’ Expectations on Automated Test Generation","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680386","https://doi.org/10.1145/3650212.3680386","Automated test generation can help developers craft high-quality software tests while mitigating the manual effort needed for writing test code. Despite significant research efforts in automated test generation for nearly 50 years, there is a lack of clarity about what practitioners expect from automated test generation tools and whether the existing research meets their needs. To address this issue, we follow a mixed-methods approach to gain insights into practitioners' expectations of automated test generation. We first conduct the qualitative analysis from semi-structured interviews with 13 professionals, followed by a quantitative survey of 339 practitioners from 46 countries across five continents. We then conduct a literature review of premier venue papers from 2022 to 2024 (in the last three years) and compare current research findings with practitioners' expectations. From this comparison, we outline future research directions for researchers to bridge the gap between automated test generation research and practitioners' expectations.","2024","2025-11-25 22:29:28","2025-11-25 22:29:28","","1618–1630","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Test Generation; Empirical Study; Practitioners' Expectations","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BK5LENWG","conferencePaper","2025","Borowczak, Mike; Borowczak, Andrea C","Integrating Generative AI into Microelectronics Education: Implications for Learning and Pedagogical Practice","Proceedings of the Great Lakes Symposium on VLSI 2025","979-8-4007-1496-2","","10.1145/3716368.3735255","https://doi.org/10.1145/3716368.3735255","Generative Artificial Intelligent (AI) agents are now common, easily accessible, and prevalent in consumer and industrial environments. This work explores the integration of AI tool in microelectronics engineering education, focusing on the results and adaptions made in a course based on an informal survey of graduate students in a semester long VLSI Electronic Design Automation seminar. Exploration of 23 anonymous student responses indicate high student familiarity with general generative AI but lower exposure to task specific tools, and lowest exposure to domain-specific EDA AI tools. Students report using AI frequently for coding assistance and understanding complex concepts, while exercising caution by verifying AI outputs. AI is perceived as beneficial and most students do not consider themselves heavily reliant on the tools for their own success. Open-ended responses highlight enthusiasm about AI’s efficiency benefits though tempered with concerns about accuracy. These course-specific insights informed course design which includes: incorporating AI literacy into VLSI/EDA curricula, designing AI-centric assignments, and updating assessment strategies to promote critical thinking and human-to-human interactions, collaboration and communication. We offer several recommendations for microelectronics educators to adapt pedagogy in alignment with evolving industry practices and student needs.","2025","2025-11-25 22:29:28","2025-11-25 22:29:28","","508–515","","","","","","","GLSVLSI '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Generative AI; AI Integration; AI Literacy; Engineering Education Pedagogy; Microelectronics Education; Student Perceptions","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"24D6RS6F","journalArticle","2025","Wang, Fanyu; Arora, Chetan; Tantithamthavorn, Chakkrit; Huang, Kaicheng; Aleti, Aldeida","Requirements-Driven Automated Software Testing: A Systematic Review","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3767739","https://doi.org/10.1145/3767739","Automated software testing has significant potential to enhance efficiency and reliability within software development processes. However, its broader adoption faces considerable challenges, particularly concerning alignment between test generation methodologies and software requirements. REquirements-Driven Automated Software Testing (REDAST) addresses this gap by systematically leveraging requirements as the foundation for automated test artifact generation. This systematic literature review (SLR) critically examines the REDAST landscape, analyzing the current state of requirements input formats, transformation techniques, generated test artifacts, evaluation methods, and prevailing limitations. We conducted a thorough analysis of 156 relevant studies selected through a rigorous multi-stage filtering process from an initial collection of 27,333 papers sourced from six major research databases. Our findings highlight the predominance of functional requirements, model-based specifications, and natural language formats. Rule-based techniques are extensively utilized, while machine learning-based approaches remain relatively underexplored. Furthermore, most existing frameworks are sequential and dependent on singular intermediate representations, and while test cases, structured textual formats, and requirements coverage are common, full automation remains rare. We identify significant gaps related to automation completeness, and dependency on input quality. This comprehensive synthesis provides a detailed overview of REDAST research and limitations, offering clear, evidence-based recommendations to guide future advancements in automated software testing.","2025-09","2025-11-25 22:29:28","2025-11-25 22:29:28","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Software Testing; Automated Test Generation; Requirements Engineering; Software Engineering; Systematic Literature Review","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PP2DGXCW","conferencePaper","2024","Wang, Zejun; Liu, Kaibo; Li, Ge; Jin, Zhi","HITS: High-coverage LLM-based Unit Test Generation via Method Slicing","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695501","https://doi.org/10.1145/3691620.3695501","Large language models (LLMs) have behaved well in generating unit tests for Java projects. However, the performance for covering the complex focal methods within the projects is poor. Complex methods comprise many conditions and loops, requiring the test cases to be various enough to cover all lines and branches. However, existing test generation methods with LLMs provide the whole method-to-test to the LLM without assistance on input analysis. The LLM has difficulty inferring the test inputs to cover all conditions, resulting in missing lines and branches. To tackle the problem, we propose decomposing the focal methods into slices and asking the LLM to generate test cases slice by slice. Our method simplifies the analysis scope, making it easier for the LLM to cover more lines and branches in each slice. We build a dataset comprising complex focal methods collected from the projects used by existing state-of-the-art approaches. Our experiment results show that our method significantly outperforms current test case generation methods with LLMs and the typical SBST method Evosuite regarding both line and branch coverage scores.","2024","2025-11-25 22:29:28","2025-11-25 22:47:25","","1258–1268","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","Analytical models; large language model; Large language models; Software testing; Test pattern generators; Java; Software; Software engineering; Natural language processing; Large Language Model; Unit Test Generation; AI for SE; Debugging; Program Decomposition; Program Slicing; Testing and Analysis; unit test generation; program decomposition; program slicing; testing and analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8GDYCZKY","conferencePaper","2024","Rajbhoj, Asha; Somase, Akanksha; Kulkarni, Piyush; Kulkarni, Vinay","Accelerating Software Development Using Generative AI: ChatGPT Case Study","Proceedings of the 17th Innovations in Software Engineering Conference","979-8-4007-1767-3","","10.1145/3641399.3641403","https://doi.org/10.1145/3641399.3641403","The Software Development Life Cycle (SDLC) comprises multiple phases, each requiring Subject Matter Experts (SMEs) with phase-specific skills. The efficacy and quality of deliverables of each phase are skill dependent. In recent times, Generative AI techniques, including Large-scale Language Models (LLMs) like GPT, have become significant players in software engineering. These models, trained on extensive text data, can offer valuable contributions to software development. Interacting with LLMs involves feeding prompts with the context information and guiding the generation of textual responses. The quality of the response is dependent on the quality of the prompt given. This paper proposes a systematic prompting approach based on meta-model concepts for SDLC phases. The approach is validated using ChatGPT for small but complex business application development. We share the approach and our experience, learnings, benefits obtained, and the challenges encountered while applying the approach using ChatGPT. Our experience indicates that Generative AI techniques, such as ChatGPT, have the potential to reduce the skills barrier and accelerate software development substantially.","2024","2025-11-25 22:29:28","2025-11-25 22:29:28","","","","","","","","","ISEC '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Bangalore, India","","","","ChatGPT; Generative AI; Large Language Models; AI in SDLC; Automated Software Development; SDLC automation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DCN8QPWW","journalArticle","2025","Kong, Jiaolong; Xie, Xiaofei; Cheng, Mingfei; Liu, Shangqing; Du, Xiaoning; Guo, Qi","ContrastRepair: Enhancing Conversation-Based Automated Program Repair via Contrastive Test Case Pairs","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3719345","https://doi.org/10.1145/3719345","Automated Program Repair (APR) aims to automatically generate patches for rectifying software bugs. Recent strides in Large Language Models (LLM), such as ChatGPT, have yielded encouraging outcomes in APR, especially within the conversation-driven APR framework. Nevertheless, the efficacy of conversation-driven APR is contingent on the quality of the feedback information. In this article, we propose ContrastRepair, a novel conversation-based APR approach that augments conversation-driven APR by providing LLMs with contrastive test pairs. A test pair consists of a failing test and a passing test, which offer contrastive feedback to the LLM. Our key insight is to minimize the difference between the generated passing test and the given failing test, which can better isolate the root causes of bugs. By providing such informative feedback, ContrastRepair enables the LLM to produce effective bug fixes. The implementation of ContrastRepair is based on the state-of-the-art LLM, ChatGPT, and it iteratively interacts with ChatGPT until plausible patches are generated. We evaluate ContrastRepair on multiple benchmark datasets, including Defects4J, QuixBugs, and HumanEval-Java. The results demonstrate that ContrastRepair significantly outperforms existing methods, achieving a new state-of-the-art in program repair. For instance, among Defects4J 1.2 and 2.0, ContrastRepair correctly repairs 143 out of all 337 bug cases, while the best-performing baseline fixes 124 bugs.","2025-10","2025-11-25 22:29:28","2025-11-25 22:29:28","","","","8","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Program Repair","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R3W2F8QC","conferencePaper","2025","Wei, Yunze; Chi, Kaiwen; Du, Shibo; Xie, Xiaohui; Geng, Ziyu; Han, Yuwei; Li, Zhen; Li, Zhanyou; Cui, Yong","Large Language Model Driven Automated Network Protocol Testing","Proceedings of the 2025 Applied Networking Research Workshop","979-8-4007-2009-3","","10.1145/3744200.3744763","https://doi.org/10.1145/3744200.3744763","Traditional network protocol testing methods face significant challenges in adapting to rapid protocol evolution. The challenges stem primarily from protocol specification analysis and customized code development for testing. To address this, we propose NeTestLLM, a Large Language Model (LLM)-powered framework that automates protocol testing through two key components: (1) a hybrid test case generator that extracts protocol specifications and produces high-coverage test cases, and (2) a retrieval-feedback-enhanced engine that translates natural language descriptions into executable code. Preliminary evaluations demonstrate that NeTestLLM achieves 94.1% coverage on protocol specification understanding. A case study with commercial network equipment validates the practical effectiveness of our approach. Our work presents the first LLM-powered framework for automated network protocol testing to keep pace with the rapid evolution of network protocols and standards.","2025","2025-11-25 22:29:28","2025-11-25 22:29:28","","32–38","","","","","","","ANRW '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Madrid, Spain","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E7N53G3K","conferencePaper","2025","Khalid, Shawal; Brown, Chris","Evaluating Capabilities and Perspectives of Generative AI Tools in Smart Contract Development","Proceedings of the 7th ACM International Symposium on Blockchain and Secure Critical Infrastructure","979-8-4007-1412-2","","10.1145/3709016.3737802","https://doi.org/10.1145/3709016.3737802","Smart contracts, self-executing agreements on the blockchain, have emerged as a transformative force in blockchain technology, automating agreements and enabling decentralized applications. However, the stakes are extraordinarily high—manual coding errors in smart contracts have repeatedly led to financial losses. Notable incidents, such as the DAO hack that resulted in a loss of approximately 50 million [36] and the Parity wallet vulnerability that froze approximately 280 million in assets [45], underscore the immense economic risks involved. To support manual development tasks, recent advancements in artificial intelligence (AI) powered by large language models (LLMs) have transformed how software is developed and maintained by automating various software engineering tasks.This research explores the capabilities of generative AI tools for efficient and secure smart contract development. The methodology involves two phases: 1) we distribute a mixed methods survey for blockchain and smart contract developers (n = 114) to investigate their perspectives towards utilizing LLMs; and 2) we evaluate the effectiveness of generative AI tools, such as ChatGPT, Google Gemini, and ChainGPT, for smart contract development. This evaluation is based on comparing the LLM-generated smart contract code with human-written code, using a diverse dataset of smart contracts gathered from GitHub. Static analysis tools and unit testing are employed to validate the accuracy, correctness, efficiency, and security of the generated code. Our findings highlight the potential of these tools to accelerate smart contract development processes, while also emphasizing the need for human oversight, contributing to the advancement of blockchain technology and its applications.","2025","2025-11-25 22:29:28","2025-11-25 22:29:28","","","","","","","","","BSCI '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Meliá Hanoi, Hanoi, Vietnam","","","","ChatGPT; Gemini; generative AI; large language models; artificial intelligence; smart contracts; blockchain; ChainGPT; software engineering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KSFWZLKB","conferencePaper","2025","Go, Gwihwan; Zhou, Chijin; Zhang, Quan; Jiang, Yu; Wei, Zhao","LSPAI: An IDE Plugin for LLM-Powered Multi-Language Unit Test Generation with Language Server Protocol","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","10.1145/3696630.3728540","https://doi.org/10.1145/3696630.3728540","Unit testing is crucial to ensure the validity of the code, and research has been conducted to advance this domain. However, existing studies fail to address industry requirements, support for multi-language static analysis and real-time unit test generation. While integrating static analysis with a Large Language Model (LLM) could address these challenges, it typically requires effort to implement across programming languages. To address this, we propose LspAi, an automated unit test generation tool that leverages language analysis tools and integrates them into a unified development environment via the Language Server Protocol. This approach equips LLM with multi-language static analysis capabilities, allowing a single tool to support unit test generation across multiple languages. We evaluated our method by comparing line coverage across different LLMs and programming languages, demonstrating superior performance and broad applicability. In projects, LspAi achieved line coverage improvements of 145% for Java, 931% for Golang, and 95.62% for Python compared to Copilot. In addition, we also share our lessons learned from applying the tool in Tencent Ltd.","2025","2025-11-25 22:29:28","2025-11-25 22:29:28","","144–149","","","","","","","FSE Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","large language model; unit testing; language server protocol","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PRT5LNIR","conferencePaper","2023","Guilherme, Vitor; Vincenzi, Auri","An initial investigation of ChatGPT unit test generation capability","Proceedings of the 8th Brazilian Symposium on Systematic and Automated Software Testing","979-8-4007-1629-4","","10.1145/3624032.3624035","https://doi.org/10.1145/3624032.3624035","Context: Software testing ensures software quality, but developers often disregard it. The use of automated testing generation is pursued to reduce the consequences of overlooked test cases in a software project. Problem: In the context of Java programs, several tools can completely automate generating unit test sets. Additionally, studies are conducted to offer evidence regarding the quality of the generated test sets. However, it is worth noting that these tools rely on machine learning and other AI algorithms rather than incorporating the latest advancements in Large Language Models (LLMs). Solution: This work aims to evaluate the quality of Java unit tests generated by an OpenAI LLM algorithm, using metrics like code coverage and mutation test score. Method: For this study, 33 programs used by other researchers in the field of automated test generation were selected. This approach was employed to establish a baseline for comparison purposes. For each program, 33 unit test sets were generated automatically, without human interference, by changing Open AI API parameters. After executing each test set, metrics such as code line coverage, mutation score, and success rate of test execution were collected to evaluate the efficiency and effectiveness of each set. Summary of Results: Our findings revealed that the OpenAI LLM test set demonstrated similar performance across all evaluated aspects compared to traditional automated Java test generation tools used in the previous research. These results are particularly remarkable considering the simplicity of the experiment and the fact that the generated test code did not undergo human analysis.","2023","2025-11-25 22:29:28","2025-11-25 22:29:28","","15–24","","","","","","","SAST '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Campo Grande, MS, Brazil","","","","software testing; automated test generation; coverage testing; experimental software engineering; mutation testing; testing tools","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I6NRVPFL","journalArticle","2025","Wang, Qing; Wang, Junjie; Li, Mingyang; Wang, Yawen; Liu, Zhe","A Roadmap for Software Testing in Open-Collaborative and AI-Powered Era","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3709355","https://doi.org/10.1145/3709355","Internet technology has given rise to an open-collaborative software development paradigm, necessitating the open-collaborative schema to software testing. It enables diverse and globally distributed contributions, but also presents significant challenges to efficient testing processes, coordination among personnel, and management of testing artifacts. At the same time, advancements in AI have enhanced testing capabilities and enabling automation, while also introducing new testing needs and unique challenges for AI-based systems. In this context, this article explores software testing in the open-collaborative and AI-powered era, focusing on the interrelated dimensions of process, personnel, and technology. Among them, process involves managing testing workflows and artifacts to improve efficiency, personnel emphasizes the role of individuals in ensuring testing quality through collaboration and contributions, while technology refers to AI methods that enhance testing capabilities and address challenges in AI-based systems. Furthermore, we delve into the challenges and opportunities arising from emerging technologies such as Large Language Models (LLMs) and the AI model-centric development paradigm.","2025-05","2025-11-25 22:29:28","2025-11-25 22:29:28","","","","5","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","LLM; Large Language Model; Software Testing; AI; Artificial Intelligence; Open Collaborative; Open Source","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NMJJ3QEW","conferencePaper","2024","Zheng, Yusheng; Yang, Yiwei; Chen, Maolin; Quinn, Andrew","Kgent: Kernel Extensions Large Language Model Agent","Proceedings of the ACM SIGCOMM 2024 Workshop on EBPF and Kernel Extensions","979-8-4007-0712-4","","10.1145/3672197.3673434","https://doi.org/10.1145/3672197.3673434","The extended Berkeley Packet Filters (eBPF) ecosystem allows for the extension of Linux and Windows kernels, but writing eBPF programs is challenging due to the required knowledge of OS internals and programming limitations enforced by the eBPF verifier. These limitations ensure that only expert kernel developers can extend their kernels, making it difficult for junior sys admins, patch makers, and DevOps personnel to maintain extensions. This paper presents Kgent, an alternative framework that alleviates the difficulty of writing an eBPF program by allowing Kernel Extensions to be written in Natural language. Kgent uses recent advances in large language models (LLMs) to synthesize an eBPF program given a user's English language prompt. To ensure that LLM's output is semantically equivalent to the user's prompt, Kgent employs a combination of LLM-empowered program comprehension, symbolic execution, and a series of feedback loops. Kgent's key novelty is the combination of these techniques. In particular, the system uses symbolic execution in a novel structure that allows it to combine the results of program synthesis and program comprehension and build on the recent success that LLMs have shown for each of these tasks individually.To evaluate Kgent, we develop a new corpus of natural language prompts for eBPF programs. We show that Kgent produces correct eBPF programs on 80%—which is an improvement of a factor of 2.67 compared to GPT-4 program synthesis baseline. Moreover, we find that Kgent very rarely synthesizes ""false positive"" eBPF programs— i.e., eBPF programs that Kgent verifies as correct but manual inspection reveals to be semantically incorrect for the input prompt. The code for Kgent is publicly accessible at https://github.com/eunomia-bpf/KEN.","2024","2025-11-25 22:29:28","2025-11-25 22:29:28","","30–36","","","","","","","eBPF '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sydney, NSW, Australia","","","","Large Language Model; eBPF; Symbolic Execution","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6MBNH669","conferencePaper","2024","Shin, Jiho; Hashtroudi, Sepehr; Hemmati, Hadi; Wang, Song","Domain Adaptation for Code Model-Based Unit Test Case Generation","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680354","https://doi.org/10.1145/3650212.3680354","Recently, deep learning-based test case generation approaches have been proposed to automate the generation of unit test cases. In this study, we leverage Transformer-based code models to generate unit tests with the help of Domain Adaptation (DA) at a project level. Specifically, we use CodeT5, a relatively small language model trained on source code data, and fine-tune it on the test generation task. Then, we apply domain adaptation to each target project data to learn project-specific knowledge (project-level DA). We use the Methods2test dataset to fine-tune CodeT5 for the test generation task and the Defects4j dataset for project-level domain adaptation and evaluation. We compare our approach with (a) CodeT5 fine-tuned on the test generation without DA, (b) the A3Test tool, and (c) GPT-4 on five projects from the Defects4j dataset. The results show that tests generated using DA can increase the line coverage by 18.62%, 19.88%, and 18.02% and mutation score by 16.45%, 16.01%, and 12.99% compared to the above (a), (b), and (c) baselines, respectively. The overall results show consistent improvements in metrics such as parse rate, compile rate, BLEU, and CodeBLEU. In addition, we show that our approach can be seen as a complementary solution alongside existing search-based test generation tools such as EvoSuite, to increase the overall coverage and mutation scores with an average of 34.42% and 6.8%, for line coverage and mutation score, respectively.","2024","2025-11-25 22:29:28","2025-11-25 22:29:28","","1211–1222","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","LLM; Test generation; Code Model; Domain Adaption; GPT; Transformers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NZDZ7J9K","conferencePaper","2025","Pophale, Swaroop; Elwasif, Wael; Bernholdt, David E.","Using a Large Language Model as a Building Block to Generate UsableValidation and Verification Suite for OpenMP","Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region","979-8-4007-1335-4","","10.1145/3712031.3712331","https://doi.org/10.1145/3712031.3712331","In the HPC area, both hardware and software move quickly. Often new hardware is developed and deployed, the corresponding software stack, including compilers and other tools, are under active development while leading edge software developers are working to port and tune their applications, all at the same time. While the software ecosystem is in flux, one of the key challenges for users is obtaining insight into the state of implementation of key features in the programming languages and models their applications are using – whether they have been implemented, and whether the implementation conforms to the specification, especially for newly implemented features (less tested by widespread use). OpenMP is one of the most prominent shared memory programming models used for on-node programming in HPC. With the shift towards accelerators (such as GPUs and FPGAs) and heterogeneous programming OpenMP features are getting more complex. It is natural to ask whether generative AI approaches, and large language models (LLMs) in particular, can help in producing validation and verification test suites to allow users better and faster insights into the availability and correctness of OpenMP features of interest. In this work, we explore the use of ChatGPT-4 to generate a suite of tests for OpenMP features. We have chosen a set of directives and clauses, a total of 78 combinations, which first appeared in OpenMP 3.0 (released in May 2008) but are also relevant for accelerators. We prompted ChatGPT to generate tests in the C and Fortran languages, for both host (CPU) and device (accelerator). On the Summit super-computer using the GNU implementation, we found that, of the 78 generated tests 67 C tests and 43 Fortran tests compiled successfully and fewer than those executed to completion. On further analysis we show that not all generated tests are valid. We document the process, results, and provide detailed analysis regarding the quality of tests generated. With the aim of providing input to a production quality validation and verification suite, we manually implement the corrections required to make the tests valid according to the current OpenMP specification. We quantify this effort as small, medium, or large, and record the lines of code changed to correct the invalid tests. With the corrected tests we validate recent implementations from HPE, AMD, and GNU on the Frontier supercomputer. Our experiment and subsequent analysis show that although LLMs are capable of producing HPC specific codes, they are limited by their understanding of the deeper semantics and restrictions of programming models such as OpenMP. Unsurprisingly more commonly used features have better support, while some OpenMP 3.0 directives such as sections and tasking are not universally supported on accelerators. We demonstrate that successful compilation and execution to completion are inadequate metrics for evaluating generated code and that, at this time, commodity LLMs require expert intervention for code verification. This points to gaps in the training data that is currently available for HPC. We demonstrate that with ""small"" effort 37% of generated invalid C tests and 63% of generated invalid Fortran tests could be corrected. This improves productivity of test generation as we circumvent writing from scratch and the common programming errors associated with it.","2025","2025-11-25 22:29:28","2025-11-25 22:29:28","","131–141","","","","","","","HPCASIA '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","large language model; generative AI; OpenMP; testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"97SNLIZX","conferencePaper","2024","Chu, Zhumin; Ai, Qingyao; Tu, Yiteng; Li, Haitao; Liu, Yiqun","Automatic Large Language Model Evaluation via Peer Review","Proceedings of the 33rd ACM International Conference on Information and Knowledge Management","979-8-4007-0436-9","","10.1145/3627673.3679677","https://doi.org/10.1145/3627673.3679677","The impressive performance of large language models (LLMs) has attracted considerable attention from the academic and industrial communities. Besides how to construct and train LLMs, how to effectively evaluate and compare the capacity of LLMs has also been well recognized as an important yet difficult problem. Existing paradigms rely on either human annotators or model-based evaluators to evaluate the performance of LLMs on different tasks. However, these paradigms often suffer from high cost, low generalizability, and inherited biases in practice, which make them incapable of supporting the sustainable development of LLMs in the long term. In order to address these issues, inspired by the peer review systems widely used in the academic publication process, we propose a novel framework that can automatically evaluate LLMs through a peer-review process. Specifically, for the evaluation of a specific task, we first construct a small qualification exam to select ""reviewers” from a couple of powerful LLMs. Then, to actually evaluate the ""submissions"" written by different candidate LLMs, i.e., the evaluatees, we use the reviewer LLMs to rate or compare the submissions. The final ranking of evaluatee LLMs is generated based on the results provided by all reviewers. We conducted extensive experiments on both text summarization and non-factoid question-answering tasks with eleven LLMs including GPT-4. The results demonstrate the existence of biasness when evaluating using a single LLM. Also, our PRE model outperforms all the baselines, illustrating the effectiveness of the peer review mechanism.","2024","2025-11-25 22:29:29","2025-11-25 22:29:29","","384–393","","","","","","","CIKM '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Boise, ID, USA","","","","large language model; automatic evaluation; peer review","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LJU3L75K","conferencePaper","2024","Zhang, Cen; Zheng, Yaowen; Bai, Mingqiang; Li, Yeting; Ma, Wei; Xie, Xiaofei; Li, Yuekang; Sun, Limin; Liu, Yang","How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680355","https://doi.org/10.1145/3650212.3680355","Fuzz drivers are essential for library API fuzzing. However, automatically generating fuzz drivers is a complex task, as it demands the creation of high-quality, correct, and robust API usage code. An LLM-based (Large Language Model) approach for generating fuzz drivers is a promising area of research. Unlike traditional program analysis-based generators, this text-based approach is more generalized and capable of harnessing a variety of API usage information, resulting in code that is friendly for human readers. However, there is still a lack of understanding regarding the fundamental issues on this direction, such as its effectiveness and potential challenges. To bridge this gap, we conducted the first in-depth study targeting the important issues of using LLMs to generate effective fuzz drivers. Our study features a curated dataset with 86 fuzz driver generation questions from 30 widely-used C projects. Six prompting strategies are designed and tested across five state-of-the-art LLMs with five different temperature settings. In total, our study evaluated 736,430 generated fuzz drivers, with 0.85 billion token costs ($8,000+ charged tokens). Additionally, we compared the LLM-generated drivers against those utilized in industry, conducting extensive fuzzing experiments (3.75 CPU-year). Our study uncovered that: 1) While LLM-based fuzz driver generation is a promising direction, it still encounters several obstacles towards practical applications; 2) LLMs face difficulties in generating effective fuzz drivers for APIs with intricate specifics. Three featured design choices of prompt strategies can be beneficial: issuing repeat queries, querying with examples, and employing an iterative querying process; 3) While LLM-generated drivers can yield fuzzing outcomes that are on par with those used in the industry, there are substantial opportunities for enhancement, such as extending contained API usage, or integrating semantic oracles to facilitate logical bug detection. Our insights have been implemented to improve the OSS-Fuzz-Gen project, facilitating practical fuzz driver generation in industry.","2024","2025-11-25 22:29:29","2025-11-25 22:29:29","","1223–1235","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large Language Model; Fuzz Driver Generation; Fuzz Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A6U9DNXA","conferencePaper","2025","Zhu, Kelin; Wang, Yu; Wang, Linzhang; Li, Xuandong","Emerging Compiler Testing Based on Test Case Reuse","Proceedings of the 16th International Conference on Internetware","979-8-4007-1926-4","","10.1145/3755881.3755917","https://doi.org/10.1145/3755881.3755917","With the rapid development of computer technology, emerging programming languages and compilers are constantly being introduced. However, these new compilers often have defects due to their short development time, insufficient testing, and the challenges they face, which affect their reliability and adoption. Traditional testing methods are limited in addressing the challenges of testing these new compilers. This paper proposes a new method for testing emerging compilers based on test case reuse, utilizing large language models to convert test cases from one programming language to another. Using C++ and Carbon language as examples, historical test cases from C++ are converted to Carbon language versions to expand the test case library for the Carbon compiler. This method involves fine-tuning a large language model to transform C++ historical bugs codes into codes suitable for the Carbon compiler, followed by verification to ensure their effectiveness. By converting and reusing test cases and implementing feature conversions to generate mutations, we improve 6.63% test coverage for Carbon compiler and discover 8 bugs.","2025","2025-11-25 22:29:29","2025-11-25 22:29:29","","130–141","","","","","","","Internetware '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","LLM; Mutation Testing; Compiler; Fuzzing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2665WXJ5","bookSection","2025","Freeman, Laura; Robert, John; Wojton, Heather","The Impact of Generative AI on Test &amp; Evaluation: Challenges and Opportunities","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","","https://doi.org/10.1145/3696630.3728723","Generative Artificial Intelligence (GenAI) is transforming software development processes, including test and evaluation (T&amp;E). From automating test case design to enabling continuous testing in DevOps pipelines, AI-driven tools enhance the efficiency, accuracy, and speed of software testing. At the same time, the integration of AI components into software-reliant systems introduces new challenges for verification and validation (V&amp;V). Traditional T&amp;E methodologies must evolve to address issues such as AI bias, hallucinated outputs, and the complexity of validating non-deterministic behaviors. This position paper examines how existing T&amp;E methods must evolve to account for AI's stochastic nature, and conversely how GenAI is transforming T&amp;E practices across the software development lifecycle (SDLC).","2025","2025-11-25 22:29:29","2025-11-25 22:29:29","","1376–1380","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TMT79TZI","conferencePaper","2024","Boukhlif, Mohamed; Kharmoum, Nassim; Hanine, Mohamed","LLMs for Intelligent Software Testing: A Comparative Study","Proceedings of the 7th International Conference on Networking, Intelligent Systems and Security","979-8-4007-0929-6","","10.1145/3659677.3659749","https://doi.org/10.1145/3659677.3659749","The need for effective and timely testing processes has become critical in the constantly changing field of software development. Large Language Models (LLMs) have demonstrated promise in automating test case creation, defect detection, and other software testing tasks through the use of the capabilities of machine/deep learning and natural language processing. This work explores the field of intelligent software testing, with a focus on the use of LLMs in this context. The purpose of this comparative study is to assess the corpus of research in the field in terms of used LLMs, how to interact with them, the use of fine-tuning, and prompt engineering, and explore the different technologies and testing types automated using LLMs. The findings of this study not only contribute to the growing body of knowledge on intelligent software testing but also guide fellow researchers and industry engineers in selecting the most suitable LLM for their specific testing needs.","2024","2025-11-25 22:29:29","2025-11-25 22:29:29","","","","","","","","","NISS '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Meknes, AA, Morocco","","","","Natural Language Processing; Comparative Study; Large Language Models; Software Testing; Test Case Generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D25LW2D8","conferencePaper","2024","Yang, Lin; Yang, Chen; Gao, Shutao; Wang, Weijing; Wang, Bo; Zhu, Qihao; Chu, Xiao; Zhou, Jianyi; Liang, Guangtai; Wang, Qianxiang; Chen, Junjie","On the Evaluation of Large Language Models in Unit Test Generation","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695529","https://doi.org/10.1145/3691620.3695529","Unit testing is an essential activity in software development for verifying the correctness of software components. However, manually writing unit tests is challenging and time-consuming. The emergence of Large Language Models (LLMs) offers a new direction for automating unit test generation. Existing research primarily focuses on closed-source LLMs (e.g., ChatGPT and CodeX) with fixed prompting strategies, leaving the capabilities of advanced open-source LLMs with various prompting settings unexplored. Particularly, open-source LLMs offer advantages in data privacy protection and have demonstrated superior performance in some tasks. Moreover, effective prompting is crucial for maximizing LLMs' capabilities. In this paper, we conduct the first empirical study to fill this gap, based on 17 Java projects, five widely-used open-source LLMs with different structures and parameter sizes, and comprehensive evaluation metrics. Our findings highlight the significant influence of various prompt factors, show the performance of open-source LLMs compared to the commercial GPT-4 and the traditional Evosuite, and identify limitations in LLM-based unit test generation. We then derive a series of implications from our study to guide future research and practical use of LLM-based unit test generation.","2024","2025-11-25 22:29:29","2025-11-25 22:47:48","","1607–1619","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","large language model; Large language models; Software testing; Test pattern generators; Software; Software engineering; Syntactics; Defect detection; Writing; Codes; Large Language Model; Unit Test Generation; unit test generation; Empirical Study; empirical study; Training data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N8ZWJU5A","journalArticle","2025","Wang, Shenao; Zhao, Yanjie; Hou, Xinyi; Wang, Haoyu","Large Language Model Supply Chain: A Research Agenda","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3708531","https://doi.org/10.1145/3708531","The rapid advancement of large language models (LLMs) has revolutionized artificial intelligence, introducing unprecedented capabilities in natural language processing and multimodal content generation. However, the increasing complexity and scale of these models have given rise to a multifaceted supply chain that presents unique challenges across infrastructure, foundation models, and downstream applications. This article provides the first comprehensive research agenda of the LLM supply chain, offering a structured approach to identify critical challenges and opportunities through the dual lenses of software engineering (SE) and security and privacy (S&amp;P). We begin by establishing a clear definition of the LLM supply chain, encompassing its components and dependencies. We then analyze each layer of the supply chain, presenting a vision for robust and secure LLM development, reviewing the current state of practices and technologies, and identifying key challenges and research opportunities. This work aims to bridge the existing research gap in systematically understanding the multifaceted issues within the LLM supply chain, offering valuable insights to guide future efforts in this rapidly evolving domain.","2025-05","2025-11-25 22:29:29","2025-11-25 22:29:29","","","","5","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; LLM Supply Chain","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M4JFAPZJ","conferencePaper","2025","Verdecchia, Roberto; Cruciani, Emilio; Bertolino, Antonia; Miranda, Breno","Energy-Aware Software Testing","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: New Ideas and Emerging Results","979-8-3315-3711-1","","10.1109/ICSE-NIER66352.2025.00026","https://doi.org/10.1109/ICSE-NIER66352.2025.00026","Our planet urges for a more responsible use of its resources, and since information technology contributes substantially to the global energy consumption, software engineering research has promptly embraced this request and is actively working towards more sustainable processes. An indispensable activity in software development is testing, which is known to be very costly in terms of time and effort. On top of this, a recent study by Zaidman has shown that software testing can be a voracious energy consumer as well. In this work we introduce the very concept of energy-aware testing as the adoption of strategies designed to reduce the energy consumption of existing practices. We discuss some possible strategies and, as an example, we conduct a first study of an energy-aware variant of a simple similarity-based test prioritization approach considering both energy consumption and test suite effectiveness, which provides evidence of perceptible savings. We encourage future research in energy-aware software testing that needs to address further studies and to think up more strategies.","2025","2025-11-25 22:29:29","2025-11-25 22:29:29","","101–105","","","","","","","ICSE-NIER '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5GEUK6FV","journalArticle","2025","Bandi, Ajay","Pedagogical Evaluation of Generative AI Course for Technologists","J. Comput. Sci. Coll.","","1937-4771","","","Generative AI is a transformative technology that impacts various fields, including software development, data analytics, and cybersecurity. To address this, we have designed and developed a Generative AI course for technologists, integrating foundational knowledge of various Gen AI architecture models with hands-on practical experience using Python libraries, including HuggingFace. This paper discusses the detailed course structure and assessments. A pedagogical evaluation approach is followed to identify the challenges encountered in the course and how to overcome them. The results demonstrate that the Generative AI Course for Technologists effectively equips students with technical expertise and critical thinking skills through a balanced combination of theoretical concepts and practical exercises, such as chatbot development and prompt engineering. The course addresses challenges like hardware limitations and API integration by proposing future improvements, including a dedicated Python module and access to cloud-based GPU tools, ensuring learners are well-prepared to navigate and ethically apply Generative AI in real-world contexts.","2025-04","2025-11-25 22:29:29","2025-11-25 22:29:29","","99–110","","6","40","","","","","","","","","","","","","","","","","Place: Evansville, IN, USA Publisher: Consortium for Computing Sciences in Colleges","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H42DK49K","conferencePaper","2025","Zosimadis, Ilias; Stamelos, Ioannis","LLM-Enhanced Test Case Prioritization for Complex Software Systems","Proceedings of the 28th Pan-Hellenic Conference on Progress in Computing and Informatics","979-8-4007-1317-0","","10.1145/3716554.3716561","https://doi.org/10.1145/3716554.3716561","This paper presents a novel approach to test case prioritization using Large Language Models (LLMs) for complex software systems. Traditional prioritization methods often struggle with the dynamic nature of modern software development and the large amounts of unstructured data generated during the software lifecycle. Our method leverages LLMs to analyze diverse data sources, including code changes, user feedback, and system documentation, creating a more adaptive and context-aware prioritization strategy. We applied our approach to an Internet of Things (IoT) based system for motion tracking in ten-pin bowling. The experimental results show significant improvements over a baseline Additional Statement Coverage method. Our LLM-enhanced approach achieved a 12.12% higher Average Percentage of Faults Detected (APFD) score and reduced test suite execution time by 26 %. These findings demonstrate the potential of LLMs to enhance software testing practices, particularly in early fault detection and efficient resource utilization. The paper discusses implementation details, evaluation metrics, and future directions for integrating this approach into continuous integration and deployment pipelines.","2025","2025-11-25 22:29:29","2025-11-25 22:29:29","","46–50","","","","","","","PCI '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GFTPEMG8","conferencePaper","2025","Flores-Saviaga, Claudia; Hanrahan, Benjamin V.; Imteyaz, Kashif; Clarke, Steven; Savage*, Saiph","The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired","Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems","979-8-4007-1394-1","","10.1145/3706598.3714008","https://doi.org/10.1145/3706598.3714008","The rapid adoption of generative AI in software development has impacted the industry, yet its effects on developers with visual impairments remain largely unexplored. To address this gap, we used an Activity Theory framework to examine how developers with visual impairments interact with AI coding assistants. For this purpose, we conducted a study where developers who are visually impaired completed a series of programming tasks using a generative AI coding assistant. We uncovered that, while participants found the AI assistant beneficial and reported significant advantages, they also highlighted accessibility challenges. Specifically, the AI coding assistant often exacerbated existing accessibility barriers and introduced new challenges. For example, it overwhelmed users with an excessive number of suggestions, leading developers who are visually impaired to express a desire for “AI timeouts.” Additionally, the generative AI coding assistant made it more difficult for developers to switch contexts between the AI-generated content and their own code. Despite these challenges, participants were optimistic about the potential of AI coding assistants to transform the coding experience for developers with visual impairments. Our findings emphasize the need to apply activity-centered design principles to generative AI assistants, ensuring they better align with user behaviors and address specific accessibility needs. This approach can enable the assistants to provide more intuitive, inclusive, and effective experiences, while also contributing to the broader goal of enhancing accessibility in software development.","2025","2025-11-25 22:29:29","2025-11-25 22:29:29","","","","","","","","","CHI '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","generative ai; accessibility; ai coding assistants; assistive technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LM3S7RGD","conferencePaper","2024","Deljouyi, Amirhossein","Understandable Test Generation Through Capture/Replay and LLMs","Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings","979-8-4007-0502-1","","10.1145/3639478.3639789","https://doi.org/10.1145/3639478.3639789","Automatic unit test generators, particularly search-based software testing (SBST) tools such as EvoSuite, efficiently generate unit test suites with acceptable coverage. Although this removes the burden of writing unit tests from developers, these generated tests often pose challenges in terms of comprehension for developers. In my doctoral research, I aim to investigate strategies to address the issue of comprehensibility in generated test cases and improve the test suite in terms of effectiveness. To achieve this, I introduce four projects leveraging Capture/Replay and Large Language Model (LLM) techniques.Capture/Replay carves information from End-to-End (E2E) tests, enabling the generation of unit tests containing meaningful test scenarios and actual test data. Moreover, the growing capabilities of large language models (LLMs) in language analysis and transformation play a significant role in improving readability in general. Our proposed approach involves leveraging E2E test scenario extraction alongside an LLM-guided approach to enhance test case understandability, augment coverage, and establish comprehensive mock and test oracles.In this research, we endeavor to conduct both a quantitative analysis and a user evaluation of the quality of the generated tests in terms of executability, coverage, and understandability.","2024","2025-11-25 22:29:29","2025-11-25 22:48:10","","261–263","","","","","","","ICSE-Companion '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","Software testing; Test pattern generators; Software engineering; Writing; automatic test generation; large language models; Large Language Models; Automatic Test Generation; unit testing; carving and replaying; readability; understandability; Unit Testing; Carving and Replaying; Generators; Medical services; Readability; Statistical analysis; Understandability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AU5IHF7N","conferencePaper","2025","Roy Chowdhury, Sujoy; Sridhara, Giriprasad; Raghavan, A K; Bose, Joy; Mazumdar, Sourav; Singh, Hamender; Sugumaran, Srinivasan Bajji; Britto, Ricardo","Static Program Analysis Guided LLM Based Unit Test Generation","Proceedings of the 8th International Conference on Data Science and Management of Data (12th ACM IKDD CODS and 30th COMAD)","979-8-4007-1124-4","","10.1145/3703323.3703742","https://doi.org/10.1145/3703323.3703742","We describe a novel approach to automating unit test generation for Java methods using large language models (LLMs). Existing LLM-based approaches rely on sample usage(s) of the method to test (focal method) and/or provide the entire class of the focal method as input prompt and context. The former approach is often not viable due to the lack of sample usages, especially for newly written focal methods. The latter approach does not scale well enough; the bigger the complexity of the focal method and larger associated class, the harder it is to produce adequate test code (due to factors such as exceeding the prompt and context lengths of the underlying LLM). We show that augmenting prompts with concise and precise context information obtained by program analysis increases the effectiveness of generating unit test code through LLMs. We validate our approach on a large commercial Java project and a popular open-source Java project.","2025","2025-11-25 22:29:29","2025-11-25 22:29:29","","279–283","","","","","","","CODS-COMAD '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Large Language Models; Static Analysis; Unit Test Case Generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5VY4HPAP","conferencePaper","2025","Korraprolu, Brahma Reddy; Pinninti, Pavitra; Reddy, Y. Raghu","Test Case Generation for Requirements in Natural Language - An LLM Comparison Study","Proceedings of the 18th Innovations in Software Engineering Conference","979-8-4007-1424-5","","10.1145/3717383.3717389","https://doi.org/10.1145/3717383.3717389","The rapid evolution of Large Language Models (LLMs) have opened new possibilities in automating tasks across the software developing life cycle, including test case generation This paper presents a comparative analysis of six LLMs in the context of generating test cases for technical requirements written in natural language (in this case English). We compare publicly available general purpose LLMs viz., BARD, ChatGPT3.5, Claude, Gemini, ChatGPT4.o (Omni) and Llama3. The generated test cases are tested against a Simulink model created for the corresponding set of requirements. The coverage metrics thus generated are used for a quantitative comparison of the LLMs.","2025","2025-11-25 22:29:29","2025-11-25 22:29:29","","","","","","","","","ISEC '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","LLM; Large Language Models; Test Case Generation; Requirements","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9T2W8WBE","journalArticle","2025","Zhang, Junwei; Hu, Xing; Xia, Xin; Cheung, Shing-Chi; Li, Shanping","Automated Unit Test Generation via Chain of Thought Prompt and Reinforcement Learning from Coverage Feedback","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3745765","https://doi.org/10.1145/3745765","Recently, large language models (LLMs) have shown promising results in code generation, and several automated test generation approaches based on LLMs have been proposed. Although these approaches achieve promising performance, they suffer from two limitations. First, they lack the intrinsic understanding of the semantic intricacies and logical constructs inherent to the focal method. Second, they ignore the diversity of the generated tests and generate tests with limited code coverage.To alleviate these two limitations, in this work, we propose a novel approach named TestCTRL that optimizes LLMs for unit test generation by the chain-of-thought (CoT) prompt and reinforcement learning (RL) strategy. Specifically, we first build a new CoT dataset, containing the focal methods, corresponding unit tests, and CoT prompts. The CoT prompt includes the intention and possible test input values. Then, the CoT dataset is used to fine-tune one LLM (i.e., CodeLlama 7B) that can be seen as the policy model in RL. Meanwhile, we fine-tune another LLM (i.e., CodeGPT) as the reward model by predicting the line coverage of the focal method and its test. Moreover, we employ the Proximal Policy Optimization (PPO) algorithm to optimize the policy model and generate unit tests. We use the Defects4J benchmark to evaluate our approach from three perspectives (i.e., naturalness, validity, and code coverage). To avoid data leakage threats, we filtered out data from the CoT dataset that have the same focal method and test case names as those in the Defects4J. The experimental results demonstrate that TestCTRL outperforms state-of-the-art baselines in line and branch coverages, respectively. Besides, TestCTRL improves bug detection performance. We also investigate the reason for the proposed approach's superiority.","2025-07","2025-11-25 22:29:29","2025-11-25 22:29:29","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Unit Test Generation; Chain of Thought; Reinforcement Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"843H23PN","conferencePaper","2024","Chen, Yinghao; Hu, Zehao; Zhi, Chen; Han, Junxiao; Deng, Shuiguang; Yin, Jianwei","ChatUniTest: A Framework for LLM-Based Test Generation","Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering","979-8-4007-0658-5","","10.1145/3663529.3663801","https://doi.org/10.1145/3663529.3663801","Unit testing is an essential yet frequently arduous task. Various automated unit test generation tools have been introduced to mitigate this challenge. Notably, methods based on large language models (LLMs) have garnered considerable attention and exhibited promising results in recent years. Nevertheless, LLM-based tools encounter limitations in generating accurate unit tests. This paper presents ChatUniTest, an LLM-based automated unit test generation framework. ChatUniTest incorporates an adaptive focal context mechanism to encompass valuable context in prompts and adheres to a generation-validation-repair mechanism to rectify errors in generated unit tests. Subsequently, we have developed ChatUniTest Core, a common library that implements core workflow, complemented by the ChatUniTest Toolchain, a suite of seamlessly integrated tools enhancing the capabilities of ChatUniTest. Our effectiveness evaluation reveals that ChatUniTest outperforms TestSpark and EvoSuite in half of the evaluated projects, achieving the highest overall line coverage. Furthermore, insights from our user study affirm that ChatUniTest delivers substantial value to various stakeholders in the software testing domain. ChatUniTest is available at https://github.com/ZJU-ACES-ISE/ChatUniTest, and the demo video is available at https://www.youtube.com/watch?v=GmfxQUqm2ZQ.","2024","2025-11-25 22:29:29","2025-11-25 22:29:29","","572–576","","","","","","","FSE 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","Large Language Models; Automatic Unit Testing Generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CIFKPSQ9","journalArticle","2025","Liu, Jinwei; Li, Chao; Chen, Rui; Li, Shaofeng; Gu, Bin; Yang, Mengfei","STRUT: Structured Seed Case Guided Unit Test Generation for C Programs using LLMs","Proc. ACM Softw. Eng.","","","10.1145/3728970","https://doi.org/10.1145/3728970","Unit testing plays a crucial role in bug detection and ensuring software correctness. It helps developers identify errors early in development, thereby reducing software defects. In recent years, large language models (LLMs) have demonstrated significant potential in automating unit test generation. However, using LLMs to generate unit tests faces many challenges. 1) The execution pass rate of the test cases generated by LLMs is low. 2) The test case coverage is inadequate, making it challenging to detect potential risks in the code. 3) Current research methods primarily focus on languages such as Java and Python, while studies on C programming are scarce, despite its importance in the real world. To address these challenges, we propose STRUT, a novel unit test generation method. STRUT utilizes structured test cases as a bridge between complex programming languages and LLMs. Instead of directly generating test code, STRUT guides LLMs to produce structured test cases, thereby alleviating the limitations of LLMs when generating code for programming languages with complex features. First, STRUT analyzes the context of focal methods and constructs structured seed test cases for them. These seed test cases then guide LLMs to generate a set of structured test cases. Subsequently, a rule-based approach is employed to convert the structured set of test cases into executable test code. We conducted a comprehensive evaluation of STRUT, which achieved an impressive execution pass rate of 96.01%, along with 77.67% line coverage and 63.60% branch coverage. This performance significantly surpasses that of the LLMs-based baseline methods and the symbolic execution tool SunwiseAUnit. These results highlight STRUT's superior capability in generating high-quality unit test cases by leveraging the strengths of LLMs while addressing their inherent limitations.","2025-06","2025-11-25 22:29:29","2025-11-25 22:29:29","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; Automatic Unit Test Generation; Structured Test Cases","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AUPKMLF7","conferencePaper","2025","Tan, Yifan; Tan, Cheng; Mi, Zeyu; Chen, Haibo","PipeLLM: Fast and Confidential Large Language Model Services with Speculative Pipelined Encryption","Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1","979-8-4007-0698-1","","10.1145/3669940.3707224","https://doi.org/10.1145/3669940.3707224","Confidential computing on GPUs, like NVIDIA H100, mitigates the security risks of outsourced Large Language Models (LLMs) by implementing strong isolation and data encryption. Nonetheless, this encryption incurs a significant performance overhead, reaching up to 52.8% and 88.2% throughput drop when serving OPT-30B and OPT-66B, respectively. To address this challenge, we introduce PipeLLM, a user-transparent runtime system. PipeLLM removes the overhead by overlapping the encryption and GPU computation through pipelining-an idea inspired by the CPU instruction pipelining-thereby effectively concealing the latency increase caused by encryption. The primary technical challenge is that, unlike CPUs, the encryption module lacks prior knowledge of the specific data needing encryption until it is requested by the GPUs. To this end, we propose speculative pipelined encryption to predict the data requiring encryption by analyzing the serving patterns of LLMs. Further, we have developed an efficient, low-cost pipeline relinquishing approach for instances of incorrect predictions. Our experiments show that compared with vanilla systems without confidential computing (e.g., vLLM, PEFT, and FlexGen), PipeLLM incurs modest overhead ( &lt; 19.6% in throughput) across various LLM sizes, from 13B to 175B. PipeLLM's source code is available at https://github.com/SJTU-IPADS/PipeLLM.","2025","2025-11-25 22:29:29","2025-11-25 22:29:29","","843–857","","","","","","","ASPLOS '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Rotterdam, Netherlands","","","","large language model; confidential virtual machine; nvidia confidential computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DXK2F5IG","bookSection","2025","Lin, Feng; Kim, Dong Jae; Chen, Tse-Hsun (Peter)","SOEN-101: Code Generation by Emulating Software Process Models Using Large Language Model Agents","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00140","Software process models are essential to facilitate collaboration and communication among software teams to solve complex development tasks. Inspired by these software engineering practices, we present FlowGen – a code generation framework that emulates software process models based on multiple Large Language Model (LLM) agents. We emulate three process models, FlowGenWaterfall, FlowGenTDD, and FlowGenScrum, by assigning LLM agents to embody roles (i.e., requirement engineer, architect, developer, tester, and scrum master) that correspond to everyday development activities and organize their communication patterns. The agents work collaboratively using chain-of-thought and prompt composition with continuous self-refinement to improve the code quality. We use GPT3.5 as our underlying LLM and several baselines (RawGPT, CodeT, Reflexion) to evaluate code generation on four benchmarks: HumanEval, HumanEval-ET, MBPP, and MBPP-ET. Our findings show that FlowGenScrum excels compared to other process models, achieving a Pass@1 of 75.2, 65.5, 82.5, and 56.7 in HumanEval, HumanEval-ET, MBPP, and MBPP-ET, respectively (an average of 15% improvement over RawGPT). Compared with other state-of-the-art techniques, FlowGenScrum achieves a higher Pass@1 in MBPP compared to CodeT, with both outperforming Reflexion. Notably, integrating CodeT into FlowGenScrum resulted in statistically significant improvements, achieving the highest Pass@1 scores. Our analysis also reveals that the development activities impacted code smell and exception handling differently, with design and code review adding more exception handling and reducing code smells. Finally, FlowGen models maintain stable Pass@1 scores across GPT3.5 versions and temperature values, highlighting the effectiveness of software process models in enhancing the quality and stability of LLM-generated code.","2025","2025-11-25 22:29:29","2025-11-25 22:29:29","","1527–1539","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H73KHQMB","journalArticle","2025","Ji, Pin; Feng, Yang; Zhang, Ruohao; Xue, Ruichen; Zhang, Yichi; Huang, Weitao; Liu, Jia; Zhao, Zhihong","NLPLego: Assembling Test Generation for Natural Language Processing Applications","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3691631","https://doi.org/10.1145/3691631","With the development of Deep Learning, Natural Language Processing (NLP) applications have reached or even exceeded human-level capabilities in certain tasks. Although NLP applications have shown good performance, they can still have bugs like traditional software and even lead to serious consequences. Inspired by Lego blocks and syntax structure analysis, we propose an assembling test generation method for NLP applications or models and implement it in NLPLego. The key idea of NLPLego is to assemble the sentence skeleton and adjuncts in order by simulating the building of Lego blocks to generate multiple grammatically and semantically correct sentences based on one seed sentence. The sentences generated by NLPLego have derivation relations and different degrees of variation. These characteristics make it well-suited for integration with metamorphic testing theory, addressing the challenge of test oracle absence in NLP application testing. To validate NLPLego, we conduct experiments on three commonly used NLP tasks (i.e., machine reading comprehension, sentiment analysis, and semantic similarity measures), focusing on the efficiency of test generation and the quality and effectiveness of generated tests. We select five advanced NLP models and one popular industrial NLP software as the tested subjects. Given seed tests from SQuAD 2.0, SST, and QQP, NLPLego successfully detects 1,732, 3,140, and 261,879 incorrect behaviors with around 93.1% precision in three tasks, respectively. The experiment results show that NLPLego can efficiently generate high-quality tests for multiple NLP tasks to detect erroneous behaviors effectively. In the case study, we analyze the testing results provided by NLPLego to obtain intuitive representations of the different NLP capabilities of the tested subjects. The case study confirms that NLPLego can provide developers with clarity on the direction to improve NLP models or applications, laying the foundation for enhancing performance.","2025-01","2025-11-25 22:29:29","2025-11-25 22:29:29","","","","2","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Natural Language Processing; Test Generation; Automated Testing; Metamorphic Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"67ME7BQN","conferencePaper","2024","Wang, Yin; Fan, Ming; Zhang, Xicheng; Shi, Jifei; Qiu, Zhaoyu; Wang, Haijun; Liu, Ting","LIReDroid: LLM-Enhanced Test Case Generation for Static Sensitive Behavior Replication","Proceedings of the 15th Asia-Pacific Symposium on Internetware","979-8-4007-0705-6","","10.1145/3671016.3671404","https://doi.org/10.1145/3671016.3671404","Malicious Android applications often employ covert behaviors to exfiltrate sensitive data, thereby compromising user privacy. Traditional detection techniques predominantly utilize static analysis of the source code to detect such sensitive behaviors, yet they are frequently plagued by elevated false positive rates. While dynamic analysis methods offer greater precision, they contend with the challenge of limited coverage. This paper introduces LIReDroid, a hybrid testing approach that aims to replicate sensitive behaviors identified in static analysis call chains. LIReDroid firstly analyze the application’s static invocation chain. Then LIReDroid devises a prompt word model for the generation of test instructions and injection script code. Ultimately, sensitive API call chains are dynamically invoked through code injection, with their activation being meticulously recorded. We presented preliminary experimental results to substantiate the efficacy of LIReDroid. Given these results, we outline future research directions for LIReDroid.","2024","2025-11-25 22:29:29","2025-11-25 22:29:29","","81–84","","","","","","","Internetware '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Macau, China","","","","Large Language Model; Test Case Generation; Android Application Security; Sensitive Behavior Reproduction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GPEJ4GI4","conferencePaper","2024","Greiner, Sandra; Bühlmann, Noah; Ohrndorf, Manuel; Tsigkanos, Christos; Nierstrasz, Oscar; Kehrer, Timo","Automated Generation of Code Contracts: Generative AI to the Rescue?","Proceedings of the 23rd ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences","979-8-4007-1211-1","","10.1145/3689484.3690738","https://doi.org/10.1145/3689484.3690738","Design by Contract represents an established, lightweight paradigm for engineering reliable and robust software systems by specifying verifiable expectations and obligations between software components. Due to its laborious nature, developers hardly adopt Design by Contract in practice. A plethora of research on (semi-)-automated inference to reduce the manual burden has not improved the adoption of so-called code contracts in practice. This paper examines the potential of Generative AI to automatically generate code contracts in terms of pre- and postconditions for any Java project without requiring any additional auxiliary artifact. To fine-tune two state-of-the-art Large Language Models, CodeT5 and CodeT5+, we derive a dataset of more than 14k Java methods comprising contracts in form of Java Modeling Language (JML) annotations, and train the models on the task of generating contracts. We examine the syntactic and semantic validity of the contracts generated for software projects not used in the fine-tuning and find that more than 95% of the generated contracts are syntactically correct and exhibit remarkably high completeness and semantic correctness. To this end, our fully automated method sets the stage for future research and eventual broader adoption of Design by Contract in software development practice.","2024","2025-11-25 22:29:29","2025-11-25 22:29:29","","1–14","","","","","","","GPCE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Pasadena, CA, USA","","","","Generative AI; Large Language Models; Design by Contract; Software Verification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YXIHWM25","conferencePaper","2024","Sapozhnikov, Arkadii; Olsthoorn, Mitchell; Panichella, Annibale; Kovalenko, Vladimir; Derakhshanfar, Pouria","TestSpark: IntelliJ IDEA's Ultimate Test Generation Companion","Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings","979-8-4007-0502-1","","10.1145/3639478.3640024","https://doi.org/10.1145/3639478.3640024","Writing software tests is laborious and time-consuming. To address this, prior studies introduced various automated test-generation techniques. A well-explored research direction in this field is unit test generation, wherein artificial intelligence (AI) techniques create tests for a method/class under test. While many of these techniques have primarily found applications in a research context, existing tools (e.g., EvoSuite, Randoop, and AthenaTest) are not user-friendly and are tailored to a single technique. This paper introduces Test-Spark, a plugin for IntelliJ IDEA that enables users to generate unit tests with only a few clicks directly within their Integrated Development Environment (IDE). Furthermore, TestSpark also allows users to easily modify and run each generated test and integrate them into the project workflow. TestSpark leverages the advances of search-based test generation tools, and it introduces a technique to generate unit tests using Large Language Models (LLMs) by creating a feedback cycle between the IDE and the LLM. Since TestSpark is an open-source (https://github.com/JetBrains-Research/TestSpark), extendable, and well-documented tool, it is possible to add new test generation methods into the plugin with the minimum effort. This paper also explains our future studies related to TestSpark and our preliminary results. Demo video: https://youtu.be/0F4PrxWfiXo","2024","2025-11-25 22:29:29","2025-11-25 22:48:01","","30–34","","","","","","","ICSE-Companion '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","Test pattern generators; Software; Software engineering; Writing; Unit Test Generation; large language models; Large Language Models; unit test generation; intellij idea plugin; IntelliJ IDEA Plugin","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"43IS6L24","conferencePaper","2025","Roy, Nimisha; Horielko, Oleksandr; Omojokun, Olufisayo","Benchmarking of Generative AI Tools in Software Engineering Education: Formative Insights for Curriculum Integration","Proceedings of the 2025 ACM Conference on International Computing Education Research V.2","979-8-4007-1341-5","","10.1145/3702653.3744328","https://doi.org/10.1145/3702653.3744328","Generative Artificial Intelligence (Gen-AI) has revolutionized software engineering (SE) by automating tasks across design, coding, and testing [1] [2]. Tools like ChatGPT and GitHub Copilot streamline code generation, architectural modeling, debugging, and test-case creation [3] [4]. Despite their rapid adoption in industry, the pedagogical implications of these tools in computing education have not been systematically examined. This study solves the existing gap by conducting a comprehensive benchmarking study of Gen-AI tools across four core SE phases— design documentation, feature implementation, debugging support, and testing — to address two research questions:RQ1: What strengths and limitations do Gen-AI tools exhibit in each phase?RQ2: How can insights from benchmarking inform effective integration of Gen-AI into SE curricula?To answer these questions, a diverse set of Gen-AI tools is evaluated, ranging from design-focused assistants such as Lucidchart, Mermaid.js and UIzard; implementation-oriented systems including GitHub Copilot, TabNine, Codeium and Supermaven; debugging supports like GPT-4 and Claude 3.5 Sonnet; and testing frameworks such as Testim, Mabl and Applitools—while also surveying emerging platforms (as of summer 2024) like Replit, Postman, Visily, Gemini, Eraser.io and others. For each tool and development phase, we applied phase-specific metrics: in design documentation, we assessed diagram accuracy, completeness, user effort, and IDE integration; in feature implementation, we measured pattern-based code generation quality, code-completion effectiveness, refactoring robustness, and UI/UX scaffolding; in debugging, we evaluated error-detection accuracy, hallucination rates, and clarity of explanatory feedback; and in testing, we examined test-case relevance and defect-detection coverage. Across all phases, we tracked prompt engineering complexity as a key mediating factor influencing tool performance.Our evaluation reveals speed-fidelity trade-offs: Code-completion assistants accelerate boilerplate generation but demand manual oversight to ensure cross-file consistency and manage higher-order abstractions; diagramming tools can produce precise UML models with minimal effort— but at the cost of iterative prompt refinement for complex cases; LLM debuggers deliver context-sensitive fixes yet suffer from nontrivial hallucination rates; testing generators exhibit wide variance in edge-case coverage. On average, tools needed 2.4 prompt iterations for usable diagrams and 1.5 prompts for bug fixes, underscoring the human effort in guiding AI.We recommend a scaffolded framework for integrating Gen-AI into SE education by: embedding AI tools into hands-on assignments, to explore tasks in a controlled context; by structuring small team projects in which one subgroup uses AI assistants while the other completes the same tasks manually (covering design, implementation, debugging and testing) to surface contrasts in workflow, tool strengths, and human reasoning; by requiring students to maintain a reflective journal documenting their AI usage and prompt-engineering strategies, fostering metacognitive insight into how tool inputs shape outputs; and by equipping learners with decision making criteria, teaching them to evaluate AI assistants according to task fit- preparing them to leverage AI responsibly across SE phases in its evolving landscape.","2025","2025-11-25 22:29:29","2025-11-25 22:29:29","","3","","","","","","","ICER '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Generative AI; Benchmarking; Hallucination; Productivity; Prompt Engineering; Software Engineering Education","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EHLFBZTS","conferencePaper","2024","Olsthoorn, Mitchell; Stallenberg, Dimitri; Panichella, Annibale","Syntest-JavaScript: Automated Unit-Level Test Case Generation for JavaScript","Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing","979-8-4007-0562-5","","10.1145/3643659.3643928","https://doi.org/10.1145/3643659.3643928","Over the last decades, various tools (e.g., AUSTIN and EvoSuite) have been developed to automate the process of unit-level test case generation. Most of these tools are designed for statically-typed languages, such as C and Java. However, as is shown in recent Stack Overflow developer surveys, the popularity of dynamically-typed languages, such as JavaScript and Python, has been increasing and is dominating the charts. Only recently, tools for automated test case generation of dynamically-typed languages have started to emerge (e.g., Pynguin for Python). However, to the best of our knowledge, there is no tool that focuses on automated test case generation for server-side JavaScript. To this aim, we introduce SynTest-JavaScript, a user-friendly tool for automated unit-level test case generation for (server-side) JavaScript. To showcase the effectiveness of SynTest-JavaScript, we empirically evaluate it on five large open-source JavaScript projects and one artificial one.","2024","2025-11-25 22:29:29","2025-11-25 22:29:29","","21–24","","","","","","","SBFT '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","software testing; test case generation; search-based software testing; fuzzing; javascript; syntest","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PTR2R64L","journalArticle","2025","Davis, Matthew C.; Wei, Amy; Myers, Brad A.; Sunshine, Joshua","TerzoN: Human-in-the-Loop Software Testing with a Composite Oracle","Proc. ACM Softw. Eng.","","","10.1145/3729359","https://doi.org/10.1145/3729359","Software testing is difficult, tedious, and may consume 28%–50% of software engineering labor. Automatic test generators aim to ease this burden but have important trade-offs. Fuzzers use an implicit oracle that can detect obviously invalid results, but the oracle problem has no general solution, and an implicit oracle cannot automatically evaluate correctness. Test suite generators like EvoSuite use the program under test as the oracle and therefore cannot evaluate correctness. Property-based testing tools evaluate correctness, but users have difficulty coming up with properties to test and understanding whether their properties are correct. Consequently, practitioners create many test suites manually and often use an example-based oracle to tediously specify correct input and output examples. To help bridge the gaps among various oracle and tool types, we present the Composite Oracle, which organizes various oracle types into a hierarchy and renders a single test result per example execution. To understand the Composite Oracle’s practical properties, we built TerzoN, a test suite generator that includes a particular instantiation of the Composite Oracle. TerzoN displays all the test results in an integrated view composed from the results of three types of oracles and finds some types of test assertion inconsistencies that might otherwise lead to misleading test results. We evaluated TerzoN in a randomized controlled trial with 14 professional software engineers with a popular industry tool, fast-check, as the control. Participants using TerzoN elicited 72% more bugs (p &lt; 0.01), accurately described more than twice the number of bugs (p &lt; 0.01) and tested 16% more quickly (p &lt; 0.05) relative to fast-check.","2025-06","2025-11-25 22:29:29","2025-11-25 22:29:29","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","software testing; automatic test generation; user study; composite oracle; Empirical software engineering; experiments; human subjects; usable testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V7DHGFXS","conferencePaper","2025","Patel, Smit; Yadavally, Aashish; Dhulipala, Hridya; Nguyen, Tien N.","Planning a Large Language Model for Static Detection of Runtime Errors in Code Snippets","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00102","https://doi.org/10.1109/ICSE55347.2025.00102","Large Language Models (LLMs) have been excellent in generating and reasoning about source code and natural-language texts. They can recognize patterns, syntax, and semantics in code, making them effective in several software engineering tasks. However, they exhibit weaknesses in reasoning about the program execution. They primarily operate on static code representations, failing to capture the dynamic behavior and state changes that occur during program execution.In this paper, we advance the capabilities of LLMs in reasoning about dynamic program behaviors. We propose Orca, a novel approach that instructs an LLM to autonomously formulate a plan to navigate through a control flow graph (CFG) for predictive execution of (in)complete code snippets. It acts as a predictive interpreter to ""execute"" the code. In Orca, we guide the LLM to pause at the branching point, focusing on the state of the symbol tables for variables' values, thus minimizing error propagation in the LLM's computation. We instruct the LLM not to stop at each step in its execution plan, resulting the use of only one prompt for the entire predictive interpreter, thus much cost-saving. As a downstream task, we use Orca to statically identify any runtime errors for online code snippets. Early detection of runtime errors and defects in these snippets is crucial to prevent costly fixes later in the development cycle after they were adapted into a codebase. Our empirical evaluation showed that Orca is effective and improves over the state-of-the-art approaches in predicting the execution traces and in static detection of runtime errors.","2025","2025-11-25 22:29:29","2025-11-25 22:29:29","","872–884","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","execution prediction; large language model (LLM) planning; runtime error static detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8FZWSRA9","conferencePaper","2025","Yeung, Steven","A comparative study of rule-based, machine learning and large language model approaches in automated writing evaluation (AWE)","Proceedings of the 15th International Learning Analytics and Knowledge Conference","979-8-4007-0701-8","","10.1145/3706468.3706566","https://doi.org/10.1145/3706468.3706566","Automated Writing Evaluation (AWE) tools have proved beneficial to writing development. Research on AWE methods is essential for improving tool performance and further comparative studies are needed as new methods emerge. This study examines the performance of several AWE approaches, comparing rule-based and statistical methods, machine learning (ML) models, and a large language model (LLM). These three AWE methods were applied to a representative sample of academic essays from the TOEFL11 dataset to compare their assessment performance. Results show that the selected LLM, GPT-4, outperformed the other two approaches in terms of QWK and Pearson’s correlation coefficient, while the Support Vector Machine (SVM) model in the ML approach had the highest accuracy and the lowest mean absolute error. This paper provides a detailed comparison of these three approaches and discusses implications for educational practice and future research around AWE.","2025","2025-11-25 22:29:29","2025-11-25 22:29:29","","984–991","","","","","","","LAK '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","large language model; generative AI; automated essay scoring; automated writing evaluation; machine learning; Rule-based method","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4RZXKWWT","conferencePaper","2024","Zhang, Yichi; Liu, Zixi; Feng, Yang; Xu, Baowen","Leveraging Large Language Model to Assist Detecting Rust Code Comment Inconsistency","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695010","https://doi.org/10.1145/3691620.3695010","Rust is renowned for its robust memory safety capabilities, yet its distinctive memory management model poses substantial challenges in both writing and understanding programs. Within Rust source code, comments are employed to clearly delineate conditions that might cause panic behavior, thereby warning developers about potential hazards associated with specific operations. Therefore, comments are particularly crucial for documenting Rust's program logic and design. Nevertheless, as modern software frequently undergoes updates and modifications, maintaining the accuracy and relevance of these comments becomes a labor-intensive endeavor.In this paper, inspired by the remarkable capabilities of Large Language Models (LLMs) in understanding software programs, we propose a code-comment inconsistency detection tool, namely RustC4, that combines program analysis and LLM-driven techniques to identify inconsistencies in code comments. RustC4 leverages LLMs' ability to interpret natural language descriptions within code comments, facilitating the extraction of design constraints. Program analysis techniques are then employed to accurately verify the implementation of these constraints. To evaluate the effectiveness of RustC4, we construct a dataset from 12 large-scale real-world Rust projects. The experiment results demonstrate that RustC4 is effective in detecting 176 real inconsistent cases and 23 of them have been confirmed and fixed by developers by the time this paper was submitted.","2024","2025-11-25 22:29:29","2025-11-25 22:29:29","","356–366","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","large language model; bug detection; code comment inconsistency; program analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WZR8D54P","journalArticle","2025","Stalnaker, Trevor; Wintersgill, Nathan; Chaparro, Oscar; Heymann, Laura A.; Di Penta, Massimiliano; German, Daniel M; Poshyvanyk, Denys","Developer Perspectives on Licensing and Copyright Issues Arising from Generative AI for Software Development","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3743133","https://doi.org/10.1145/3743133","Despite the utility that Generative AI (GenAI) tools provide for tasks such as writing code, the use of these tools raises important legal questions and potential risks, particularly those associated with copyright law. As lawmakers and regulators respond to these questions, the views of users can offer relevant perspectives. In this paper, we provide: (1) a survey of 574 developers on the licensing and copyright aspects of GenAI for coding, as well as follow-up interviews; (2) a snapshot of developers’ views at a time when GenAI and perceptions of it were rapidly evolving; and (3) an analysis of developers’ perspectives, yielding insights and recommendations that can inform future regulatory decisions in this evolving field. Our results show the benefits developers derive from GenAI, how they view the use of AI-generated code as similar to using other existing code, the varied opinions they have on who should own or be compensated for such code, that they are concerned about data leakage via GenAI, and other findings, providing organizations and policymakers with valuable insights into how the technology is being used and the concerns that stakeholders believe warrant attention.","2025-06","2025-11-25 22:29:30","2025-11-25 22:29:30","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","large language models; generative ai; machine learning; open-source software; qualitative research","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9SN9VUPD","bookSection","2025","Alian, Parsa; Nashid, Noor; Shahbandeh, Mobina; Shabani, Taha; Mesbah, Ali","Feature-Driven End-to-End Test Generation","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00141","End-to-end (E2E) testing is essential for ensuring web application quality. However, manual test creation is time-consuming, and current test generation techniques produce incoherent tests. In this paper, we present AutoE2E, a novel approach that leverages Large Language Models (LLMs) to automate the generation of semantically meaningful feature-driven E2E test cases for web applications. AutoE2E intelligently infers potential features within a web application and translates them into executable test scenarios. Furthermore, we address a critical gap in the research community by introducing E2EBench, a new benchmark for automatically assessing the feature coverage of E2E test suites. Our evaluation on E2EBench demonstrates that AutoE2E achieves an average feature coverage of 79%, outperforming the best baseline by 558%, highlighting its effectiveness in generating high-quality, comprehensive test cases.","2025","2025-11-25 22:29:30","2025-11-25 22:29:30","","450–462","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HUQY9ZSS","conferencePaper","2024","Chernyshev, Maxim; Baig, Zubair; Doss, Robin Ram Mohan","Towards Large Language Model (LLM) Forensics Using LLM-based Invocation Log Analysis","Proceedings of the 1st ACM Workshop on Large AI Systems and Models with Privacy and Safety Analysis","979-8-4007-1209-8","","10.1145/3689217.3690616","https://doi.org/10.1145/3689217.3690616","Large Language Models (LLMs) have fostered the emergence of software application architectures that improve user experiences powered by generative artificial intelligence. A range of cyber attacks are possible against an LLM. A novel approach to digital forensic analysis of LLM-integrated applications is presented for prompt injection attacks. The forensic analysis process is invoked through LLM log analysis. We propose LLM invocation logging as a critical component for enhancing digital forensic readiness in LLM-integrated applications and evaluate 13 state-of-the-art LLMs for this analysis task. Our findings demonstrate the potential utility of selected LLMs in the context of prompt-to-SQL attacks, influenced by sampling temperature and context window size parameters. We also identify limitations of our work and propose key areas for future research, for ongoing contribution to the emerging field of LLM forensics.","2024","2025-11-25 22:29:30","2025-11-25 22:29:30","","89–96","","","","","","","LAMPS '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Salt Lake City, UT, USA","","","","digital forensics; large language model (llm); llm-integrated applications; log analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2FNE76R5","journalArticle","2024","Li, Jialong; Zhang, Mingyue; Li, Nianyu; Weyns, Danny; Jin, Zhi; Tei, Kenji","Generative AI for Self-Adaptive Systems: State of the Art and Research Roadmap","ACM Trans. Auton. Adapt. Syst.","","1556-4665","10.1145/3686803","https://doi.org/10.1145/3686803","Self-adaptive systems (SASs) are designed to handle changes and uncertainties through a feedback loop with four core functionalities: monitoring, analyzing, planning, and execution. Recently, generative artificial intelligence (GenAI), especially the area of large language models, has shown impressive performance in data comprehension and logical reasoning. These capabilities are highly aligned with the functionalities required in SASs, suggesting a strong potential to employ GenAI to enhance SASs. However, the specific benefits and challenges of employing GenAI in SASs remain unclear. Yet, providing a comprehensive understanding of these benefits and challenges is complex due to several reasons: limited publications in the SAS field, the technological and application diversity within SASs, and the rapid evolution of GenAI technologies. To that end, this article aims to provide researchers and practitioners a comprehensive snapshot that outlines the potential benefits and challenges of employing GenAI’s within SAS. Specifically, we gather, filter, and analyze literature from four distinct research fields and organize them into two main categories to potential benefits: (i) enhancements to the autonomy of SASs centered around the specific functions of the MAPE-K feedback loop, and (ii) improvements in the interaction between humans and SASs within human-on-the-loop settings. From our study, we outline a research roadmap that highlights the challenges of integrating GenAI into SASs. The roadmap starts with outlining key research challenges that need to be tackled to exploit the potential for applying GenAI in the field of SAS. The roadmap concludes with a practical reflection, elaborating on current shortcomings of GenAI and proposing possible mitigation strategies.†","2024-09","2025-11-25 22:29:30","2025-11-25 22:29:30","","","","3","19","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Generative AI; Large Language Model; survey; diffusion model; MAPE; Self-Adaptive Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TXNZKMWM","conferencePaper","2024","Khatiri, Sajad; Saurabh, Prasun; Zimmermann, Timothy; Munasinghe, Charith; Birchler, Christian; Panichella, Sebastiano","SBFT Tool Competition 2024 - CPS-UAV Test Case Generation Track","Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing","979-8-4007-0562-5","","10.1145/3643659.3643931","https://doi.org/10.1145/3643659.3643931","While simulation-based testing is critical for ensuring the safety of autonomous Unmanned Aerial Vehicles (UAVs), it has not been adequately researched yet. The UAV Testing Competition organized by the Search-Based and Fuzz Testing (SBFT) workshop is an initiative designed to inspire and encourage the software testing Community to direct their attention toward UAVs as a rapidly emerging and crucial domain. It provides a simple software platform and case study to facilitate their onboarding in the UAV domain and help them develop their first test generation tools for UAVs.In this first edition of the competition, 7 tools were submitted, evaluated, and compared extensively against each other and the baseline approach. We evaluated their test generation performance for 6 different case studies using our novel benchmarking infrastructure. The generated test suites were scored and ranked based on the number and severity of the revealed faults, and the complexity, diversity, and execution time of the test cases. This paper describes the competition context, its platform, the competing tools, and the evaluation process and results.","2024","2025-11-25 22:29:30","2025-11-25 22:29:30","","29–32","","","","","","","SBFT '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","software testing; test case generation; search based software engineering; tool competition; unmanned aerial vehicles","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AV9H4JYY","journalArticle","2025","Zhang, Yuwei; Jin, Zhi; Xing, Ying; Li, Ge; Liu, Fang; Zhu, Jiaxin; Dou, Wensheng; Wei, Jun","PATCH: Empowering Large Language Model with Programmer-Intent Guidance and Collaborative-Behavior Simulation for Automatic Bug Fixing","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3718739","https://doi.org/10.1145/3718739","Bug fixing holds significant importance in software development and maintenance. Recent research has made substantial strides in exploring the potential of large language models (LLMs) for automatically resolving software bugs. However, a noticeable gap in existing approaches lies in the oversight of collaborative facets intrinsic to bug resolution, treating the process as a single-stage endeavor. Moreover, most approaches solely take the buggy code snippet as input for LLMs during the patch generation stage. To mitigate the aforementioned limitations, we introduce a novel stage-wise framework named PATCH. Specifically, we first augment the buggy code snippet with corresponding dependence context and intent information to better guide LLMs in generating the correct candidate patches. Additionally, by taking inspiration from bug management practices, we decompose the bug-fixing task into four distinct stages: bug reporting, bug diagnosis, patch generation, and patch verification. These stages are performed interactively by LLMs, aiming to simulate the collaborative behavior of programmers during the resolution of software bugs. By harnessing these collective contributions, PATCH effectively enhances the bug-fixing capability of LLMs. We implement PATCH by employing the powerful dialogue-based LLM ChatGPT. Our evaluation on the widely used bug-fixing benchmark BFP demonstrates that PATCH has achieved better performance than state-of-the-art LLMs.","2025-02","2025-11-25 22:29:30","2025-11-25 22:29:30","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Bug Fixing; Bug Management; Multi-Agent Collaboration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BLADNENJ","conferencePaper","2025","Alrabah, Abdulrahman; Alawini, Abdussalam","CodeLens: A Generative AI Framework for Automated Feedback on SQL Assignments","Proceedings of the 4th International Workshop on Data Systems Education: Bridging Education Practice with Education Research","979-8-4007-1918-9","","10.1145/3735091.3737534","https://doi.org/10.1145/3735091.3737534","The integration of Generative AI in computing education presents a unique opportunity to complement the learning experience for students across different educational levels. This paper presents an AI-assisted framework designed to support database students by employing instructions designed to guide the Large Language Model’s behavior. The GenAI model is customized to act like an instructor that provides guiding feedback without revealing the solution. It utilizes Chat Completion API and fine-tuning methodologies, integrating information including problem statements, correct queries, database schema (structures that define how data is organized in a database). The outcome is a tool that detects semantic errors and formulates helpful feedback on students’ SQL queries. Preliminary implementations in courses such as Database Systems have demonstrated the framework’s influence, resulting in a reduction in SQL problem submissions. The framework’s correctness and effectiveness are evaluated by testing the models on problem sets, with experts assessing the generated responses. Ultimately, this work contribute to the field of Database Systems and Education by showcasing the practical application, adaptability, and effectiveness of AI models in computing education, providing a learning environment that leads to better outcomes for students tackling computing problems.","2025","2025-11-25 22:29:30","2025-11-25 22:29:30","","29–34","","","","","","","DataEd '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GDZNLFXS","conferencePaper","2025","Xhyra, Alind","Automatic Test Case Generation for Smart Human-Centric Ecosystems","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings","979-8-3315-3683-1","","10.1109/ICSE-Companion66252.2025.00040","https://doi.org/10.1109/ICSE-Companion66252.2025.00040","Smart Human-Centric Ecosystems (SHcES) are an important class of systems that impact on our everyday life. They span from smart homes to smart cities, smart grids, autonomous vehicles, smart schools, and smart healthcare systems. A SHcES comprises a wide range of autonomous and heterogeneous hardware and software systems that interact explicitly and implicitly in a shared environment. The core elements of SHcES are humans. Humans interact with the systems within the SHcES both explicitly through system interfaces and implicitly by freely acting in the ecosystem. For instance, humans interact with many systems in a smart city even by simply standing or moving. Classic testing approaches assume that users interact with the system only through the system interfaces, and thus miss many relevant elements of the sequences of human actions that comprise test cases of SHcES. The complete freedom of human actions and the vast variety of interactions that characterize the human behavior in SHcES make it challenging to generate sequences of human actions that constitute the backbone of test cases for SHcES.We hypothesize that sequences of human actions in SHcES depend on the personality, and this relation helps us automatically generate complete sequences of human actions that comprise test cases for SHcES. We assume that it is possible to infer the personality of humans by observing the human actions in the SHcES up to a given instant and use this information to infer the most likely human actions that can follow. We use this information to automatically generate test sequences for SHcES. With the use of personality models, we can inject new personalities in the SHcES, to test how different personalities and social groups interact in the context of new scenarios and system behavior before they happen in the field.","2025","2025-11-25 22:29:30","2025-11-25 22:29:30","","135–139","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","test case generation; testing; human-centric systems; personality","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TLNEBYC6","conferencePaper","2024","Mezzaro, Simone; Gambi, Alessio; Fraser, Gordon","An Empirical Study on How Large Language Models Impact Software Testing Learning","Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering","979-8-4007-1701-7","","10.1145/3661167.3661273","https://doi.org/10.1145/3661167.3661273","Software testing is a challenging topic in software engineering education and requires creative approaches to engage learners. For example, the Code Defenders game has students compete over a Java class under test by writing effective tests and mutants. While such gamified approaches deal with problems of motivation and engagement, students may nevertheless require help to put testing concepts into practice. The recent widespread diffusion of Generative AI and Large Language Models raises the question of whether and how these disruptive technologies could address this problem, for example, by providing explanations of unclear topics and guidance for writing tests. However, such technologies might also be misused or produce inaccurate answers, which would negatively impact learning. To shed more light on this situation, we conducted the first empirical study investigating how students learn and practice new software testing concepts in the context of the Code Defenders testing game, supported by a smart assistant based on a widely known, commercial Large Language Model. Our study shows that students had unrealistic expectations about the smart assistant, “blindly” trusting any output it generated, and often trying to use it to obtain solutions for testing exercises directly. Consequently, students who resorted to the smart assistant more often were less effective and efficient than those who did not. For instance, they wrote 8.6% fewer tests, and their tests were not useful in 78.0% of the cases. We conclude that giving unrestricted and unguided access to Large Language Models might generally impair learning. Thus, we believe our study helps to raise awareness about the implications of using Generative AI and Large Language Models in Computer Science Education and provides guidance towards developing better and smarter learning tools.","2024","2025-11-25 22:29:30","2025-11-25 22:29:30","","555–564","","","","","","","EASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Salerno, Italy","","","","ChatGPT; Generative AI; Computer Science Education; Smart Learning Assistant","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PMB5H2AT","conferencePaper","2024","Logacheva, Evanfiya; Hellas, Arto; Prather, James; Sarsa, Sami; Leinonen, Juho","Evaluating Contextually Personalized Programming Exercises Created with Generative AI","Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 1","979-8-4007-0475-8","","10.1145/3632620.3671103","https://doi.org/10.1145/3632620.3671103","Programming skills are typically developed through completing various hands-on exercises. Such programming problems can be contextualized to students’ interests and cultural backgrounds. Prior research in educational psychology has demonstrated that context personalization of exercises stimulates learners’ situational interests and positively affects their engagement. However, creating a varied and comprehensive set of programming exercises for students to practice on is a time-consuming and laborious task for computer science educators. Previous studies have shown that large language models can generate conceptually and contextually relevant programming exercises. Thus, they offer a possibility to automatically produce personalized programming problems to fit students’ interests and needs. This article reports on a user study conducted in an elective introductory programming course that included contextually personalized programming exercises created with GPT-4. The quality of the exercises was evaluated by both the students and the authors. Additionally, this work investigated student attitudes towards the created exercises and their engagement with the system. The results demonstrate that the quality of exercises generated with GPT-4 was generally high. What is more, the course participants found them engaging and useful. This suggests that AI-generated programming problems can be a worthwhile addition to introductory programming courses, as they provide students with a practically unlimited pool of practice material tailored to their personal interests and educational needs.","2024","2025-11-25 22:29:30","2025-11-25 22:29:30","","95–113","","","","","","","ICER '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Melbourne, VIC, Australia","","","","generative AI; large language models; automatic exercise generation; context personalization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KMTEMJFZ","journalArticle","2025","Cai, Yufan; Hou, Zhe; Sanan, David; Luan, Xiaokun; Lin, Yun; Sun, Jun; Dong, Jin Song","Automated Program Refinement: Guide and Verify Code Large Language Model with Refinement Calculus","Proc. ACM Program. Lang.","","","10.1145/3704905","https://doi.org/10.1145/3704905","Recently, the rise of code-centric Large Language Models (LLMs) has reshaped the software engineering world with low-barrier tools like Copilot that can easily generate code. However, there is no correctness guarantee for the code generated by LLMs, which suffer from the hallucination problem, and their output is fraught with risks. Besides, the end-to-end process from specification to code through LLMs is a non-transparent and uncontrolled black box. This opacity makes it difficult for users to understand and trust the generated code. Addressing these challenges is both necessary and critical. In contrast, program refinement transforms high-level specification statements into executable code while preserving correctness. Traditional tools for program refinement are primarily designed for formal methods experts and lack automation and extensibility. We apply program refinement to guide LLM and validate the LLM-generated code while transforming refinement into a more accessible and flexible framework. To initiate this vision, we propose Refine4LLM, an approach that aims to: (1) Formally refine the specifications, (2) Automatically prompt and guide the LLM using refinement calculus, (3) Interact with the LLM to generate the code, (4) Verify that the generated code satisfies the constraints, thus guaranteeing its correctness, (5) Learn and build more advanced refinement laws to extend the refinement calculus. We evaluated Refine4LLM against the state-of-the-art baselines on program refinement and LLMs benchmarks.The experiment results show that Refine4LLM can efficiently generate more robust code and reduce the time for refinement and verification.","2025-01","2025-11-25 22:29:30","2025-11-25 22:29:30","","","","POPL","9","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Program Refinement; Program Synthesis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J3TR5CG9","conferencePaper","2025","Dong, Chunhao; Jiang, Yanjie; Zhang, Yuxia; Zhang, Yang; Liu, Hui","ChatGPT-Based Test Generation for Refactoring Engines Enhanced by Feature Analysis on Examples","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00210","https://doi.org/10.1109/ICSE55347.2025.00210","Software refactoring is widely employed to improve software quality. However, conducting refactorings manually is tedious, time-consuming, and error-prone. Consequently, automated and semi-automated tool support is highly desirable for software refactoring in the industry, and most of the main-stream IDEs provide powerful tool support for refactoring. However, complex refactoring engines are prone to errors, which in turn may result in imperfect and incorrect refactorings. To this end, in this paper, we propose a ChatGPT-based approach to testing refactoring engines. We first manually analyze bug reports and test cases associated with refactoring engines, and construct a feature library containing fine-grained features that may trigger defects in refactoring engines. The approach automatically generates prompts according to both predefined prompt templates and features randomly selected from the feature library, requesting ChatGPT to generate test programs with the requested features. Test programs generated by ChatGPT are then forwarded to multiple refactoring engines for differential testing. To the best of our knowledge, it is the first approach in testing refactoring engines that guides test program generation with features derived from existing bugs. It is also the first approach in this line that exploits LLMs in the generation of test programs. Our initial evaluation of four main-stream refactoring engines suggests that the proposed approach is effective. It identified a total of 115 previously unknown bugs besides 28 inconsistent refactoring behaviors among different engines. Among the 115 bugs, 78 have been manually confirmed by the original developers of the tested engines, i.e., IntelliJ IDEA, Eclipse, VScode-Java, and NetBeans.","2025","2025-11-25 22:29:30","2025-11-25 22:47:05","","2714–2725","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","Test pattern generators; Software engineering; Industries; Software quality; Chatbots; ChatGPT; Testing; Computer bugs; differential testing; refactoring; refactoring engines; Libraries; Engines","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YWQEL7N7","conferencePaper","2024","Kimmel, Bailey; Geisert, Austin Lee; Yaro, Lily; Gipson, Brendan; Hotchkiss, Ronald Taylor; Osae-Asante, Sidney Kwame; Vaught, Hunter; Wininger, Grant; Yamaguchi, Chase","Enhancing Programming Error Messages in Real Time with Generative AI","Extended Abstracts of the CHI Conference on Human Factors in Computing Systems","979-8-4007-0331-7","","10.1145/3613905.3647967","https://doi.org/10.1145/3613905.3647967","Generative AI is changing the way that many disciplines are taught, including computer science. Researchers have shown that generative AI tools are capable of solving programming problems, writing extensive blocks of code, and explaining complex code in simple terms. Particular promise has been shown in using generative AI to enhance programming error messages. Both students and instructors have complained for decades that these messages are often cryptic and difficult to understand. Yet recent work has shown that students make fewer repeated errors when enhanced via GPT-4. We extend this work by implementing feedback from ChatGPT for all programs submitted to our automated assessment tool, Athene, providing help for compiler, run-time, and logic errors. Our results indicate that adding generative AI to an automated assessment tool does not necessarily make it better and that design of the interface matters greatly to the usability of the feedback that GPT-4 provided.","2024","2025-11-25 22:29:30","2025-11-25 22:29:30","","","","","","","","","CHI EA '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Honolulu, HI, USA","","","","LLM; ChatGPT; Large Language Models; AI; Artificial Intelligence; Automatic Code Generation; Codex; Copilot; CS1; GitHub; GPT-4; HCI; Introductory Programming; Novice Programming; OpenAI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C5GIREK6","journalArticle","2025","Li, Tao; Cui, Chenhui; Huang, Rubing; Towey, Dave; Ma, Lei","Large Language Models for Automated Web-Form-Test Generation: An Empirical Study","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3735553","https://doi.org/10.1145/3735553","Testing web forms is an essential activity for ensuring the quality of web applications. It typically involves evaluating the interactions between users and forms. Automated test-case generation remains a challenge for web-form testing: Due to the complex, multi-level structure of web pages, it can be difficult to automatically capture their inherent contextual information for inclusion in the tests. Large Language Models (LLMs) have shown great potential for contextual text generation. This motivated us to explore how they could generate automated tests for web forms, making use of the contextual information within form elements. To the best of our knowledge, no comparative study examining different LLMs has yet been reported for web-form-test generation. To address this gap in the literature, we conducted a comprehensive empirical study investigating the effectiveness of 11 LLMs on 146 web forms from 30 open-source Java web applications. In addition, we propose three HTML-structure-pruning methods to extract key contextual information. The experimental results show that different LLMs can achieve different testing effectiveness, with the GPT-4, GLM-4, and Baichuan2 LLMs generating the best web-form tests. Compared with GPT-4, the other LLMs had difficulty generating appropriate tests for the web forms: Their successfully-submitted rates (SSRs) — the proportions of the LLMs-generated web-form tests that could be successfully inserted into the web forms and submitted — decreased by 9.10% to 74.15%. Our findings also show that, for all LLMs, when the designed prompts include complete and clear contextual information about the web forms, more effective web-form tests were generated. Specifically, when using Parser-Processed HTML for Task Prompt (PH-P), the SSR averaged 70.63%, higher than the 60.21% for Raw HTML for Task Prompt (RH-P) and 50.27% for LLM-Processed HTML for Task Prompt (LH-P). With RH-P, GPT-4’s SSR was 98.86%, outperforming models like LLaMa2 (7B) with 34.47% and GLM-4V with 0%. Similarly, with PH-P, GPT-4 reached an SSR of 99.54%, the highest among all models and prompt types. Finally, this paper also highlights strategies for selecting LLMs based on performance metrics, and for optimizing the prompt design to improve the quality of the web-form tests.","2025-05","2025-11-25 22:29:30","2025-11-25 22:29:30","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Empirical Study; Automated Web-Form Testing; Java Web Applications; Large Language Models (LLMs); Web-Form-Test Generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PGXEVTV6","conferencePaper","2024","Ouedraogo, Wendkuuni C.; Kabore, Kader; Tian, Haoye; Song, Yewei; Koyuncu, Anil; Klein, Jacques; Lo, David; Bissyande, Tegawende F.","LLMs and Prompting for Unit Test Generation: A Large-Scale Evaluation","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695330","https://doi.org/10.1145/3691620.3695330","Unit testing, essential for identifying bugs, is often neglected due to time constraints. Automated test generation tools exist but typically lack readability and require developer intervention. Large Language Models (LLMs) like GPT and Mistral show potential in test generation, but their effectiveness remains unclear.This study evaluates four LLMs and five prompt engineering techniques, analyzing 216 300 tests for 690 Java classes from diverse datasets. We assess correctness, readability, coverage, and bug detection, comparing LLM-generated tests to EvoSuite. While LLMs show promise, improvements in correctness are needed. The study highlights both the strengths and limitations of LLMs, offering insights for future research.","2024","2025-11-25 22:29:30","2025-11-25 22:47:37","","2464–2465","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","Large language models; Test pattern generators; Java; Software engineering; Codes; Faces; Testing; prompt engineering; automatic test generation; Computer bugs; large language models; Large Language Models; Automatic Test Generation; Prompt engineering; Prompt Engineering; empirical evaluation; unit tests; Unit Tests; Time factors; Empirical Evaluation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XFYJP5PQ","conferencePaper","2025","Tanaka, Kyoji; Hisazumi, Kenji","Automated Scenario Generation for Autonomous Robots using Large Language Model","Proceedings of the 2025 14th International Conference on Software and Computer Applications","979-8-4007-1012-4","","10.1145/3731806.3731861","https://doi.org/10.1145/3731806.3731861","The development of autonomous robots has increasingly emphasized the importance of identifying potential issues early through efficient validation and simulation-based testing. Despite this progress, significant challenges remain in automating test scenario generation and addressing the diversity and complexity of testing environments. This study introduces a method for scenario-based testing using the Hakoniwa platform, which supports IoT and robot software development in a virtual environment, in combination with Large Language Models (LLM) to achieve a more precise drone simulation tailored to user requirements.","2025","2025-11-25 22:29:30","2025-11-25 22:29:30","","348–352","","","","","","","ICSCA '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Autonomous Robots; IoT in Robotics; Large Language Models (LLM); Scenario-Based Testing; Simulation Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CHXXUIZD","bookSection","2025","Rico, Sergio; Öberg, Lena-Maria","Challenges and Opportunities for Generative AI in Software Engineering: A Managerial View","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","","https://doi.org/10.1145/3696630.3728718","Generative artificial intelligence (GenAI) has shown a strong potential to automate tasks, increase productivity, and enhance software quality. Managers have an important role in adopting GenAI as they lead strategic planning and team coordination. This study examines the software engineering tasks that managers consider suitable for large language models (LLMs), as well as the role of management and education in software engineering practices. The study reviews current applications in practice and explores the challenges encountered during implementation. We conducted a workshop with managers from diverse organizations to collect insights on the growing use of LLMs in areas such as code development, debugging, documentation, and training. The discussion highlighted several key concerns, including intellectual property issues, quality assurance, and integration difficulties. Overall, our findings suggest that LLMs can potentially transform software engineering practices if technical and organizational challenges are carefully addressed.","2025","2025-11-25 22:29:30","2025-11-25 22:29:30","","1338–1344","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RDW2L4U2","conferencePaper","2024","Alian, Parsa; Nashid, Noor; Shahbandeh, Mobina; Mesbah, Ali","Semantic Constraint Inference for Web Form Test Generation","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680332","https://doi.org/10.1145/3650212.3680332","Automated test generation for web forms has been a longstanding challenge, exacerbated by the intrinsic human-centric design of forms and their complex, device-agnostic structures. We introduce an innovative approach, called FormNexus, for automated web form test generation, which emphasizes deriving semantic insights from individual form elements and relations among them, utilizing textual content, DOM tree structures, and visual proximity. The insights gathered are transformed into a new conceptual graph, the Form Entity Relation Graph (FERG), which offers machine-friendly semantic information extraction. Leveraging LLMs, FormNexus adopts a feedback-driven mechanism for generating and refining input constraints based on real-time form submission responses. The culmination of this approach is a robust set of test cases, each produced by methodically invalidating constraints, ensuring comprehensive testing scenarios for web forms. This work bridges the existing gap in automated web form testing by intertwining the capabilities of LLMs with advanced semantic inference methods. Our evaluation demonstrates that FormNexus combined with GPT-4 achieves 89% coverage in form submission states. This outcome significantly outstrips the performance of the best baseline model by a margin of 25%.","2024","2025-11-25 22:29:30","2025-11-25 22:29:30","","932–944","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large Language Models; Test Input Generation; Web Forms","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2GMPEYWQ","journalArticle","2024","Ryan, Gabriel; Jain, Siddhartha; Shang, Mingyue; Wang, Shiqi; Ma, Xiaofei; Ramanathan, Murali Krishna; Ray, Baishakhi","Code-Aware Prompting: A Study of Coverage-Guided Test Generation in Regression Setting using LLM","Proc. ACM Softw. Eng.","","","10.1145/3643769","https://doi.org/10.1145/3643769","Testing plays a pivotal role in ensuring software quality, yet conventional Search Based Software Testing (SBST) methods often struggle with complex software units, achieving suboptimal test coverage. Recent work using large language models (LLMs) for test generation have focused on improving generation quality through optimizing the test generation context and correcting errors in model outputs, but use fixed prompting strategies that prompt the model to generate tests without additional guidance. As a result LLM-generated testsuites still suffer from low coverage. In this paper, we present SymPrompt, a code-aware prompting strategy for LLMs in test generation. SymPrompt’s approach is based on recent work that demonstrates LLMs can solve more complex logical problems when prompted to reason about the problem in a multi-step fashion. We apply this methodology to test generation by deconstructing the testsuite generation process into a multi-stage sequence, each of which is driven by a specific prompt aligned with the execution paths of the method under test, and exposing relevant type and dependency focal context to the model. Our approach enables pretrained LLMs to generate more complete test cases without any additional training. We implement SymPrompt using the TreeSitter parsing framework and evaluate on a benchmark challenging methods from open source Python projects. SymPrompt enhances correct test generations by a factor of 5 and bolsters relative coverage by 26% for CodeGen2. Notably, when applied to GPT-4, SymPrompt improves coverage by over 2x compared to baseline prompting strategies.","2024-07","2025-11-25 22:29:30","2025-11-25 22:29:30","","","","FSE","1","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; Test Generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CGBBIZT4","conferencePaper","2024","Jiang, Nan; Wu, Yi","RepairCAT: Applying Large Language Model to Fix Bugs in AI-Generated Programs","Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair","979-8-4007-0577-9","","10.1145/3643788.3648020","https://doi.org/10.1145/3643788.3648020","Automated program repair has been a crucial and popular domain for years, and with the development of large language models (LLMs) and the trend of using LLMs for code generation, there comes the new challenge of fixing bugs in LLM-generated (AI-generated) programs. In this work, we introduce RepairCAT, a simple and neat framework for fine-tuning large language models for automated repairing Python programs. Our experiments built on StarCoder-1B successfully generated patches fixing the failed test cases for 14 out of 100 bugs in the Python programs, 2 of which passed all the public test cases and were considered plausible.","2024","2025-11-25 22:29:30","2025-11-25 22:29:30","","58–60","","","","","","","APR '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","large language model; automated program repair","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CX3J9V96","conferencePaper","2024","Prather, James; Reeves, Brent N; Leinonen, Juho; MacNeil, Stephen; Randrianasolo, Arisoa S; Becker, Brett A.; Kimmel, Bailey; Wright, Jared; Briggs, Ben","The Widening Gap: The Benefits and Harms of Generative AI for Novice Programmers","Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 1","979-8-4007-0475-8","","10.1145/3632620.3671116","https://doi.org/10.1145/3632620.3671116","Novice programmers often struggle through programming problem solving due to a lack of metacognitive awareness and strategies. Previous research has shown that novices can encounter multiple metacognitive difficulties while programming, such as forming incorrect conceptual models of the problem or having a false sense of progress after testing their solution. Novices are typically unaware of how these difficulties are hindering their progress. Meanwhile, many novices are now programming with generative AI (GenAI), which can provide complete solutions to most introductory programming problems, code suggestions, hints for next steps when stuck, and explain cryptic error messages. Its impact on novice metacognition has only started to be explored. Here we replicate a previous study that examined novice programming problem solving behavior and extend it by incorporating GenAI tools. Through 21 lab sessions consisting of participant observation, interview, and eye tracking, we explore how novices are coding with GenAI tools. Although 20 of 21 students completed the assigned programming problem, our findings show an unfortunate divide in the use of GenAI tools between students who did and did not struggle. Some students who did not struggle were able to use GenAI to accelerate, creating code they already intended to make, and were able to ignore unhelpful or incorrect inline code suggestions. But for students who struggled, our findings indicate that previously known metacognitive difficulties persist, and that GenAI unfortunately can compound them and even introduce new metacognitive difficulties. Furthermore, struggling students often expressed cognitive dissonance about their problem solving ability, thought they performed better than they did, and finished with an illusion of competence. Based on our observations from both groups, we propose ways to scaffold the novice GenAI experience and make suggestions for future work.","2024","2025-11-25 22:29:30","2025-11-25 22:29:30","","469–486","","","","","","","ICER '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Melbourne, VIC, Australia","","","","ChatGPT; generative AI; large language models; Copilot; CS1; metacognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BGXYL9H5","conferencePaper","2025","Bernstein, Seth; Rahman, Ashfin; Sharifi, Nadia; Terbish, Ariunjargal; MacNeil, Stephen","Beyond the Benefits: A Systematic Review of the Harms and Consequences of Generative AI in Computing Education","Proceedings of the 25th Koli Calling International Conference on Computing Education Research","979-8-4007-1599-0","","10.1145/3769994.3770036","https://doi.org/10.1145/3769994.3770036","Generative artificial intelligence (GenAI) has already had a big impact on computing education with prior research identifying many benefits. However, recent studies have also identified potential risks and harms. To continue maximizing AI benefits while addressing the harms and unintended consequences, we conducted a systematic literature review of research focusing on the risks, harms, and unintended consequences of GenAI in computing education. Our search of ACM DL, IEEE Xplore, and Scopus (2022-2025) resulted in 1,677 papers, which were then filtered to 224 based on our inclusion and exclusion criteria. Guided by best practices for systematic reviews, four reviewers independently extracted publication year, learner population, research method, contribution type, GenAI technology, and educational task information from each paper. We then coded each paper for concrete harm categories such as academic integrity, cognitive effects, and trust issues. Our analysis shows patterns in how and where harms appear, highlights methodological gaps and opportunities for more rigorous evidence, and identifies under-explored harms and student populations. By synthesizing these insights, we intend to equip educators, computing students, researchers, and developers with a clear picture of the harms associated with GenAI in computing education.","2025","2025-11-25 22:29:30","2025-11-25 22:29:30","","","","","","","","","Koli Calling '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","generative AI; large language models; computing education; harms","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BQJTL2Q3","journalArticle","2025","Wang, Chenxu; Liu, Tianming; Zhao, Yanjie; Yang, Minghui; Wang, Haoyu","LLMDroid: Enhancing Automated Mobile App GUI Testing Coverage with Large Language Model Guidance","Proc. ACM Softw. Eng.","","","10.1145/3715763","https://doi.org/10.1145/3715763","With the rapid development of Large Language Models (LLMs), their integration into automated mobile GUI testing has emerged as a promising research direction. However, existing LLM-based testing approaches face significant challenges, including time inefficiency and high costs due to constant LLM querying. To address these issues, this paper introduces LLMDroid, a novel testing framework designed to enhance existing automated mobile GUI testing tools by leveraging LLMs more efficiently. The workflow of LLMDroid comprises two main stages: Autonomous Exploration and LLM Guidance. During Autonomous Exploration, LLMDroid utilizes existing testing tools while leveraging LLMs to summarize explored pages. When code coverage growth slows, it transitions to LLM Guidance to strategically direct testing towards unexplored functionalities. This approach minimizes LLM interactions while maximizing their impact on test coverage. We applied LLMDroid to three popular open-source Android testing tools and evaluated it on 14 top-listed apps from Google Play. Results demonstrate an average increase of 26.16% in code coverage and 29.31% in activity coverage. Furthermore, our evaluation under different LLMs reveals that LLMDroid outperform existing step-wise approaches with significant cost efficiency, achieving optimal performance at 0.49 per hour using GPT-4o among tested models, with a cost-effective alternative achieving 94% of this performance at just 0.03 per hour. These findings highlight LLMDroid’s effectiveness in enhancing automated mobile app testing and its potential for widespread adoption.","2025-06","2025-11-25 22:29:30","2025-11-25 22:29:30","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; Code Coverage; GUI Testing; Mobile App Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3XRJMWKK","conferencePaper","2024","Alshahwan, Nadia; Harman, Mark; Marginean, Alexandru; Tal, Rotem; Wang, Eddy","Observation-Based Unit Test Generation at Meta","Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering","979-8-4007-0658-5","","10.1145/3663529.3663838","https://doi.org/10.1145/3663529.3663838","TestGen automatically generates unit tests, carved from serialized observations of complex objects, observed during app execution. We describe the development and deployment of TestGen at Meta. In particular, we focus on the scalability challenges overcome during development in order to deploy observation-based test carving at scale in industry. So far, TestGen has landed 518 tests into production, which have been executed 9,617,349 times in continuous integration, finding 5,702 faults. Meta is currently in the process of more widespread deployment. Our evaluation reveals that, when carving its observations from 4,361 reliable end-to-end tests, TestGen was able to generate tests for at least 86% of the classes covered by end-to-end tests. Testing on 16 Kotlin Instagram app-launch-blocking tasks demonstrated that the TestGen tests would have trapped 13 of these before they became launch blocking.","2024","2025-11-25 22:29:30","2025-11-25 22:29:30","","173–184","","","","","","","FSE 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","unit testing; Automated test generation; test carving","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"68PT27C6","bookSection","2025","Harman, Mark; O'Hearn, Peter; Sengupta, Shubho","Harden and Catch for Just-in-Time Assured LLM-Based Software Testing: Open Research Challenges","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","","https://doi.org/10.1145/3696630.3734199","Despite decades of research and practice in automated software testing, several fundamental concepts remain ill-defined and under-explored, yet offer enormous potential real-world impact. We show that these concepts raise exciting new challenges in the context of Large Language Models for software test generation. More specifically, we formally define and investigate the properties of hardening and catching tests. A hardening test is one that seeks to protect against future regressions, while a catching test is one that catches such a regression or a fault in new functionality introduced by a code change. Hardening tests can be generated at any time and may become catching tests when a future regression is caught. We also define and motivate the Catching 'Just-in-Time' (JiTTest) Challenge, in which tests are generated 'just-in-time' to catch new faults before they land into production. We show that any solution to Catching JiTTest generation can also be repurposed to catch latent faults in legacy code. We enumerate possible outcomes for hardening and catching tests and JiTTests, and discuss open research problems, deployment options, and initial results from our work on automated LLM-based hardening at Meta. This paper1 was written to accompany the keynote by the authors at the ACM International Conference on the Foundations of Software Engineering (FSE) 2025.","2025","2025-11-25 22:29:30","2025-11-25 22:29:30","","1–17","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W3I5YKLV","conferencePaper","2024","Li, Guochang; Zhi, Chen; Chen, Jialiang; Han, Junxiao; Deng, Shuiguang","Exploring Parameter-Efficient Fine-Tuning of Large Language Model on Automated Program Repair","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695066","https://doi.org/10.1145/3691620.3695066","Automated Program Repair (APR) aims to fix bugs by generating patches. And existing work has demonstrated that ""pre-training and fine-tuning"" paradigm enables Large Language Models (LLMs) improve fixing capabilities on APR. However, existing work mainly focuses on Full-Model Fine-Tuning (FMFT) for APR and limited research has been conducted on the execution-based evaluation of Parameter-Efficient Fine-Tuning (PEFT) for APR. Comparing to FMFT, PEFT can reduce computing resource consumption without compromising performance and has been widely adopted to other software engineering tasks.To fill this gap, we enhance the existing APR dataset by employing prompt engineering to create an instruction dataset, APR-Instruction, at first. Secondly, we fine-tune four pre-trained LLMs using four different PEFT methods with APR-Instruction. The best fine-tuned model fixes 58% more bugs than the state-of-the-art LLM-based APR techniques. The results also show that (IA)3 improves the creativity of LLMs more effectively through fine-tuning and achieves the highest fixing capability compared to the other three PEFT methods. Thirdly, we explore the optimal configuration of PEFT hyperparameters, and assess the impact of instruction dataset size, showing that a larger number of parameters and a larger training dataset do not necessarily result in better performance for PEFT. Lastly, we analyze peak memory usage and trainable parameters to show the efficiency of PEFT.This work provides a comprehensive exploration of PEFT on APR and suggests potentially promising directions for extension to other software engineering downstream tasks. APR-Instruction, PEFT weights, and the fine-tuning code are publicly available as open-source resources.","2024","2025-11-25 22:29:30","2025-11-25 22:29:30","","719–731","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","large language model; automated program repair; execution-based evaluation; parameter-effective fine-tuning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VIC9T8SS","conferencePaper","2025","Nie, Allen; Chandak, Yash; Suzara, Miroslav; Malik, Ali; Woodrow, Juliette; Peng, Matt; Sahami, Mehran; Brunskill, Emma; Piech, Chris","The GPT Surprise: Offering Large Language Model Chat in a Massive Coding Class Reduced Engagement But May Increase Adopters' Exam Performances","Proceedings of the Twelfth ACM Conference on Learning @ Scale","979-8-4007-1291-3","","10.1145/3698205.3733960","https://doi.org/10.1145/3698205.3733960","Large language models (LLMs) are quickly being adopted in a wide range of learning experiences, especially via ubiquitous and broadly accessible chat interfaces like ChatGPT. This type of interface is readily available to students and teachers around the world. Coding education is an interesting test case, both because LLMs have strong performance on coding tasks, and because LLM-powered support tools are rapidly becoming part of the workflow of professional software engineers. To help understand the impact of generic LLM use on coding education, we conducted a large-scale randomized control trial with 5,831 students from 146 countries in an online coding class in which we provided some students with access to a chat interface with GPT-4. Under some assumptions, we estimate positive benefits on exam performance for adopters, the students who used the tool, but over all students, the advertisement of GPT-4 led to a significant average decrease in exam participation. We observe similar decreases in other forms of course engagement. However, this decrease is modulated by the student's country of origin. Offering access to LLMs to students from low human development index countries increased their exam participation rate on average. Our results suggest there may be promising benefits to using LLMs in an introductory coding class, but also potential harms for engagement, which makes their longer term impact on student success unclear. Our work highlights the need for additional investigations to help understand the potential impact of future adoption and integration of LLMs into classrooms.","2025","2025-11-25 22:29:31","2025-11-25 22:29:31","","376–380","","","","","","","L@S '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Palermo, Italy","","","","generative ai; causal inference; randomized control trial","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X2EPSVQN","journalArticle","2025","Jackson, Victoria; Vasilescu, Bogdan; Russo, Daniel; Ralph, Paul; Prikladnicki, Rafael; Izadi, Maliheh; D’Angelo, Sarah; Inman, Sarah; Andrade, Anielle; van der Hoek, André","The Impact of Generative AI on Creativity in Software Development: A Research Agenda","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3708523","https://doi.org/10.1145/3708523","As GenAI becomes embedded in developer toolchains and practices, and routine code is increasingly generated, human creativity will be increasingly important for generating competitive advantage. This article uses the McLuhan tetrad alongside scenarios of how GenAI may disrupt software development more broadly, to identify potential impacts GenAI may have on creativity within software development. The impacts are discussed along with a future research agenda comprising five connected themes that consider how individual capabilities, team capabilities, the product, unintended consequences, and society can be affected.","2025-05","2025-11-25 22:29:31","2025-11-25 22:29:31","","","","5","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Generative AI; Creativity; Foundational Models; Software Development","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KB2IUT97","conferencePaper","2024","Missaoui, Sondess; Gerasimou, Simos; Matragkas, Nicholas","Semantic Data Augmentation for Deep Learning Testing Using Generative AI","Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering","979-8-3503-2996-4","","10.1109/ASE56229.2023.00194","https://doi.org/10.1109/ASE56229.2023.00194","The performance of state-of-the-art Deep Learning models heavily depends on the availability of well-curated training and testing datasets that sufficiently capture the operational domain. Data augmentation is an effective technique in alleviating data scarcity, reducing the time-consuming and expensive data collection and labelling processes. Despite their potential, existing data augmentation techniques primarily focus on simple geometric and colour space transformations, like noise, flipping and resizing, producing datasets with limited diversity. When the augmented dataset is used for testing the Deep Learning models, the derived results are typically uninformative about the robustness of the models. We address this gap by introducing GenFuzzer, a novel coverage-guided data augmentation fuzzing technique for Deep Learning models underpinned by generative AI. We demonstrate our approach using widely-adopted datasets and models employed for image classification, illustrating its effectiveness in generating informative datasets leading up to a 26% increase in widely-used coverage criteria.","2024","2025-11-25 22:29:31","2025-11-25 22:47:50","","1694–1698","","","","","","","ASE '23","","","","IEEE Press","Echternach, Luxembourg","","","","","","","","","","","","Semantics; generative AI; Generative AI; coverage guided fuzzing; data augmentation; deep learning testing; safe AI; Data Augmentation; Deep Learning Testing; Robustness; Deep learning; Safe AI; Data models; Training; Labeling; Coverage Guided Fuzzing; Data augmentation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NQXYYYS7","conferencePaper","2024","Shi, Billy; Kristensson, Per Ola","Pay Attention! Human-Centric Improvements of LLM-based Interfaces for Assisting Software Test Case Development","Adjunct Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology","979-8-4007-0718-6","","10.1145/3672539.3686341","https://doi.org/10.1145/3672539.3686341","Implementing automation testing is difficult and as a consequence there is a growing desire for semi-automated software testing systems with humans in the loop. Leveraging the growth of LLMs, recent research has demonstrated LLMs’ potential to improve performance on test generation, reporting, and bug triaging. However, relatively little work has explored the interactivity issues that emerge in semi-automated LLM-assisted software test case development. To fill this gap, we present two user studies (N1 = 16, N2 = 24) that investigate productivity, creativity, and user attention in three semi-automated LLM-assisted interaction strategies: (1) pre-emptive prompting; (2) buffered response; and (3) guided input. We find that pre-emptively prompting the user significantly enhances branch coverage and task creativity by more than 30% while reducing user’s off-task idle time by up to 48.7%. We conclude by suggesting concrete research directions applying mixed-initiative principles for LLM-based interactive systems for semi-automated software testing.","2024","2025-11-25 22:29:31","2025-11-25 22:29:31","","","","","","","","","UIST Adjunct '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Pittsburgh, PA, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"46UIHPGG","journalArticle","2024","Steinhöfel, Dominic; Zeller, Andreas","Language-Based Software Testing","Commun. ACM","","0001-0782","10.1145/3631520","https://doi.org/10.1145/3631520","Constraints over grammar elements can make test generation easier than ever.","2024-03","2025-11-25 22:29:31","2025-11-25 22:29:31","","80–84","","4","67","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P2AFVDYM","journalArticle","2025","Zhang, Junwei; Hu, Xing; Gao, Shan; Xia, Xin; Lo, David; Li, Shanping","Less Is More: On the Importance of Data Quality for Unit Test Generation","Proc. ACM Softw. Eng.","","","10.1145/3715778","https://doi.org/10.1145/3715778","Unit testing is crucial for software development and maintenance. Effective unit testing ensures and improves software quality, but writing unit tests is time-consuming and labor-intensive. Recent studies have proposed deep learning (DL) techniques or large language models (LLMs) to automate unit test generation. These models are usually trained or fine-tuned on large-scale datasets. Despite growing awareness of the importance of data quality, there has been limited research on the quality of datasets used for test generation. To bridge this gap, we systematically examine the impact of noise on the performance of learning-based test generation models. We first apply the open card sorting method to analyze the most popular and largest test generation dataset, Methods2Test, to categorize eight distinct types of noise. Further, we conduct detailed interviews with 17 domain experts to validate and assess the importance, reasonableness, and correctness of the noise taxonomy. Then, we propose CleanTest, an automated noise-cleaning framework designed to improve the quality of test generation datasets. CleanTest comprises three filters: a rule-based syntax filter, a rule-based relevance filter, and a model-based coverage filter. To evaluate its effectiveness, we apply CleanTest on two widely-used test generation datasets, i.e., Methods2Test and Atlas. Our findings indicate that 43.52% and 29.65% of datasets contain noise, highlighting its prevalence. Finally, we conduct comparative experiments using four LLMs (i.e., CodeBERT, AthenaTest, StarCoder, and CodeLlama7B) to assess the impact of noise on test generation performance. The results show that filtering noise positively influences the test generation ability of the models. Fine-tuning the four LLMs with the filtered Methods2Test dataset, on average, improves its performance by 67% in branch coverage, using the Defects4J benchmark. For the Atlas dataset, the four LLMs improve branch coverage by 39%. Additionally, filtering noise improves bug detection performance, resulting in a 21.42% increase in bugs detected by the generated tests.","2025-06","2025-11-25 22:29:31","2025-11-25 22:29:31","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Unit Test Generation; Large Language Models; Dataset Quality","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z7IP4CTQ","conferencePaper","2024","Oishwee, Sahrima Jannat; Stakhanova, Natalia; Codabux, Zadia","Large Language Model vs. Stack Overflow in Addressing Android Permission Related Challenges","Proceedings of the 21st International Conference on Mining Software Repositories","979-8-4007-0587-8","","10.1145/3643991.3644933","https://doi.org/10.1145/3643991.3644933","The Android permission system regulates access to sensitive mobile device resources such as camera and location. To access these resources, third-party developers need to request permissions. However, the Android permission system is complex and fast-evolving, presenting developers with numerous challenges surrounding compatibility issues, misuse of permissions, and vulnerabilities related to permissions. Our study aims to explore whether Large Language Models (LLMs) can serve as a reliable tool to assist developers in using Android permissions correctly and securely, thereby reducing the risks of misuse and security vulnerabilities in apps. In our study, we analyzed 1,008 Stack Overflow questions related to Android permissions and their accepted answers. In parallel, we generate answers to these questions using a popular LLM tool, ChatGPT. We focused on how well the ChatGPT's responses align with the accepted answers on Stack Overflow. Our findings show that above 50% of ChatGPT's answers align with Stack Overflow's accepted answers. ChatGPT offers better-aligned responses for challenges related to Documentation and Conceptual Understanding, while it provides less aligned answers for Debugging-related issues. In addition, we found that ChatGPT provides more consistent answers for 73.27% questions. Our study demonstrates the potential for using LLMs such as ChatGPT as a supporting tool to help developers navigate Android permission-related problems.","2024","2025-11-25 22:29:31","2025-11-25 22:29:31","","373–383","","","","","","","MSR '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","Android permissions; large language model (LLM); stack overflow","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7XM5FIFQ","conferencePaper","2023","Prather, James; Denny, Paul; Leinonen, Juho; Becker, Brett A.; Albluwi, Ibrahim; Craig, Michelle; Keuning, Hieke; Kiesler, Natalie; Kohn, Tobias; Luxton-Reilly, Andrew; MacNeil, Stephen; Petersen, Andrew; Pettit, Raymond; Reeves, Brent N.; Savelka, Jaromir","The Robots Are Here: Navigating the Generative AI Revolution in Computing Education","Proceedings of the 2023 Working Group Reports on Innovation and Technology in Computer Science Education","979-8-4007-0405-5","","10.1145/3623762.3633499","https://doi.org/10.1145/3623762.3633499","Recent advancements in artificial intelligence (AI) and specifically generative AI (GenAI) are threatening to fundamentally reshape computing and society. Largely driven by large language models (LLMs), many tools are now able to interpret and generate both natural language instructions and source code. These capabilities have sparked urgent questions in the computing education community around how educators should adapt their pedagogy to address the challenges and to leverage the opportunities presented by this new technology. In this working group report, we undertake a comprehensive exploration of generative AI in the context of computing education and make five significant contributions. First, we provide a detailed review of the literature on LLMs in computing education and synthesise findings from 71 primary articles, nearly 80% of which have been published in the first 8 months of 2023. Second, we report the findings of a survey of computing students and instructors from across 20 countries, capturing prevailing attitudes towards GenAI/LLMs and their use in computing education contexts. Third, to understand how pedagogy is already changing, we offer insights collected from in-depth interviews with 22 computing educators from five continents. Fourth, we use the ACM Code of Ethics to frame a discussion of ethical issues raised by the use of large language models in computing education, and we provide concrete advice for policy makers, educators, and students. Finally, we benchmark the performance of several current GenAI models/tools on various computing education datasets, and highlight the extent to which the capabilities of current models are rapidly improving.There is little doubt that LLMs and other forms of GenAI will have a profound impact on computing education over the coming years. However, just as the technology will continue to improve, so will our collective knowledge about how to leverage these new models and tools in educational settings. We expect many important conversations around this topic will emerge as the community explores how to provide more effective, inclusive, and personalised learning experiences. Our aim is that this report will serve as a focal point for both researchers and practitioners who are exploring, adapting, using, and evaluating GenAI and LLM-based tools in computing classrooms.","2023","2025-11-25 22:29:31","2025-11-25 22:29:31","","108–159","","","","","","","ITiCSE-WGR '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Turku, Finland","","","","large language models; artificial intelligence; llm; generative ai; ai; chatgpt; code generation; codex; computer programming; copilot; cs1; curriculum; github; gpt; gpt-3; gpt-4; llms; novice programming; openai; pedagogical practices; programming","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VYUJDDXN","conferencePaper","2024","Shu, Dong; Zhao, Haoran; Liu, Xukun; Demeter, David; Du, Mengnan; Zhang, Yongfeng","LawLLM: Law Large Language Model for the US Legal System","Proceedings of the 33rd ACM International Conference on Information and Knowledge Management","979-8-4007-0436-9","","10.1145/3627673.3680020","https://doi.org/10.1145/3627673.3680020","In the rapidly evolving field of legal analytics, finding relevant cases and accurately predicting judicial outcomes are challenging because of the complexity of legal language, which often includes specialized terminology, complex syntax, and historical context. Moreover, the subtle distinctions between similar and precedent cases require a deep understanding of legal knowledge. Researchers often conflate these concepts, making it difficult to develop specialized techniques to effectively address these nuanced tasks. In this paper, we introduce the Law Large Language Model (LawLLM), a multi-task model specifically designed for the US legal domain to address these challenges. LawLLM excels at Similar Case Retrieval (SCR), Precedent Case Recommendation (PCR), and Legal Judgment Prediction (LJP). By clearly distinguishing between precedent and similar cases, we provide essential clarity, guiding future research in developing specialized strategies for these tasks. We propose customized data preprocessing techniques for each task that transform raw legal data into a trainable format. Furthermore, we also use techniques such as in-context learning (ICL) and advanced information retrieval methods in LawLLM. The evaluation results demonstrate that LawLLM consistently outperforms existing baselines in both zero-shot and few-shot scenarios, offering unparalleled multi-task capabilities and filling critical gaps in the legal domain. Code and data are available at https://github.com/Tizzzzy/Law_LLM.","2024","2025-11-25 22:29:31","2025-11-25 22:29:31","","4882–4889","","","","","","","CIKM '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Boise, ID, USA","","","","large language models; natural language processing; legal system; multitask learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TS6KXZDW","journalArticle","2024","Russo, Daniel","Navigating the Complexity of Generative AI Adoption in Software Engineering","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3652154","https://doi.org/10.1145/3652154","This article explores the adoption of Generative Artificial Intelligence (AI) tools within the domain of software engineering, focusing on the influencing factors at the individual, technological, and social levels. We applied a convergent mixed-methods approach to offer a comprehensive understanding of AI adoption dynamics. We initially conducted a questionnaire survey with 100 software engineers, drawing upon the Technology Acceptance Model, the Diffusion of Innovation Theory, and the Social Cognitive Theory as guiding theoretical frameworks. Employing the Gioia methodology, we derived a theoretical model of AI adoption in software engineering: the Human-AI Collaboration and Adaptation Framework. This model was then validated using Partial Least Squares–Structural Equation Modeling based on data from 183 software engineers. Findings indicate that at this early stage of AI integration, the compatibility of AI tools within existing development workflows predominantly drives their adoption, challenging conventional technology acceptance theories. The impact of perceived usefulness, social factors, and personal innovativeness seems less pronounced than expected. The study provides crucial insights for future AI tool design and offers a framework for developing effective organizational implementation strategies.","2024-06","2025-11-25 22:29:31","2025-11-25 22:29:31","","","","5","33","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Generative AI; large language models; empirical software engineering; technology adaption","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VTYR5NFN","journalArticle","2025","Ji, Zhenlan; Ma, Pingchuan; Li, Zongjie; Wang, Zhaoyu; Wang, Shuai","Causality-Aided Evaluation and Explanation of Large Language Model-Based Code Generation","Proc. ACM Softw. Eng.","","","10.1145/3728938","https://doi.org/10.1145/3728938","While code generation has been widely used in various software development scenarios, the quality of the generated code is not guaranteed. This has been a particular concern in the era of large language models (LLM)-based code generation, where LLMs, deemed a complex and powerful black-box model, are instructed by a high-level natural language specification, namely a prompt, to generate code. Nevertheless, effectively evaluating and explaining the code generation capability of LLMs is inherently challenging, given the complexity of LLMs and the lack of transparency. Inspired by recent progress in causality analysis and its software engineering applications, this paper proposes a causality-driven approach to systematically analyze prompt-code causal relationships. However, this endeavor faces three key technical challenges: (1) representing textual prompts and code in a canonical form, (2) establishing causal relations between high-level concepts and code features, and (3) systematically analyzing diverse prompt variations. To address these challenges, we first propose a novel causal graph-based representation of the prompt and the generated code, which is established over the fine-grained, human-understandable concepts in the input prompts. The formed causal graph is then used to identify the causal relations between the prompt and the derived code. We illustrate the insights that our framework can provide by studying over four popular LLMs with over 12 prompt adjustment strategies. The results of these studies illustrate the potential of our technique to provide insights into LLM effectiveness and aid end-users in understanding predictions. Additionally, we demonstrate that our approach provides actionable insights to improve the quality of the LLM-generated code by properly calibrating the prompt.","2025-06","2025-11-25 22:29:31","2025-11-25 22:29:31","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; Causality; Code Generation; Explainability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XDZ8VDS4","conferencePaper","2024","Ouaazki, Abdessalam; Bergram, Kristoffer; Farah, Juan Carlos; Gillet, Denis; Holzer, Adrian","Generative AI-Enabled Conversational Interaction to Support Self-Directed Learning Experiences in Transversal Computational Thinking","Proceedings of the 6th ACM Conference on Conversational User Interfaces","979-8-4007-0511-3","","10.1145/3640794.3665542","https://doi.org/10.1145/3640794.3665542","As computational thinking (CT) becomes increasingly acknowledged as an important skill in education, self-directed learning (SDL) emerges as a key strategy for developing this capability. The advent of generative AI (GenAI) conversational agents has disrupted the landscape of SDL. However, many questions still arise about several user experience aspects of these agents. This paper focuses on two of these questions: personalization and long-term support. As such, the first part of this study explores the effectiveness of personalizing GenAI through prompt-tuning using a CT-based prompt for solving programming challenges. The second part focuses on identifying the strengths and weaknesses of a GenAI model in a semester-long programming project. Our findings indicate that while prompt-tuning could hinder ease of use and perceived learning assistance, it might lead to higher learning outcomes. Results from a thematic analysis also indicate that GenAI is useful for programming and debugging, but it presents challenges such as over-reliance and diminishing utility over time.","2024","2025-11-25 22:29:31","2025-11-25 22:29:31","","","","","","","","","CUI '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Luxembourg, Luxembourg","","","","Chatbots; ChatGPT; Education; Generative AI; Student Perceptions; Programming","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RLDCUTMF","conferencePaper","2024","Zhang, Elisa; Sun, Shiyu; Xing, Yunlong; Sun, Kun","Poster: Repairing Bugs with the Introduction of New Variables: A Multi-Agent Large Language Model","Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security","979-8-4007-0636-3","","10.1145/3658644.3691412","https://doi.org/10.1145/3658644.3691412","Trained on billions of tokens, large language models (LLMs) have a broad range of empirical knowledge which enables them to generate software patches with complex repair patterns. We leverage the powerful code-fixing capabilities of LLMs and propose VarPatch, a multi-agent conversational automated program repair (APR) technique that iteratively queries the LLM to generate software patches by providing various prompts and context information. VarPatch focuses on the variable addition repair pattern, as previous APR tools struggle to introduce and use new variables to fix buggy code. Additionally, we summarize commonly used APIs and identify four repair patterns involving new variable addition. Our evaluation on the Defects4J 1.2 dataset shows that VarPatch can repair 69% more bugs than baseline tools and over 8 times more bugs than GPT-4.","2024","2025-11-25 22:29:31","2025-11-25 22:29:31","","4961–4963","","","","","","","CCS '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Salt Lake City, UT, USA","","","","large language model; automated program repair; multiple agents","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R7XS4X6Q","conferencePaper","2024","Choudhuri, Rudrajit; Liu, Dylan; Steinmacher, Igor; Gerosa, Marco; Sarma, Anita","How Far Are We? The Triumphs and Trials of Generative AI in Learning Software Engineering","Proceedings of the IEEE/ACM 46th International Conference on Software Engineering","979-8-4007-0217-4","","10.1145/3597503.3639201","https://doi.org/10.1145/3597503.3639201","Conversational Generative AI (convo-genAI) is revolutionizing Software Engineering (SE) as engineers and academics embrace this technology in their work. However, there is a gap in understanding the current potential and pitfalls of this technology, specifically in supporting students in SE tasks. In this work, we evaluate through a between-subjects study (N=22) the effectiveness of ChatGPT, a convo-genAI platform, in assisting students in SE tasks. Our study did not find statistical differences in participants' productivity or self-efficacy when using ChatGPT as compared to traditional resources, but we found significantly increased frustration levels. Our study also revealed 5 distinct faults arising from violations of Human-AI interaction guidelines, which led to 7 different (negative) consequences on participants.","2024","2025-11-25 22:29:31","2025-11-25 22:29:31","","","","","","","","","ICSE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","ChatGPT; generative AI; software engineering; empirical study","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IQGFDW8A","book","2024","","A-TEST 2024: Proceedings of the 15th ACM International Workshop on Automating Test Case Design, Selection and Evaluation","","979-8-4007-1109-1","","","","Welcome to the 15th ACM International Workshop on Automating Test Case Design, Selection and Evaluation (A-TEST), co-located with ECOOP and ISSTA 2024, in Vienna, on 19th of September, 2024. This year's theme of the workshop is ""Using DSLs for testing and the testing of DSLs"", right at intersection of the topics of the two main conferences. A-TEST'24 received six submissions, three of which were accepted for presentation. All papers were reviewed by three program committee members. Next to the regular session with papers the workshop features a hands-on session, titled ""Testing DSLs with DSLs in Rascal and TESTAR"", showcasing state-of-the-art automated testing techniques applied to a DSL implementation in the context of the Rascal language workbench. In particular, it highlights how a (formal) model of a DSL can function as an oracle for regular acceptance tests, and as a driver from scriptless UI testing, using tools like TESTAR.","2024","2025-11-25 22:29:31","2025-11-25 22:29:31","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K4GEZF5U","conferencePaper","2024","Boucher, Josiah D; Smith, Gillian; Telliel, Yunus Doğan","Is Resistance Futile?: Early Career Game Developers, Generative AI, and Ethical Skepticism","Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems","979-8-4007-0330-0","","10.1145/3613904.3641889","https://doi.org/10.1145/3613904.3641889","This paper presents a study that examines developer perceptions and usage of generative AI (GAI) in a summer professional development program for game development interns focused on mobile game design. GAI applications are in common usage worldwide, yet the impacts of this technology in game development remain relatively underexplored. Through a qualitative study using ethnographic interviews and participatory observation, this paper explores how GAI impacted the workflows, creative processes, and professional identities of early career game developers. We present a case of GAI integration that was not a straightforward adoption. Focusing on the interns’ resistance, negotiation, and reimagining, we show that the interns were actively developing a new professional culture both with and against generative AI. For the interns, their ethical commitments to fellow game developers and the future of their profession were as important as their practical concerns about usability, utility, and efficacy of GAI tools.","2024","2025-11-25 22:29:31","2025-11-25 22:29:31","","","","","","","","","CHI '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Honolulu, HI, USA","","","","Generative AI; Creativity Support; Future of GAI; Games/Play; Professional Communities; Programming/Development Support; Qualitative Methods","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3DADZEYP","conferencePaper","2025","Haque, Summit; Hundhausen, Christopher","Generative AI Access, Usage, and Perceptions: An Empirical Comparison of Computing Students In The United States and Bangladesh","Proceedings of the 2025 ACM Conference on International Computing Education Research V.1","979-8-4007-1340-8","","10.1145/3702652.3744231","https://doi.org/10.1145/3702652.3744231","In recent years, Large Language Model-based AI (GenAI) assistants have begun to transform the computer programming process. Researchers in computing education are studying these tools by assessing their capabilities, analyzing associated risks and opportunities, and developing guidelines for their effective use. One concern that has received little attention thus far is the potentially disparate impacts of GenAI tools on computing students with unequal resources and opportunities in different regions across the globe. Is GenAI technology creating a digital divide among computing students from different regions? This research presents a comparative study between undergraduate computing students from the United States and Bangladesh with respect to their access to GenAI assistants, usage behavior, and concerns about these tools. We collected study data through a questionnaire distributed to undergraduate computing education students from multiple universities in both countries (n = 534). The study results reveal significant differences (p &lt;.05) between the access, use, and attitudes of students from the two countries, suggesting the need to develop strategies for bridging the gap between the regions. This research aims to inform computing education researchers about GenAI disparities among computing students from different regions and to promote research to address this challenge.","2025","2025-11-25 22:29:31","2025-11-25 22:29:31","","109–124","","","","","","","ICER '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Generative AI; Comparative study; Computing education students; Digital divide; Exploratory study; LLM-based AI assistant; Quantitative analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P8FP4WS3","conferencePaper","2025","Yin, Xin; Ni, Chao; Xu, Xiaodan; Yang, Xiaohu","What You See Is What You Get: Attention-Based Self-Guided Automatic Unit Test Generation","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00105","https://doi.org/10.1109/ICSE55347.2025.00105","Software defects heavily affect software's functionalities and may cause huge losses. Recently, many AI-based approaches have been proposed to detect defects, which can be divided into two categories: software defect prediction and automatic unit test generation. While these approaches have made great progress in software defect detection, they still have several limitations in practical application, including the low confidence of prediction models and the inefficiency of unit testing models.To address these limitations, we propose a WYSIWYG (i.e., What You See Is What You Get) approach: Attention-based Self-guided Automatic Unit Test GenERation (AUGER), which contains two stages: defect detection and error triggering. In the former stage, AUGER first detects the proneness of defects. Then, in the latter stage, it guides to generate unit tests for triggering such an error with the help of critical information obtained by the former stage. To evaluate the effectiveness of AUGER, we conduct a large-scale experiment by comparing with the state-of-the-art (SOTA) approaches on the widely used datasets (i.e., Bears, Bugs.jar, and Defects4J). AUGER makes great improvements by 4.7% to 35.3% and 17.7% to 40.4% in terms of F1-score and Precision in defect detection, and can trigger 23 to 84 more errors than SOTAs in unit test generation. Besides, we also conduct a further study to verify the generalization in practical usage by collecting a new dataset from real-world projects.","2025","2025-11-25 22:29:31","2025-11-25 22:29:31","","1039–1051","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","unit test generation; defect; error-triggering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CXKSHZFW","conferencePaper","2024","Wang, Dawei; Zhou, Geng; Chen, Li; Li, Dan; Miao, Yukai","ProphetFuzz: Fully Automated Prediction and Fuzzing of High-Risk Option Combinations with Only Documentation via Large Language Model","Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security","979-8-4007-0636-3","","10.1145/3658644.3690231","https://doi.org/10.1145/3658644.3690231","Vulnerabilities related to option combinations pose a significant challenge in software security testing due to their vast search space. Previous research primarily addressed this challenge through mutation or filtering techniques, which inefficiently treated all option combinations as having equal potential for vulnerabilities, thus wasting considerable time on non-vulnerable targets and resulting in low testing efficiency. In this paper, we utilize carefully designed prompt engineering to drive the large language model (LLM) to predict high-risk option combinations (i.e., more likely to contain vulnerabilities) and perform fuzz testing automatically without human intervention. We developed a tool called ProphetFuzz and evaluated it on a dataset comprising 52 programs collected from three related studies. The entire experiment consumed 10.44 CPU years. ProphetFuzz successfully predicted 1748 high-risk option combinations at an average cost of only 8.69 per program. Results show that after 72 hours of fuzzing, ProphetFuzz discovered 364 unique vulnerabilities associated with 12.30% of the predicted high-risk option combinations, which was 32.85% higher than that found by state-of-the-art in the same timeframe. Additionally, using ProphetFuzz, we conducted persistent fuzzing on the latest versions of these programs, uncovering 140 vulnerabilities, with 93 confirmed by developers and 21 awarded CVE numbers.","2024","2025-11-25 22:29:31","2025-11-25 22:29:31","","735–749","","","","","","","CCS '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Salt Lake City, UT, USA","","","","large language model; fuzzing; option-aware; vulnerability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JAZF4NB2","conferencePaper","2023","Faruk, Lawal Ibrahim Dutsinma; Rohan, Rohani; Ninrutsirikun, Unhawa; Pal, Debajyoti","University Students’ Acceptance and Usage of Generative AI (ChatGPT) from a Psycho-Technical Perspective","Proceedings of the 13th International Conference on Advances in Information Technology","979-8-4007-0849-7","","10.1145/3628454.3629552","https://doi.org/10.1145/3628454.3629552","The emergence of ChatGPT as a generative AI tool has revolutionized the educational scenario by bringing in unprecedented changes. In this respect exploring the factors that affect the adoption and acceptance of ChatGPT services for educational purpose is of utmost importance. Accordingly, in this work we take a hybrid psycho-technical approach by considering the technological (perceived usefulness, ease of use and facilitating conditions), contextual (perceived humanness and novelty value), and psychological (agreeableness, extraversion, openness, conscientiousness, and neuroticism) gratifications of ChatGPT use. Data is collected from a sample of university students who use ChatGPT regularly across two Asian countries. The data analysis is done using Partial Least Squares Structural Equation Modelling. Results indicate that among the technical factors only perceived usefulness successfully predicts ChatGPT usage. Both the contextual factors of humanness and novelty use significantly explain ChatGPT usage. Finally, among the psychological factors’ openness, agreeableness, and neuroticism determine the usage scenario, however, the later two are found to be negatively associated with ChatGPT usage.","2023","2025-11-25 22:29:31","2025-11-25 22:29:31","","","","","","","","","IAIT '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Bangkok, Thailand","","","","ChatGPT; personality; higher education; novelty value; perceived humanness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XUL49TRC","conferencePaper","2024","Tsai, Yunda; Liu, Mingjie; Ren, Haoxing","RTLFixer: Automatically Fixing RTL Syntax Errors with Large Language Model","Proceedings of the 61st ACM/IEEE Design Automation Conference","979-8-4007-0601-1","","10.1145/3649329.3657353","https://doi.org/10.1145/3649329.3657353","This paper presents RTLFixer, a novel framework enabling automatic syntax errors fixing for Verilog code with Large Language Models (LLMs). Despite LLM's promising capabilities, our analysis indicates that approximately 55% of errors in LLM-generated Verilog are syntax-related, leading to compilation failures. To tackle this issue, we introduce a novel debugging framework that employs Retrieval-Augmented Generation (RAG) and ReAct prompting, enabling LLMs to act as autonomous agents in interactively debugging the code with feedback. This framework demonstrates exceptional proficiency in resolving syntax errors, successfully correcting about 98.5% of compilation errors in our debugging dataset, comprising 212 erroneous implementations derived from the VerilogEval benchmark. Our method leads to 32.3% and 10.1% increase in pass@1 success rates in the VerilogEval-Machine and VerilogEval-Human benchmarks, respectively. The source code and benchmark are available at https://github.com/NVlabs/RTLFixer.","2024","2025-11-25 22:29:31","2025-11-25 22:29:31","","","","","","","","","DAC '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: San Francisco, CA, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DBZAQ923","conferencePaper","2024","Xiao, Danni; Guo, Yimeng; Li, Yanhui; Chen, Lin","Optimizing Search-Based Unit Test Generation with Large Language Models: An Empirical Study","Proceedings of the 15th Asia-Pacific Symposium on Internetware","979-8-4007-0705-6","","10.1145/3671016.3674813","https://doi.org/10.1145/3671016.3674813","Search-based unit test generation methods have been considered effective and widely applied, and Large Language Models (LLMs) have also demonstrated their powerful generation ability. Therefore, some scholars have proposed using LLMs to enhance search-based unit test generation methods and have preliminarily confirmed that LLMs can help alleviate the problem of test coverage plateaus. However, it is still unclear when and how LLMs should intervene in the time-consuming test generation process. This paper explores the application of LLMs at various stages of search-based test generation (SBTG) (including the initial stage, the test generation period, and the test coverage plateaus), as well as strategies for controlling the frequency of LLM intervention. A comprehensive empirical study was conducted on 486 Python benchmark modules from 27 projects. The experimental results show that 1) LLM intervention has a positive effect at any stage, whether to improve coverage over a fixed period or to reduce the time to reach a specific coverage; 2) a reasonable intervention frequency is crucial for LLMs to have a positive effect on SBTG. This work can better help understand when and how LLMs should be applied in SBTG and provide valuable suggestions for developers in practice.","2024","2025-11-25 22:29:31","2025-11-25 22:29:31","","71–80","","","","","","","Internetware '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Macau, China","","","","Large Language Model; Search-based Testing; Unit Test","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9PSD3BET","conferencePaper","2025","Duan, Zhangqi; Fernandez, Nigel; Hicks, Alexander; Lan, Andrew","Test Case-Informed Knowledge Tracing for Open-ended Coding Tasks","Proceedings of the 15th International Learning Analytics and Knowledge Conference","979-8-4007-0701-8","","10.1145/3706468.3706500","https://doi.org/10.1145/3706468.3706500","Open-ended coding tasks, which ask students to construct programs according to certain specifications, are common in computer science education. Student modeling can be challenging since their open-ended nature means that student code can be diverse. Traditional knowledge tracing (KT) models that only analyze response correctness may not fully capture nuances in student knowledge from student code. In this paper, we introduce Test case-Informed Knowledge Tracing for Open-ended Coding (TIKTOC), a framework to simultaneously analyze and predict both open-ended student code and whether the code passes each test case. We augment the existing CodeWorkout dataset with the test cases used for a subset of the open-ended coding questions, and propose a multi-task learning KT method to simultaneously analyze and predict 1) whether a student’s code submission passes each test case and 2) the student’s open-ended code, using a large language model as the backbone. We quantitatively show that these methods outperform existing KT methods for coding that only use the overall score a code submission receives. We also qualitatively demonstrate how test case information, combined with open-ended code, helps us gain fine-grained insights into student knowledge.","2025","2025-11-25 22:29:31","2025-11-25 22:29:31","","238–248","","","","","","","LAK '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Large Language Models; Computer Science Education; Open-ended Coding Questions; Test Cases","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SQ8Z5WNQ","journalArticle","2024","Shankar, Shreya; Li, Haotian; Asawa, Parth; Hulsebos, Madelon; Lin, Yiming; Zamfirescu-Pereira, J. D.; Chase, Harrison; Fu-Hinthorn, Will; Parameswaran, Aditya G.; Wu, Eugene","spade: Synthesizing Data Quality Assertions for Large Language Model Pipelines","Proc. VLDB Endow.","","2150-8097","10.14778/3685800.3685835","https://doi.org/10.14778/3685800.3685835","Large language models (LLMs) are being increasingly deployed as part of pipelines that repeatedly process or generate data of some sort. However, a common barrier to deployment are the frequent and often unpredictable errors that plague LLMs. Acknowledging the inevitability of these errors, we propose data quality assertions to identify when LLMs may be making mistakes. We present spade, a method for automatically synthesizing data quality assertions that identify bad LLM outputs. We make the observation that developers often identify data quality issues during prototyping prior to deployment, and attempt to address them by adding instructions to the LLM prompt over time. spade therefore analyzes histories of prompt versions over time to create candidate assertion functions and then selects a minimal set that fulfills both coverage and accuracy requirements. In testing across nine different real-world LLM pipelines, spade efficiently reduces the number of assertions by 14% and decreases false failures by 21% when compared to simpler baselines. spade has been deployed as an offering within LangSmith, LangChain's LLM pipeline hub, and has been used to generate data quality assertions for over 2000 pipelines across a spectrum of industries.","2024-08","2025-11-25 22:29:31","2025-11-25 22:29:31","","4173–4186","","12","17","","","","","","","","","","","","","","","","","Publisher: VLDB Endowment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LHZZDRW3","conferencePaper","2025","Prather, James; Leinonen, Juho; Kiesler, Natalie; Gorson Benario, Jamie; Lau, Sam; MacNeil, Stephen; Norouzi, Narges; Opel, Simone; Pettit, Vee; Porter, Leo; Reeves, Brent N.; Savelka, Jaromir; Smith, IV, David H.; Strickroth, Sven; Zingaro, Daniel","Beyond the Hype: A Comprehensive Review of Current Trends in Generative AI Research, Teaching Practices, and Tools","2024 Working Group Reports on Innovation and Technology in Computer Science Education","979-8-4007-1208-1","","10.1145/3689187.3709614","https://doi.org/10.1145/3689187.3709614","Generative AI (GenAI) is advancing rapidly, and the literature in computing education is expanding almost as quickly. Initial responses to GenAI tools were mixed between panic and utopian optimism. Many were fast to point out the opportunities and challenges of GenAI. Researchers reported that these new tools are capable of solving most introductory programming tasks and are causing disruptions throughout the curriculum. These tools can write and explain code, enhance error messages, create resources for instructors, and even provide feedback and help for students like a traditional teaching assistant. In 2024, new research started to emerge on the effects of GenAI usage in the computing classroom. These new data involve the use of GenAI to support classroom instruction at scale and to teach students how to code with GenAI. In support of the former, a new class of tools is emerging that can provide personalized feedback to students on their programming assignments or teach both programming and prompting skills at the same time. With the literature expanding so rapidly, this report aims to summarize and explain what is happening on the ground in computing classrooms. We provide a systematic literature review; a survey of educators and industry professionals; and interviews with educators using GenAI in their courses, educators studying GenAI, and researchers who create GenAI tools to support computing education. The triangulation of these methods and data sources expands the understanding of GenAI usage and perceptions at this critical moment for our community.","2025","2025-11-25 22:29:31","2025-11-25 22:29:31","","300–338","","","","","","","ITiCSE 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Milan, Italy","","","","large language models; artificial intelligence; generative ai; computing education; pedagogical practices; genai; teaching computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HQV9QBGY","journalArticle","2024","Jiang, Weipeng; Zhai, Juan; Ma, Shiqing; Zhang, Xiaoyu; Shen, Chao","COS℡LO: Contrastive Testing for Embedding-Based Large Language Model as a Service Embeddings","Proc. ACM Softw. Eng.","","","10.1145/3643767","https://doi.org/10.1145/3643767","Large language models have gained significant popularity and are often provided as a service (i.e., LLMaaS). Companies like OpenAI and Google provide online APIs of LLMs to allow downstream users to create innovative applications. Despite its popularity, LLM safety and quality assurance is a well-recognized concern in the real world, requiring extra efforts for testing these LLMs. Unfortunately, while end-to-end services like ChatGPT have garnered rising attention in terms of testing, the LLMaaS embeddings have comparatively received less scrutiny. We state the importance of testing and uncovering problematic individual embeddings without considering downstream applications. The abstraction and non-interpretability of embedded vectors, combined with the black-box inaccessibility of LLMaaS, make testing a challenging puzzle. This paper proposes COS℡LO, a black-box approach to reveal potential defects in abstract embedding vectors from LLMaaS by contrastive testing. Our intuition is that high-quality LLMs can adequately capture the semantic relationships of the input texts and properly represent their relationships in the high-dimensional space. For the given interface of LLMaaS and seed inputs, COS℡LO can automatically generate test suites and output words with potential problematic embeddings. The idea is to synthesize contrastive samples with guidance, including positive and negative samples, by mutating seed inputs. Our synthesis guide will leverage task-specific properties to control the mutation procedure and generate samples with known partial relationships in the high-dimensional space. Thus, we can compare the expected relationship (oracle) and embedding distance (output of LLMs) to locate potential buggy cases. We evaluate COS℡LO on 42 open-source (encoder-based) language models and two real-world commercial LLMaaS. Experimental results show that COS℡LO can effectively detect semantic violations, where more than 62% of violations on average result in erroneous behaviors (e.g., unfairness) of downstream applications.","2024-07","2025-11-25 22:29:32","2025-11-25 22:29:32","","","","FSE","1","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Contrastive Testing; Embeddings; LLMaaS","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TJTT4VRB","conferencePaper","2025","Al Hashimi, Hussein Ali","An Empirical Exploration of Cybersecurity Threats and Mitigation Strategies in Software Testing","Proceedings of the 2025 14th International Conference on Software and Computer Applications","979-8-4007-1012-4","","10.1145/3731806.3731851","https://doi.org/10.1145/3731806.3731851","Software testing is a fundamental process of software development since it ensures the superior quality of the software products developed and that they are delivered without defects and with the required functionality. As software applications become increasingly integral to business operations and everyday life, the proliferation of cyber threats poses significant threats that can compromise data integrity, user privacy, and overall system functionality. This study investigates common cybersecurity threats, including SQL injection, cross-site scripting, and vulnerabilities associated with third-party libraries, by analyzing data collected from surveys with industry professionals alongside a review of existing literature. The study's findings reveal a critical gap between awareness of cybersecurity threats and the effective implementation of security practices within the testing lifecycle. While many organizations recognize the importance of cybersecurity, there is often a lack of structured methodologies to incorporate security testing alongside functional testing. The paper identifies key factors influencing the successful integration of security measures, including organizational culture, resource allocation, and the use of automated testing tools. This paper proposes a framework by using an online questionnaire survey to enhance cybersecurity resilience during software testing, emphasizing best practices such as threat modeling, continuous security integration, and the adoption of DevSecOps methodologies. This framework aims to foster a proactive security mindset among development and testing teams, ensuring that security is not an afterthought but an integral component of the software development lifecycle. By shedding light on the current state of cybersecurity in software testing and offering actionable insights for practitioners, this research contributes to the growing body of knowledge in the field. The paper ultimately aims to promote safer digital environments by equipping organizations with the necessary tools and strategies to mitigate cybersecurity threats in their software applications effectively.","2025","2025-11-25 22:29:32","2025-11-25 22:29:32","","96–106","","","","","","","ICSCA '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Software Testing; Cybersecurity; Empirical Survey; Security Threats and Practices","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GZC28TRQ","bookSection","2025","Deng, Wenqian; Liang, Jie; Wu, Zhiyong; Fu, Jingzhou; Wang, Mingzhe; Jiang, Yu","Coni: Detecting Database Connector Bugs via State-Aware Test Case Generation","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00003","Database connectors are widely used in many applications to facilitate flexible and convenient database interactions. Potential bugs in database connectors can lead to various abnormal behaviors within applications, such as returning incorrect results or experiencing unexpected connection interruption. However, existing DBMS fuzzing works cannot be directly applied to testing database connectors as they mainly focus on SQL generation and use a small subset of connector interfaces. Automated test case generation also struggles to generate effective test cases that explore intricate interactions of database connectors due to a lack of domain knowledge.The main challenge in testing database connectors is generating semantically correct test cases that can trigger various connector state transitions. To address that, we propose Coni, a framework designed for detecting logic bugs of database connectors with state-aware test case generation. First, we define the database connector state model by analyzing the corresponding standard specification. Building upon this model, Coni generates interface call sequences within test cases to encompass various state transitions. After that, Coni generates suitable parameter values based on the parameter information and contextual information collected during runtime. Then the test cases are executed on a target and a reference database connector. Inconsistent results indicate potential bugs. We evaluated Coni on 5 widely-used JDBC database connectors, namely MySQL Connector/J, MariaDB Connector/J, AWS JDBC Driver for MySQL, PGJDBC, and PG JDBC NG. In total, Coni reported 44 previously unknown bugs, of which 34 have been confirmed.","2025","2025-11-25 22:29:32","2025-11-25 22:29:32","","667–678","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3EE259BI","conferencePaper","2024","Denny, Paul; Leinonen, Juho; Prather, James; Luxton-Reilly, Andrew; Amarouche, Thezyrie; Becker, Brett A.; Reeves, Brent N.","Prompt Problems: A New Programming Exercise for the Generative AI Era","Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1","979-8-4007-0423-9","","10.1145/3626252.3630909","https://doi.org/10.1145/3626252.3630909","Large language models (LLMs) are revolutionizing the field of computing education with their powerful code-generating capabilities. Traditional pedagogical practices have focused on code writing tasks, but there is now a shift in importance towards reading, comprehending and evaluating LLM-generated code. Alongside this shift, an important new skill is emerging – the ability to solve programming tasks by constructing good prompts for code-generating models. In this work we introduce a new type of programming exercise to hone this nascent skill: 'Prompt Problems'. Prompt Problems are designed to help students learn how to write effective prompts for AI code generators. A student solves a Prompt Problem by crafting a natural language prompt which, when provided as input to an LLM, outputs code that successfully solves a specified programming task. We also present a new web-based tool called Promptly which hosts a repository of Prompt Problems and supports the automated evaluation of prompt-generated code. We deploy Promptly in one CS1 and one CS2 course and describe our experiences, which include student perceptions of this new type of activity and their interactions with the tool. We find that students are enthusiastic about Prompt Problems, and appreciate how the problems engage their computational thinking skills and expose them to new programming constructs. We discuss ideas for the future development of new variations of Prompt Problems, and the need to carefully study their integration into classroom practice.","2024","2025-11-25 22:29:32","2025-11-25 22:29:32","","296–302","","","","","","","SIGCSE 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Portland, OR, USA","","","","prompt engineering; large language models; artificial intelligence; generative ai; llms; ai code generation; prompt problems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DTKXMCTG","journalArticle","2024","Wen, Cheng; Cai, Yuandao; Zhang, Bin; Su, Jie; Xu, Zhiwu; Liu, Dugang; Qin, Shengchao; Ming, Zhong; Cong, Tian","Automatically Inspecting Thousands of Static Bug Warnings with Large Language Model: How Far Are We?","ACM Trans. Knowl. Discov. Data","","1556-4681","10.1145/3653718","https://doi.org/10.1145/3653718","Static analysis tools for capturing bugs and vulnerabilities in software programs are widely employed in practice, as they have the unique advantages of high coverage and independence from the execution environment. However, existing tools for analyzing large codebases often produce a great deal of false warnings over genuine bug reports. As a result, developers are required to manually inspect and confirm each warning, a challenging, time-consuming, and automation-essential task.This article advocates a fast, general, and easily extensible approach called Llm4sa that automatically inspects a sheer volume of static warnings by harnessing (some of) the powers of Large Language Models (LLMs). Our key insight is that LLMs have advanced program understanding capabilities, enabling them to effectively act as human experts in conducting manual inspections on bug warnings with their relevant code snippets. In this spirit, we propose a static analysis to effectively extract the relevant code snippets via program dependence traversal guided by the bug warning reports themselves. Then, by formulating customized questions that are enriched with domain knowledge and representative cases to query LLMs, Llm4sa can remove a great deal of false warnings and facilitate bug discovery significantly. Our experiments demonstrate that Llm4sa is practical in automatically inspecting thousands of static warnings from Juliet benchmark programs and 11 real-world C/C++ projects, showcasing a high precision (81.13%) and a recall rate (94.64%) for a total of 9,547 bug warnings. Our research introduces new opportunities and methodologies for using the LLMs to reduce human labor costs, improve the precision of static analyzers, and ensure software trustworthiness","2024-06","2025-11-25 22:29:32","2025-11-25 22:29:32","","","","7","18","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","static analysis; Large language model; AI for program analysis; false alarms; static bug warning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9BC4K9NR","conferencePaper","2024","Liu, Zhe; Chen, Chunyang; Wang, Junjie; Chen, Mengzhuo; Wu, Boyu; Tian, Zhilin; Huang, Yuekai; Hu, Jun; Wang, Qing","Testing the Limits: Unusual Text Inputs Generation for Mobile App Crash Detection with Large Language Model","Proceedings of the IEEE/ACM 46th International Conference on Software Engineering","979-8-4007-0217-4","","10.1145/3597503.3639118","https://doi.org/10.1145/3597503.3639118","Mobile applications have become a ubiquitous part of our daily life, providing users with access to various services and utilities. Text input, as an important interaction channel between users and applications, plays an important role in core functionality such as search queries, authentication, messaging, etc. However, certain special text (e.g., -18 for Font Size) can cause the app to crash, and generating diversified unusual inputs for fully testing the app is highly demanded. Nevertheless, this is also challenging due to the combination of explosion dilemma, high context sensitivity, and complex constraint relations. This paper proposes InputBlaster which leverages the LLM to automatically generate unusual text inputs for mobile app crash detection. It formulates the unusual inputs generation problem as a task of producing a set of test generators, each of which can yield a batch of unusual text inputs under the same mutation rule. In detail, InputBlaster leverages LLM to produce the test generators together with the mutation rules serving as the reasoning chain, and utilizes the in-context learning schema to demonstrate the LLM with examples for boosting the performance. InputBlaster is evaluated on 36 text input widgets with cash bugs involving 31 popular Android apps, and results show that it achieves 78% bug detection rate, with 136% higher than the best baseline. Besides, we integrate it with the automated GUI testing tool and detect 37 unseen crashes in real-world apps.","2024","2025-11-25 22:29:32","2025-11-25 22:48:00","","","","","","","","","ICSE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","large language model; Computer bugs; Cognition; Large language model; Android GUI testing; in-context learning; Mobile applications; Generators; Task analysis; Sensitivity; Maintenance engineering; In-context learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9KJJ5255","conferencePaper","2025","Borghoff, Uwe M.; Minas, Mark; Schopp, Jannis","Generative AI in Student Software Development Projects: A User Study on Experiences and Self-Assessment","Proceedings of the 6th European Conference on Software Engineering Education","979-8-4007-1282-1","","10.1145/3723010.3723012","https://doi.org/10.1145/3723010.3723012","The way software is developed is changing rapidly due to the general availability of generative AI tools. As a result, the software engineering education that is part of every computer science program needs to change. Especially in software engineering courses, such AI tools need to be used and practiced in a meaningful and useful way. The programming project is one such course at our university, and the curriculum will be expanded accordingly in the future. In this paper we describe our approach and a user study among the participants of the last programming project, in which we collected experiences with the use of current AI tools, in particular highlighting their usefulness and limitations. Our study focuses on identifying which aspects of the course students used AI tools for, evaluating successful applications, and uncovering remaining challenges.","2025","2025-11-25 22:29:32","2025-11-25 22:29:32","","161–170","","","","","","","ECSEE '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","experiments; AI support; AI-based tutoring; software development project course; software engineering education","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PNW9S4SF","conferencePaper","2024","Su, Yanqi; Liao, Dianshu; Xing, Zhenchang; Huang, Qing; Xie, Mulong; Lu, Qinghua; Xu, Xiwei","Enhancing Exploratory Testing by Large Language Model and Knowledge Graph","Proceedings of the IEEE/ACM 46th International Conference on Software Engineering","979-8-4007-0217-4","","10.1145/3597503.3639157","https://doi.org/10.1145/3597503.3639157","Exploratory testing leverages the tester's knowledge and creativity to design test cases for effectively uncovering system-level bugs from the end user's perspective. Researchers have worked on test scenario generation to support exploratory testing based on a system knowledge graph, enriched with scenario and oracle knowledge from bug reports. Nevertheless, the adoption of this approach is hindered by difficulties in handling bug reports of inconsistent quality and varied expression styles, along with the infeasibility of the generated test scenarios. To overcome these limitations, we utilize the superior natural language understanding (NLU) capabilities of Large Language Models (LLMs) to construct a System KG of User Tasks and Failures (SysKG-UTF). Leveraging the system and bug knowledge from the KG, along with the logical reasoning capabilities of LLMs, we generate test scenarios with high feasibility and coherence. Particularly, we design chain-of-thought (CoT) reasoning to extract human-like knowledge and logical reasoning from LLMs, simulating a developer's process of validating test scenario feasibility. Our evaluation shows that our approach significantly enhances the KG construction, particularly for bug reports with low quality. Furthermore, our approach generates test scenarios with high feasibility and coherence. The user study further proves the effectiveness of our generated test scenarios in supporting exploratory testing. Specifically, 8 participants find 36 bugs from 8 seed bugs in two hours using our test scenarios, a significant improvement over the 21 bugs found by the state-of-the-art baseline.","2024","2025-11-25 22:29:32","2025-11-25 22:29:32","","","","","","","","","ICSE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","prompt engineering; AI chain; exploratory testing; knowledge graph","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZZJ2XAHE","conferencePaper","2024","Di, Peng; Li, Jianguo; Yu, Hang; Jiang, Wei; Cai, Wenting; Cao, Yang; Chen, Chaoyu; Chen, Dajun; Chen, Hongwei; Chen, Liang; Fan, Gang; Gong, Jie; Gong, Zi; Hu, Wen; Guo, Tingting; Lei, Zhichao; Li, Ting; Li, Zheng; Liang, Ming; Liao, Cong; Liu, Bingchang; Liu, Jiachen; Liu, Zhiwei; Lu, Shaojun; Shen, Min; Wang, Guangpei; Wang, Huan; Wang, Zhi; Xu, Zhaogui; Yang, Jiawei; Ye, Qing; Zhang, Gehao; Zhang, Yu; Zhao, Zelin; Zheng, Xunjin; Zhou, Hailian; Zhu, Lifu; Zhu, Xianying","CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model","Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice","979-8-4007-0501-4","","10.1145/3639477.3639719","https://doi.org/10.1145/3639477.3639719","Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM 2. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high-quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodefuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software development process where CodeFuse has been successfully deployed. The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes. In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts.","2024","2025-11-25 22:29:32","2025-11-25 22:47:09","","418–429","","","","","","","ICSE-SEIP '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","Software engineering; Industries; Codes; Benchmark testing; chinese prompts; code large language models; multi-lingual; Training; Task analysis; Computer languages; Chinese prompts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DX56YZ39","conferencePaper","2025","Gong, Rushi; Jiang, Rui; Guo, Chuanlei; Hu, Wanqing; Li, Yanyan","Roles emerging during the knowledge construction process in collaborative learning: Does a generative AI-support chatbot matter?","Proceedings of the 2024 16th International Conference on Education Technology and Computers","979-8-4007-1781-9","","10.1145/3702163.3702165","https://doi.org/10.1145/3702163.3702165","Students’ emerging roles in computer supported collaborative learning (CSCL) are crucial in revealing what learning characteristics and states students present during their collaborative knowledge construction. Previous researchers have unveiled the fact that pedagogical scaffoldings such as AI chatbots play a pivotal role in students’ role emerging, but with the prevalence of generative AI (GAI), there is also an urgent need to investigate whether GAI chatbots influence students’ emerging roles during the knowledge construction process in collaborative learning. Therefore, this study conducted a quasi-experiment, using an integration of cluster analysis, chi-square test, case analysis, and content analysis to investigate whether and how a GAI chatbot affected students’ emerging roles in their online collaborative knowledge construction. Results demonstrated statistical significance that the GAI chatbot and the traditional static scripts did not have a distinct difference in students’ emerging roles. However, qualitative data showed that the GAI chatbot had an impact on the allocation of roles and that there were perceptual differences in how students with the same roles experienced the writing process and collaborative atmosphere under different support conditions. The study will provide insights into how GAI chatbots can be adapted for future development and application in a collaborative learning context with consideration of students’ roles.","2025","2025-11-25 22:29:32","2025-11-25 22:29:32","","8–16","","","","","","","ICETC '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Computer Supported Collaborative Learning; Generative AI Chatbot; Knowledge Construction; Students’ Roles","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WEUQANEH","conferencePaper","2025","Ou, Xianfei; Li, Cong; Jiang, Yanyan; Xu, Chang","The Mutators Reloaded: Fuzzing Compilers with Large Language Model Generated Mutation Operators","Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4","979-8-4007-0391-1","","10.1145/3622781.3674171","https://doi.org/10.1145/3622781.3674171","Crafting high-quality mutators-the core of mutation-based fuzzing that shapes the search space-is challenging. It requires human expertise and creativity, and their implementation demands knowledge of compiler internals. This paper presents MetaMut framework for developing new, useful mutators for compiler fuzzing. It integrates our compiler-domain knowledge into prompts and processes that can best harness the capabilities of a large language model. With MetaMut, we have successfully created 118 semantic-aware mutators at approximately $0.5 each, with only moderate human effort. With these mutators, our fuzzer uncovered 131 bugs in GCC and Clang, 129 of which were confirmed or fixed. The success of MetaMut suggests that the integration of AI into software and system engineering tasks traditionally thought to require expert human intervention could be a promising research direction.","2025","2025-11-25 22:29:32","2025-11-25 22:29:32","","298–312","","","","","","","ASPLOS '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Hilton La Jolla Torrey Pines, La Jolla, CA, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TXZ372TG","conferencePaper","2025","Vasilevski, Kirill; Rajbahadur, Gopi Krishnan; Oliva, Gustavo A.; Rombaut, Benjamin; Gallaba, Keheliya; Cogo, Filipe R.; Lin, Jiahuei (Justina); Lin, Dayi; Zhang, Haoxiang; Chen, Bouyan; Thangarajah, Kishanthan; Hassan, Ahmed E.; Jiang, Zhen Ming (Jack)","The Hitchhikers Guide to Production-ready Trustworthy Foundation Model Powered Software (FMware)","Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2","979-8-4007-1454-2","","10.1145/3711896.3736572","https://doi.org/10.1145/3711896.3736572","Foundation Models (FMs) such as Large Language Models (LLMs) are reshaping the software industry by enabling FMware, systems that integrate these FMs as core components. In this KDD 2025 tutorial, we present a comprehensive exploration of FMware that combines a curated catalogue of challenges with real-world production concerns. We first discuss the state of research and practice in building FMware. We further examine the difficulties in selecting suitable models, aligning high-quality domain-specific data, engineering robust prompts, and orchestrating autonomous agents. We then address the complex journey from impressive demos to production-ready systems by outlining issues in system testing, optimization, deployment, and integration with legacy software. Drawing on our industrial experience and recent research in the area, we provide actionable insights and a technology roadmap for overcoming these challenges. Attendees will gain practical strategies to enable the creation of trustworthy FMware in the evolving technology landscape.","2025","2025-11-25 22:29:32","2025-11-25 22:29:32","","6162–6172","","","","","","","KDD '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Toronto ON, Canada","","","","software engineering; llms; foundation models; production ready systems; productioniztion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IH25W8AR","conferencePaper","2023","Cardoso, Ana Paula; Santos, Cleicy Priscilla; Collins, Eliane; Lima, Kelen; Quiroga, Pablo; Griego, Marlon","Evaluation of Automatic Test Case Generation for the Android Operating System using Deep Reinforcement Learning","Proceedings of the XXII Brazilian Symposium on Software Quality","979-8-4007-0786-5","","10.1145/3629479.3629503","https://doi.org/10.1145/3629479.3629503","The industry of large-scale software for mobile devices, such as the Android operating system, presents significant challenges regarding software validation and testing. This is due to the need to test on various devices, operating system versions, connections, and different hardware configurations. As a result, the manual creation of test cases can be a time-consuming process, and test cases can become outdated with updates in the Android version. To tackle these challenges, automatic test case generation emerges as an effective solution to streamline test creation and updates. In this context, Artificial Intelligence (AI) techniques, such as Deep Reinforcement Learning (DRL), have been explored to optimize this process and ensure adequate coverage of system requirements. This study evaluated the performance of the DRL state-of-the-art tool for test case generation DRL-MOBTEST [3] in an industry scenario context to generate test cases for Android functional applications (apps). The tool was performed in nine native apps (clock, maps, calculator, wallpaper, calendar, contacts, YouTube, drive, and files) regarding the functionalities coverage. The results showed a coverage range of 74.43%, and we compared it with the random Android SDK tool Monkey in five applications, revealing a trend of 63.52% improvement. The DRL-MOBTEST tool achieved the coverage of basic application paths through the creation of different test input types, such as symbols, numbers, and letters. It enables professionals to focus on complex scenarios and improve software quality across different devices and hardware configurations. However, it’s worth noting that human supervision is still necessary despite the advances offered by automated tools.","2023","2025-11-25 22:29:32","2025-11-25 22:29:32","","228–235","","","","","","","SBQS '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Brasília, Brazil","","","","Automatic Test Generation; AI; Reinforcement Learning; Android; Test Automation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RKYKL77G","journalArticle","2024","Alrifai, Rad","Using Generative AI to Design Programming Assignments in Introduction to Computer Science","J. Comput. Sci. Coll.","","1937-4771","","","Programming stands as an essential requisite in computer science education. Recognizing the challenges students face in learning programming effectively, the proposed assignment aims to integrate generative artificial intelligence (AI) tools to teach students introductory programming constructs. Generative AI has gained an increasing popularity in recent years. Several available Generative AI implementations can now help students learn programming essentials and debugging skills.","2024-04","2025-11-25 22:29:32","2025-11-25 22:29:32","","103–106","","6","39","","","","","","","","","","","","","","","","","Place: Evansville, IN, USA Publisher: Consortium for Computing Sciences in Colleges","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"73Q9IGFV","conferencePaper","2024","Li, Xinjian; Chen, Xiaohu; Wang, Chengtao; Chen, Fei","Research on Automatic Classification and Grading of Industrial Data Based on Large Language Model","Proceedings of the 2023 4th International Conference on Big Data Economy and Information Management","979-8-4007-1666-9","","10.1145/3659211.3659335","https://doi.org/10.1145/3659211.3659335","Classification and grading of data in the industrial field,It is the first task of data-security governance.This paper revolves around classification and grading of industrial data,which is not enough automation and refinement and defect of huge labor input,finding ways to change the definition of traditional isolated fields for classification and grading by using the Large Language Model.The data is graded and weighted according to the semantic association analysis of horizontal fields, the sharing of vertical cumulative fields, and the vertical inheritance of artificial classification and grading base for clustering.By calling the ChatGPT interface for experiment, common personnel data and data of cigarette industrial packages and silk in tobacco companies are selected for verification, which is more refined than traditional classification, and the classification is more conducive to business data sharing. .","2024","2025-11-25 22:29:32","2025-11-25 22:29:32","","719–725","","","","","","","BDEIM '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Zhengzhou, China","","","","Large Language Model; Classification and grading of data; Data security; Industrial data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AP3MU9KU","conferencePaper","2024","Shi, Haochen; Sun, Zhiyuan; Yuan, Xingdi; Côté, Marc-Alexandre; Liu, Bang","OPEx: A Large Language Model-Powered Framework for Embodied Instruction Following","Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems","979-8-4007-0486-4","","","","Embodied Instruction Following (EIF) is crucial for understanding natural language in a practical context, requiring agents to follow verbal instructions for complex tasks. Traditionally, EIF relies heavily on expert annotations for learning, which are costly and sometimes unattainable. Recent research shows Large Language Models (LLMs) can use their reasoning ability to help in EIF with minimal examples, but applying LLMs directly faces issues like hallucinations and partially observable environment. To bridge the gap, we introduce OPEx, a new LLM-based method for EIF that needs far less specific data. OPEx uses three LLMs for different roles: observing to gather environment data, planning by breaking down instructions, and executing tasks with learned skills. Our tests reveal OPEx significantly outperforms the FILM baseline, with 90% less training data for planning tasks and achieving up to 38% performance gain when FILM is trained on identical data.","2024","2025-11-25 22:29:32","2025-11-25 22:29:32","","2465–2467","","","","","","","AAMAS '24","","","","International Foundation for Autonomous Agents and Multiagent Systems","Richland, SC","","","","","","","","event-place: Auckland, New Zealand","","","","large language models; embodied instruction following; grounded planning; in context learning; language grounding","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"843LZTV2","journalArticle","2025","Wen, Jin; Hu, Qiang; Guo, Yuejun; Cordy, Maxime; Le Traon, Yves","Variable Renaming-Based Adversarial Test Generation for Code Model: Benchmark and Enhancement","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3723353","https://doi.org/10.1145/3723353","Robustness testing is essential for evaluating deep learning models, particularly under unforeseen circumstances. Adversarial test generation, a fundamental approach in robustness testing, is prevalent in computer vision and natural language processing, and it has gained considerable attention in code tasks recently. The Variable Renaming-Based Adversarial Test Generation (VRTG), which deceives models by altering variable names, is a key focus. VRTG involves substitution construction and variable name searching, but its systematic design remains a challenge due to the empirical nature of these components. This paper introduces the first benchmark to examine the impact of various substitutions and search algorithms on VRTG effectiveness, exploring improvements for existing VRTGs. Our benchmark includes three substitution construction types, six substitution position rank ways and seven search algorithms. Analysis of four code understanding tasks and three pre-trained code models using our benchmark reveals that combining RNNS and Genetic Algorithm with code-based substitution is more effective for VRTG construction. Notably, this method outperforms the advanced black-box variable renaming test generation technique, ALERT, by up to 22.57%.","2025-03","2025-11-25 22:29:32","2025-11-25 22:29:32","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F7WP4BS5","book","2024","","ISSTA 2024: Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","","979-8-4007-0612-7","","","","Welcome to the 33rd edition of the International Symposium on Software Testing and Analysis, ISSTA 2024, held on September 16–20, 2024 in Vienna, Austria. ISSTA 2024 is co-located with ECOOP and MPLR 2024. ISSTA brings together academics, industrial researchers, and practitioners from all over the world working on testing and analyzing software systems.","2024","2025-11-25 22:29:32","2025-11-25 22:29:32","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LRHA8DYR","journalArticle","2025","Huang, Li; Sun, Weifeng; Yan, Meng; Liu, Zhongxin; Lei, Yan; Lo, David","Neuron Semantic-Guided Test Generation for Deep Neural Networks Fuzzing","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3688835","https://doi.org/10.1145/3688835","In recent years, significant progress has been made in testing methods for deep neural networks (DNNs) to ensure their correctness and robustness. Coverage-guided criteria, such as neuron-wise, layer-wise, and path-/trace-wise, have been proposed for DNN fuzzing. However, existing coverage-based criteria encounter performance bottlenecks for several reasons: ❶ Testing Adequacy: Partial neural coverage criteria have been observed to achieve full coverage using only a small number of test inputs. In this case, increasing the number of test inputs does not consistently improve the quality of models. ❷ Interpretability: The current coverage criteria lack interpretability. Consequently, testers are unable to identify and understand which incorrect attributes or patterns of the model are triggered by the test inputs. This lack of interpretability hampers the subsequent debugging and fixing process. Therefore, there is an urgent need for a novel fuzzing criterion that offers improved testing adequacy, better interpretability, and more effective failure detection capabilities for DNNs.To alleviate these limitations, we propose NSGen, an approach for DNN fuzzing that utilizes neuron semantics as guidance during test generation. NSGen identifies critical neurons, translates their high-level semantic features into natural language descriptions, and then assembles them into human-readable DNN decision paths (representing the internal decision of the DNN). With these decision paths, we can generate more fault-revealing test inputs by quantifying the similarity between original test inputs and mutated test inputs for fuzzing. We evaluate NSGen on popular DNN models (VGG16_BN, ResNet50, and MobileNet_v2) using CIFAR10, CIFAR100, Oxford 102 Flower, and ImageNet datasets. Compared to 12 existing coverage-guided fuzzing criteria, NSGen outperforms all baselines, increasing the number of triggered faults by 21.4% to 61.2% compared to the state-of-the-art coverage-guided fuzzing criterion. This demonstrates NSGen's effectiveness in generating fault-revealing test inputs through guided input mutation, highlighting its potential to enhance DNN testing and interpretability.","2025-12","2025-11-25 22:29:32","2025-11-25 22:29:32","","","","1","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","fuzzing; Deep learning testing; test input generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F54F5MIS","conferencePaper","2024","Roberts, Jordan; Mohamed, Abdallah","Generative AI in CS Education: Literature Review through a SWOT Lens","Proceedings of the 26th Western Canadian Conference on Computing Education","979-8-4007-0997-5","","10.1145/3660650.3660657","https://doi.org/10.1145/3660650.3660657","The rapid growth of generative artificial intelligence (AI) models introduced challenges for educators, students and administrators across the academic sphere related to how to manage and regulate these tools. While some oppose their use, many researchers have begun to approach the topic of educational AI use from a different perspective. Despite being in its early stages; this field of research has produced notable insights into the capabilities and limitations of models like ChatGPT. This paper utilizes a SWOT analysis framework to analyze and consolidate existing literature, with a specific focus on Computer Science education. Through the analysis of this literature, we have created a set of use cases and guidelines to aid in the future development of strategies and tools within this field. Our findings indicate that while some concerns are valid, such as AI's ability to generate plagiarized work, we identified several promising avenues and opportunities for careful integration of this technology into education.","2024","2025-11-25 22:29:32","2025-11-25 22:29:32","","","","","","","","","WCCCE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Kelowna, BC, Canada","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FM5BQ235","conferencePaper","2024","Han, Yongqi; Du, Qingfeng; Huang, Ying; Wu, Jiaqi; Tian, Fulong; He, Cheng","The Potential of One-Shot Failure Root Cause Analysis: Collaboration of the Large Language Model and Small Classifier","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695475","https://doi.org/10.1145/3691620.3695475","Failure root cause analysis (RCA), which systematically identifies underlying faults, is essential for ensuring the reliability of widely adopted microservice-based applications and cloud-native systems. However, manual analysis by simple rules faces significant burdens due to the heterogeneous nature of resource entities and the massive amount of observability data. Furthermore, existing approaches for automating RCA struggle to perform in-depth fault analysis without extensive fault labels. To address the scarcity of fault labels, we examine an extreme RCA scenario where each fault type has only one example (one-shot). We propose LasRCA, a framework for one-hot RCA in cloud-native systems that leverages the collaboration of the large language model (LLM) and the small classifier. In the training stage, LasRCA initially trains a small classifier based on one-shot fault examples. The small classifier then iteratively selects high-confusion samples and receives feedback on their fault types from LLM-driven fault labeling. These samples are applied to retrain the small classifier. In the inference stage, LasRCA performs a joint RCA through the collaboration of the LLM and small classifier, achieving a trade-off between effectiveness and cost. Experiment results on public datasets with heterogeneous nature and prevalent fault types show the effectiveness of LasRCA in one-shot RCA.","2024","2025-11-25 22:29:32","2025-11-25 22:48:04","","931–943","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","Large language models; Software testing; Software engineering; Software reliability; Software performance; cloud-native systems; multimodal data; root cause analysis; Collaboration; Training; Cloud computing; Observability; Root cause analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YITWJDQG","conferencePaper","2023","Laaber, Christoph; Yue, Tao; Ali, Shaukat; Schwitalla, Thomas; Nygård, Jan F.","Automated Test Generation for Medical Rules Web Services: A Case Study at the Cancer Registry of Norway","Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","979-8-4007-0327-0","","10.1145/3611643.3613882","https://doi.org/10.1145/3611643.3613882","The Cancer Registry of Norway (CRN) collects, curates, and manages data related to cancer patients in Norway, supported by an interactive, human-in-the-loop, socio-technical decision support software system. Automated software testing of this software system is inevitable; however, currently, it is limited in CRN’s practice. To this end, we present an industrial case study to evaluate an AI-based system-level testing tool, i.e., EvoMaster, in terms of its effectiveness in testing CRN’s software system. In particular, we focus on GURI, CRN’s medical rule engine, which is a key component at the CRN. We test GURI with EvoMaster’s black-box and white-box tools and study their test effectiveness regarding code coverage, errors found, and domain-specific rule coverage. The results show that all EvoMaster tools achieve a similar code coverage; i.e., around 19% line, 13% branch, and 20% method; and find a similar number of errors; i.e., 1 in GURI’s code. Concerning domain-specific coverage, EvoMaster’s black-box tool is the most effective in generating tests that lead to applied rules; i.e., 100% of the aggregation rules and between 12.86% and 25.81% of the validation rules; and to diverse rule execution results; i.e., 86.84% to 89.95% of the aggregation rules and 0.93% to 1.72% of the validation rules pass, and 1.70% to 3.12% of the aggregation rules and 1.58% to 3.74% of the validation rules fail. We further observe that the results are consistent across 10 versions of the rules. Based on these results, we recommend using EvoMaster’s black-box tool to test GURI since it provides good results and advances the current state of practice at the CRN. Nonetheless, EvoMaster needs to be extended to employ domain-specific optimization objectives to improve test effectiveness further. Finally, we conclude with lessons learned and potential research directions, which we believe are applicable in a general context.","2023","2025-11-25 22:29:32","2025-11-25 22:29:32","","1937–1948","","","","","","","ESEC/FSE 2023","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: San Francisco, CA, USA","","","","test generation; automated software testing; cancer registry; electronic health records; REST APIs; rule engine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JXNZRT68","conferencePaper","2024","Xue, Zhiyi; Li, Liangguo; Tian, Senyue; Chen, Xiaohong; Li, Pingping; Chen, Liangyu; Jiang, Tingting; Zhang, Min","Domain Knowledge is All You Need: A Field Deployment of LLM-Powered Test Case Generation in FinTech Domain","Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings","979-8-4007-0502-1","","10.1145/3639478.3643087","https://doi.org/10.1145/3639478.3643087","Despite the promise of automation, general-purpose Large Language Models (LLMs) face difficulties in generating complete and accurate test cases from informal software requirements, primarily due to challenges in interpreting unstructured text and producing diverse, relevant scenarios. This paper argues that incorporating domain knowledge significantly improves LLM performance in test case generation. We report on the successful deployment of our LLM-powered tool, LLM4Fin, in the FinTech domain, showcasing the crucial role of domain knowledge in addressing the aforementioned challenges. We demonstrate two methods for integrating domain knowledge: implicit incorporation through model fine-tuning, and explicit incorporation with algorithm design. This combined approach delivers remarkable results, achieving up to 98.18% improvement in test scenario coverage and reducing generation time from 20 minutes to 7 seconds.","2024","2025-11-25 22:29:32","2025-11-25 22:47:15","","314–315","","","","","","","ICSE-Companion '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","Software; Software engineering; Automation; Accuracy; Faces; Large Language Models; Knowledge engineering; Natural Language; Software algorithms; Software Requirements; Test Case Generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AVVANU65","conferencePaper","2022","Sun, Jiao; Liao, Q. Vera; Muller, Michael; Agarwal, Mayank; Houde, Stephanie; Talamadupula, Kartik; Weisz, Justin D.","Investigating Explainability of Generative AI for Code through Scenario-based Design","Proceedings of the 27th International Conference on Intelligent User Interfaces","978-1-4503-9144-3","","10.1145/3490099.3511119","https://doi.org/10.1145/3490099.3511119","What does it mean for a generative AI model to be explainable? The emergent discipline of explainable AI (XAI) has made great strides in helping people understand discriminative models. Less attention has been paid to generative models that produce artifacts, rather than decisions, as output. Meanwhile, generative AI (GenAI) technologies are maturing and being applied to application domains such as software engineering. Using scenario-based design and question-driven XAI design approaches, we explore users’ explainability needs for GenAI in three software engineering use cases: natural language to code, code translation, and code auto-completion. We conducted 9 workshops with 43 software engineers in which real examples from state-of-the-art generative AI models were used to elicit users’ explainability needs. Drawing from prior work, we also propose 4 types of XAI features for GenAI for code and gathered additional design ideas from participants. Our work explores explainability needs for GenAI for code and demonstrates how human-centered approaches can drive the technical development of XAI in novel domains.","2022","2025-11-25 22:29:32","2025-11-25 22:29:32","","212–228","","","","","","","IUI '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Helsinki, Finland","","","","generative AI; explainable AI; human-centered AI; scenario based design; software engineering tooling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q5XM69D8","conferencePaper","2024","Katzy, Jonathan; Popescu, Razvan; Van Deursen, Arie; Izadi, Maliheh","An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets","Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering","979-8-4007-0609-7","","10.1145/3650105.3652298","https://doi.org/10.1145/3650105.3652298","Does the training of large language models potentially infringe upon code licenses? Furthermore, are there any datasets available that can be safely used for training these models without violating such licenses? In our study, we assess the current trends in the field and the importance of incorporating code into the training of large language models. Additionally, we examine publicly available datasets to see whether these models can be trained on them without the risk of legal issues in the future. To accomplish this, we compiled a list of 53 large language models trained on file-level code. We then extracted their datasets and analyzed how much they overlap with a dataset we created, consisting exclusively of strong copyleft code.Our analysis revealed that every dataset we examined contained license inconsistencies, despite being selected based on their associated repository licenses. We analyzed a total of 514 million code files, discovering 38 million exact duplicates present in our strong copyleft dataset. Additionally, we examined 171 million file-leading comments, identifying 16 million with strong copyleft licenses and another 11 million comments that discouraged copying without explicitly mentioning a license. Based on the findings of our study, which highlights the pervasive issue of license inconsistencies in large language models trained on code, our recommendation for both researchers and the community is to prioritize the development and adoption of best practices for dataset creation and management.","2024","2025-11-25 22:29:32","2025-11-25 22:29:32","","74–85","","","","","","","FORGE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","large language models; software engineering; machine learning; foundation models; code licensing; datasets; ML4SE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9DLZLQJU","conferencePaper","2024","Lu, Yao; Liu, Shang; Zhang, Qijun; Xie, Zhiyao","RTLLM: An Open-Source Benchmark for Design RTL Generation with Large Language Model","Proceedings of the 29th Asia and South Pacific Design Automation Conference","979-8-3503-9354-5","","10.1109/ASP-DAC58780.2024.10473904","https://doi.org/10.1109/ASP-DAC58780.2024.10473904","Inspired by the recent success of large language models (LLMs) like ChatGPT, researchers start to explore the adoption of LLMs for agile hardware design, such as generating design RTL based on natural-language instructions. However, in existing works, their target designs are all relatively simple and in a small scale, and proposed by the authors themselves, making a fair comparison among different LLM solutions challenging. In addition, many prior works only focus on the design correctness, without evaluating the design qualities of generated design RTL. In this work, we propose an open-source benchmark named RTLLM, for generating design RTL with natural language instructions. To systematically evaluate the auto-generated design RTL, we summarized three progressive goals, named syntax goal, functionality goal, and design quality goal. This benchmark can automatically provide a quantitative evaluation of any given LLM-based solution. Furthermore, we propose an easy-to-use yet surprisingly effective prompt engineering technique named self-planning, which proves to significantly boost the performance of GPT-3.5 in our proposed benchmark.","2024","2025-11-25 22:29:32","2025-11-25 22:29:32","","722–727","","","","","","","ASPDAC '24","","","","IEEE Press","Incheon, Republic of Korea","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V5LDFACS","conferencePaper","2024","Agarwal, Shivali; Chimalakonda, Sridhar; Krishnan, Saravanan; Kanvar, Vini; Shah, Samveg","Tutorial Report on Legacy Software Modernization: A Journey From Non-AI to Generative AI Approaches","Proceedings of the 17th Innovations in Software Engineering Conference","979-8-4007-1767-3","","10.1145/3641399.3641434","https://doi.org/10.1145/3641399.3641434","Dealing with ageing software is a reality of the industry, and even open source software systems. This is a great opportunity for the software engineering researchers to apply the traditional techniques of program analysis to solve problems of refactoring and modernization. The generative AI advancements have opened up a whole new world of possibilities for software engineering tasks such as code generation, code translation, bug fixing among others. Industry is keen on exploring scalable solutions for refactoring, automated testing and now automatic code generation. In this tutorial, we aim to (i) provide a background and overview of legacy software modernization and its importance amidst the emergence of AI-Assisted software and Generative AI (ii) discuss the challenges being faced by industry due to monolithic legacy code and systems (iii) introduce architectural and technological paradigms to modernize this legacy or ageing software (iv) highlight the research and engineering problems that remain to be solved in this space discussing the opportunities for the software engineering research community.","2024","2025-11-25 22:29:32","2025-11-25 22:29:32","","","","","","","","","ISEC '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Bangalore, India","","","","Code LLMs; Legacy Software Modernization; Program Analysis; Refactoring","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7FKHGUFP","conferencePaper","2024","Corradini, Davide; Montolli, Zeno; Pasqua, Michele; Ceccato, Mariano","DeepREST: Automated Test Case Generation for REST APIs Exploiting Deep Reinforcement Learning","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695511","https://doi.org/10.1145/3691620.3695511","Automatically crafting test scenarios for REST APIs helps deliver more reliable and trustworthy web-oriented systems. However, current black-box testing approaches rely heavily on the information available in the API's formal documentation, i.e., the Open API Specification (OAS for short). While useful, the OAS mostly covers syntactic aspects of the API (e.g., producer-consumer relations between operations, input value properties, and additional constraints in natural language), and it lacks a deeper understanding of the API business logic. Missing semantics include implicit ordering (logic dependency) between operations and implicit input-value constraints. These limitations hinder the ability of black-box testing tools to generate truly effective test cases automatically.This paper introduces DeepREST, a novel black-box approach for automatically testing REST APIs. It leverages deep reinforcement learning to uncover implicit API constraints, that is, constraints hidden from API documentation. Curiosity-driven learning guides an agent in the exploration of the API and learns an effective order to test its operations. This helps identify which operations to test first to take the API in a testable state and avoid failing API interactions later. At the same time, experience gained on successful API interactions is leveraged to drive accurate input data generation (i.e., what parameters to use and how to pick their values). Additionally, DeepREST alternates exploration with exploitation by mutating successful API interactions to improve test coverage and collect further experience.Our empirical validation suggests that the proposed approach is very effective in achieving high test coverage and fault detection and superior to a state-of-the-art baseline.","2024","2025-11-25 22:29:32","2025-11-25 22:29:32","","1383–1394","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","automated blackbox testing; deep reinforcement learning; REST API testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5GBP88C5","conferencePaper","2024","El Haji, Khalid; Brandt, Carolin; Zaidman, Andy","Using GitHub Copilot for Test Generation in Python: An Empirical Study","Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)","979-8-4007-0588-5","","10.1145/3644032.3644443","https://doi.org/10.1145/3644032.3644443","Writing unit tests is a crucial task in software development, but it is also recognized as a time-consuming and tedious task. As such, numerous test generation approaches have been proposed and investigated. However, most of these test generation tools produce tests that are typically difficult to understand. Recently, Large Language Models (LLMs) have shown promising results in generating source code and supporting software engineering tasks. As such, we investigate the usability of tests generated by GitHub Copilot, a proprietary closed-source code generation tool that uses an LLM. We evaluate GitHub Copilot's test generation abilities both within and without an existing test suite, and we study the impact of different code commenting strategies on test generations.Our investigation evaluates the usability of 290 tests generated by GitHub Copilot for 53 sampled tests from open source projects. Our findings highlight that within an existing test suite, approximately 45.28% of the tests generated by Copilot are passing tests; 54.72% of generated tests are failing, broken, or empty tests. Furthermore, if we generate tests using Copilot without an existing test suite in place, we observe that 92.45% of the tests are failing, broken, or empty tests. Additionally, we study how test method comments influence the usability of test generations.","2024","2025-11-25 22:29:32","2025-11-25 22:48:12","","45–55","","","","","","","AST '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","Test pattern generators; Software; Syntactics; Writing; Codes; Source coding; Runtime","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QGA733HF","bookSection","2025","Luna, Jose; Tan, Ivan; Xie, Xiaofei; Jiang, Lingxiao","Navigating Governance Paradigms: A Cross-Regional Comparative Study of Generative AI Governance Processes &amp; Principles","Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society","","","","","As Generative Artificial Intelligence (GenAI) technologies evolve at an unprecedented rate, global governance approaches struggle to keep pace with the technology, highlighting a critical issue in the governance adaptation of significant challenges. Depicting the nuances of nascent and diverse governance approaches based on risks, rules, outcomes, principles, or a mix, across different regions around the globe, is fundamental to discern discrepancies and convergences, and to shed light on specific limitations that need to be addressed, thereby facilitating the safe and trustworthy adoption of GenAI. In response to the need and the evolving nature of GenAI, this paper seeks to provide a collective view of different governance approaches around the world. Our research introduces a Harmonized GenAI Framework, ""H-GenAIGF"", based on the current governance approaches of six regions: (European Union (EU), United States (US), China (CN), Canada (CA), United Kingdom (UK), and Singapore (SG)). We have identified four constituents, fifteen processes, twenty-five sub-processes, and nine principles that aid the governance of GenAI, thus providing a comprehensive perspective on the current state of GenAI governance. In addition, we present a comparative analysis to facilitate identification of common ground and distinctions based on coverage of the processes by each region. The results show that risk-based approaches allow for better coverage of the processes, followed by mixed approaches. Other approaches lag behind, covering less than 50% of the processes. Most prominently, the analysis demonstrates that amongst the regions, only one process aligns across all approaches, highlighting the lack of consistent and executable provisions. Moreover, our case study on ChatGPT reveals process coverage deficiency, showing that harmonization of approaches is necessary to find alignment for GenAI governance.","2025","2025-11-25 22:29:33","2025-11-25 22:29:33","","917–931","","","","","","","","","","","AAAI Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QCPU8MVE","conferencePaper","2024","Paduraru, Ciprian; Stefanescu, Alin; Jianu, Augustin","Unit Test Generation using Large Language Models for Unity Game Development","Proceedings of the 1st ACM International Workshop on Foundations of Applied Software Engineering for Games","979-8-4007-0674-5","","10.1145/3663532.3664466","https://doi.org/10.1145/3663532.3664466","Challenges related to game quality, whether occurring during initial release or after updates, can result in player dissatisfaction, media scrutiny, and potential financial setbacks. These issues may stem from factors like software bugs, performance bottlenecks, or security vulnerabilities. Despite these challenges, game developers often rely on manual playtesting, highlighting the need for more robust and automated processes in game development. This research explores the application of Large Language Models (LLMs) for automating unit test creation in game development, with a specific focus on strongly typed programming languages like C++ and C#, widely used in the industry. The study centers around fine-tuning Code Llama, an advanced code generation model, to address common scenarios encountered in game development, including game engines and specific APIs or backends. Although the prototyping and evaluations primarily occurred within the Unity game engine, the proposed methods can be adapted to other internal or publicly available solutions. The evaluation outcomes demonstrate the effectiveness of these methods in enhancing existing unit test suites or automatically generating new tests based on natural language descriptions of class contexts and targeted methods.","2024","2025-11-25 22:29:33","2025-11-25 22:29:33","","7–13","","","","","","","FaSE4Games 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","large language models; unit testing; game development","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CI7MSY6V","book","2023","","ISSTA 2023: Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis","","979-8-4007-0221-1","","","","It is our great pleasure to welcome you to ISSTA 2023, the 32nd edition of the International Symposium on Software Testing and Analysis, to be held on July 18–20, 2023 in Seattle, USA. The symposium has become a premier scientific event in the expanding area of software testing and analysis, with a strong appeal to researchers from all continents.","2023","2025-11-25 22:29:33","2025-11-25 22:29:33","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WF6IXGGH","book","2023","","SAST '23: Proceedings of the 8th Brazilian Symposium on Systematic and Automated Software Testing","","979-8-4007-1629-4","","","","","2023","2025-11-25 22:29:33","2025-11-25 22:29:33","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YEGCGVYD","conferencePaper","2023","Pham, Khang; Nguyen, Vu; Nguyen, Tien","Application of Natural Language Processing Towards Autonomous Software Testing","Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering","978-1-4503-9475-8","","10.1145/3551349.3563241","https://doi.org/10.1145/3551349.3563241","The process of creating test cases from requirements written in natural language (NL) requires intensive human efforts and can be tedious, repetitive, and error-prone. Thus, many studies have attempted to automate that process by utilizing Natural Language Processing (NLP) approaches. Furthermore, with the advent of massive language models and transfer learning techniques, people have introduced various advancements in NLP-assisted software testing with promising results. More notably, in recent years, not only have researchers been engrossed in solving the above task, but many companies have also embedded the feature to translate from human language to test cases their products. This paper presents an overview of NLP-assisted solutions being used in both the literature and the software testing industry.","2023","2025-11-25 22:29:33","2025-11-25 22:29:33","","","","","","","","","ASE '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Rochester, MI, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NEMXB6TD","journalArticle","2023","El-Deeb, Ahmed","The Recent Wave of Generative AI Systems: What Does This Tell Us About What AI Can Do Now?","SIGSOFT Softw. Eng. Notes","","0163-5948","10.1145/3599975.3599979","https://doi.org/10.1145/3599975.3599979","No, this paper content is not generated by AI. Not sure if that question came across my editor's mind while reviewing submissions. It's a rightful question nowadays after the wave of generative AI apps that swamped our lives between chatGPT to King Charles coronation afterparty videos. Finally, AI is here in our hands that everyone of us can touch and integrate in daily life; after long being a fancy research topic. The question now is how this is going to shape our industry? Or more proactively, how generative AI can help us improve our craft and processes. This paper surveys key areas where generative AI can help us shape our industry and the way we work.","2023-06","2025-11-25 22:29:33","2025-11-25 22:29:33","","13","","3","48","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M7BFSVTN","conferencePaper","2023","Tihanyi, Norbert; Bisztray, Tamas; Jain, Ridhi; Ferrag, Mohamed Amine; Cordeiro, Lucas C.; Mavroeidis, Vasileios","The FormAI Dataset: Generative AI in Software Security through the Lens of Formal Verification","Proceedings of the 19th International Conference on Predictive Models and Data Analytics in Software Engineering","979-8-4007-0375-1","","10.1145/3617555.3617874","https://doi.org/10.1145/3617555.3617874","This paper presents the FormAI dataset, a large collection of 112,000 AI-generated compilable and independent C programs with vulnerability classification. We introduce a dynamic zero-shot prompting technique constructed to spawn diverse programs utilizing Large Language Models (LLMs). The dataset is generated by GPT-3.5-turbo and comprises programs with varying levels of complexity. Some programs handle complicated tasks like network management, table games, or encryption, while others deal with simpler tasks like string manipulation. Every program is labeled with the vulnerabilities found within the source code, indicating the type, line number, and vulnerable function name. This is accomplished by employing a formal verification method using the Efficient SMT-based Bounded Model Checker (ESBMC), which uses model checking, abstract interpretation, constraint programming, and satisfiability modulo theories to reason over safety/security properties in programs. This approach definitively detects vulnerabilities and offers a formal model known as a counterexample, thus eliminating the possibility of generating false positive reports. We have associated the identified vulnerabilities with Common Weakness Enumeration (CWE) numbers. We make the source code available for the 112,000 programs, accompanied by a separate file containing the vulnerabilities detected in each program, making the dataset ideal for training LLMs and machine learning algorithms. Our study unveiled that according to ESBMC, 51.24% of the programs generated by GPT-3.5 contained vulnerabilities, thereby presenting considerable risks to software safety and security.","2023","2025-11-25 22:29:33","2025-11-25 22:29:33","","33–43","","","","","","","PROMISE 2023","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: San Francisco, CA, USA","","","","Large Language Models; Dataset; Artificial Intelligence; Formal Verification; Software Security; Vulnerability Classification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SRWJ5WA8","conferencePaper","2025","Wang, Shuai; Yu, Yinan; Feldt, Robert; Parthasarathy, Dhasarathy","Automating a Complete Software Test Process Using LLMs: An Automotive Case Study","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00211","https://doi.org/10.1109/ICSE55347.2025.00211","Vehicle API testing verifies whether the interactions between a vehicle's internal systems and external applications meet expectations, ensuring that users can access and control various vehicle functions and data. However, this task is inherently complex, requiring the alignment and coordination of API systems, communication protocols, and even vehicle simulation systems to develop valid test cases. In practical industrial scenarios, inconsistencies, ambiguities, and interdependencies across various documents and system specifications pose significant challenges. This paper presents a system designed for the automated testing of in-vehicle APIs. By clearly defining and segmenting the testing process, we enable Large Language Models (LLMs) to focus on specific tasks, ensuring a stable and controlled testing workflow. Experiments conducted on over 100 APIs demonstrate that our system effectively automates vehicle API testing. The results also confirm that LLMs can efficiently handle mundane tasks requiring human judgment, making them suitable for complete automation in similar industrial contexts.","2025","2025-11-25 22:29:33","2025-11-25 22:46:52","","373–384","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","large language model; Large language models; software testing; Software testing; Manuals; Software engineering; Automation; Writing; Testing; test automation; vehicle API testing; Transforms; Automotive engineering; Web servers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FYFT4XW5","conferencePaper","2023","Lemieux, Caroline; Inala, Jeevana Priya; Lahiri, Shuvendu K.; Sen, Siddhartha","CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-Trained Large Language Models","Proceedings of the 45th International Conference on Software Engineering","978-1-6654-5701-9","","10.1109/ICSE48619.2023.00085","https://doi.org/10.1109/ICSE48619.2023.00085","Search-based software testing (SBST) generates high-coverage test cases for programs under test with a combination of test case generation and mutation. SBST's performance relies on there being a reasonable probability of generating test cases that exercise the core logic of the program under test. Given such test cases, SBST can then explore the space around them to exercise various parts of the program. This paper explores whether Large Language Models (LLMs) of code, such as OpenAI's Codex, can be used to help SBST's exploration. Our proposed algorithm, CodaMosa, conducts SBST until its coverage improvements stall, then asks Codex to provide example test cases for under-covered functions. These examples help SBST redirect its search to more useful areas of the search space. On an evaluation over 486 benchmarks, CodaMosa achieves statistically significantly higher coverage on many more benchmarks (173 and 279) than it reduces coverage on (10 and 4), compared to SBST and LLM-only baselines.","2023","2025-11-25 22:29:33","2025-11-25 22:47:07","","919–931","","","","","","","ICSE '23","","","","IEEE Press","Melbourne, Victoria, Australia","","","","","","","","","","","","large language model; Software testing; Test pattern generators; Software; Software engineering; Codes; Benchmark testing; python; codex; automated testing; test suite generation; search based software testing; Space exploration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZG36MERY","conferencePaper","2024","Bergsmann, Severin; Schmidt, Alexander; Fischer, Stefan; Ramler, Rudolf","First Experiments on Automated Execution of Gherkin Test Specifications with Collaborating LLM Agents","Proceedings of the 15th ACM International Workshop on Automating Test Case Design, Selection and Evaluation","979-8-4007-1109-1","","10.1145/3678719.3685692","https://doi.org/10.1145/3678719.3685692","Gherkin is a domain-specific language for describing test scenarios in natural language, which are the basis for automated acceptance testing. The emergence of Large Language Models (LLMs) has opened up new possibilities for processing such test specifications and for generating executable test code. This paper investigates the feasibility of employing LLMs to execute Gherkin test specifications utilizing the AutoGen multi-agent framework. Our findings show that our LLM agent system is able to automatically run the given test scenarios by autonomously exploring the system under test, generating executable test code on the fly, and evaluating execution results. We observed high success rates for executing simple as well as more complex test scenarios, but we also identified difficulties regarding failure scenarios and fault detection.","2024","2025-11-25 22:29:33","2025-11-25 22:29:33","","12–15","","","","","","","A-TEST 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Test Automation; Domain Specific Language; LLMs","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MJEYL8MD","journalArticle","2025","Hossain, Soneya Binta; Taylor, Raygan; Dwyer, Matthew","Doc2OracLL: Investigating the Impact of Documentation on LLM-Based Test Oracle Generation","Proc. ACM Softw. Eng.","","","10.1145/3729354","https://doi.org/10.1145/3729354","Code documentation is a critical artifact of software development, bridging human understanding and machine- readable code. Beyond aiding developers in code comprehension and maintenance, documentation also plays a critical role in automating various software engineering tasks, such as test oracle generation (TOG). In Java, Javadoc comments offer structured, natural language documentation embedded directly within the source code, typically describing functionality, usage, parameters, return values, and exceptional behavior. While prior research has explored the use of Javadoc comments in TOG alongside other information, such as the method under test (MUT), their potential as a stand-alone input source, the most relevant Javadoc components, and guidelines for writing effective Javadoc comments for automating TOG remain less explored. In this study, we investigate the impact of Javadoc comments on TOG through a comprehensive analysis. We begin by fine-tuning 10 large language models using three different prompt pairs to assess the role of Javadoc comments alongside other contextual information. Next, we systematically analyze the impact of different Javadoc comment’s components on TOG. To evaluate the generalizability of Javadoc comments from various sources, we also generate them using the GPT-3.5 model. We perform a thorough bug detection study using Defects4J dataset to understand their role in real-world bug detection. Our results show that incorporating Javadoc comments improves the accuracy of test oracles in most cases, aligning closely with ground truth. We find that Javadoc comments alone can match or even outperform approaches that utilize the MUT implementation. Additionally, we identify that the description and the return tag are the most valuable components for TOG. Finally, our approach, when using only Javadoc comments, detects between 19% and 94% more real-world bugs in Defects4J than prior methods, establishing a new state-of-the-art. To further guide developers in writing effective documentation, we conduct a detailed qualitative study on when Javadoc comments are helpful or harmful for TOG.","2025-06","2025-11-25 22:29:33","2025-11-25 22:29:33","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","large language model; software testing; documentation; test oracles","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PQ8CRR2F","journalArticle","2025","Li, Suwan; Bu, Lei; Liu, Shangqing; Bai, Guangdong; Xie, Fuman; Chen, Kai; Yue, Chang","VUI Testing of VPA Apps via Behavior Model-Enhanced LLM Agents","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3771555","https://doi.org/10.1145/3771555","With the increasing adoption of smart speakers, Virtual Personal Assistant (VPA) applications have become integral to daily life, enabling users to access news, entertainment, and smart device control through Voice User Interfaces (VUI). However, many VPA apps suffer from quality issues, such as unexpected terminations and failures to process common user commands, highlighting the urgent need for systematic and efficient VUI testing. Existing chatbot-style and model-based testing approaches lack global and semantic awareness, resulting in ineffective test case generation and inefficient state exploration.To address these challenges, we introduce Elevate, a model-enhanced, LLM-driven VPA testing framework that employs a multi-agent architecture to enhance VUI behavior testing. Elevate comprises three specialized LLM agents—Observer, Generator, and Planner—that collaboratively perform state extraction, test case generation, and guided state exploration. Additionally, a deterministic finite automaton (DFA)-based behavior model is designed to abstract app behavior and provide structured guidance to LLM agents, enhancing testing performance. Elevate also incorporates a feedback mechanism that refines testing strategies based on observed behaviors, ensuring continuous improvement.Implemented using GPT-4-Turbo and DeepSeek-R1, Elevate has been evaluated on problem detection, sentence/semantic coverage, and large-scale testing. Experimental results show that Elevate outperforms state-of-the-art methods (Vitas and LLM-based chatbots), detecting at least 18 and 37 more problems, respectively, and achieving over 10% and 30% higher state coverage. In a large-scale evaluation on 4,000 Alexa skills, Elevate further demonstrated 15% higher coverage than Vitas, confirming its effectiveness, scalability, and potential for widespread application in VUI testing.","2025-10","2025-11-25 22:29:33","2025-11-25 22:29:33","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Model-based Testing; VUI Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z6EU2TFI","conferencePaper","2024","Tanzil, Minaoar Hossain; Khan, Junaed Younus; Uddin, Gias","ChatGPT Incorrectness Detection in Software Reviews","Proceedings of the IEEE/ACM 46th International Conference on Software Engineering","979-8-4007-0217-4","","10.1145/3597503.3639194","https://doi.org/10.1145/3597503.3639194","We conducted a survey of 135 software engineering (SE) practitioners to understand how they use Generative AI-based chatbots like ChatGPT for SE tasks. We find that they want to use ChatGPT for SE tasks like software library selection but often worry about the truthfulness of ChatGPT responses. We developed a suite of techniques and a tool called CID (ChatGPT Incorrectness Detector) to automatically test and detect the incorrectness in ChatGPT responses. CID is based on the iterative prompting to ChatGPT by asking it contextually similar but textually divergent questions (using an approach that utilizes metamorphic relationships in texts). The underlying principle in CID is that for a given question, a response that is different from other responses (across multiple incarnations of the question) is likely an incorrect response. In a benchmark study of library selection, we show that CID can detect incorrect responses from ChatGPT with an F1-score of 0.74 – 0.75.","2024","2025-11-25 22:29:33","2025-11-25 22:29:33","","","","","","","","","ICSE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","large language model; testing; chatGPT; hallucination","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6DSXCH7U","journalArticle","2025","von Lucke, Jörn; Frank, Sander","A few Thoughts on the Use of ChatGPT, GPT 3.5, GPT-4 and LLMs in Parliaments: Reflecting on the results of experimenting with LLMs in the parliamentarian context","Digit. Gov.: Res. Pract.","","","10.1145/3665333","https://doi.org/10.1145/3665333","Starting in November 2022 with the free provision of ChatGPT, large language models (LLM) are now publicly available. This has significantly increased the number of publications that scope potential changes caused by the application of generative artificial intelligence (AI) in various societal domains. The private use of AI and the economic integration of generative LLMs have increased significantly. However, for parliamentarians and parliamentary professionals, the technology often remains abstract, impacting everyday work only peripherally. Due to the special responsibility of parliaments, governments, and administrations as the organizational instances of society, and through the inherent legitimations by society itself, there is a necessity to examine the implications of the use of generative LLMs within these institutions and traditional structures as well as their influence on political system logic. The article analyzes the responses that the generative LLMs GPT 3.5 and GPT 4 have provided via ChatGPT, based on the same input command (prompt) over different times. The responses help to assess how LLMs can be used in the parliamentary context, to reflect what dangers exist as well as to respond to the question on how a business model of an AI department in parliament might look like. Furthermore, it shall be explored whether there are fluctuations in the quality of the responses and how these should be evaluated against the backdrop of the need for accurate and precise workflows in parliamentary operations. Ultimately, the article aims to provide an answer as to whether the application of ChatGPT together with the LLMs GPT-3.5 and GPT-4 could already deliver this necessary quality and consistency for the parliamentarian working environment today.","2025-06","2025-11-25 22:29:33","2025-11-25 22:29:33","","","","2","6","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","ChatGPT; Large language model; GPT-4; GPT 3.5; parliament","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QLUZ8B7Z","conferencePaper","2024","Xia, Chunqiu Steven; Zhang, Lingming","Automated Program Repair via Conversation: Fixing 162 out of 337 Bugs for $0.42 Each using ChatGPT","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680323","https://doi.org/10.1145/3650212.3680323","Automated Program Repair (APR) aims to automatically generate patches for buggy programs. Traditional APR techniques suffer from a lack of patch variety as they rely heavily on handcrafted or mined bug fixing patterns and cannot easily generalize to other bug/fix types. To address this limitation, recent APR work has been focused on leveraging modern Large Language Models (LLMs) to directly generate patches for APR. Such LLM-based APR tools work by first constructing an input prompt built using the original buggy code and then querying the LLM to either fill-in (cloze-style APR) the correct code at the bug location or to produce a completely new code snippet as the patch. While the LLM-based APR tools are able to achieve state-of-the-art results, they still follow the classic Generate and Validate (GV) repair paradigm of first generating lots of patches by sampling from the same initial prompt and then validating each one afterwards. This not only leads to many repeated patches that are incorrect, but also misses the crucial and yet previously ignored information in test failures as well as in plausible patches. To address these aforementioned limitations, we propose ChatRepair, the first fully automated conversation-driven APR approach that interleaves patch generation with instant feedback to perform APR in a conversational style. ChatRepair first feeds the LLM with relevant test failure information to start with, and then learns from both failures and successes of earlier patching attempts of the same bug for more powerful APR. For earlier patches that failed to pass all tests, we combine the incorrect patches with their corresponding relevant test failure information to construct a new prompt for the LLM to generate the next patch. In this way, we can avoid making the same mistakes. For earlier patches that passed all the tests (i.e., plausible patches), we further ask the LLM to generate alternative variations of the original plausible patches. In this way, we can further build on and learn from earlier successes to generate more plausible patches to increase the chance of having correct patches. While our approach is general, we implement ChatRepair using state-of-the-art dialogue-based LLM – ChatGPT. Our evaluation on the widely studied Defects4j dataset shows that ChatRepair is able to achieve the new state-of-the-art in repair performance, achieving 114 and 48 correct fixes on Defects4j 1.2 and 2.0 respectively. By calculating the cost of accessing ChatGPT, we can fix 162 out of 337 bugs for $0.42 each!","2024","2025-11-25 22:29:33","2025-11-25 22:29:33","","819–831","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large Language Model; Automated Program Repair","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FX4LFNQV","journalArticle","2025","Ling, Yuchen; Yu, Shengcheng; Fang, Chunrong; Zhou, Quan; Chen, Zhenyu","LLM-based Crowdsourced Test Report Clustering","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3765756","https://doi.org/10.1145/3765756","The openness of crowdsourced testing introduces diversity in testing results. However, it also leads to a large volume of test reports, many of which highlight the same recurring issues. While these reports provide valuable feedback, their redundancy makes it inefficient for developers to review the reports and identify bugs. Crowdsourced test report clustering has been proposed to mitigate this problem, allowing developers to focus only on the representative reports from each cluster. However, existing methods primarily rely on embedding features extracted from reports for clustering, which limits their ability to generate accurate and interpretable clusters due to a lack of deeper semantic understanding of the reports.To address the aforementioned challenge, we propose LLMCluster, a novel method for crowdsourced test report clustering based on large language models (LLMs). LLMCluster employs an iterative clustering strategy. In each iteration, LLMCluster processes a subset of reports by instructing the LLM to disregard surface-level variations in expression, analyze the core issue in each report, and group reports addressing the same issue into new or existing clusters. After the iterative clustering process, LLMCluster applies correction algorithms to ensure the completeness and validity of the clustering result. Finally, LLMCluster utilizes the LLM to generate concise summaries for each cluster, making the results more intuitive and interpretable. Experimental results show that LLMCluster outperforms state-of-the-art methods across six commonly used clustering evaluation metrics. Additionally, the cluster summaries generated by LLMCluster semantically align well with manually written summaries.","2025-09","2025-11-25 22:29:33","2025-11-25 22:29:33","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Crowdsourced Testing; Report Clustering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LXRC3E9T","conferencePaper","2024","Feldt, Robert; Kang, Sungmin; Yoon, Juyeon; Yoo, Shin","Towards Autonomous Testing Agents via Conversational Large Language Models","Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering","979-8-3503-2996-4","","10.1109/ASE56229.2023.00148","https://doi.org/10.1109/ASE56229.2023.00148","Software testing is an important part of the development cycle, yet it requires specialized expertise and substantial developer effort to adequately test software. Recent discoveries of the capabilities of large language models (LLMs) suggest that they can be used as automated testing assistants, and thus provide helpful information and even drive the testing process. To highlight the potential of this technology, we present a taxonomy of LLM-based testing agents based on their level of autonomy, and describe how a greater level of autonomy can benefit developers in practice. An example use of LLMs as a testing assistant is provided to demonstrate how a conversational framework for testing can help developers. This also highlights how the often criticized ""hallucination"" of LLMs can be beneficial for testing. We identify other tangible benefits that LLM-driven testing agents can bestow, and also discuss potential limitations.","2024","2025-11-25 22:29:33","2025-11-25 22:48:07","","1688–1693","","","","","","","ASE '23","","","","IEEE Press","Echternach, Luxembourg","","","","","","","","","","","","large language model; software testing; Software testing; Automation; Testing; artificial intelligence; machine learning; test automation; Drives; Middleware; Oral communication; Taxonomy","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M6WLLRZ9","journalArticle","2025","Guan, Hao; Bai, Guangdong; Liu, Yepang","CrossProbe: LLM-Empowered Cross-Project Bug Detection for Deep Learning Frameworks","Proc. ACM Softw. Eng.","","","10.1145/3728984","https://doi.org/10.1145/3728984","Deep Learning (DL) models may introduce reliability challenges in the underlying DL frameworks. These frameworks may be prone to bugs that can lead to crash or wrong results, particularly when involving complex model architectures and substantial computational demands. Such framework bugs can disrupt DL applications, impacting customer experience and potentially causing financial losses. Traditional approaches to testing DL frameworks face limitations in adapting to the vast search space of model structures, diverse APIs, and the complexity of hybrid programming and hardware environments. Recent advancements using Large Language Models (LLMs) have improved DL framework fuzzing, but their efficacy depends heavily on the quality and diversity of input prompts, which are often constructed using single-framework data. In this paper, we propose an innovative approach for enhancing test generation for DL frameworks by leveraging “mirroring issues”—analogous bugs identified across different frameworks with common functionalities. Our approach is inspired by the fact that DL frameworks, such as PyTorch and TensorFlow, often share common bugs due to dependencies, developer errors, or edge-case inputs. We develop CrossProbe that utilizes LLMs to effectively learn from existing issues of one framework and transfer the acquired knowledge to generate test cases for finding mirroring issues in another framework, thus enabling cross-framework bug detection. To overcome the challenges of test case generation arising from the incompatible functionalities and different implementations between frameworks, we introduce three processes: alignment, screening, and distinction. These processes help mitigate transfer errors by establishing API pair databases, filtering unsuitable cases, and highlighting cross-framework distinctions. Experiments demonstrate that CrossProbe is efficient by saving 36.3% iterations of generation, and achieves a 25.0% higher success rate in issue transferring compared to existing state-of-the-art LLM-based testing techniques. CrossProbe detects 24 unique bugs using its transferred knowledge. Out of them, 19 are previously unknown and each requires cross-framework knowledge in deep learning for identification.","2025-06","2025-11-25 22:29:33","2025-11-25 22:29:33","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Deep Learning Frameworks; Library Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G7CVMTGI","conferencePaper","2024","Xue, Yuankai; Chen, Hanlin; Bai, Gina R.; Tairas, Robert; Huang, Yu","Does ChatGPT Help With Introductory Programming?An Experiment of Students Using ChatGPT in CS1","Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training","979-8-4007-0498-7","","10.1145/3639474.3640076","https://doi.org/10.1145/3639474.3640076","Generative AI, notably ChatGPT, has garnered attention in computer science education. This paper presents a controlled experiment that explores ChatGPT's role in CS1 in a classroom setting. Specifically, we aim to investigate the impact of ChatGPT on student learning outcomes and their behaviors when working on programming assignments. Participants were tasked with creating a UML diagram and subsequently implementing its design through programming, followed by a closed-book post-evaluation and a post-survey. All the participants were required to screen-record the whole process. In total, 56 participants were recruited, with 48 successful screen recordings. Participants in the Experimental Group can access ChatGPT 3.5 and other online resources, such as Google and Stack Overflow when creating the UML diagram and programming; however, participants in the Control Group can access all online resources except for ChatGPT (i.e., the only design variable is the access to ChatGPT). Finally, we measured and analyzed participants' learning outcomes through their UML diagram, programming, and post-evaluation scores. We also analyzed the time participants took to complete the tasks and their interactions with ChatGPT and other resources from the screen recordings. After finishing the tasks, student participants also provided their perceptions of using ChatGPT in CS1 through a post-survey.With rigorous quantitative and qualitative analysis, we found that (1) using ChatGPT does not present a significant impact on students' learning performance in the CS1 assignment-style tasks; (2) once using ChatGPT, students' tendency to explore other traditional educational resources is largely reduced (though available) and they tend to rely solely on ChatGPT, and this reliance on ChatGPT did not guarantee enhanced learning performance; (3) the majority of students hold neutral views on ChatGPT's role in CS1 programming but most of them raised concerns about its potential ethical issues and inconsistent performance across different tasks. We hope this study can help educators and students better understand the impact of ChatGPT in CS1 and inspire future work to provide proper guidelines for using ChatGPT in introductory programming classes.","2024","2025-11-25 22:29:33","2025-11-25 22:29:33","","331–341","","","","","","","ICSE-SEET '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","ChatGPT; generative AI; CS1; CS education; OOP","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"954NY7UZ","conferencePaper","2024","He, Yifeng; Huang, Jiabo; Rong, Yuyang; Guo, Yiwen; Wang, Ethan; Chen, Hao","UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large Language Models for Program Testing","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680342","https://doi.org/10.1145/3650212.3680342","The remarkable capability of large language models (LLMs) in generating high-quality code has drawn increasing attention in the software testing community. However, existing code LLMs often demonstrate unsatisfactory capabilities in generating accurate, complete tests since they were trained on code snippets collected without differentiating between code for testing and for other purposes. In this paper, we present a large-scale dataset, UniTSyn, which can enhance LLMs for Unit Test Synthesis. Associating tests with the tested functions is crucial for LLMs to infer the expected behavior and the logic paths to be verified. By leveraging Language Server Protocol, UniTSyn achieves the challenging goal of collecting focal-test pairs without per-project execution setups or per-language heuristics, which tend to be fragile and difficult to scale. Containing 2.7 million focal-test pairs across five mainstream programming languages, it can enhance the test generation ability of LLMs. Our experiments demonstrate that, by building an autoregressive LLM based on UniTSyn, we can achieve significant benefits in learning and understanding unit test representations, resulting in improved generation accuracy and code coverage across all the evaluated programming languages.","2024","2025-11-25 22:29:33","2025-11-25 22:29:33","","1061–1072","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large language models; software testing; test case generation; dataset","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZUC85GNQ","conferencePaper","2024","Qiu, Yuxin; Hu, Jie; Zhang, Qian; Yin, Heng","Calico: Automated Knowledge Calibration and Diagnosis for Elevating AI Mastery in Code Tasks","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680399","https://doi.org/10.1145/3650212.3680399","Recent advancements in large language models (LLMs) have exhibited promising capabilities in addressing various tasks such as defect detection and program repair. Despite their prevalence, LLMs still face limitations in effectively handling these tasks. Common strategies to adapt them and improve their performance for specific tasks involve fine-tuning models based on user data or employing in-context learning with examples of desired inputs and outputs. However, they pose challenges for practical adoption due to the need for extensive computational resources, high-quality data, and continuous maintenance. Furthermore, neither strategy can explain or reason about the deficiencies of LLMs in the given tasks. We propose Calico to address the high cost of fine-tuning, eliminate the necessity for task-specific examples, and provide explanations of LLM deficiency. At the heart of Calico is an evolutionary approach that interleaves knowledge calibration and AI deficiency diagnosis. The key essence of Calico is as follows. First, it focuses on identifying knowledge gaps in LLMs’ program comprehension. Second, it conducts automated code refactoring to integrate the overlooked knowledge into the source code for mitigating those gaps. Third, it employs what-if analysis and counterfactual reasoning to determine a minimum set of overlooked knowledge necessary to improve the performance of LLMs in code tasks. We have extensively evaluated Calico over 8,938 programs on three most commonly seen code tasks. Our experimental results show that vanilla ChatGPT cannot fully understand code structures. With knowledge calibration, Calico improves it by 20% and exhibits comparable proficiency compared to fine-tuned LLMs. Deficiency diagnosis contributes to 8% reduction in program sizes while ensuring performance. These impressive results demonstrate the feasibility of utilizing a vanilla LLM for automated software engineering (SE) tasks, thereby avoiding the high computational costs associated with a fine-tuned model.","2024","2025-11-25 22:29:33","2025-11-25 22:29:33","","1785–1797","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","large language model; software testing; Software engineering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VW4GKEN8","journalArticle","2024","Richards, Mike; Waugh, Kevin; Slaymaker, Mark; Petre, Marian; Woodthorpe, John; Gooch, Daniel","Bob or Bot: Exploring ChatGPT's Answers to University Computer Science Assessment","ACM Trans. Comput. Educ.","","","10.1145/3633287","https://doi.org/10.1145/3633287","Cheating has been a long-standing issue in university assessments. However, the release of ChatGPT and other free-to-use generative AI tools has provided a new and distinct method for cheating. Students can run many assessment questions through the tool and generate a superficially compelling answer, which may or may not be accurate.&nbsp;We ran a dual-anonymous “quality assurance” marking exercise across four end-of-module assessments across a distance university computer science (CS) curriculum. Each marker received five ChatGPT-generated scripts alongside 10 student scripts. A total of 90 scripts were marked; every ChatGPT-generated script for the undergraduate modules received at least a passing grade (&gt;40%), with all of the introductory module CS1 scripts receiving a distinction (&gt;85%). None of the ChatGPT-taught postgraduate scripts received a passing grade (&gt;50%). We also present the results of interviewing the markers and of running our sample scripts through a GPT-2 detector and the TurnItIn AI detector, which both identified every ChatGPT-generated script but differed in the number of false positives. As such, we contribute a baseline understanding of how the public release of generative AI is likely to significantly impact quality assurance processes. Our analysis demonstrates that in most cases, across a range of question formats, topics, and study levels, ChatGPT is at least capable of producing adequate answers for undergraduate assessment.","2024-01","2025-11-25 22:29:33","2025-11-25 22:29:33","","","","1","24","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","ChatGPT; generative AI; cheating; quality assurance; university assessment’","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R7CPE3UG","conferencePaper","2024","Yang, Boyang; Tian, Haoye; Pian, Weiguo; Yu, Haoran; Wang, Haitao; Klein, Jacques; Bissyandé, Tegawendé F.; Jin, Shunfu","CREF: An LLM-Based Conversational Software Repair Framework for Programming Tutors","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680328","https://doi.org/10.1145/3650212.3680328","With the proven effectiveness of Large Language Models (LLMs) in code-related tasks, researchers have explored their potential for program repair. However, existing repair benchmarks might have influenced LLM training data, potentially causing data leakage. To evaluate LLMs’ realistic repair capabilities, (i) we introduce an extensive, non-crawled benchmark TutorCode, comprising 1,239 C++ defect codes and associated information such as tutor guidance, solution description, failing test cases, and the corrected code. Our work assesses LLM’s repair performance on TutorCode, measuring repair correctness (TOP-5 and AVG-5) and patch precision (RPSR). (ii) We then provide a comprehensive investigation into which types of extra information can help LLMs improve their repair performance. Among these types, tutor guidance was the most effective information. To fully harness LLMs’ conversational capabilities and the benefits of augmented information, (iii) we introduce a novel conversational semi-automatic repair framework CREF assisting human programming tutors. It demonstrates a remarkable AVG-5 improvement of 17.2%-24.6% compared to the baseline, achieving an impressive AVG-5 of 76.6% when utilizing GPT-4. These results highlight the potential for enhancing LLMs’ repair capabilities through tutor interactions and historical conversations. The successful application of CREF in a real-world educational setting demonstrates its effectiveness in reducing tutors’ workload and improving students’ learning experience, showing promise for code review and other software engineering tasks.","2024","2025-11-25 22:29:33","2025-11-25 22:29:33","","882–894","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large Language Model; Program Repair; Open Source","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CVS3M4NU","journalArticle","2025","Feng, Qiong; Ma, Xiaotian; Sheng, Jiayi; Feng, Ziyuan; Song, Wei; Liang, Peng","Integrating Various Software Artifacts for Better LLM-based Bug Localization and Program Repair","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3770581","https://doi.org/10.1145/3770581","LLMs have garnered considerable attention for their potential to streamline Automated Program Repair (APR). LLM-based approaches can either insert the correct code using an infilling-style technique or directly generate patches when provided with buggy methods, aiming for plausible patches to pass all tests. However, most of LLM-based APR methods rely on a single type of software information, such as issue descriptions or error stack traces, without fully leveraging a combination of diverse software artifacts. Human developers, in contrast, often use a range of information — such as debugging data, issue discussions, and error stack traces — to diagnose and fix bugs. Despite this, many LLM-based approaches do not explore which specific types of software information best assist in localizing and repairing software bugs. Addressing this gap is crucial for advancing LLM-based APR techniques.To investigate this and mimic the way human developers fix bugs, we propose DEVLoRe (short for DEVeloper Localization and Repair). In this framework, LLMs first use issue content (description and discussion) and stack error traces to localize buggy methods, then rely on debug information in buggy methods and issue content and stack error to localize buggy lines and generate valid patches. We evaluated the effectiveness of issue content, error stack traces, and debugging information in bug localization and automatic program repair. Our results show that while issue content and error stack is particularly effective in assisting LLMs with fault localization and program repair respectively, different types of software artifacts complement each other in addressing various bugs. By incorporating these three types of artifacts and using the Defects4J v2.0 dataset for evaluation, DEVLoRe successfully localizes 49.3% of single-method bugs and generates 56.0% plausible patches. Additionally, DEVLoRe can localize 47.6% of non-single-method bugs and generates 14.5% plausible patches. Moreover, our framework streamlines the end-to-end process from buggy source code to a complete repair, and achieves a 39.7% and 17.1% of single-method and non-single-method bug repair rate, outperforming current state-of-the-art APR methods. Furthermore, we re-implemented and evaluated our framework, demonstrating its effectiveness in resolving 9 unique issues compared to other state-of-the-art frameworks using the same or more advanced models on SWE-bench Lite. We also discussed whether a leading framework for Python code can be directly applied to Java code, or vice versa. The source code and experimental results of this work for replication are available at .","2025-10","2025-11-25 22:29:33","2025-11-25 22:29:33","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Automatic Program Repair; Fault Localization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NLBJG6PF","conferencePaper","2024","Eom, Jueon; Jeong, Seyeon; Kwon, Taekyoung","Fuzzing JavaScript Interpreters with Coverage-Guided Reinforcement Learning for LLM-Based Mutation","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680389","https://doi.org/10.1145/3650212.3680389","JavaScript interpreters, crucial for modern web browsers, require an effective fuzzing method to identify security-related bugs. However, the strict grammatical requirements for input present significant challenges. Recent efforts to integrate language models for context- aware mutation in fuzzing are promising but lack the necessary coverage guidance to be fully effective. This paper presents a novel technique called CovRL (Coverage-guided Reinforcement Learning) that combines Large Language Models (LLMs) with Reinforcement Learning (RL) from coverage feedback. Our fuzzer, CovRL-Fuzz, integrates coverage feedback directly into the LLM by leveraging the Term Frequency-Inverse Document Frequency (TF-IDF) method to construct a weighted coverage map. This map is key in calculating the fuzzing reward, which is then applied to the LLM-based mutator through reinforcement learning. CovRL-Fuzz, through this approach, enables the generation of test cases that are more likely to discover new coverage areas, thus improving bug detection while minimizing syntax and semantic errors, all without needing extra post-processing. Our evaluation results show that CovRL-Fuzz outperforms the state-of-the-art fuzzers in enhancing code coverage and identifying bugs in JavaScript interpreters: CovRL-Fuzz identified 58 real-world security-related bugs in the latest JavaScript interpreters, including 50 previously unknown bugs and 15 CVEs.","2024","2025-11-25 22:29:33","2025-11-25 22:29:33","","1656–1668","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","coverage; large language model; fuzzing; reinforcement learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BZJ4WXSS","journalArticle","2025","Shang, Ye; Zhang, Quanjun; Fang, Chunrong; Gu, Siqi; Zhou, Jianyi; Chen, Zhenyu","A Large-Scale Empirical Study on Fine-Tuning Large Language Models for Unit Testing","Proc. ACM Softw. Eng.","","","10.1145/3728951","https://doi.org/10.1145/3728951","Unit testing plays a pivotal role in software development, improving software quality and reliability. However, generating effective test cases manually is time-consuming, prompting interest in unit testing research. Recently, Large Language Models (LLMs) have shown potential in various unit testing tasks, including test generation, assertion generation, and test evolution, but existing studies are limited in scope and lack a systematic evaluation of the effectiveness of LLMs. To bridge this gap, we present a large-scale empirical study on fine-tuning LLMs for unit testing. Our study involves three unit testing tasks, five benchmarks, eight evaluation metrics, and 37 popular LLMs across various architectures and sizes, consuming over 3,000 NVIDIA A100 GPU hours. We focus on three key research questions: (1) the performance of LLMs compared to state-of-the-art methods, (2) the impact of different factors on LLM performance, and (3) the effectiveness of fine-tuning versus prompt engineering. Our findings reveal that LLMs outperform existing state-of-the-art approaches on all three unit testing tasks across nearly all metrics, highlighting the potential of fine-tuning LLMs in unit testing tasks. Furthermore, large-scale, decoder-only models achieve the best results across tasks, while encoder-decoder models perform better under the same parameter scale. Additionally, the comparison of the performance between fine-tuning and prompt engineering approaches reveals the considerable potential capability of the prompt engineering approach in unit testing tasks. We then discuss the concerned issues on the test generation task, including data leakage issues, bug detection capabilities, and metrics comparisons. Finally, we further pinpoint carious practical guidelines for LLM-based approaches to unit testing tasks in the near future. Overall, our work demonstrates the promising future of fine-tuning LLMs on unit testing tasks and reduces the manual efforts of unit testing experts in practical scenarios.","2025-06","2025-11-25 22:29:33","2025-11-25 22:29:33","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; AI for SE; Software Testing; Unit Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZTV9VGRU","conferencePaper","2025","An, Junwen","LLM-Assisted Dialect-Agnostic SQL Query Parsing","Companion Proceedings of the 2025 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for Humanity","979-8-4007-2141-0","","10.1145/3758316.3763249","https://doi.org/10.1145/3758316.3763249","Query analysis and rewriting tools, which rely on analyzing the Abstract Syntax Trees (ASTs) of queries, are essential in database workflows. However, due to the diverse SQL dialects, traditional grammar-based parsers often fail when encountering dialect-specific syntax. Although Large Language Models (LLMs) show promise in understanding SQL queries, they struggle in accurately generating ASTs. To address this, we propose SQLFlex, a hybrid approach that iteratively uses a grammar-based parser and, upon failure, employs an LLM to segment the query into smaller, parsable parts. SQLFlex successfully parsed 96.37% of queries across eight dialects on average, and demonstrated its practicality in SQL linting and test case reduction.","2025","2025-11-25 22:29:33","2025-11-25 22:29:33","","31–33","","","","","","","SPLASH Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Singapore, Singapore","","","","Large Language Model; Parser; SQL Dialect","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HL3MJE94","journalArticle","2025","Xia, Chunqiu Steven; Deng, Yinlin; Dunn, Soren; Zhang, Lingming","Demystifying LLM-Based Software Engineering Agents","Proc. ACM Softw. Eng.","","","10.1145/3715754","https://doi.org/10.1145/3715754","Recent advancements in large language models (LLMs) have significantly advanced the automation of software development tasks, including code synthesis, program repair, and test generation. More recently, researchers and industry practitioners have developed various autonomous LLM agents to perform end-to-end software development tasks. These agents are equipped with the ability to use tools, run commands, observe feedback from the environment, and plan for future actions. However, the complexity of these agent-based approaches, together with the limited abilities of current LLMs, raises the following question: Do we really have to employ complex autonomous software agents? To attempt to answer this question, we build Agentless – an agentless approach to automatically resolve software development issues. Compared to the verbose and complex setup of agent-based approaches, Agentless employs a simplistic three-phase process of localization, repair, and patch validation, without letting the LLM decide future actions or operate with complex tools. Our results on the popular SWE-bench Lite benchmark show that surprisingly the simplistic Agentless is able to achieve both the highest performance (32.00%, 96 correct fixes) and low cost ($0.70) compared with all existing open-source software agents at the time of paper submission! Agentless also achieves more than 50% solve rate when using Claude 3.5 Sonnet on the new SWE-bench Verified benchmark. In fact, Agentless has already been adopted by OpenAI as the go-to approach to showcase the real-world coding performance of both GPT-4o and the new o1 models; more recently, Agentless has also been used by DeepSeek to evaluate their newest DeepSeek V3 and R1 models. Furthermore, we manually classified the problems in SWE-bench Lite and found problems with exact ground truth patches or insufficient/misleading issue descriptions. As such, we construct SWE-bench Lite-𝑆 by excluding such problematic issues to perform more rigorous evaluation and comparison. Our work highlights the currently overlooked potential of a simplistic, cost-effective technique in autonomous software development. We hope Agentless will help reset the baseline, starting point, and horizon for autonomous software agents, and inspire future work along this crucial direction. We have open-sourced Agentless at: https://github.com/OpenAutoCoder/Agentless","2025-06","2025-11-25 22:29:33","2025-11-25 22:29:33","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Automated Program Repair; AI Software Engineer; Autonomous Programming","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T5TBLLQ3","conferencePaper","2024","Lops, Andrea; Narducci, Fedelucio; Ragone, Azzurra; Trizio, Michelantonio","AgoneTest: Automated creation and assessment of Unit tests leveraging Large Language Models","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695318","https://doi.org/10.1145/3691620.3695318","Software correctness is crucial, with unit testing playing an indispensable role in the software development lifecycle. However, creating unit tests is time-consuming and costly, underlining the need for automation. Leveraging Large Language Models (LLMs) for unit test generation is a promising solution, but existing studies focus on simple, small-scale scenarios, leaving a gap in understanding LLMs' performance in real-world applications, particularly regarding integration and assessment efficacy at scale. Here, we present AgoneTest, a system focused on automatically generating and evaluating complex class-level test suites. Our contributions include a scalable automated system, a newly developed dataset for rigorous evaluation, and a detailed methodology for test quality assessment.","2024","2025-11-25 22:29:33","2025-11-25 22:46:49","","2440–2441","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","large language model; Large language models; software testing; Software testing; Test pattern generators; Software; Software engineering; Automation; Software development management; Large Language Model; Debugging; Automatic Assessment; Software Testing; automatic assessment; Automatic programming","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7C8TJR7Q","conferencePaper","2024","Shan, Shiwen; Huo, Yintong; Su, Yuxin; Li, Yichen; Li, Dan; Zheng, Zibin","Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize Configuration Errors via Logs","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3652106","https://doi.org/10.1145/3650212.3652106","Configurable software systems are prone to configuration errors, resulting in significant losses to companies. However, diagnosing these errors is challenging due to the vast and complex configuration space. These errors pose significant challenges for both experienced maintainers and new end-users, particularly those without access to the source code of the software systems. Given that logs are easily accessible to most end-users, we conduct a preliminary study to outline the challenges and opportunities of utilizing logs in localizing configuration errors. Based on the insights gained from the preliminary study, we propose an LLM-based two-stage strategy for end-users to localize the root-cause configuration properties based on logs. We further implement a tool, LogConfigLocalizer, aligned with the design of the aforementioned strategy, hoping to assist end-users in coping with configuration errors through log analysis. To the best of our knowledge, this is the first work to localize the root-cause configuration properties for end-users based on Large Language Models (LLMs) and logs. We evaluate the proposed strategy on Hadoop by LogConfigLocalizer and prove its efficiency with an average accuracy as high as 99.91%. Additionally, we also demonstrate the effectiveness and necessity of different phases of the methodology by comparing it with two other variants and a baseline tool. Moreover, we validate the proposed methodology through a practical case study to demonstrate its effectiveness and feasibility.","2024","2025-11-25 22:29:33","2025-11-25 22:29:33","","13–25","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large Language Model; Configuration Errors; Log Analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8J24MLCH","conferencePaper","2024","Qi, Fei; Hou, Yingnan; Lin, Ning; Bao, Shanshan; Xu, Nuo","A Survey of Testing Techniques Based on Large Language Models","Proceedings of the 2024 International Conference on Computer and Multimedia Technology","979-8-4007-1826-7","","10.1145/3675249.3675298","https://doi.org/10.1145/3675249.3675298","With the development of software testing technology, Large Language Model (LLM) driven testing method have gradually become an emerging trend in the field of software testing. This paper presents a comprehensive review of LLM-based testing techniques. The results of 19 studies using LLM to optimize testing techniques are analyzed from the perspective of software testing. This paper discusses in detail how to use LLM to optimize test techniques for generating automated test code and generating diverse input in software test tasks. It also summarizes the challenges and opportunities faced by this field. The above conclusions can identify the shortcomings of LLM-based software testing technology and the direction of future research.","2024","2025-11-25 22:29:34","2025-11-25 22:29:34","","280–284","","","","","","","ICCMT '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sanming, China","","","","LLM; Pre-trained Large Language Model; Software Testing Techniques","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XNMQC5GJ","conferencePaper","2024","Yu, Zeliang; Wen, Ming; Guo, Xiaochen; Jin, Hai","Maltracker: A Fine-Grained NPM Malware Tracker Copiloted by LLM-Enhanced Dataset","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680397","https://doi.org/10.1145/3650212.3680397","As the largest package registry, Node Package Manager (NPM) has become the prime target for various supply chain attacks recently and has been flooded with numerous malicious packages, posing significant security risks to end-users. Learning-based methods have demonstrated promising performance with good adaptability to various types of attacks. However, they suffer from two main limitations. First, they often utilize metadata features or coarse-grained code features extracted at the package level while overlooking complex code semantics. Second, the dataset used to train the model often suffers from a lack of variety both in quantity and diversity, and thus cannot detect significant types of attacks. To address these problems, we introduce Maltracker, a learningbased NPM malware tracker based on fine-grained features empowered by LLM-enhanced dataset. First, Maltracker constructs precise call graphs to extract suspicious functions that are reachable to a pre-defined set of sensitive APIs, and then utilizes community detection algorithm to identify suspicious code gadgets based on program dependency graph, from which fine-grained features are then extracted. To address the second limitation, we extend the dataset using advanced large language models (LLM) to translate malicious functions from other languages (e.g., C/C++, Python, and Go) into JavaScript. Evaluations shows that Maltracker can achieve an improvement of about 12.6% in terms of F1-score at the package level and 31.0% at the function level compared with the SOTA learning-based methods. Moreover, the key components of 𝑀𝑎𝑙𝑡𝑟𝑎𝑐𝑘𝑒𝑟 all contribute to the effectiveness of its performance. Finally, Maltracker has also detected 230 new malicious packages in NPM and received 61 thanks letters, among which some contain new malicious behaviors that cannot be detected by existing tools.","2024","2025-11-25 22:29:34","2025-11-25 22:29:34","","1759–1771","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large Language Model; Code Translation; Malware Detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PRY3RVN7","conferencePaper","2024","Wu, Liangxuan; Zhao, Yanjie; Hou, Xinyi; Liu, Tianming; Wang, Haoyu","ChatGPT Chats Decoded: Uncovering Prompt Patterns for Superior Solutions in Software Development Lifecycle","Proceedings of the 21st International Conference on Mining Software Repositories","979-8-4007-0587-8","","10.1145/3643991.3645069","https://doi.org/10.1145/3643991.3645069","The advent of Large Language Models (LLMs) like ChatGPT has markedly transformed software development, aiding tasks from code generation to issue resolution with their human-like text generation. Nevertheless, the effectiveness of these models greatly depends on the nature of the prompts given by developers. Therefore, this study delves into the DevGPT dataset, a rich collection of developer-ChatGPT dialogues, to unearth the patterns in prompts that lead to effective problem resolutions. The underlying motivation for this research is to enhance the collaboration between human developers and AI tools, thereby improving productivity and problem-solving efficacy in software development. Utilizing a combination of textual analysis and data-driven approaches, this paper seeks to identify the attributes of prompts that are associated with successful interactions, providing crucial insights for the strategic employment of ChatGPT in software engineering environments.","2024","2025-11-25 22:29:34","2025-11-25 22:29:34","","142–146","","","","","","","MSR '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","large language model; LLM; ChatGPT; data mining","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P68RZQB6","conferencePaper","2025","Leinonen, Juho; Denny, Paul; Kiljunen, Olli; MacNeil, Stephen; Sarsa, Sami; Hellas, Arto","LLM-itation is the Sincerest Form of Data: Generating Synthetic Buggy Code Submissions for Computing Education","Proceedings of the 27th Australasian Computing Education Conference","979-8-4007-1425-2","","10.1145/3716640.3716647","https://doi.org/10.1145/3716640.3716647","There is a great need for data in computing education research. Data is needed to understand how students behave, to train models of student behavior to optimally support students, and to develop and validate new assessment tools and learning analytics techniques. However, relatively few computing education datasets are shared openly, often due to privacy regulations and issues in making sure the data is anonymous. Large language models (LLMs) offer a promising approach to create large-scale, privacy-preserving synthetic data, which can be used to explore various aspects of student learning, develop and test educational technologies, and support research in areas where collecting real student data may be challenging or impractical. This work explores generating synthetic buggy code submissions for introductory programming exercises using GPT-4o. We compare the distribution of test case failures between synthetic and real student data from two courses to analyze the accuracy of the synthetic data in mimicking real student data. Our findings suggest that LLMs can be used to generate synthetic incorrect submissions that are not significantly different from real student data with regard to test case failure distributions. Our research contributes to the development of reliable synthetic datasets for computing education research and teaching, potentially accelerating progress in the field while preserving student privacy.","2025","2025-11-25 22:29:34","2025-11-25 22:29:34","","56–63","","","","","","","ACE '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","generative AI; prompt engineering; large language models; LLMs; bugs; data generation; genAI; GPT-4o; submissions; synthetic data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DC2AYCWU","conferencePaper","2024","Leotta, Maurizio; Yousaf, Hafiz Zeeshan; Ricca, Filippo; Garcia, Boni","AI-Generated Test Scripts for Web E2E Testing with ChatGPT and Copilot: A Preliminary Study","Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering","979-8-4007-1701-7","","10.1145/3661167.3661192","https://doi.org/10.1145/3661167.3661192","Automated testing is vital for ensuring the reliability of web applications. This paper presents a preliminary study on leveraging artificial intelligence (AI) models, specifically ChatGPT and Github Copilot, to generate test scripts for web end-to-end testing. Through experimentation, we evaluated the feasibility and effectiveness of AI language models in generating test scripts based on natural language descriptions of user interactions with web applications. Our preliminary results show that AI-based generation generally provides an advantage over fully manual test scripts development. Starting from test cases clearly defined in Gherkin, a reduction in development time is always observable. In some cases, this reduction is statistically significant (e.g., Manual vs. a particular use of ChatGPT). These results are valid provided that the tester has some skills in manual test script development and is therefore able to modify the code produced by the AI-generation tools. This study contributes to the exploration of AI-driven solutions in web test scripts generation and lays the foundation for future research in this domain.","2024","2025-11-25 22:29:34","2025-11-25 22:29:34","","339–344","","","","","","","EASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Salerno, Italy","","","","LLM; ChatGPT; Test Automation; E2E Testing; Empirical Study.; GitHub Copilot; Selenium WebDriver","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GR24TW83","conferencePaper","2024","Sun, Simin","Enhancing Software Design and Developer Experience Via LLMs","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695606","https://doi.org/10.1145/3691620.3695606","This research explores the transformative potential of generative AI in software development. Generative AI is revolutionizing the field by offering capabilities to automatically generate, refactor, and test code. Through the use of action research, new methods and tools based on generative AI models are studied and developed. The initial focus is on the models' ability to comprehend high-level design concepts. Subsequently, the research moves into the augmented generation of software artifacts. Finally, organization-specific or task-specific methods are introduced to enhance software developers' productivity and experience.","2024","2025-11-25 22:29:34","2025-11-25 22:47:17","","2498–2501","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","LLM; Software; Software engineering; generative AI; Generative AI; Software development management; Codes; Productivity; log analysis; Log Analysis; CI/CD; Software design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"687Y3PKV","journalArticle","2025","Yang, Yaopeng; Li, Chuanyi; Han, Zhifeng; Li, Rui; Xu, Kui; Li, Qingyuan; Zhong, Wenkang; Shen, Zongwen; Fei, Zhiwei; Ge, Jidong; Luo, Bin","Patch Generation in APR: A Survey from the Perspectives of Utilizing LLMs and Using APR-Specific Information","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3764584","https://doi.org/10.1145/3764584","Automated Program Repair (APR) is a crucial task in software development and maintenance, aiming to patch software bugs automatically without human intervention. The rise of Large Language Models (LLMs) has significantly advanced APR. However, as researchers delve deeper into APR as a downstream task for LLMs, a key challenge remains: how to effectively integrate APR-specific information to complement LLMs capabilities. This survey revisits 124 existing APR studies, most from 2021 to 2024, from two perspectives: Utilizing LLMs and APR-specific Information. First, we define the concept of APR-specific information. Then, we summarize techniques for utilizing LLMs in patch generation in four dimensions. Next, we explore critical factors influencing the effectiveness of generating patches by LLMs, such as prompting context and fine-tuning configurations. After that, we focus on the evaluation of generated patches, including benchmarks, reasoning cost scaling, etc. Furthermore, we distill all APR-specific information (e.g., bug-fix pairs and error messages), highlighting their unique importance and features. Finally, we comprehensively outline challenges, limitations, and potential future research directions in APR in the LLM era. To conclude, by adopting the two perspectives, we aim to provide valuable insights into mining and leveraging APR-specific information in utilizing LLMs process for the APR community.","2025-08","2025-11-25 22:29:34","2025-11-25 22:29:34","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Automated Program Repair; APR-specific information","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FS4Y4IW9","journalArticle","2024","Li, Ningke; Li, Yuekang; Liu, Yi; Shi, Ling; Wang, Kailong; Wang, Haoyu","Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models","Proc. ACM Program. Lang.","","","10.1145/3689776","https://doi.org/10.1145/3689776","Large language models (LLMs) have revolutionized language processing, but face critical challenges with security, privacy, and generating hallucinations — coherent but factually inaccurate outputs. A major issue is fact-conflicting hallucination (FCH), where LLMs produce content contradicting ground truth facts. Addressing FCH is difficult due to two key challenges: 1) Automatically constructing and updating benchmark datasets is hard, as existing methods rely on manually curated static benchmarks that cannot cover the broad, evolving spectrum of FCH cases. 2) Validating the reasoning behind LLM outputs is inherently difficult, especially for complex logical relations. To tackle these challenges, we introduce a novel logic-programming-aided metamorphic testing technique for FCH detection. We develop an extensive and extensible framework that constructs a comprehensive factual knowledge base by crawling sources like Wikipedia, seamlessly integrated into Drowzee. Using logical reasoning rules, we transform and augment this knowledge into a large set of test cases with ground truth answers. We test LLMs on these cases through template-based prompts, requiring them to provide reasoned answers. To validate their reasoning, we propose two semantic-aware oracles that assess the similarity between the semantic structures of the LLM answers and ground truth. Our approach automatically generates useful test cases and identifies hallucinations across six LLMs within nine domains, with hallucination rates ranging from 24.7% to 59.8%. Key findings include LLMs struggling with temporal concepts, out-of-distribution knowledge, and lack of logical reasoning capabilities. The results show that logic-based test cases generated by Drowzee effectively trigger and detect hallucinations. To further mitigate the identified FCHs, we explored model editing techniques, which proved effective on a small scale (with edits to fewer than 1000 knowledge pieces). Our findings emphasize the need for continued community efforts to detect and mitigate model hallucinations.","2024-10","2025-11-25 22:29:34","2025-11-25 22:29:34","","","","OOPSLA2","8","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Software Testing; Hallucination","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YRMPDR4C","conferencePaper","2024","García, Boni; Leotta, Maurizio; Ricca, Filippo; Whitehead, Jim","Use of ChatGPT as an Assistant in the End-to-End Test Script Generation for Android Apps","Proceedings of the 15th ACM International Workshop on Automating Test Case Design, Selection and Evaluation","979-8-4007-1109-1","","10.1145/3678719.3685691","https://doi.org/10.1145/3678719.3685691","Automated testing is crucial in software development to ensure that applications perform as intended. However, generating automated End-to-End (E2E) tests can be time-consuming and challenging, especially for junior developers. This study investigates the use of ChatGPT, a popular Generative Artificial Intelligence (GenAI) model, as an assistant in developing automated E2E test scripts for Android apps. We present an empirical study that compares the effort required to create E2E test scripts and the resulting reliability of these tests using two treatments: manually and assisted by ChatGPT. We used Gherkin, a domain-specific language that allows non-technical practitioners to define test scenarios using a human-readable syntax. Our findings indicate that using ChatGPT significantly reduces the time required to develop automated test scripts without compromising the reliability of the scripts. Statistical analysis shows a notable reduction in development time for the ChatGPT-assisted group compared to the manual group, with a large effect size. While the reliability of the tests did not show a significant difference between the two groups, the results suggest practical benefits in terms of efficiency.","2024","2025-11-25 22:29:34","2025-11-25 22:29:34","","5–11","","","","","","","A-TEST 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Empirical Study; Android; E2E Automated Testing; GenAI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IX5TJQ3S","conferencePaper","2024","Zhang, Yichi","Detecting Code Comment Inconsistencies using LLM and Program Analysis","Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering","979-8-4007-0658-5","","10.1145/3663529.3664458","https://doi.org/10.1145/3663529.3664458","Code comments are the most important medium for documenting program logic and design. Nevertheless, as modern software undergoes frequent updates and modifications, maintaining the accuracy and relevance of comments becomes a labor-intensive endeavor. Drawing inspiration from the remarkable performance of Large Language Model (LLM) in comprehending software programs, this paper introduces a program analysis based and LLM-driven methodology for identifying inconsistencies in code comments. Our approach capitalizes on LLMs' ability to interpret natural language descriptions within code comments, enabling the extraction of design constraints. Subsequently, we employ program analysis techniques to accurately identify the implementation of these constraints. We instantiate this methodology using GPT 4.0, focusing on three prevalent types of constraints. In the experiment on 13 open-source projects, our approach identified 160 inconsistencies, and 23 of them have been confirmed and fixed by the developers.","2024","2025-11-25 22:29:34","2025-11-25 22:29:34","","683–685","","","","","","","FSE 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","large language model; bug detection; code comment inconsistency; program analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CJ5M4TJJ","conferencePaper","2024","Liu, Kaibo; Han, Yudong; Liu, Yiyang; Chen, Zhenpeng; Zhang, Jie M.; Sarro, Federica; Huang, Gang; Ma, Yun","TrickyBugs: A Dataset of Corner-case Bugs in Plausible Programs","Proceedings of the 21st International Conference on Mining Software Repositories","979-8-4007-0587-8","","10.1145/3643991.3644870","https://doi.org/10.1145/3643991.3644870","We call a program that passes existing tests but still contains bugs as a buggy plausible program. Bugs in such a program can bypass the testing environment and enter the production environment, causing unpredictable consequences. Therefore, discovering and fixing such bugs is a fundamental and critical problem. However, no existing bug dataset is purposed to collect this kind of bug, posing significant obstacles to relevant research. To address this gap, we introduce TrickyBugs, a bug dataset with 3,043 buggy plausible programs sourced from human-written submissions of 324 real-world competition coding tasks. We identified the buggy plausible programs from approximately 400,000 submissions, and all the bugs in TrickyBugs were not previously detected. We hope that TrickyBugs can effectively facilitate research in the fields of automated program repair, fault localization, test generation, and test adequacy.","2024","2025-11-25 22:29:34","2025-11-25 22:29:34","","113–117","","","","","","","MSR '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","software testing; test generation; benchmark; program repair; test adequacy","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D8VWN6RS","conferencePaper","2025","Li, Cong; Yin, Yihan; Wu, Xintong; Zhu, Jingchen; Gao, Zhutianya; Niu, Dimin; Wu, Qiang; Si, Xin; Xie, Yuan; Zhang, Chen; Sun, Guangyu","H2-LLM: Hardware-Dataflow Co-Exploration for Heterogeneous Hybrid-Bonding-based Low-Batch LLM Inference","Proceedings of the 52nd Annual International Symposium on Computer Architecture","979-8-4007-1261-6","","10.1145/3695053.3731008","https://doi.org/10.1145/3695053.3731008","Low-batch large language model (LLM) inference has been extensively applied to edge-side generative tasks, such as personal chat helper, virtual assistant, reception bot, private edge server, etc. To efficiently handle both prefill and decoding stages in LLM inference, near-memory processing (NMP) enabled heterogeneous computation paradigm has been proposed. However, existing NMP designs typically embed processing engines into DRAM dies, resulting in limited computation capacity, which in turn restricts their ability to accelerate edge-side low-batch LLM inference.To tackle this problem, we propose H ( ^text2 )-LLM, a Hybrid-bonding-based Heterogeneous accelerator for edge-side low-batch LLM inference. To balance the trade-off between computation capacity and bandwidth intrinsic to hybrid-bonding technology, we propose H ( ^text2 )-LLM’s architecture and extract its architecture design space. We further propose a data-centric dataflow abstraction to fully exploit the heterogeneous architecture’s acceleration opportunities in low-batch LLM inference. Based on the whole design space, we propose a design space exploration (DSE) framework to automatically find out the optimal design. Compared with existing in-die NMP-based heterogeneous accelerators, H ( ^text2 )-LLM&nbsp;achieves 2.72 × geomean speedup and 1.48 × geomean better energy efficiency. H ( ^text2 )-LLM’s data-centric dataflow exploration framework is open-sourced at https://github.com/leesou/H2-LLM-ISCA-2025.","2025","2025-11-25 22:29:34","2025-11-25 22:29:34","","194–210","","","","","","","ISCA '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Large Language Model; Hybrid Bonding; Near-Memory Processing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QACQA67G","bookSection","2025","Shen, Qianwen; Ma, Zeyu; Chen, Mingang","From Chart to QA Pairs: A Context-Aware Generation Framework for Chart-Containing Documents","Proceedings of the 2025 2nd International Conference on Generative Artificial Intelligence and Information Security","979-8-4007-1345-3","","","https://doi.org/10.1145/3728725.3728741","While Large Language Models (LLMs) and Vision Language Models (VLMs) have demonstrated impressive capabilities across various general tasks, developing domain-specific model remains an urgent need. This requires fine-tuning with high-quality data. Extraction of question-answer (QA) pairs from domain knowledge documents is a crucial prerequisite for fine-tuning, especially when these documents containing complex charts and unstructured data. This paper introduces a framework for extracting QA pairs from charts, which combines OCR and VLM to fully leverage contextual information in the documents and generate deep, contextually relevant questions. Our method first uses OCR for initial chart recognition, followed by a VLM-based “Caption-Reflection” paradigm to reduce misrecognition. We then design a context localization module which combines local and global contexts to generate relevant QA pairs. Additionally, we construct a specialized chart knowledge base to guide the generation of QA pairs. Experimental results show significant improvement in both chart recognition performance and QA pairs generation quality: the average precision increases from 0.52 to 0.90, the average F1 Score improves from 0.64 to 0.94, the average FPR decreases from 0.55 to 0 respectively. The generated QA pairs perform well in terms of statistical relevance and contextual consistency.","2025","2025-11-25 22:29:34","2025-11-25 22:29:34","","101–107","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VV2K3FFH","journalArticle","2025","Rahman, Shanto; Kuhar, Sachit; Cirisci, Berk; Garg, Pranav; Wang, Shiqi; Ma, Xiaofei; Deoras, Anoop; Ray, Baishakhi","UTFix: Change Aware Unit Test Repairing using LLM","Proc. ACM Program. Lang.","","","10.1145/3720419","https://doi.org/10.1145/3720419","Software updates, including bug repair and feature additions, are frequent in modern applications but they often leave test suites outdated, resulting in undetected bugs and increased chances of system failures. A recent study by Meta revealed that 14%-22% of software failures stem from outdated tests that fail to reflect changes in the codebase. This highlights the need to keep tests in sync with code changes to ensure software reliability. In this paper, we present UTFix, a novel approach for repairing unit tests when their corresponding focal methods undergo changes. UTFix addresses two critical issues: assertion failure and reduced code coverage caused by changes in the focal method. Our approach leverages language models to repair unit tests by providing contextual information such as static code slices, dynamic code slices, and failure messages. We evaluate UTFix on our generated synthetic benchmarks (Tool-Bench), and real-world benchmarks. Tool- Bench includes diverse changes from popular open-source Python GitHub projects, where UTFix successfully repaired 89.2% of assertion failures and achieved 100% code coverage for 96 tests out of 369 tests. On the real-world benchmarks, UTFix repairs 60% of assertion failures while achieving 100% code coverage for 19 out of 30 unit tests. To the best of our knowledge, this is the first comprehensive study focused on unit test in evolving Python projects. Our contributions include the development of UTFix, the creation of Tool-Bench and real-world benchmarks, and the demonstration of the effectiveness of LLM-based methods in addressing unit test failures due to software evolution.","2025-04","2025-11-25 22:29:34","2025-11-25 22:29:34","","","","OOPSLA1","9","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; Software Testing; Change Aware Test Repair; Unit Tests","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E8YMBFEA","journalArticle","2025","Liu, Puzhuo; Sun, Chengnian; Zheng, Yaowen; Feng, Xuan; Qin, Chuan; Wang, Yuncheng; Xu, Zhenyang; Li, Zhi; Di, Peng; Jiang, Yu; Sun, Limin","LLM-Powered Static Binary Taint Analysis","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3711816","https://doi.org/10.1145/3711816","This article proposes LATTE, the first static binary taint analysis that is powered by a large language model (LLM). LATTE is superior to the state of the art (e.g., Emtaint, Arbiter, Karonte) in three aspects. First, LATTE is fully automated while prior static binary taint analyzers need rely on human expertise to manually customize taint propagation rules and vulnerability inspection rules. Second, LATTE is significantly effective in vulnerability detection, demonstrated by our comprehensive evaluations. For example, LATTE has found 37 new bugs in real-world firmware, which the baselines failed to find. Moreover, 10 of them have been assigned CVE numbers. Lastly, LATTE incurs remarkably low engineering cost, making it a cost-efficient and scalable solution for security researchers and practitioners. We strongly believe that LATTE opens up a new direction to harness the recent advance in LLMs to improve vulnerability analysis for binary programs.","2025-02","2025-11-25 22:29:34","2025-11-25 22:29:34","","","","3","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","large language model; vulnerability; binary; taint analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CGFPRP3B","conferencePaper","2023","Fulcini, Tommaso; Torchiano, Marco","Is ChatGPT Capable of Crafting Gamification Strategies for Software Engineering Tasks?","Proceedings of the 2nd International Workshop on Gamification in Software Development, Verification, and Validation","979-8-4007-0373-7","","10.1145/3617553.3617887","https://doi.org/10.1145/3617553.3617887","Gamification has gained significant attention in the last decade for its potential to enhance engagement and motivation in various domains. During the last year ChatGPT, a state-of-the-art large language model has received even more attention both in the field of scientific research and in common use by individuals or companies. In this study, we investigate the possibility of adopting ChatGPT as a tool for designing gamification platforms in the Software Engineering domain. Leveraging the capabilities of ChatGPT, we assess how good is it at generating effective suggestions and ideas for designers or developers. To evaluate ChatGPT's potential as a gamification platform creator we narrowed the context to one particular Software Engineering activity, asking for possible aspects of the activity to be gamified. Each proposed aspect was subsequently unraveled by ChatGPT both asking in a shared and separate context, first following the conversational nature of the model, then applying a validated design framework. The study assesses ChatGPT's ability to select and integrate game elements to build a thriving gamification environment by framing the design of the platform to a state-of-the-art conceptual framework. To evaluate the goodness of the design choices made we relied both on the Octalysis framework and on personal experience. The findings of the papers show that ChatGPT can only create simple playful experiences not very effective. Although, by instructing the model with more specific desired mechanics and dynamics, it is possible to guide it toward the application of the ideas suggested. We argue that ChatGPT is not capable of building a gamified environment on its own, but it could still be used to build the foundation of a gamification platform as long as the designers refine and rough out the advice gained from a user-centered solution.","2023","2025-11-25 22:29:34","2025-11-25 22:29:34","","22–28","","","","","","","Gamify 2023","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: San Francisco, CA, USA","","","","Large Language Model; Software Engineering; Artificial Intelligence; Gamification; Software Lifecycle","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7NT3I5Z2","conferencePaper","2025","Gao, Hongyan; Yang, Yibiao; Sun, Maolin; Wu, Jiangchang; Zhou, Yuming; Xu, Baowen","ClozeMaster: Fuzzing Rust Compiler by Harnessing LLMs for Infilling Masked Real Programs","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00175","https://doi.org/10.1109/ICSE55347.2025.00175","Ensuring the reliability of the Rust compiler is of paramount importance, given increasing adoption of Rust for critical systems development, due to its emphasis on memory and thread safety. However, generating valid test programs for the Rust compiler poses significant challenges, given Rust's complex syntax and strict requirements. With the growing popularity of large language models (LLMs), much research in software testing has explored using LLMs to generate test cases. Still, directly using LLMs to generate Rust programs often results in a large number of invalid test cases. Existing studies have indicated that test cases triggering historical compiler bugs can assist in software testing. Our investigation into Rust compiler bug issues supports this observation. Inspired by existing work and our empirical research, we introduce a bracket-based masking and filling strategy called clozeMask. The clozeMask strategy involves extracting test code from historical issue reports, identifying and masking code snippets with specific structures, and using an LLM to fill in the masked portions for synthesizing new test programs. This approach harnesses the generative capabilities of LLMs while retaining the ability to trigger Rust compiler bugs. It enables comprehensive testing of the compiler's behavior, particularly exploring edge cases. We implemented our approach as a prototype ClozeMaster. ClozeMaster has identified 27 confirmed bugs for rustc and mrustc, of which 10 have been fixed by developers. Furthermore, our experimental results indicate that ClozeMaster outperforms existing fuzzers in terms of code coverage and effectiveness.","2025","2025-11-25 22:29:34","2025-11-25 22:47:06","","1422–1435","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","large language model; Large language models; Software engineering; Syntactics; Codes; Prototypes; Computer bugs; Fuzzing; fuzzing; bug detection; rust compiler; Reliability; Safety; Instruction sets; Rust Compiler","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BCBZCRZU","journalArticle","2025","Yang, Chen; Chen, Junjie; Lin, Bin; Wang, Ziqi; Zhou, Jianyi","Advancing Code Coverage: Incorporating Program Analysis with Large Language Models","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3748505","https://doi.org/10.1145/3748505","Automatic test generation plays a critical role in software quality assurance. While the recent advances in Search-Based Software Testing (SBST) and Large Language Models (LLMs) have shown promise in generating useful tests, these techniques still struggle to cover certain branches. Reaching these hard-to-cover branches usually requires constructing complex objects and resolving intricate inter-procedural dependencies in branch conditions, which poses significant challenges for existing techniques. In this work, we propose ℡PA, a novel technique aimed at addressing these challenges. Its key insight lies in extracting real usage scenarios of the target method under test to learn how to construct complex objects and extracting methods entailing inter-procedural dependencies with hard-to-cover branches to learn the semantics of branch constraints. To enhance efficiency and effectiveness, ℡PA identifies a set of ineffective tests as counter-examples for LLMs and employs a feedback-based process to iteratively refine these counter-examples. Then, ℡PA integrates program analysis results and counter-examples into the prompt, guiding LLMs to gain deeper understandings of the semantics of the target method and generate diverse tests that can reach the hard-to-cover branches. Our experimental results on 27 open-source Python projects demonstrate that ℡PA significantly outperforms the state-of-the-art SBST and LLM-enhanced techniques, achieving an average improvement of 34.10% and 25.93% in terms of branch coverage.","2025-07","2025-11-25 22:29:34","2025-11-25 22:29:34","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; Test Generation; Program Analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3C6GEY5F","conferencePaper","2025","Xu, Junjielong; Fu, Ying; Tan, Shin Hwei; He, Pinjia","Aligning the Objective of LLM-Based Program Repair","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00169","https://doi.org/10.1109/ICSE55347.2025.00169","Large language models (LLMs) have achieved decent results on automated program repair (APR). However, the next token prediction training objective of decoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction objective of current infilling-style methods, which impedes LLMs from fully leveraging pre-trained knowledge for program repair. In addition, while some LLMs can locate and repair bugs in certain functions using the related artifacts (e.g., test cases), existing methods still depend on statement-level fault localization methods to provide a list of buggy hunks for repair. This restriction hinders LLMs from exploring potential patches beyond the given locations.In this paper, we investigate a new approach to adapt LLMs to program repair. Our core insight is that LLM's APR capability can be greatly improved by simply aligning the output to their training objective and allowing them to refine the whole program without first identifying faulty statements. Based on this insight, we designed D4C, a straightforward prompting framework for APR. D4C can repair 180 bugs correctly in Defects4J, with each patch being sampled only 10 times. This surpasses the SOTA APR methods with perfect fault localization by 10% and reduces the patch sampling number by 90%. Our findings reveal that (1) objective alignment is crucial for fully exploiting LLM's pre-trained capability, and (2) replacing the traditional localize-buggy-hunks-then-repair workflow with direct debugging is more effective for LLM-based APR methods. Thus, we believe this paper introduces a new mindset for harnessing LLMs in APR.","2025","2025-11-25 22:29:34","2025-11-25 22:29:34","","2548–2560","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","large language model; automated program repair; objective alignment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BNUJ742R","conferencePaper","2025","Tang, Xiaoxuan; Chen, Xinfang; Chen, Dajun; Zhou, Sheng; Jiang, Wei; Li, Yong","TestFlow: Advancing Mobile UI Testing through Multi-Step Reinforcement Learning","Proceedings of the 34th ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-1474-0","","10.1145/3713081.3732930","https://doi.org/10.1145/3713081.3732930","GUI Agents have demonstrated promising applications in mobile UI testing. However, for complex testing tasks, UI agents tend to fail due to their greedy approach in executing step-by-step operations, leading to error accumulation and neglecting long-horizon dependencies. To address these limitations, we propose TestFlow, a novel multi-modal UI testing model that combines Supervised Fine-Tuning with a Task-aware Reinforcement Learning framework. Our approach implements a two-phase training pipeline designed to optimize long-horizon instruction compliance and complex task completion. Additionally, we develop a tailor-made reward function that integrates both process and outcome rewards to improve the completion rate of multi-step tasks. The experimental results demonstrate that TestFlow significantly outperforms the baseline methods, achieving 33. 69% WTSR and 55. 37% SSR in cross-page test scenarios. These improvements highlight the practical value of TestFlow in addressing the challenges of modern mobile app testing, particularly in industrial settings requiring high adaptability and reliability.","2025","2025-11-25 22:29:34","2025-11-25 22:29:34","","190–194","","","","","","","ISSTA Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","test automation; LLM-based Agent; vision-language Models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IV34ZZM5","conferencePaper","2025","Du, Yalong; Wang, Chaozheng; Wang, Huaijin","Programming Language Techniques for Bridging LLM Code Generation Semantic Gaps","Proceedings of the 1st ACM SIGPLAN International Workshop on Language Models and Programming Languages","979-8-4007-2148-9","","10.1145/3759425.3763383","https://doi.org/10.1145/3759425.3763383","Large Language Models have demonstrated remarkable capabilities in automated code generation, yet their statistical nature and black-box characteristics create significant semantic gaps manifested through syntax errors, semantic hallucinations, and reliability concerns. This position paper argues that principled integration of Programming Language (PL) techniques is essential for bridging these gaps. Through structured program representations, formal correctness guarantees, and robust verification mechanisms, PL techniques can elevate LLM-generated code from statistical pattern matching to truly reliable and trustworthy levels. This integration is crucial for developing systems that generate code that is not only functionally correct but also interpretable, verifiable, and ultimately trustworthy.","2025","2025-11-25 22:29:34","2025-11-25 22:29:34","","40–45","","","","","","","LMPL '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Singapore, Singapore","","","","Large Language Model; Code Generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SX7C9IJV","conferencePaper","2023","Deng, Yinlin; Xia, Chunqiu Steven; Peng, Haoran; Yang, Chenyuan; Zhang, Lingming","Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models","Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0221-1","","10.1145/3597926.3598067","https://doi.org/10.1145/3597926.3598067","Deep Learning (DL) systems have received exponential growth in popularity and have become ubiquitous in our everyday life. Such systems are built on top of popular DL libraries, e.g., TensorFlow and PyTorch which provide APIs as building blocks for DL systems. Detecting bugs in these DL libraries is critical for almost all downstream DL systems in ensuring effectiveness/safety for end users. Meanwhile, traditional fuzzing techniques can be hardly effective for such a challenging domain since the input DL programs need to satisfy both the input language (e.g., Python) syntax/semantics and the DL API input/shape constraints for tensor computations. To address these limitations, we propose TitanFuzz – the first approach to directly leveraging Large Language Models (LLMs) to generate input programs for fuzzing DL libraries. LLMs are titanic models trained on billions of code snippets and can autoregressively generate human-like code snippets. Our key insight is that modern LLMs can also include numerous code snippets invoking DL library APIs in their training corpora, and thus can implicitly learn both language syntax/semantics and intricate DL API constraints for valid DL program generation. More specifically, we use both generative and infilling LLMs (e.g., Codex/InCoder) to generate and mutate valid/diverse input DL programs for fuzzing. Our experimental results demonstrate that TitanFuzz can achieve 30.38%/50.84% higher code coverage than state-of-the-art fuzzers on TensorFlow/PyTorch. Furthermore, TitanFuzz is able to detect 65 bugs, with 44 already confirmed as previously unknown bugs. This paper demonstrates that modern titanic LLMs can be leveraged to directly perform both generation-based and mutation-based fuzzing studied for decades, while being fully automated, generalizable, and applicable to domains challenging for traditional approaches (such as DL systems). We hope TitanFuzz can stimulate more work in this promising direction of LLMs for fuzzing.","2023","2025-11-25 22:29:34","2025-11-25 22:29:34","","423–435","","","","","","","ISSTA 2023","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Seattle, WA, USA","","","","Large Language Model; Test Generation; Fuzz Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QGH8Z7XS","conferencePaper","2024","Xue, Zhipeng; Gao, Zhipeng; Wang, Shaohua; Hu, Xing; Xia, Xin; Li, Shanping","SelfPiCo: Self-Guided Partial Code Execution with LLMs","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680368","https://doi.org/10.1145/3650212.3680368","Code executability plays a vital role in software debugging and testing (e.g., detecting runtime exceptions or assertion violations). However, code execution, especially partial or arbitrary code execution, is a non-trivial task due to missing definitions and complex third-party dependencies. To make partial code (such as code snippets posted on the web or code fragments deep inside complex software projects) executable, the existing study has proposed a machine learning model to predict the undefined element types and inject the pre-defined dummy values into execution. However, the performance of their tool is limited due to its simply designed dummy values and the inability to continue learning. In this paper, we design and implement a novel framework, named SelfPiCo (Self-Guided Partial Code Executor), to dynamically guide partial code execution by incorporating the open-source LLM (i.e., Code Llama) within an interactive loop. Particularly, SelfPiCo leverages few-shot in-context learning and chain-of-thought reasoning to elicit human knowledge and logical reasoning based on fine-tuning the Code Llama model. SelfPiCo continuously learns from code execution results and refines its predictions step after step. Our evaluations demonstrate that SelfPiCo can execute 72.7% and 83.3% of all lines in the open-source code and Stack Overflow snippets, outperforming the most recent state-of-the-art Lexecutor by 37.9% and 33.5%, respectively. Moreover, SelfPiCo successfully detected 18 and 33 runtime type error issues by executing the partial code from eight GitHub software projects and 43 Stack Overflow posts, demonstrating the practical usage and potential application of our framework in practice.","2024","2025-11-25 22:29:34","2025-11-25 22:29:34","","1389–1401","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large Language Model; Prompt Engineering; Dynamic Analysis; Partial Code Execution","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"85TGKM5G","journalArticle","2025","Kim, Nam Wook; Ahn, Yongsu; Myers, Grace; Bach, Benjamin","How Good Is ChatGPT in Giving Advice on Your Visualization Design?","ACM Trans. Comput.-Hum. Interact.","","1073-0516","10.1145/3745768","https://doi.org/10.1145/3745768","Data visualization creators often lack formal training, resulting in a knowledge gap in design practice. Large-language models such as ChatGPT, with their vast internet-scale training data, offer transformative potential to address this gap. In this study, we used both qualitative and quantitative methods to investigate how well ChatGPT can address visualization design questions. First, we quantitatively compared the ChatGPT-generated responses with anonymous online Human replies to data visualization questions on the VisGuides user forum. Next, we conducted a qualitative user study examining the reactions and attitudes of practitioners toward ChatGPT as a visualization design assistant. Participants were asked to bring their visualizations and design questions and received feedback from both Human experts and ChatGPT in randomized order. Our findings from both studies underscore ChatGPT’s strengths—particularly its ability to rapidly generate diverse design options—while also highlighting areas for improvement, such as nuanced contextual understanding and fluid interaction dynamics beyond the chat interface. Drawing on these insights, we discuss design considerations for future LLM-based design feedback systems.","2025-10","2025-11-25 22:29:34","2025-11-25 22:29:34","","","","5","32","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","LLM; ChatGPT; AI; data visualization; design feedback","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LK3VNLLW","journalArticle","2024","Feng, Xiaoning; Han, Xiaohong; Chen, Simin; Yang, Wei","LLMEffiChecker: Understanding and Testing Efficiency Degradation of Large Language Models","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3664812","https://doi.org/10.1145/3664812","Large Language Models (LLMs) have received much recent attention due to their human-level accuracy. While existing works mostly focus on either improving accuracy or testing accuracy robustness, the computation efficiency of LLMs, which is of paramount importance due to often vast generation demands and real-time requirements, has surprisingly received little attention. In this article, we make the first attempt to understand and test potential computation efficiency robustness in state-of-the-art LLMs. By analyzing the working mechanism and implementation of 20,543 public-accessible LLMs, we observe a fundamental property in LLMs that could be manipulated in an adversarial manner to reduce computation efficiency significantly. Our interesting observation is that the output length determines the computation efficiency of LLMs instead of the input, where the output length depends on two factors: an often sufficiently large yet pessimistic pre-configured threshold controlling the max number of iterations and a runtime-generated end of sentence (EOS) token. Our key motivation is to generate test inputs that could sufficiently delay the generation of EOS such that LLMs would have to go through enough iterations to satisfy the pre-configured threshold. We present LLMEffiChecker, which can work under both white-box setting and black-box setting. In the white-box scenario, LLMEffiChecker develops a gradient-guided technique that searches for a minimal and unnoticeable perturbation at character-level, token-level, and structure-level. In the black-box scenario, LLMEffiChecker employs a causal inference-based approach to find critical tokens and similarly applies three levels of imperceptible perturbation to them. Both the white-box and black-box settings effectively delay the appearance of EOS, compelling these inputs to reach the naturally unreachable threshold. To demonstrate the effectiveness of LLMEffiChecker, we conduct a systematic evaluation on nine publicly available LLMs: Google T5, AllenAI WMT14, Helsinki-NLP translator, Facebook FairSeq, UNICAMP-DL translator, MarianMT, Google FLAN-T5, MBZUAI LaMini-GPT, and Salesforce CodeGen. Experimental results show that LLMEffiChecker can increase on average LLMs’ response latency and energy consumption by 325% to 3,244% and 344% to 3,616%, respectively, by perturbing just one character or token in the input sentence. Our case study shows that inputs generated by LLMEffiChecker significantly affect the battery power in real-world mobile devices (i.e., drain more than 30 times battery power than normal inputs).","2024-08","2025-11-25 22:29:34","2025-11-25 22:29:34","","","","7","33","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","large language model; software testing; Machine learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"65SJKKLB","conferencePaper","2025","Fang, Zihan; Li, Jiliang; Liang, Anda; Bai, Gina R.; Huang, Yu","A Comparative Study on ChatGPT and Checklist as Support Tools for Unit Testing Education","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","10.1145/3696630.3727244","https://doi.org/10.1145/3696630.3727244","Testing is widely practiced in software engineering, and many tools have been developed to support students in learning testing. Prior research suggests that a lightweight testing checklist improves learning outcomes but doesn't address students' challenges in writing test code that matches their intentions or design. Meanwhile, generative AI tools (e.g., ChatGPT) bring new promise as another form of software assistance tool. In this study, we examined the impact of various support tools (checklist, ChatGPT, or both) on unit testing among 42 students. Our results indicated that using these tools individually or in combination produced a comparable effect on student performance in unit testing. Students preferred using the checklist but acknowledged ChatGPT's effectiveness in accelerating task completion and addressing programming language challenges. While ChatGPT demonstrated potential benefits for testing education, it did not overcome the implementation challenges identified in the previous study. Moreover, reliance on ChatGPT may hinder students' deeper engagement with new concepts, which is crucial for comprehensive learning, as they often interact superficially with AI-generated responses without employing the critical thinking necessary to evaluate the information provided. Therefore, we proposed recommendations for both students and instructors on adapting to learning and teaching in the AI era and offer insights into the evolving role of AI in education.","2025","2025-11-25 22:29:35","2025-11-25 22:29:35","","871–882","","","","","","","FSE Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","ChatGPT; unit testing; checklist; testing education","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K6CATHY6","journalArticle","2025","Ouyang, Shuyin; Zhang, Jie M.; Harman, Mark; Wang, Meng","An Empirical Study of the Non-Determinism of ChatGPT in Code Generation","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3697010","https://doi.org/10.1145/3697010","There has been a recent explosion of research on Large Language Models (LLMs) for software engineering tasks, in particular code generation. However, results from LLMs can be highly unstable; non-deterministically returning very different code for the same prompt. Such non-determinism affects the correctness and consistency of the generated code, undermines developers’ trust in LLMs, and yields low reproducibility in LLM-based papers. Nevertheless, there is no work investigating how serious this non-determinism threat is.To fill this gap, this article conducts an empirical study on the non-determinism of ChatGPT in code generation. We chose to study ChatGPT because it is already highly prevalent in the code generation research literature. We report results from a study of 829 code generation problems across three code generation benchmarks (i.e., CodeContests, APPS and HumanEval) with three aspects of code similarities: semantic similarity, syntactic similarity, and structural similarity. Our results reveal that ChatGPT exhibits a high degree of non-determinism under the default setting: the ratio of coding tasks with zero equal test output across different requests is 75.76%, 51.00% and 47.56% for three different code generation datasets (i.e., CodeContests, APPS and HumanEval), respectively. In addition, we find that setting the temperature to 0 does not guarantee determinism in code generation, although it indeed brings less non-determinism than the default configuration (temperature (=) 1). In order to put LLM-based research on firmer scientific foundations, researchers need to take into account non-determinism in drawing their conclusions.","2025-01","2025-11-25 22:29:35","2025-11-25 22:29:35","","","","2","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","large language model; code generation; non-determinism","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VRP28ZZK","journalArticle","2025","Chi, Jianlei; Wang, Xiaotian; Huang, Yuhan; Yu, Lechen; Cui, Di; Sun, Jianguo; Sun, Jun","REACCEPT: Automated Co-evolution of Production and Test Code Based on Dynamic Validation and Large Language Models","Proc. ACM Softw. Eng.","","","10.1145/3728930","https://doi.org/10.1145/3728930","Synchronizing production and test code, known as PT co-evolution, is critical for software quality. Given the significant manual effort involved, researchers have tried automating PT co-evolution using predefined heuristics and machine learning models. However, existing solutions are still incomplete. Most approaches only detect and flag obsolete test cases, leaving developers to manually update them. Meanwhile, existing solutions may suffer from low accuracy, especially when applied to real-world software projects. In this paper, we propose ReAccept, a novel approach leveraging large language models (LLMs), retrievalaugmented generation (RAG), and dynamic validation to fully automate PT co-evolution with high accuracy. ReAccept employs an experience-guided approach to generate prompt templates for the identification and subsequent update processes. After updating a test case, ReAccept performs dynamic validation by checking syntax, verifying semantics, and assessing test coverage. If the validation fails, ReAccept leverages the error messages to iteratively refine the patch. To evaluate ReAccept's effectiveness, we conducted extensive experiments with a dataset of 537 Java projects and compared ReAccept's performance with several stateof-the-art methods. The evaluation results show that ReAccept achieved an update accuracy of 60.16% on the correctly identified obsolete test code, surpassing the state-of-the-art technique CEPROT by 90%. These findings demonstrate that ReAccept can effectively maintain test code, improve overall software quality, and significantly reduce maintenance effort.","2025-06","2025-11-25 22:29:35","2025-11-25 22:29:35","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Test Generation; Dynamic Validation; Product-Test Co-evolution","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HZNMZAG5","conferencePaper","2023","Guerino, Lucca; Vincenzi, Auri","An Experimental Study Evaluating Cost, Adequacy, and Effectiveness of Pynguin's Test Sets","Proceedings of the 8th Brazilian Symposium on Systematic and Automated Software Testing","979-8-4007-1629-4","","10.1145/3624032.3624034","https://doi.org/10.1145/3624032.3624034","Context: Software testing is a very relevant step in quality assurance, but developers frequently overlook it. We pursued testing automation to minimize the impact of missing test cases in a software project. Problem: However, for Python programs, there are not many tools able to fully automate the generation of unit test sets, and the one available demands studies to provide evidence of the quality of the generated test set. Solution: This work aims to evaluate the quality of different unit test generation algorithms for Python, implemented in a tool named Pynguin. Method: In the analysis of the selected programs, the Pynguin test generation tool is executed with each of its algorithms, including random, as a way to generate complete unit test sets. Then, we evaluate each generated test set’s efficacy, efficiency, and cost. We use four different fault models, implemented by four mutation testing tools, to measure efficacy. We use line and branch coverage to measure efficiency, the number of test cases, and test set execution time to measure cost. Summary of Results: We identified that RANDOM test set performed worst concerning all evaluated aspects, DYNAMOSA and MOSA, the two algorithms that generate the best test sets regarding efficacy, efficiency, and cost. By combining all Pynguin smart algorithms (DYNAMOSA, MIO, MOSA, WHOLE-SUITE), the resultant test set overcomes the individual test sets efficiency by around 1%, for coverage and efficacy by 4.5% on average, concerning previous mutation score, at a reasonable cost, without a test set minimization.","2023","2025-11-25 22:29:35","2025-11-25 22:29:35","","5–14","","","","","","","SAST '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Campo Grande, MS, Brazil","","","","software testing; automated test generation; coverage testing; experimental software engineering; mutation testing; testing tools","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WUHN6LRM","conferencePaper","2024","Chen, Yang; Jabbarvand, Reyhaneh","Can ChatGPT Repair Non-Order-Dependent Flaky Tests?","Proceedings of the 1st International Workshop on Flaky Tests","979-8-4007-0558-8","","10.1145/3643656.3643900","https://doi.org/10.1145/3643656.3643900","Regression testing helps developers check whether the latest code changes break software functionality. Flaky tests, which can non-deterministically pass or fail on the same code version, may mislead developers' concerns, resulting in missing some bugs or spending time pinpointing bugs that do not exist. Existing flakiness detection and mitigation techniques have primarily focused on general order-dependent (OD) and implementation-dependent (ID) flaky tests. There is also a dearth of research on repairing test flakiness, out of which, mostly have focused on repairing OD flaky tests, and a few have explored repairing a subcategory of non-order-dependent (NOD) flaky tests that are caused by asynchronous waits. As a result, there is a demand for devising techniques to reproduce, detect, and repair NOD flaky tests. Large language models (LLMs) have shown great effectiveness in several programming tasks. To explore the potential of LLMs in addressing NOD flakiness, this paper investigates the possibility of using ChatGPT to repair different categories of NOD flaky tests. Our comprehensive study on 118 from the IDoFT dataset shows that ChatGPT, despite as a leading LLM with notable success in multiple code generation tasks, is ineffective in repairing NOD test flakiness, even by following the best practices for prompt crafting. We investigated the reasons behind the failure of using ChatGPT in repairing NOD tests, which provided us valuable insights about the next step to advance the field of NOD test flakiness repair.","2024","2025-11-25 22:29:35","2025-11-25 22:47:05","","22–29","","","","","","","FTW '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","Large language models; software testing; Software testing; Codes; Computer bugs; large language models; Large Language Models; Debugging; Software Testing; test flakiness; Test Flakiness; Prevention and mitigation; Maintenance engineering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8KLSI7KE","journalArticle","2025","Kabir, Azmain; Wang, Shaowei; Tian, Yuan; Chen, Tse-Hsun (Peter); Asaduzzaman, Muhammad; Zhang, Wenbin","ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code Snippets Using LLMs","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3702979","https://doi.org/10.1145/3702979","Technical Q&amp;A sites are valuable for software developers seeking knowledge, but the code snippets they provide are often uncompilable and incomplete due to unresolved types and missing libraries. This poses a challenge for users who wish to reuse or analyze these snippets. Existing methods either do not focus on creating compilable code or have low success rates. To address this, we propose ZS4C, a lightweight approach for zero-shot synthesis of compilable code from incomplete snippets using Large Language Models (LLMs). ZS4C operates in two stages: first, it uses an LLM, like GPT-3.5, to identify missing import statements in a snippet; second, it collaborates with a validator (e.g., compiler) to fix compilation errors caused by incorrect imports and syntax issues. We evaluated ZS4C on the StatType-SO benchmark and a new dataset, Python-SO, which includes 539 Python snippets from Stack Overflow across the 20 most popular Python libraries. ZS4C significantly outperforms existing methods, improving the compilation rate from 63% to 95.1% compared to the state-of-the-art SnR, marking a 50.1% improvement. On average, ZS4C can infer more accurate import statements (with an F1 score of 0.98) than SnR, with an improvement of 8.5% in the F1.","2025-04","2025-11-25 22:29:35","2025-11-25 22:29:35","","","","4","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","ChatGPT; Large Language Model; Prompt Engineering; API inference; incomplete code snippets; Program synthesis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4IJ4Y2SZ","bookSection","2025","Hans, Sandeep; Kumar, Atul; Yasue, Toshiaki; Ono, Kouichi; Krishnan, Saravanan; Sondhi, Devika; Satoh, Fumiko; Mitchell, Gerald; Kumar, Sachin; Saha, Diptikalyan","Automated Testing of COBOL to Java Transformation","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","","https://doi.org/10.1145/3696630.3728548","Recent advances in Large Language Model (LLM) based Generative AI techniques have made it feasible to translate enterprise-level code from legacy languages such as COBOL to modern languages such as Java or Python. While the results of LLM-based automatic transformation are encouraging, the resulting code cannot be trusted to correctly translate the original code, making manual validation of translated Java code from COBOL a necessary but time-consuming and labor-intensive process. In this paper, we share our experience of developing a testing framework for IBM Watsonx Code Assistant for Z (WCA4Z) [5], an industrial tool designed for COBOL to Java translation. The framework automates the process of testing the functional equivalence of the translated Java code against the original COBOL programs in an industry context. Our framework uses symbolic execution to generate unit tests for COBOL, mocking external calls and transforming them into JUnit tests to validate semantic equivalence with translated Java. The results not only help identify and repair any detected discrepancies but also provide feedback to improve the AI model.","2025","2025-11-25 22:29:35","2025-11-25 22:29:35","","227–237","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P2NYVRFS","journalArticle","2025","Wen, Jinfeng; Chen, Zhenpeng; Zhu, Zixi; Sarro, Federica; Liu, Yi; Ping, Haodi; Wang, Shangguang","LLM-Based Misconfiguration Detection for AWS Serverless Computing","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3745766","https://doi.org/10.1145/3745766","Serverless computing is a popular cloud computing paradigm that enables developers to build applications at the function level, known as serverless applications. The Serverless Application Model (AWS SAM) is the most widely adopted configuration schema. However, misconfigurations pose a significant challenge due to the complexity of serverless configurations and the limitations of traditional data-driven techniques. Recent advancements in Large Language Models (LLMs), pre-trained on large-scale public data, offer promising potential for identifying and explaining misconfigurations. In this paper, we present SlsDetector, the first framework that harnesses the capabilities of LLMs to perform static misconfiguration detection in serverless applications. SlsDetector utilizes effective prompt engineering with zero-shot prompting to identify configuration issues. It designs multi-dimensional constraints aligned with serverless configuration characteristics and leverages the Chain of Thought technique to enhance LLM inferences, alongside generating structured responses. We evaluate SlsDetector on a curated dataset of 110 configuration files, which includes correct configurations, real-world misconfigurations, and intentionally injected errors. Our results show that SlsDetector, based on ChatGPT-4o (one of the most representative LLMs), achieves a precision of 72.88%, recall of 88.18%, and F1-score of 79.75%, outperforming state-of-the-art data-driven methods by 53.82, 17.40, and 49.72 percentage points, respectively. We further investigate the generalization capability of SlsDetector across recent LLMs, including Llama 3.1 (405B) Instruct Turbo, Gemini 1.5 Pro, and DeepSeek V3, with consistently high effectiveness.","2025-06","2025-11-25 22:29:35","2025-11-25 22:29:35","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large language model; Serverless computing; Software configuration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DZF27JX3","conferencePaper","2024","Wen, Xin-Cheng","Collaboration to Repository-Level Vulnerability Detection","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3685562","https://doi.org/10.1145/3650212.3685562","Large Language Model (LLM)-based methods have proven to be effective for many software engineering domains, with a potential for substantial productivity effective for software vulnerability detection. However, due to the limitation of the length of input contexts of LLM, the existing LLM-based methods mainly focus on detecting function-level and leveraging the in-file context information for vulnerability detection (i.e., intra-procedural vulnerabilities), ignoring the more complex inter-procedural vulnerability detection scenarios in practice. For instance, in real-world scenarios, developers routinely engage with program analysis to detect vulnerabilities that span multiple cross-file information within repositories. Since complex processes tend to have redundancy dependencies from spanning multiple files in the repository level and invoking multiple static analysis tools, the ideal goal of vulnerability detection is to extract the vulnerability-related information from the repository and provide potential possible explanations for vulnerability triggers. However, such a goal is hard to achieve, and thus in this work, we design three works through multi-agent collaboration to approach the goal of repository-level vulnerability detection.","2024","2025-11-25 22:29:35","2025-11-25 22:29:35","","1926–1928","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large Language Model; Software Vulnerability Detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XR4LUCEQ","conferencePaper","2025","Zhao, Shanhui; Wen, Hao; Du, Wenjie; Liang, Cheng; Liu, Yunxin; Ye, Xiaozhou; Ouyang, Ye; Li, Yuanchun","LLM-Explorer: Towards Efficient and Affordable LLM-based Exploration for Mobile Apps","Proceedings of the 31st Annual International Conference on Mobile Computing and Networking","979-8-4007-1129-9","","10.1145/3680207.3723494","https://doi.org/10.1145/3680207.3723494","Large language models (LLMs) have opened new opportunities for automated mobile app exploration, an important and challenging problem that used to suffer from the difficulty of generating meaningful UI interactions. However, existing LLM-based exploration approaches rely heavily on LLMs to generate actions in almost every step, leading to a huge cost of token fees and computational resources. We argue that such extensive usage of LLMs is neither necessary nor effective, since many actions during exploration do not require, or may even be biased by the abilities of LLMs. Further, based on the insight that a precise and compact knowledge plays the central role for effective exploration, we introduce LLM-Explorer, a new exploration agent designed for efficiency and affordability. LLM-Explorer uses LLMs primarily for maintaining the knowledge instead of generating actions, and knowledge is used to guide action generation in a LLM-less manner. Based on a comparison with 5 strong baselines on 20 typical apps, LLM-Explorer was able to achieve the fastest and highest coverage among all automated app explorers, with over 148x lower cost than the state-of-the-art LLM-based approach.","2025","2025-11-25 22:29:35","2025-11-25 22:29:35","","589–603","","","","","","","ACM MOBICOM '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Kerry Hotel, Hong Kong, Hong Kong, China","","","","software testing; large language models; LLM-based agent; mobile app exploration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X7NPBNN5","journalArticle","2025","Li, Meiziniu; Li, Dongze; Liu, Jianmeng; Cao, Jialun; Tian, Yongqiang; Cheung, Shing-Chi","Enhancing Differential Testing With LLMs For Testing Deep Learning Libraries","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3735637","https://doi.org/10.1145/3735637","Differential testing offers a promising strategy to alleviate the test oracle problem by comparing the test results between alternative implementations. However, existing differential testing techniques for deep learning (DL) libraries are limited by the key challenges of finding alternative implementations (called (counterparts) ) for a given API and subsequently generating diverse test inputs. To address the two challenges, this paper introduces DLLens, an LLM-enhanced differential testing technique for DL libraries. The first challenge is addressed by an observation that DL libraries are commonly designed to support the computation of a similar set of DL algorithms. Therefore, the counterpart of a given API’s computation could be successfully synthesized through certain composition and adaptation of the APIs from another DL library. DLLens incorporates a novel counterpart synthesis workflow, leveraging a large language model (LLM) to search for valid counterparts for differential testing. To address the second challenge, DLLens incorporates a static analysis technique that extracts the path constraints from the implementations of a given API and its counterpart to guide diverse test input generation. The extraction is facilitated by LLM’s knowledge of the concerned DL library and its upstream libraries. DLLens incorporates validation mechanisms to manage the LLM’s hallucinations in counterpart synthesis and path constraint extraction.We evaluate DLLens on two popular DL libraries, TensorFlow and PyTorch. Our evaluation shows that DLLens synthesizes counterparts for 1.84 times as many APIs as those found by state-of-the-art techniques on these libraries. Moreover, under the same time budget, DLLens covers 7.23% more branches and detects 1.88 times as many bugs as state-of-the-art techniques on 200 randomly sampled APIs. DLLens has successfully detected 71 bugs in recent TensorFlow and PyTorch libraries. Among them, 59 are confirmed by developers, including 46 confirmed as previously unknown bugs, and 10 of these previously unknown bugs have been fixed in the latest version of TensorFlow and PyTorch.","2025-05","2025-11-25 22:29:35","2025-11-25 22:29:35","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Static Analysis; Deep Learning Library Testing; Differential Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W2T874SX","conferencePaper","2025","Wang, Zehao","Identifying Performance-Sensitive Configurations in Software Systems with LLM-Driven Agents","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings","979-8-3315-3683-1","","10.1109/ICSE-Companion66252.2025.00069","https://doi.org/10.1109/ICSE-Companion66252.2025.00069","Configuration settings are essential for tailoring software behavior to meet specific performance requirements. However, incorrect configurations are common, and identifying those that impact system performance is challenging due to the vast number and complexity of possible settings. In this work, we present PerfAware, a lightweight framework that leverages Large Language Models (LLMs) to efficiently identify performance-sensitive configurations with minimal overhead. PerfAware employs LLM agents to simulate interactions between developers and performance engineers using advanced prompting techniques such as prompt chaining and retrieval-augmented generation (RAG). Our evaluation of seven open-source Java systems demonstrates that PerfAware achieves an average accuracy of 70.55% in classifying performance-sensitive configurations, outperforming both our LLM baseline (58.47%) and the previous state-of-the-art method (60.87%). Notably, our prompt chaining technique improves precision significantly while maintaining similar recall levels. In summary, PerfAware significantly reduces manual effort in classifying performance-sensitive configurations and offers valuable insights for future LLM-based code analysis research.","2025","2025-11-25 22:29:35","2025-11-25 22:29:35","","222–223","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","large language model; configuration; multi-agent","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N3XWSF63","conferencePaper","2025","Fu, Yulong; Zhang, Yingtong","UAgent: Adversarial Co-evolution for Targeted Bug Revelation in Unit Testing","Proceedings of the 2025 Workshop on Recent Advances in Resilient and Trustworthy MAchine Learning-DriveN Systems","979-8-4007-1909-7","","10.1145/3733821.3763027","https://doi.org/10.1145/3733821.3763027","Unit testing remains crucial for software quality assurance, yet manually creating effective tests requires significant expertise. Only 17% of 82,447 GitHub projects contain test cases, revealing low adoption rates. While automated tools like Pynguin and EvoSuite reduce effort, their dependence on code coverage metrics is problematic due to weak correlation with defect detection. Current limitations include: (1) Defects in LLM-generated tests, particularly for strongly-typed languages (2) Inadequate simulation of progressive attacks in traditional mutation testing To address these, we propose UAgent - a novel adversarial co-evolution framework with: - TG Agent (Defender): Creates test suites via reflective closed-loop iteration - MG Agent (Attacker): Generates coupled mutations exposing defensive gaps - Resilience Evolution Mechanism: Enables continuous adaptation through agent competition By transforming test generation into a dynamic adaptation process, UAgent establishes a new paradigm for resilient software validation, addressing fundamental gaps in current test automation methodologies through biologically-inspired learning mechanisms. Key contributions include: - First application of adversarial ML principles to test generation - Framework-agnostic implementation for Python/Java ecosystems","2025","2025-11-25 22:29:35","2025-11-25 22:29:35","","51–56","","","","","","","ARTMAN '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","test automation; adversarial testing; ML robustness; mutation analysis; software resilience certification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B7LNNAPH","conferencePaper","2025","Miczek, Dennis; Gabbireddy, Divyesh; Saha, Suman","Leveraging LLM to Strengthen ML-Based Cross-Site Scripting Detection","Proceedings of the 2025 ACM Workshop on Wireless Security and Machine Learning","979-8-4007-1531-0","","10.1145/3733965.3733969","https://doi.org/10.1145/3733965.3733969","According to the Open Web Application Security Project (OWASP), Cross-Site Scripting (XSS) is a critical security vulnerability. Despite decades of research, XSS remains among the top 10 security vulnerabilities. Researchers have proposed various techniques to protect systems from XSS attacks, with machine learning (ML) being one of the most widely used methods. An ML model is trained on a dataset to identify potential XSS threats, making its effectiveness highly dependent on the size and diversity of the training data.A variation of XSS is obfuscated XSS, where attackers apply obfuscation techniques to alter the code's structure, making it challenging for security systems to detect its malicious intent. Our study's random forest model trained on traditional (non-obfuscated) XSS data achieved 99.8% accuracy. However, when tested against obfuscated XSS samples, accuracy dropped to 81.9%, underscoring the importance of training ML models with obfuscated data to improve their effectiveness in detecting XSS attacks. A significant challenge is to generate highly complex obfuscated code despite the availability of several public tools. These tools can only produce obfuscation up to certain levels of complexity.In our proposed system, we fine-tune a Large Language Model (LLM) to generate complex obfuscated XSS payloads automatically. By transforming original XSS samples into diverse obfuscated variants, we create challenging training data for ML model evaluation. Our approach achieved a 99.5% accuracy rate with the obfuscated dataset. We also found that the obfuscated samples generated by the LLM were 28.1% more complex than those created by other tools, significantly improving the model's ability to handle advanced XSS attacks and making it more effective for real-world application security.","2025","2025-11-25 22:29:35","2025-11-25 22:29:35","","14–19","","","","","","","WiseML '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: USA","","","","large language model; cross-site scripting; fine-tuning; machine learning model; obfuscation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M8Y9VGE3","conferencePaper","2025","Shen, Xiangmin; Wang, Lingzhi; Li, Zhenyuan; Chen, Yan; Zhao, Wencheng; Sun, Dawei; Wang, Jiashui; Ruan, Wei","PentestAgent: Incorporating LLM Agents to Automated Penetration Testing","Proceedings of the 20th ACM Asia Conference on Computer and Communications Security","979-8-4007-1410-8","","10.1145/3708821.3733882","https://doi.org/10.1145/3708821.3733882","Penetration testing is a critical technique for identifying security vulnerabilities, traditionally performed manually by skilled security specialists. This complex process involves gathering information about the target system, identifying entry points, exploiting the system, and reporting findings. Despite its effectiveness, manual penetration testing is time-consuming and expensive, often requiring significant expertise and resources that many organizations cannot afford. While automated penetration testing methods have been proposed, they often fall short in real-world applications due to limitations in flexibility, adaptability, and implementation.Recent advancements in large language models offer new opportunities for enhancing penetration testing through increased intelligence and automation. However, current LLM-based approaches still face significant challenges, including limited penetration testing knowledge and a lack of comprehensive automation capabilities. To address these gaps, we propose PentestAgent, a novel LLM-based automated penetration testing framework that leverages the power of LLMs and various LLM-based techniques like retrieval augmented generation to enhance penetration testing knowledge and automate various tasks. Our framework leverages multi-agent collaboration to automate intelligence gathering, vulnerability analysis, and exploitation stages, reducing manual intervention. We evaluate PentestAgent using a comprehensive benchmark, demonstrating superior performance in task completion and overall efficiency.","2025","2025-11-25 22:29:35","2025-11-25 22:29:35","","375–391","","","","","","","ASIA CCS '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Large Language Model; Agent; Penetration Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LUBP68MG","journalArticle","2025","Ma, Jie; He, Ningyu; Xi, Jinwen; Xing, Mingzhe; Wang, Haoyu; Gao, Ying; Yue, Yinliang","OpDiffer: LLM-Assisted Opcode-Level Differential Testing of Ethereum Virtual Machine","Proc. ACM Softw. Eng.","","","10.1145/3728946","https://doi.org/10.1145/3728946","As Ethereum continues to thrive, the Ethereum Virtual Machine (EVM) has become the cornerstone powering tens of millions of active smart contracts. Intuitively, security issues in EVMs could lead to inconsistent behaviors among smart contracts or even denial-of-service of the entire blockchain network. However, to the best of our knowledge, only a limited number of studies focus on the security of EVMs. Moreover, they suffer from 1) insufficient test input diversity and invalid semantics; and 2) the inability to automatically identify bugs and locate root causes. To bridge this gap, we propose OpDiffer, a differential testing framework for EVM, which takes advantage of LLMs and static analysis methods to address the above two limitations. We conducted the largest-scale evaluation, covering nine EVMs and uncovering 26 previously unknown bugs, 22 of which have been confirmed by developers and three have been assigned CNVD IDs. Compared to state-of-the-art baselines, OpDiffer can improve code coverage by at most 71.06%, 148.40% and 655.56%, respectively. Through an analysis of real-world deployed Ethereum contracts, we estimate that 7.21% of the contracts could trigger our identified EVM bugs under certain environmental settings, potentially resulting in severe negative impact on the Ethereum ecosystem.","2025-06","2025-11-25 22:29:35","2025-11-25 22:29:35","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Differential Testing; Ethereum Virtual Machine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KBNMLZ7J","conferencePaper","2024","Wu, Yonghao; Li, Zheng; Zhang, Jie M.; Liu, Yong","ConDefects: A Complementary Dataset to Address the Data Leakage Concern for LLM-Based Fault Localization and Program Repair","Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering","979-8-4007-0658-5","","10.1145/3663529.3663815","https://doi.org/10.1145/3663529.3663815","With the growing interest on Large Language Models (LLMs) for fault localization and program repair, ensuring the integrity and generalizability of the LLM-based methods becomes paramount. The code in existing widely-adopted benchmarks for these tasks was written before the bloom of LLMs and may be included in the training data of existing popular LLMs, thereby suffering from the threat of data leakage, leading to misleadingly optimistic performance metrics. To address this issue, we introduce ConDefects, a dataset developed as a complement to existing datasets, meticulously curated with real faults to eliminate such overlap. ConDefects contains 1,254 Java faulty programs and 1,625 Python faulty programs. All these programs are sourced from the online competition platform AtCoder and were produced between October 2021 and September 2023. We pair each fault with fault locations and the corresponding repaired code versions, making it tailored for fault localization and program repair related research. We also provide interfaces for selecting subsets based on different time windows and coding task difficulties. While inspired by LLM-based tasks, ConDefects can be adopted for benchmarking ALL types of fault localization and program repair methods. The dataset is publicly available, and a demo video can be found at https://www.youtube.com/watch?v=22j15Hj5ONk.","2024","2025-11-25 22:29:35","2025-11-25 22:29:35","","642–646","","","","","","","FSE 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","Large Language Model; Dataset; Program Repair; Fault Localization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AJR5K8IU","bookSection","2025","Tamanna, Salma Begum; Uddin, Gias; Wang, Song; Xia, Lan; Zhang, Longyu","ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are We There Yet?","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00145","Hallucinations, the tendency to produce irrelevant/incorrect responses, are prevalent concerns in generative AI-based tools like ChatGPT. Although hallucinations in ChatGPT are studied for textual responses, it is unknown how ChatGPT hallucinates for technical texts that contain both textual and technical terms. We surveyed 47 software engineers and produced a benchmark of 412 Q&amp;A pairs from the bug reports of two OSS projects. We find that a RAG-based ChatGPT (i.e., ChatGPT tuned with the benchmark issue reports) is 36.4% correct when producing answers to the questions, due to two reasons 1) limitations to understand complex technical contents in code snippets like stack traces, and 2) limitations to integrate contexts denoted in the technical terms and texts. We present CHIME (ChatGPT Inaccuracy Mitigation Engine) whose underlying principle is that if we can preprocess the technical reports better and guide the query validation process in ChatGPT, we can address the observed limitations. CHIME uses context-free grammar (CFG) to parse stack traces in technical reports. CHIME then verifies and fixes ChatGPT responses by applying metamorphic testing and query transformation. In our benchmark, CHIME shows 30.3% more correction over ChatGPT responses. In a user study, we find that the improved responses with CHIME are considered more useful than those generated from ChatGPT without CHIME.","2025","2025-11-25 22:29:35","2025-11-25 22:29:35","","2290–2302","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"754QPCAK","conferencePaper","2025","Wang, Ruoxi; Li, Kun; Xu, Minghui; Zhang, Yue; Xu, Kaidi; Liu, Chunchi; Xiao, Yinhao; Cheng, Xiuzhen","LIFT: Automating Symbolic Execution Optimization with Large Language Models for AI Networks","Proceedings of the 2nd Workshop on Networks for AI Computing","979-8-4007-2082-6","","10.1145/3748273.3749202","https://doi.org/10.1145/3748273.3749202","Dynamic Symbolic Execution (DSE) is a key technique in program analysis, widely used in software testing, vulnerability discovery, and formal verification. In distributed AI systems, DSE plays a crucial role in identifying hard-to-detect bugs, especially those arising from complex network communication patterns. However, traditional approaches to symbolic execution are often hindered by scalability issues and inefficiencies, particularly in large-scale systems. This paper introduces LIFT (Large-language-model Integrated Functional-equivalent-IR Transformation), a novel framework that leverages Large Language Models (LLMs) to automate the optimization of Intermediate Representations (IRs) in symbolic execution. LIFT addresses the challenges of symbolic execution by providing a scalable, context-sensitive solution for IR transformation. The framework consists of two phases: IR Analysis and Optimization, where LLMs optimize time-intensive IR blocks, and Symbolic Execution and Validation, which includes benchmarking and semantic verification to ensure correctness and generalizability. Experiments on real-world binaries demonstrated significant performance improvements, including a 53.5% reduction in execution time for bigtest and a 10.24% reduction for random, along with reductions in IR statements, PUT instructions, and temporary variables. These results demonstrate that LLMs simplify IRs while maintaining functional correctness, enhancing symbolic execution in distributed AI systems.","2025","2025-11-25 22:29:35","2025-11-25 22:29:35","","37–42","","","","","","","NAIC '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Coimbra, Portugal","","","","angr; Binary Analysis; Dynamic Symbolic Execution (DSE); Intermediate Representation (IR); Large Language Model (LLM)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T7FLNS7S","journalArticle","2025","Yang, Boyang; Tian, Haoye; Ren, Jiadong; Zhang, Hongyu; Klein, Jacques; Bissyande, Tegawende; Le Goues, Claire; Jin, Shunfu","MORepair: Teaching LLMs to Repair Code via Multi-Objective Fine-Tuning","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3735129","https://doi.org/10.1145/3735129","Within the realm of software engineering, specialized tasks on code, such as program repair, present unique challenges, necessitating fine-tuning Large language models&nbsp;(LLMs) to unlock state-of-the-art performance. Fine-tuning approaches proposed in the literature for LLMs on program repair tasks generally overlook the need to reason about the logic behind code changes, beyond syntactic patterns in the data. High-performing fine-tuning experiments also usually come at very high computational costs. With MORepair, we propose a novel perspective on the learning focus of LLM fine-tuning for program repair: we not only adapt the LLM parameters to the syntactic nuances of the task of code transformation (objective ➊), but we also specifically fine-tune the LLM with respect to the logical reason behind the code change in the training data (objective ➋). Such a multi-objective fine-tuning will instruct LLMs to generate high-quality patches.We apply MORepair to fine-tune four open-source LLMs with different sizes and architectures. Experimental results on function-level and repository-level repair benchmarks show that the implemented fine-tuning effectively boosts LLM repair performance by 11.4% to 56.0%. We further show that our fine-tuning strategy yields superior performance compared to the state-of-the-art approaches, including standard fine-tuning, Fine-tune-CoT, and RepairLLaMA.","2025-05","2025-11-25 22:29:35","2025-11-25 22:29:35","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Program Repair; Open Source; Fine-tuning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G2BA3PTX","bookSection","2025","Chen, Junkai; Pan, Zhiyuan; Hu, Xing; Li, Zhenhao; Li, Ge; Xia, Xin","Reasoning Runtime Behavior of a Program with LLM: How Far Are We?","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00012","Large language models for code (i.e., code LLMs) have shown strong code understanding and generation capabilities. To evaluate the capabilities of code LLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval and ClassEval). Code reasoning is one of the most essential abilities of code LLMs (i.e., predicting code execution behaviors such as program output and execution path), but existing benchmarks for code reasoning are not sufficient. Typically, they focus on predicting the input and output of a program, ignoring the evaluation of the intermediate behavior during program execution, as well as the logical consistency (e.g., the model should not give the correct output if the prediction of execution path is wrong) when performing the reasoning. To address these problems, in this paper, we propose a framework, namely ℛEval, for evaluating code reasoning abilities and consistency of code LLMs with program execution. We utilize existing code benchmarks and adapt them to new benchmarks within our framework. A large-scale empirical study is conducted and most LLMs show unsatisfactory performance on both Runtime Behavior Reasoning (i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation (i.e., an average IC score of 10.3). Evaluation results of current code LLMs reflect the urgent need for the community to strengthen the code reasoning capability of code LLMs. Our code, data and ℛEval leaderboard are available at https://r-eval.github.io.","2025","2025-11-25 22:29:35","2025-11-25 22:29:35","","1869–1881","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2NPT6Z27","conferencePaper","2024","López, José Antonio Hernández; Földiák, Máté; Varró, Dániel","Text2VQL: Teaching a Model Query Language to Open-Source Language Models with ChatGPT","Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems","979-8-4007-0504-5","","10.1145/3640310.3674091","https://doi.org/10.1145/3640310.3674091","While large language models (LLMs) like ChatGPT has demonstrated impressive capabilities in addressing various software engineering tasks, their use in a model-driven engineering (MDE) context is still in an early stage. Since the technology is proprietary and accessible solely through an API, its use may be incompatible with the strict protection of intellectual properties in industrial models. While there are open-source LLM alternatives, they often lack the power of proprietary models and require extensive data fine-tuning to realize their full potential. Furthermore, open-source datasets tailored for MDE tasks are scarce, posing challenges for training such models effectively.In this work, we introduce Text2VQL, a framework that generates graph queries captured in the VIATRA Query Language (VQL) from natural language specifications using open-source LLMs. Initially, we create a high-quality synthetic dataset comprising pairs of queries and their corresponding natural language descriptions using ChatGPT and VIATRA parser. Leveraging this dataset, we use parameter-efficient tuning to specialize three open-source LLMs, namely, DeepSeek Coder 1b, DeepSeek Coder 7b, and CodeLlama 7b for VQL query generation. Our experimental evaluation demonstrates that the fine-tuned models outperform the base models in query generation, highlighting the usefulness of our synthetic dataset. Moreover, one of the fine-tuned models achieves performance comparable to ChatGPT.","2024","2025-11-25 22:29:35","2025-11-25 22:29:35","","13–24","","","","","","","MODELS '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Linz, Austria","","","","ChatGPT; large language model (LLM); model query language; query generation; VIATRA Query Language (VQL)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NICXUUWM","conferencePaper","2025","Parasaram, Nikhil; Yan, Huijie; Yang, Boyu; Flahy, Zineb; Qudsi, Abriele; Ziaber, Damian; Barr, Earl T.; Mechtaev, Sergey","The Fact Selection Problem in LLM-Based Program Repair","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00162","https://doi.org/10.1109/ICSE55347.2025.00162","Recent research has shown that incorporating bug-related facts, such as stack traces and GitHub issues, into prompts enhances the bug-fixing capabilities of large language models (LLMs). Considering the ever-increasing context window of these models, a critical question arises: what and how many facts should be included in prompts to maximise the chance of correctly fixing bugs? To answer this question, we conducted a large-scale study, employing over 19K prompts featuring various combinations of seven diverse facts to rectify 314 bugs from open-source Python projects within the BugsInPy benchmark. Our findings revealed that each fact, ranging from simple syntactic details like code context to semantic information previously unexplored in the context of LLMs such as angelic values, is beneficial. Specifically, each fact aids in fixing some bugs that would remain unresolved or only be fixed with a low success rate without it. Importantly, we discovered that the effectiveness of program repair prompts is non-monotonic over the number of used facts; using too many facts leads to subpar outcomes. These insights led us to define the fact selection problem: determining the optimal set of facts for inclusion in a prompt to maximise LLM's performance on a given task instance. We found that there is no one-size-fits-all set of facts for bug repair. Therefore, we developed a basic statistical model, named Maniple, which selects facts specific to a given bug to include in the prompt. This model significantly surpasses the performance of the best generic fact set. To underscore the significance of the fact selection problem, we benchmarked MANIPLE against the state-of-the-art zero-shot, non-conversational LLM-based bug repair methods. On our testing dataset of 157 bugs, Maniple repairs 88 bugs, 17% above the best configuration.","2025","2025-11-25 22:29:35","2025-11-25 22:48:02","","2574–2586","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","Large language models; Semantics; Software engineering; Syntactics; Software development management; prompt engineering; Computer bugs; large language models; Benchmark testing; automated program repair; Python; Systematics; Maintenance engineering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K9YKKGNU","journalArticle","2025","Wong, Wai Kin; Wu, Daoyuan; Wang, Huaijin; Li, Zongjie; Liu, Zhibo; Wang, Shuai; Tang, Qiyi; Nie, Sen; Wu, Shi","DecLLM: LLM-Augmented Recompilable Decompilation for Enabling Programmatic Use of Decompiled Code","Proc. ACM Softw. Eng.","","","10.1145/3728958","https://doi.org/10.1145/3728958","Decompilers are widely used in reverse engineering (RE) to convert compiled executables into human-readable pseudocode and support various security analysis tasks. Existing decompilers, such as IDA Pro and Ghidra, focus on enhancing the readability of decompiled code rather than its recompilability, which limits further programmatic use, such as for CodeQL-based vulnerability analysis that requires compilable versions of the decompiled code. Recent LLM-based approaches for enhancing decompilation results, while useful for human RE analysts, unfortunately also follow the same path. In this paper, we explore, for the first time, how off-the-shelf large language models (LLMs) can be used to enable recompilable decompilation—automatically correcting decompiler outputs into compilable versions. We first show that this is non-trivial through a pilot study examining existing rule-based and LLM-based approaches. Based on the lessons learned, we design DecLLM, an iterative LLM-based repair loop that utilizes both static recompilation and dynamic runtime feedback as oracles to iteratively fix decompiler outputs. We test DecLLM on popular C benchmarks and real-world binaries using two mainstream LLMs, GPT-3.5 and GPT-4, and show that off-the-shelf LLMs can achieve an upper bound of around 70% recompilation success rate, i.e., 70 out of 100 originally non-recompilable decompiler outputs are now recompilable. We also demonstrate the practical applicability of the recompilable code for CodeQL-based vulnerability analysis, which is impossible to perform directly on binaries. For the remaining 30% of hard cases, we further delve into their errors to gain insights for future improvements in decompilation-oriented LLM design.","2025-06","2025-11-25 22:29:35","2025-11-25 22:29:35","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Recompilable Decompilation; Reverse Engineering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y7EY9WSY","journalArticle","2025","Zhang, Yakun; Liu, Chen; Xie, Xiaofei; Lin, Yun; Dong, Jin Song; Hao, Dan; Zhang, Lu","GUI Test Migration via Abstraction and Concretization","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3726525","https://doi.org/10.1145/3726525","GUI test migration aims to produce test cases with events and assertions to test specific functionalities of a target app. Existing migration approaches typically focus on the widget-mapping paradigm that maps widgets from source apps to target apps. However, since different apps may implement the same functionality in different ways, direct mapping may result in incomplete or buggy test cases, thus significantly impacting the effectiveness of testing the target functionality and the practical applicability of migration approaches.In this paper, we propose a new migration paradigm (i.e., the abstraction-concretization paradigm) that first abstracts the test logic for the target functionality and then utilizes this logic to generate the concrete GUI test case. Furthermore, we introduce MACdroid, the first approach that migrates GUI test cases based on this paradigm. Specifically, we propose an abstraction technique that utilizes source test cases from source apps targeting the same functionality to extract a general test logic for that functionality. Then, we propose a concretization technique that utilizes the general test logic to guide an LLM in generating the corresponding GUI test case (including events and assertions) for the target app. We evaluate MACdroid on two widely-used datasets (including 31 apps, 34 functionalities, and 123 test cases). On the FrUITeR dataset, the test cases generated by MACdroid successfully test 64% of the target functionalities, improving the baselines by 191%. On the Lin dataset, MACdroid successfully tests 75% of the target functionalities, outperforming the baselines by 42%. These results underscore the effectiveness of MACdroid in GUI test migration.","2025-04","2025-11-25 22:29:35","2025-11-25 22:29:35","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large language model; Functional GUI testing; Test migration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VG94DMFM","journalArticle","2024","Mendonça, Nabor C.","Evaluating ChatGPT-4 Vision on Brazil's National Undergraduate Computer Science Exam","ACM Trans. Comput. Educ.","","","10.1145/3674149","https://doi.org/10.1145/3674149","The recent integration of visual capabilities into Large Language Models (LLMs) has the potential to play a pivotal role in science and technology education, where visual elements such as diagrams, charts, and tables are commonly used to improve the learning experience. This study investigates the performance of ChatGPT-4 Vision, OpenAI’s most advanced visual model at the time the study was conducted, on the Bachelor in Computer Science section of Brazil’s 2021 National Undergraduate Exam (ENADE). By presenting the model with the exam’s open and multiple-choice questions in their original image format and allowing for reassessment in response to differing answer keys, we were able to evaluate the model’s reasoning and self-reflecting capabilities in a large-scale academic assessment involving textual and visual content. ChatGPT-4 Vision significantly outperformed the average exam participant, positioning itself within the top 10 best score percentile. While it excelled in questions that incorporated visual elements, it also encountered challenges with question interpretation, logical reasoning, and visual acuity. A positive correlation between the model’s performance in multiple-choice questions and the performance distribution of the human participants suggests multimodal LLMs can provide a useful tool for question testing and refinement. However, the involvement of an independent expert panel to review cases of disagreement between the model and the answer key revealed some poorly constructed questions containing vague or ambiguous statements, calling attention to the critical need for improved question design in future exams. Our findings suggest that while ChatGPT-4 Vision shows promise in multimodal academic evaluations, human oversight remains crucial for verifying the model’s accuracy and ensuring the fairness of high-stakes educational exams. The paper’s research materials are publicly available at .","2024-08","2025-11-25 22:29:36","2025-11-25 22:29:36","","","","3","24","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","ChatGPT-4 vision; computer science education; educational assessment; Multimodal generative AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XHGNSTKC","conferencePaper","2024","Gonzalez-Barahona, Jesus M.","IDEs in the age of LLMs and XR","Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments","979-8-4007-0580-9","","10.1145/3643796.3648457","https://doi.org/10.1145/3643796.3648457","Let's imagine that in a few years generative AI has changed software development dramatically, taking charge of most of the programming tasks. Let's also assume that extended reality devices became ubiquitous, being the preferred interface for interacting with computers. This paper proposes how this situation would impact IDEs, by exploring how the development process would be affected, and analyzing which tools would be needed for supporting developers.","2024","2025-11-25 22:29:36","2025-11-25 22:29:36","","66–69","","","","","","","IDE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","LLM; generative AI; AR; extended reality; IDE; software development; VR; XR","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"73NCK8U3","conferencePaper","2024","Kabir, Samia; Udo-Imeh, David N.; Kou, Bonan; Zhang, Tianyi","Is Stack Overflow Obsolete? An Empirical Study of the Characteristics of ChatGPT Answers to Stack Overflow Questions","Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems","979-8-4007-0330-0","","10.1145/3613904.3642596","https://doi.org/10.1145/3613904.3642596","Q&amp;A platforms have been crucial for the online help-seeking behavior of programmers. However, the recent popularity of ChatGPT is altering this trend. Despite this popularity, no comprehensive study has been conducted to evaluate the characteristics of ChatGPT’s answers to programming questions. To bridge the gap, we conducted the first in-depth analysis of ChatGPT answers to 517 programming questions on Stack Overflow and examined the correctness, consistency, comprehensiveness, and conciseness of ChatGPT answers. Furthermore, we conducted a large-scale linguistic analysis, as well as a user study, to understand the characteristics of ChatGPT answers from linguistic and human aspects. Our analysis shows that 52% of ChatGPT answers contain incorrect information and 77% are verbose. Nonetheless, our user study participants still preferred ChatGPT answers 35% of the time due to their comprehensiveness and well-articulated language style. However, they also overlooked the misinformation in the ChatGPT answers 39% of the time. This implies the need to counter misinformation in ChatGPT answers to programming questions and raise awareness of the risks associated with seemingly correct answers.","2024","2025-11-25 22:29:36","2025-11-25 22:29:36","","","","","","","","","CHI '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Honolulu, HI, USA","","","","large language model; stack overflow; chatgpt; a; misinformation; q&amp","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"89I2M77M","journalArticle","2024","Xu, Congying; Terragni, Valerio; Zhu, Hengcheng; Wu, Jiarong; Cheung, Shing-Chi","MR-Scout: Automated Synthesis of Metamorphic Relations from Existing Test Cases","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3656340","https://doi.org/10.1145/3656340","Metamorphic Testing (MT) alleviates the oracle problem by defining oracles based on metamorphic relations (MRs) that govern multiple related inputs and their outputs. However, designing MRs is challenging, as it requires domain-specific knowledge. This hinders the widespread adoption of MT. We observe that developer-written test cases can embed domain knowledge that encodes MRs. Such encoded MRs could be synthesized for testing not only their original programs but also other programs that share similar functionalities.In this article, we propose MR-Scout to automatically synthesize MRs from test cases in open-source software (OSS) projects. MR-Scout first discovers MR-encoded test cases (MTCs), and then synthesizes the encoded MRs into parameterized methods (called codified MRs), and filters out MRs that demonstrate poor quality for new test case generation. MR-Scout discovered over 11,000 MTCs from 701 OSS projects. Experimental results show that over 97% of codified MRs are of high quality for automated test case generation, demonstrating the practical applicability of MR-Scout. Furthermore, codified-MRs-based tests effectively enhance the test adequacy of programs with developer-written tests, leading to 13.52% and 9.42% increases in line coverage and mutation score, respectively. Our qualitative study shows that 55.76% to 76.92% of codified MRs are easily comprehensible for developers.","2024-06","2025-11-25 22:29:36","2025-11-25 22:29:36","","","","6","33","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Software testing; automated test case generation; metamorphic relation; metamorphic testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W48ZD4RT","journalArticle","2025","Bouzid, Rezika; Khoury, Raphaël","Assessing the Effectiveness of ChatGPT in Secure Code Development: A Systematic Literature Review","ACM Comput. Surv.","","0360-0300","10.1145/3744553","https://doi.org/10.1145/3744553","ChatGPT, a Large Language Model (LLM) maintained by OpenAI, has demonstrated a remarkable ability to seemingly comprehend and contextually generate text. Among its myriad applications, its capability to autonomously generate and analyze computer code stands out as particularly promising. This functionality has piqued substantial interest due to its potential to streamline the software development process. However, this technological advancement also brings to the forefront significant apprehensions concerning the security of code produced by LLMs. In this article, we survey recent research that examines the use of ChatGPT to generate secure code, detect vulnerabilities in code, or perform other tasks related to secure code development. Beyond categorizing and synthesizing these studies, we identify important insights into ChatGPT’s potential impact on secure programming. Key findings indicate that while ChatGPT shows great promise as an aid in writing secure code, challenges remain. Its effectiveness varies across security tasks, depending on the context of experimentation (programming language, CWE, code length, etc.) and the benchmark used for comparison–whether against other LLMs, traditional analysis tools, or its own versions. The overall trend indicates that GPT-4 consistently surpasses its predecessor in most tasks.","2025-07","2025-11-25 22:29:36","2025-11-25 22:29:36","","","","12","57","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","ChatGPT; LLMs; code; security; vulnerabilities","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TESI2FWC","journalArticle","2025","Wu, Susheng; Wang, Ruisi; Cao, Yiheng; Chen, Bihuan; Zhou, Zhuotong; Huang, Yiheng; Zhao, JunPeng; Peng, Xin","Mystique: Automated Vulnerability Patch Porting with Semantic and Syntactic-Enhanced LLM","Proc. ACM Softw. Eng.","","","10.1145/3715718","https://doi.org/10.1145/3715718","Branching repositories facilitates efficient software development but can also inadvertently propagate vulnerabilities. When an original branch is patched, other unfixed branches remain vulnerable unless the patch is successfully ported. However, due to inherent discrepancies between branches, many patches cannot be directly applied and require manual intervention, which is time-consuming and leads to delays in patch porting, increasing vulnerability risks. Existing automated patch porting approaches are prone to errors, as they often overlook essential semantic and syntactic context of vulnerability and fail to detect or refine faulty patches. We propose Mystique, a novel LLM-based approach to address these limitations. Mystique first slices the semantic-related statements linked to the vulnerability while ensuring syntactic correctness, allowing it to extract the signatures for both the original patched function and the target vulnerable function. Mystique then utilizes a fine-tuned LLM to generate a fixed function, which is further iteratively checked and refined to ensure successful porting. Our evaluation shows that Mystique achieved a success rate of 0.954 at function level and of 0.924 at CVE level, outperforming state-of-the-art approaches by at least 13.2% at function level and 12.3% at CVE level. Our evaluation also demonstrates Mystique’s superior generality across various projects, bugs, and programming languages. Mystique successfully ported patches for 34 real-world vulnerable branches.","2025-06","2025-11-25 22:29:36","2025-11-25 22:29:36","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Open Source Software; Patch Porting","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W52UKFLS","conferencePaper","2024","Kouemo Ngassom, Sylvain; Moradi Dakhel, Arghavan; Tambon, Florian; Khomh, Foutse","Chain of Targeted Verification Questions to Improve the Reliability of Code Generated by LLMs","Proceedings of the 1st ACM International Conference on AI-Powered Software","979-8-4007-0685-1","","10.1145/3664646.3664772","https://doi.org/10.1145/3664646.3664772","LLM-based assistants, such as GitHub Copilot and ChatGPT, have the potential to generate code that fulfills a programming task described in a natural language description, referred to as a prompt. The widespread accessibility of these assistants enables users with diverse backgrounds to generate code and integrate it into software projects. However, studies show that code generated by LLMs is prone to bugs and may miss various corner cases in task specifications. Presenting such buggy code to users can impact their reliability and trust in LLM-based assistants. Moreover, significant efforts are required by the user to detect and repair any bug present in the code, especially if no test cases are available. In this study, we propose a self-refinement method aimed at improving the reliability of code generated by LLMs by minimizing the number of bugs before execution, without human intervention, and in the absence of test cases. Our approach is based on targeted Verification Questions (VQs) to identify potential bugs within the initial code. These VQs target various nodes within the Abstract Syntax Tree (AST) of the initial code, which have the potential to trigger specific types of bug patterns commonly found in LLM-generated code. Finally, our method attempts to repair these potential bugs by re-prompting the LLM with the targeted VQs and the initial code. Our evaluation, based on programming tasks in the CoderEval dataset, demonstrates that our proposed method outperforms state-of-the-art methods by decreasing the number of targeted errors in the code between 21% to 62% and improving the number of executable code instances to 13%.","2024","2025-11-25 22:29:36","2025-11-25 22:29:36","","122–130","","","","","","","AIware 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","Large Language Model; Hallucination; Software Development; Code Generation; Reliability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I4Q7YG22","conferencePaper","2024","Pan, Shengyi; Wang, You; Liu, Zhongxin; Hu, Xing; Xia, Xin; Li, Shanping","Automating Zero-Shot Patch Porting for Hard Forks","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3652134","https://doi.org/10.1145/3650212.3652134","Forking is a typical way of code reuse, which provides a simple way for developers to create a variant software (denoted as hard fork) by copying and modifying an existing codebase. Despite of the benefits, forking also leads to duplicate efforts in software maintenance. Developers need to port patches across the hard forks to address similar bugs or implement similar features. Due to the divergence between the source project and the hard fork, patch porting is complicated, which requires an adaption regarding different implementations of the same functionality. In this work, we take the first step to automate patch porting for hard forks under a zero-shot setting. We first conduct an empirical study of the patches ported from Vim to Neovim over the last ten years to investigate the necessities of patch porting and the potential flaws in the current practice. We then propose a large language model (LLM) based approach (namely PPatHF) to automatically port patches for hard forks on a function-wise basis. Specifically, PPatHF is composed of a reduction module and a porting module. Given the pre- and post-patch versions of a function from the reference project and the corresponding function from the target project, the reduction module first slims the input functions by removing code snippets less relevant to the patch. Then, the porting module leverages a LLM to apply the patch to the function from the target project. To better elicit the power of the LLM on patch porting, we design a prompt template to enable efficient in-context learning. We further propose an instruction-tuning based training task to better guide the LLM to port the patch and inject task-specific knowledge. We evaluate PPatHF on 310 Neovim patches ported from Vim. The experimental results show that PPatHF outperforms the baselines significantly. Specifically, PPatHF can correctly port 131 (42.3%) patches and automate 57% of the manual edits required for the developer to port the patch.","2024","2025-11-25 22:29:36","2025-11-25 22:29:36","","363–375","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large Language Model; Patch Porting; Hard Fork","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AE2S6ABM","conferencePaper","2025","Moon, Seungjae; Cha, Junseo; Park, Hyunjun; Kim, Joo-Young","Hybe: GPU-NPU Hybrid System for Efficient LLM Inference with Million-Token Context Window","Proceedings of the 52nd Annual International Symposium on Computer Architecture","979-8-4007-1261-6","","10.1145/3695053.3731051","https://doi.org/10.1145/3695053.3731051","The growth of context window size in large language model (LLM) inference poses a very distinct computational challenge of hardware inefficiency. The inefficiency arises from the computational imbalance during LLM inference between the compute-intensive prefill stage, and memory-intensive decode stage. The predominant inference hardware, GPU, boasts large number of cores to excel in the prefill stage, which processes the entire input context at once, but suffers from hardware underutilization in the decode stage, which iteratively generates one output token at a time. In conventional LLM, batching has been able to alleviate the underutilization by generating multiple tokens of different requests. However, batching becomes infeasible in models with large context windows over 100K tokens because the Key-Value (KV) activations dominate the physical memory capacity, surpassing the entire model size. In this paper, we propose Hybe, a GPU-NPU hybrid system for efficient LLM inference with a million-token context window. Hybe utilizes the preexisting GPU for the prefill stage and employs lightweight NPUs during the decode stage. Each NPU includes only the necessary computing resources to fully utilize the given memory bandwidth, thereby achieving maximum hardware efficiency. Furthermore, Hybe introduces fine-grained KV transmission, a kernel scheduling method that immediately offloads partial KV produced from the GPU to the NPU, which significantly reduces the KV memory required in the GPU. Lastly, Hybe scheduler applies stage-wise pipelining that dynamically assigns queued requests to idle hardware to minimize stalls. Hybe utilizes NVIDIA H100 GPU with inference-optimized vLLM library and implement Hybe NPU in 4nm process with equal HBM specification. Hybe achieves 2.1 × speedup for Phi-3 with 100K-token context window and 3.9 × energy efficiency for Llama-3 with 1M-token context window, over H100 GPUs with equal total device count.","2025","2025-11-25 22:29:36","2025-11-25 22:29:36","","808–820","","","","","","","ISCA '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Context window; Heterogeneous (hybrid) system; Large language model (LLM); Neural processing unit (NPU); Scheduling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X87GG3MY","conferencePaper","2024","Jiang, Yu; Liang, Jie; Ma, Fuchen; Chen, Yuanliang; Zhou, Chijin; Shen, Yuheng; Wu, Zhiyong; Fu, Jingzhou; Wang, Mingzhe; Li, Shanshan; Zhang, Quan","When Fuzzing Meets LLMs: Challenges and Opportunities","Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering","979-8-4007-0658-5","","10.1145/3663529.3663784","https://doi.org/10.1145/3663529.3663784","Fuzzing, a widely-used technique for bug detection, has seen advancements through Large Language Models (LLMs). Despite their potential, LLMs face specific challenges in fuzzing. In this paper, we identified five major challenges of LLM-assisted fuzzing. To support our findings, we revisited the most recent papers from top-tier conferences, confirming that these challenges are widespread. As a remedy, we propose some actionable recommendations to help improve applying LLM in Fuzzing and conduct preliminary evaluations on DBMS fuzzing. The results demonstrate that our recommendations effectively address the identified challenges.","2024","2025-11-25 22:29:36","2025-11-25 22:29:36","","492–496","","","","","","","FSE 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","Large Language Model; Fuzzing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NCJQHLAH","conferencePaper","2025","Zhang, Yuanliang; Xie, Yifan; Li, Shanshan; Liu, Ke; Wang, Chong; Jia, Zhouyang; Huang, Xiangbing; Song, Jie; Luo, Chaopeng; Zheng, Zhizheng; Xu, Rulin; Liu, Yitong; Zheng, Si; Liao, Xiangke","Unseen Horizons: Unveiling the Real Capability of LLM Code Generation Beyond the Familiar","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00082","https://doi.org/10.1109/ICSE55347.2025.00082","Recently, large language models (LLMs) have shown strong potential in code generation tasks. However, there are still gaps before they can be fully applied in actual software development processes. Accurately assessing the code generation capabilities of large language models has become an important basis for evaluating and improving the models. Some existing works have constructed datasets to evaluate the capabilities of these models. However, the current evaluation process may encounter the illusion of ""Specialist in Familiarity"", primarily due to three gaps: the exposure of target code, case timeliness, and dependency availability. The fundamental reason for these gaps is that the code in current datasets may have been extensively exposed and exercised during the training phase, and due to the continuous training and development of LLM, their timeliness has been severely compromised.The key to solve the problem is to, as much as possible, evaluate the LLMs using code that they have not encountered before. Thus, the fundamental idea in this paper is to draw on the concept of code obfuscation, changing code at different levels while ensuring the functionality and output. To this end, we build a code-obfuscation based benchmark ObfusEval. We first collect 1,354 raw cases from five real-world projects, including function description and code. Then we use three-level strategy (symbol, structure and semantic) to obfuscate descriptions, code and context dependencies. We evaluate four LLMs on ObfusEval and compared the effectiveness of different obfuscation strategy. We use official test suites of these projects to evaluate the generated code. The results show that after obfuscation, the average decrease ratio of test pass rate can up to 62.5%.","2025","2025-11-25 22:29:36","2025-11-25 22:29:36","","604–615","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","large language model; code dataset; code generation capability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HCVIA4T8","conferencePaper","2024","Fan, Zhiyu; Ruan, Haifeng; Mechtaev, Sergey; Roychoudhury, Abhik","Oracle-Guided Program Selection from Large Language Models","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680308","https://doi.org/10.1145/3650212.3680308","While large language models (LLMs) have shown significant advancements in code generation, their susceptibility to producing incorrect code poses a significant challenge to the adoption of LLM-generated programs. This issue largely stems from the reliance on natural language descriptions as informal oracles in code generation. Current strategies to mitigate this involve selecting the best program from multiple LLM-generated alternatives, judged by criteria like the consistency of their execution results on an LLM-generated test suite. However, this approach has crucial limitations: (1) LLMs often generate redundant tests or tests that cannot distinguish between correct and incorrect solutions, (2) the used consistency criteria, such as the majority vote, fail to foster developer trust due to the absence of transparent rationale behind the made choices. In this work, we propose a new perspective on increasing the quality of LLM-generated code via program selection using the LLM as a test oracle. Our method is based on our experimentally confirmed observation that LLMs serve more effectively as oracles when tasked with selecting the correct output from multiple choices. Leveraging this insight, we first generate distinguishing inputs that capture semantic discrepancies of programs sampled from an LLM, and record outputs produced by the programs on these inputs. An LLM then selects the most likely to be correct output from these, guided by the natural language problem description. We implemented this idea in a tool LLMCodeChoice and evaluated its accuracy in generating and selecting standalone programs. Our experiments demonstrated its effectiveness in improving pass@1 by 3.6-7% on HumanEval and MBPP benchmarks compared to the state-of-art CodeT. Most interestingly, the selected input-output specifications helped us to uncover incompleteness and ambiguities in task descriptions and also identify incorrect ground-truth implementations in the benchmarks.","2024","2025-11-25 22:29:36","2025-11-25 22:29:36","","628–640","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","large language model; differential testing; code generation; oracle inference","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F9BUV8DG","conferencePaper","2025","Zhang, Jiyang; Liu, Yu; Nie, Pengyu; Li, Junyi Jessy; Gligoric, Milos","exLong: Generating Exceptional Behavior Tests with Large Language Models","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00176","https://doi.org/10.1109/ICSE55347.2025.00176","Many popular programming languages, including C#, Java, and Python, support exceptions. Exceptions are thrown during program execution if an unwanted event happens, e.g., a method is invoked with an illegal argument value. Software developers write exceptional behavior tests (EBTs) to check that their code detects unwanted events and throws appropriate exceptions. Prior research studies have shown the importance of EBTs, but those studies also highlighted that developers put most of their efforts on ""happy paths"", e.g., paths without unwanted events. To help developers fill the gap, we present the first framework, dubbed exLong, that automatically generates EBTs. exLong is a large language model instruction fine-tuned from CodeLlama and embeds reasoning about traces that lead to throw statements, conditional expressions that guard throw statements, and non-exceptional behavior tests that execute similar traces. We compare exLong with the state-of-the-art models for test generation (CAT-LM) and one of the strongest foundation models (GPT-4o), as well as with analysis-based tools for test generation (Randoop and EvoSuite). Our results show that exLong outperforms existing models and tools. Furthermore, we contributed several pull requests to open-source projects and 23 EBTs generated by exLong were already accepted.","2025","2025-11-25 22:29:36","2025-11-25 22:47:19","","1462–1474","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","Analytical models; Large language models; Test pattern generators; Java; Software; Software engineering; Codes; large language models; Cognition; test generation; program analysis; exceptional behavior tests; Python; Quality assurance","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JQ77G7SI","bookSection","2025","Zhong, Linghan; Yuan, Samuel; Zhang, Jiyang; Liu, Yu; Nie, Pengyu; Li, Junyi Jessy; Gligoric, Milos","A Tool for Generating Exceptional Behavior Tests With Large Language Models","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","","https://doi.org/10.1145/3696630.3728608","Exceptional behavior tests (EBTs) are crucial in software development for verifying that code correctly handles unwanted events and throws appropriate exceptions. However, prior research has shown that developers often prioritize testing ""happy paths"", i.e., paths without unwanted events, over exceptional scenarios. We present exLong, a tool that automatically generates EBTs to address this gap. exLong leverages a large language model (LLM) fine-tuned from CodeLlama and incorporates reasoning about exception-throwing traces, conditional expressions that guard throw statements, and non-exceptional behavior tests that execute similar traces. Our demonstration video illustrates how exLong can effectively assist developers in creating comprehensive EBTs for their project (available at https://youtu.be/Jro8kMgplZk).","2025","2025-11-25 22:29:36","2025-11-25 22:29:36","","1193–1197","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IJESJR5X","journalArticle","2025","Wang, Dong; Yu, Junji; Shu, Honglin; Fu, Michael; Tantithamthavorn, Chakkrit; Kamei, Yasutaka; Chen, Junjie","On the Evaluation of Large Language Models in Multilingual Vulnerability Repair","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3771930","https://doi.org/10.1145/3771930","Various Deep Learning-based approaches with pre-trained language models have been proposed for automatically repairing software vulnerabilities. However, these approaches are limited to a specific programming language (C/C++). Recent advances in large language models (LLMs) offer language-agnostic capabilities and strong semantic understanding, exhibiting potential to overcome multilingual vulnerability limitation. Although some work has begun to explore LLM’s repair performance, their effectiveness is unsatisfactory. To address these limitations, we conducted a large-scale empirical study to investigate the performance of automated vulnerability repair approaches and state-of-the-art LLMs across seven programming languages. Results show GPT-4o, instruction-tuned with few-shot prompting, performs competitively against the leading approach, VulMaster. Additionally, the LLM-based approach shows superior performance in repairing unique vulnerabilities and is more likely to repair the most dangerous vulnerabilities. Instruction-tuned GPT-4o demonstrates strong generalization on vulnerabilities in previously unseen language, outperforming existing approaches. Analysis shows that Go consistently achieves the highest effectiveness across all model types, while C/C++ performs the worst. Based on findings, we discuss the promising of LLM on multilingual vulnerability repair and reasons behind LLM failed cases. This work takes the first look at repair approaches and LLMs across multiple languages, highlighting the promising future of adopting LLMs to multilingual vulnerability repair.","2025-10","2025-11-25 22:29:36","2025-11-25 22:29:36","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Multilingual Vulnerability; Vulnerability Repair","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E9QYMHWD","conferencePaper","2025","Zhao, Shengming; Wang, Jiawei","Best practice for supply chain in LLM-assisted medical applications","Proceedings of the 34th ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-1474-0","","10.1145/3713081.3731748","https://doi.org/10.1145/3713081.3731748","The application of large language models in medical applications is crucial for enhancing diagnostic accuracy, improving patient communication, and boosting healthcare efficiency. Their ability to process vast amounts of data, generate concise information, and automate tasks positions LLM applications as transformative tools. Recent studies and real-world examples underscore the importance of delivering secure and responsible LLM-assisted applications. In this manuscript, we outline our intention of uncovering best software practices in the supply chain of LLM medical applications.","2025","2025-11-25 22:29:36","2025-11-25 22:29:36","","174–177","","","","","","","ISSTA Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","large language models; software supply chain; medical system; software best practice","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6757WIBU","journalArticle","2025","Vallecillos Ruiz, Fernando; Grishina, Anastasiia; Hort, Max; Moonen, Leon","Assessing the Latent Automated Program Repair Capabilities of Large Language Models using Round-Trip Translation","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3771922","https://doi.org/10.1145/3771922","Research shows that errors in natural language can be corrected by translating texts to another language and back using language models. We explore to what extent this latent correction capability extends to Automated Program Repair (APR) by investigating Round-Trip Translation (RTT): translating code from one programming language into another programming or natural language and back, using Large Language Models (LLMs). We hypothesize that RTT restores patterns most commonly seen in the LLM’s training corpora through regression toward the mean, replacing infrequent bugs with more frequent, natural, bug-free code. To test this hypothesis, we employ nine LLMs and four common APR benchmarks in Java, and perform a detailed quantitative and qualitative analysis of RTT-generated patches. We find that RTT through English generates plausible patches for 100 of 164 bugs with GPT-4 on the HumanEval-Java benchmark, and 97 are found to be correct in our manual assessment. Moreover, RTT uniquely generates plausible patches for 46 bugs that were missed by LLMs specifically fine-tuned for APR. While this demonstrates the viability of RTT for APR, we also observe limitations, such as a lower overall bug fix rate than the state-of-the-art and diluting the original coding style. We analyze the impact of these limitations and discuss the potential of using RTT as a complementary component in APR frameworks.","2025-10","2025-11-25 22:29:36","2025-11-25 22:29:36","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","large language model; automated program repair; machine translation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ITNSR48E","conferencePaper","2025","Mougouei, Davoud; Azarnik, Ahmad; Fahmideh, Mahdi; Mougouei, Elahe; Dam, Hoa Khanh; Khan, Arif Ali; Rafi, Saima; Khan, Javed Ali; Ahmad, Aakash","A First Look at AI Trends in Value-Aligned Software Engineering Publications: Human-LLM Insights","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: Software Engineering in Society","979-8-3315-3707-4","","10.1109/ICSE-SEIS66351.2025.00014","https://doi.org/10.1109/ICSE-SEIS66351.2025.00014","Recent criticism of social media platforms by the U.S. Senate Judiciary Committee for neglecting child safety exemplifies how software can undermine human values. This is further complicated by the growing integration of Artificial Intelligence (AI) in software, which introduces inherent challenges such as biases and limited transparency. However, AI also presents opportunities to embed human values into software. To explore these opportunities, we have utilized the reasoning abilities of ChatGPT, a large language model (LLM), in combination with human expertise, to study the use of AI in publications that address human values, across some of the leading software engineering (SE) venues from 2022 to 2023. Our findings confirm the use of AI concepts - mainly General Machine Learning - in around 33% of these value-aligned publications. The value alignments largely concern pragmatic aspects of Achievement and (personal) Security, while the majority of the values receive less attention. The socially focused values of Conformity and Tradition and the personally focused value of Hedonism are rarely addressed in the SE publications.","2025","2025-11-25 22:29:36","2025-11-25 22:46:18","","82–93","","","","","","","ICSE-SEIS '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","Large language models; Software; Software engineering; Chatbots; ChatGPT; Market research; Artificial intelligence; Software Engineering; software engineering; Large Language Models (LLMs); artificial intelligence (AI); human values; large language models (LLMs); value-aligned publications; Artificial Intelligence (AI); Security; Safety; Social networking (online); Human Values; Pragmatics; Value-Aligned Publications","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E2PH49WF","conferencePaper","2024","Guan, Hao; Bai, Guangdong; Liu, Yepang","Large Language Models Can Connect the Dots: Exploring Model Optimization Bugs with Domain Knowledge-Aware Prompts","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680383","https://doi.org/10.1145/3650212.3680383","Model optimization, such as pruning and quantization, has become the de facto pre-deployment phase when deploying deep learning&nbsp;(DL) models on resource-constrained platforms. However, the complexity of DL models often leads to non-trivial bugs in model optimizers, known as model optimization bugs&nbsp;(MOBs). These MOBs are characterized by involving complex data types and layer structures inherent to DL models, causing significant hurdles in detecting them through traditional static analysis and dynamic testing techniques. In this work, we leverage Large Language Models (LLMs) with prompting techniques to generate test cases for MOB detection. We explore how LLMs can draw an understanding of the MOB domain from scattered bug instances and generalize to detect new ones, a paradigm we term as concentration and diffusion. We extract MOB domain knowledge from the artifacts of known MOBs, such as their issue reports and fixes, and design knowledge-aware prompts to guide LLMs in generating effective test cases. The domain knowledge of code structure and error description provides precise in-depth depictions of the problem domain, i.e., the concentration, and heuristic directions to generate innovative test cases, i.e., the diffusion. Our approach is implemented as a tool named YanHui and benchmarked against existing few-shot LLM-based fuzzing techniques. Test cases generated by YanHui demonstrate enhanced capability to find relevant API and data combinations for exposing MOBs, leading to an 11.4% increase in generating syntactically valid code and a 22.3% increase in generating on-target code specific to model optimization. YanHui detects 17 MOBs, and among them, five are deep MOBs that are difficult to reveal without our prompting technique.","2024","2025-11-25 22:29:36","2025-11-25 22:29:36","","1579–1591","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large Language Model; Library Testing; Model Optimization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C2W8UGVF","journalArticle","2025","Zhang, Yixuan; He, Ningyu; Gao, Jianting; Cao, Shangtong; Liu, Kaibo; Wang, Haoyu; Ma, Yun; Huang, Gang; Liu, Xuanzhe","DrWASI: LLM-assisted Differential Testing for WebAssembly System Interface Implementations","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3716379","https://doi.org/10.1145/3716379","WebAssembly (Wasm) is an emerging binary format that serves as a compilation target for over 40 programming languages. Wasm runtimes provide execution environments that enhance portability by abstracting away operating systems and hardware details. A key component in these runtimes is the WebAssembly System Interface (WASI), which manages interactions with operating systems, like file operations. Considering the critical role of Wasm runtimes, the community has aimed to detect their implementation bugs. However, no work has focused on WASI-specific bugs that can affect the original functionalities of running Wasm binaries and cause unexpected results. To fill the void, we present DrWASI, the first general-purpose differential testing framework for WASI implementations. Our approach uses a large language model to generate seeds and applies variant and environment mutation strategies to expand and enrich the test case corpus. We then perform differential testing across major Wasm runtimes. By leveraging dynamic and static information collected during and after the execution, DrWASI can identify bugs. Our evaluation shows that DrWASI uncovered 33 unique bugs, with all confirmed and 7 fixed by developers. This research represents a pioneering step in exploring a promising yet under-explored area of the Wasm ecosystem, providing valuable insights for stakeholders.","2025-02","2025-11-25 22:29:36","2025-11-25 22:29:36","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","WebAssembly; WebAssembly System Interface","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B4JK433Y","conferencePaper","2025","Kabir, Md Mahir Asef; Hassan, Sk Adnan","How Well Do ChatGPT Models Maintain Software?","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","10.1145/3696630.3728698","https://doi.org/10.1145/3696630.3728698","Since the launch of ChatGPT in 2022, people have conducted various studies to investigate its capabilities in code generation, bug-fixing, test generation, and program comprehension. While ChatGPT has demonstrated strong capabilities in several aspects of software engineering, their effectiveness in maintaining software remains under-explored. Motivated by such a lack of study, we conducted an empirical study to systematically evaluate the performance of ChatGPT in software maintenance. Specifically, we distilled 58 software maintenance tasks from 58 GitHub projects. For each task, we prompted two ChatGPT models—ChatGPT-3.5 and ChatGPT-4o—to separately revise a given Java file, in response to a prescribed maintenance request. Once the models returned results, we assessed each model's capability by comparing those revisions with developers' modifications recorded in the version history.We found that ChatGPT-3.5 correctly revised code for 30 of the 58 tasks, while ChatGPT-4o correctly fulfilled 31 tasks. Neither model fulfilled all tasks successfully mainly because they either truncated Java files unnecessarily, missed project-specific logic, or failed to cover all corner cases. This phenomenon implies that ChatGPT can help developers in software maintenance, but is unlikely to replace developers completely. Our study characterizes ChatGPT's capabilities in software maintenance and its progression across model versions. It also sheds light on ChatGPT's potential roles in future software-maintenance practices.","2025","2025-11-25 22:29:36","2025-11-25 22:29:36","","1631–1637","","","","","","","FSE Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","ChatGPT; empirical study; research; software maintenance","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RHXAPG2S","journalArticle","2025","Leesatapornwongsa, Tanakorn; Faisal, Fazle; Nath, Suman","ReproCopilot: LLM-Driven Failure Reproduction with Dynamic Refinement","Proc. ACM Softw. Eng.","","","10.1145/3729399","https://doi.org/10.1145/3729399","Failure reproduction is a crucial step for debugging software systems, but it is often challenging and time-consuming, especially when the failures are caused by complex inputs, states, or environments. In this paper, we present ReproCopilot, a tool that leverages program analysis and a large language model (LLM) to generate a workload (i.e., code and inputs) that can reproduce a given failure. ReproCopilot proposes two novel techniques: state-oriented code generation and dynamic refinement. These techniques can iteratively guide the LLM with program analysis feedback until the generated workload can successfully reproduce the target failure. We evaluate ReproCopilot on 50 real-world failures from 17 open-source projects, and show that it can reproduce 76% of them, significantly outperforming the-state-of-the-art solutions.","2025-06","2025-11-25 22:29:36","2025-11-25 22:29:36","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; Code Generation; Errors; Failures; Reproduction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BF69LP94","conferencePaper","2024","Guo, An; Zhou, Yuan; Tian, Haoxiang; Fang, Chunrong; Sun, Yunjian; Sun, Weisong; Gao, Xinyu; Luu, Anh Tuan; Liu, Yang; Chen, Zhenyu","SoVAR: Build Generalizable Scenarios from Accident Reports for Autonomous Driving Testing","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695037","https://doi.org/10.1145/3691620.3695037","Autonomous driving systems (ADSs) have undergone remarkable development and are increasingly employed in safety-critical applications. However, recently reported data on fatal accidents involving ADSs suggests that the desired level of safety has not yet been fully achieved. Consequently, there is a growing need for more comprehensive and targeted testing approaches to ensure safe driving. Scenarios from real-world accident reports provide valuable resources for ADS testing, including critical scenarios and high-quality seeds. However, existing scenario reconstruction methods from accident reports often exhibit limited accuracy in information extraction. Moreover, due to the diversity and complexity of road environments, matching current accident information with the simulation map data for reconstruction poses significant challenges.In this paper, we design and implement SoVAR, a tool for automatically generating road-generalizable scenarios from accident reports. SoVAR utilizes well-designed prompts with linguistic patterns to guide the large language model (LLM) in extracting accident information from textual data. Subsequently, it formulates and solves accident-related constraints in conjunction with the extracted accident information to generate accident trajectories. Finally, SoVAR reconstructs accident scenarios on various map structures and converts them into test scenarios to evaluate its capability to detect defects in industrial ADSs. We experiment with SoVAR, using the accident reports from the National Highway Traffic Safety Administration's (NHTSA) database to generate test scenarios for the industrial-grade ADS Apollo. The experimental findings demonstrate that SoVAR can effectively generate generalized accident scenarios across different road structures. Furthermore, the results confirm that SoVAR identified 5 distinct safety violation types that contributed to the crash of Baidu Apollo.","2024","2025-11-25 22:29:36","2025-11-25 22:29:36","","268–280","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","software testing; automatic test generation; autonomous driving system; constraint solving","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YFKJ92DL","conferencePaper","2024","Tian, Zhao; Shu, Honglin; Wang, Dong; Cao, Xuejie; Kamei, Yasutaka; Chen, Junjie","Large Language Models for Equivalent Mutant Detection: How Far Are We?","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680395","https://doi.org/10.1145/3650212.3680395","Mutation testing is vital for ensuring software quality. However, the presence of equivalent mutants is known to introduce redundant cost and bias issues, hindering the effectiveness of mutation testing in practical use. Although numerous equivalent mutant detection (EMD) techniques have been proposed, they exhibit limitations due to the scarcity of training data and challenges in generalizing to unseen mutants. Recently, large language models (LLMs) have been extensively adopted in various code-related tasks and have shown superior performance by more accurately capturing program semantics. Yet the performance of LLMs in equivalent mutant detection remains largely unclear. In this paper, we conduct an empirical study on 3,302 method-level Java mutant pairs to comprehensively investigate the effectiveness and efficiency of LLMs for equivalent mutant detection. Specifically, we assess the performance of LLMs compared to existing EMD techniques, examine the various strategies of LLMs, evaluate the orthogonality between EMD techniques, and measure the time overhead of training and inference. Our findings demonstrate that LLM-based techniques significantly outperform existing techniques (i.e., the average improvement of 35.69% in terms of F1-score), with the fine-tuned code embedding strategy being the most effective. Moreover, LLM-based techniques offer an excellent balance between cost (relatively low training and inference time) and effectiveness. Based on our findings, we further discuss the impact of model size and embedding quality, and provide several promising directions for future research. This work is the first to examine LLMs in equivalent mutant detection, affirming their effectiveness and efficiency.","2024","2025-11-25 22:29:36","2025-11-25 22:29:36","","1733–1745","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large Language Model; Mutation Testing; Empirical Study; Equivalent Mutant Detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D8KVA84X","journalArticle","2024","Mu, Fangwen; Shi, Lin; Wang, Song; Yu, Zhuohao; Zhang, Binquan; Wang, ChenXue; Liu, Shichao; Wang, Qing","ClarifyGPT: A Framework for Enhancing LLM-Based Code Generation via Requirements Clarification","Proc. ACM Softw. Eng.","","","10.1145/3660810","https://doi.org/10.1145/3660810","Large Language Models (LLMs), such as ChatGPT, have demonstrated impressive capabilities in automatically generating code from provided natural language requirements. However, in real-world practice, it is inevitable that the requirements written by users might be ambiguous or insufficient. Current LLMs will directly generate programs according to those unclear requirements, regardless of interactive clarification, which will likely deviate from the original user intents. To bridge that gap, we introduce a novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions. Specifically, ClarifyGPT first detects whether a given requirement is ambiguous by performing a code consistency check. If it is ambiguous, ClarifyGPT prompts an LLM to generate targeted clarifying questions. After receiving question responses, ClarifyGPT refines the ambiguous requirement and inputs it into the same LLM to generate a final code solution. To evaluate our ClarifyGPT, we invite ten participants to use ClarifyGPT for code generation on two benchmarks: MBPP-sanitized and MBPP-ET. The results show that ClarifyGPT elevates the performance (Pass@1) of GPT-4 from 70.96% to 80.80% on MBPP-sanitized. Furthermore, to conduct large-scale automated evaluations of ClarifyGPT across different LLMs and benchmarks without requiring user participation, we introduce a high-fidelity simulation method to simulate user responses. The results demonstrate that ClarifyGPT can significantly enhance code generation performance compared to the baselines. In particular, ClarifyGPT improves the average performance of GPT-4 and ChatGPT across five benchmarks from 62.43% to 69.60% and from 54.32% to 62.37%, respectively. A human evaluation also confirms the effectiveness of ClarifyGPT in detecting ambiguous requirements and generating high-quality clarifying questions. We believe that ClarifyGPT can effectively facilitate the practical application of LLMs in real-world development environments.","2024-07","2025-11-25 22:29:36","2025-11-25 22:29:36","","","","FSE","1","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Prompt Engineering; Code Generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6DUD8R62","journalArticle","2025","Bettscheider, Leon; Zeller, Andreas","Inferring Input Grammars from Code with Symbolic Parsing","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3776743","https://doi.org/10.1145/3776743","Generating effective test inputs for a software system requires that these inputs be valid, as they will otherwise be rejected without reaching actual functionality. In the absence of a specification for the input language, common test generation techniques rely on sample inputs, which are abstracted into matching grammars and/or evolved guided by test coverage. However, if sample inputs miss features of the input language, the chances of generating these features randomly are slim.In this work, we present the first technique for symbolically and automatically mining input grammars from the code of recursive descent parsers. So far, the complexity of parsers has made such a symbolic analysis challenging to impossible. Our realization of the symbolic parsing technique overcomes these challenges by (1) associating each parser function&nbsp;parse_ELEM() with a nonterminal&nbsp;&lt;ELEM&gt;; (2) limiting recursive calls and loop iterations, such that a symbolic analysis of&nbsp;parse_ELEM() needs to consider only a finite number of paths; and (3) for each path, create an expansion alternative for&nbsp;&lt;ELEM&gt;. Being purely static, symbolic parsing does not require seed inputs; as it mitigates path explosion, it scales to complex parsers.Our evaluation promises symbolic parsing to be highly accurate. Applied on parsers for complex languages such as TINY-C or JSON, our STALAGMITE implementation extracts grammars with an accuracy of 99–100%, widely improving over the state of the art despite requiring only the program code and no input samples. The resulting grammars cover the entire input space, allowing for comprehensive and effective test generation, reverse engineering, and documentation.","2025-11","2025-11-25 22:29:36","2025-11-25 22:29:36","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","test generation; fuzzing; Input grammars; symbolic analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PV6RGHYI","conferencePaper","2024","Decrop, Alix","Leveraging Natural Language Processing and Data Mining to Augment and Validate APIs","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3685554","https://doi.org/10.1145/3650212.3685554","APIs are increasingly prominent for modern web applications, allowing millions of users around the world to access data. Reducing the risk of API defects - and consequently failures - is key, notably for security, availability, and maintainability purposes. Documenting an API is crucial, allowing the user to better understand it. Moreover, API testing techniques often require formal documentation as input. However, documenting is a time-consuming and error-prone task, often overlooked by developers. Natural Language Processing (NLP) could assist API development, as recent Large Language Models (LLMs) demonstrated exceptional abilities to automate tasks based on their colossal training data. Data mining could also be utilized, synthesizing API information scattered across the web. Hence, I present my PhD project aimed at exploring the usage of NLP-related technologies and data mining to augment and validate APIs. The research questions of this PhD project are: (1) What types of APIs can benefit from NLP and data mining assistance? (2) What API problems can be solved with such methods? (3) How effective are the methods (i.e. LLMs) in assisting APIs? (4) How efficient are the methods in assisting APIs (i.e. time and costs)?","2024","2025-11-25 22:29:37","2025-11-25 22:29:37","","1906–1908","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","LLM; Automation; Software Testing; API; Data Mining; NLP","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BWCT3P6T","conferencePaper","2024","Luo, Yang; Yu, Richard; Zhang, Fajun; Liang, Ling; Xiong, Yongqiang","Bridging Gaps in LLM Code Translation: Reducing Errors with Call Graphs and Bridged Debuggers","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695322","https://doi.org/10.1145/3691620.3695322","When using large language models (LLMs) for code translation of complex software, numerous compilation and runtime errors can occur due to insufficient context awareness. To address this issue, this paper presents a code translation method based on call graphs and bridged debuggers: TransGraph. TransGraph first obtains the call graph of the entire code project using the Language Server Protocol, which provides a detailed description of the function call relationships in the program. Through this structured view of the code, LLMs can more effectively handle large-scale and complex codebases, significantly reducing compilation errors. Furthermore, TransGraph, combined with bridged debuggers and dynamic test case generation, significantly reduces runtime errors, overcoming the limitations of insufficient test case coverage in traditional methods. In experiments on six datasets including CodeNet and Avatar, TransGraph outperformed existing code translation methods and LLMs in terms of translation accuracy, with improvements of up to 10.2%.","2024","2025-11-25 22:29:37","2025-11-25 22:47:03","","2448–2449","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","large language model; Large language models; Software; Writing; Codes; Source coding; language server protocol; bridged debugger; call graph; code translation; compilation error; runtime error; Generators; Protocols; Runtime; Servers; Context awareness; Language Server Protocol","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LBSM3T7P","conferencePaper","2024","Xu, Zheer; Cai, Shanqing; Varma T, Mukund; Venugopalan, Subhashini; Zhai, Shumin","SkipWriter: LLM-Powered Abbreviated Writing on Tablets","Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology","979-8-4007-0628-8","","10.1145/3654777.3676423","https://doi.org/10.1145/3654777.3676423","Large Language Models (LLMs) may offer transformative opportunities for text input, especially for physically demanding modalities like handwriting. We studied a form of abbreviated handwriting by designing, developing, and evaluating a prototype, named SkipWriter, that converts handwritten strokes of a variable-length prefix-based abbreviation (e.g., ""ho a y"" as handwritten strokes) into the intended full phrase (e.g., ""how are you"" in the digital format) based on the preceding context. SkipWriter consists of an in-production handwriting recognizer and an LLM fine-tuned on this task. With flexible pen input, SkipWriter allows the user to add and revise prefix strokes when predictions do not match the user’s intent. An user evaluation demonstrated a 60% reduction in motor movements with an average speed of 25.78 WPM. We also showed that this reduction is close to the ceiling of our model in an offline simulation.","2024","2025-11-25 22:29:37","2025-11-25 22:29:37","","","","","","","","","UIST '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Pittsburgh, PA, USA","","","","Large Language Model (LLM); Handwriting; Text Entry","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y7YVR7MI","conferencePaper","2025","Rawson, Jessica; Reddivari, Sandeep","A ChatGPT-powered Prompt Engineering Framework for Generating Software Acceptance Criteria","Proceedings of the 2025 ACM Southeast Conference","979-8-4007-1277-7","","10.1145/3696673.3723078","https://doi.org/10.1145/3696673.3723078","There has been a growing interest in using Natural Language Processing (NLP), such as OpenAI's ChatGPT for software engineering tasks, including requirements engineering, software design and software testing. This paper introduces a novel prompt engineering framework that aims to utilize ChatGPT for the generation of high-quality acceptance criteria in the software development process, particularly in the implementation and maintenance stages, by using curated prompts and inputs. The paper describes the development and possible implementation of the proposed framework.","2025","2025-11-25 22:29:37","2025-11-25 22:29:37","","282–288","","","","","","","ACMSE 2025","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Southeast Missouri State University, Cape Girardeau, MO, USA","","","","ChatGPT; prompt engineering; acceptance criteria; requirements engineering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F8PAJ2ZY","journalArticle","2025","Jang, Sujin; Ryou, Yeonhee; Lee, Heewon; Heo, Kihong","UnitCon: Synthesizing Targeted Unit Tests for Java Runtime Exceptions","Proc. ACM Softw. Eng.","","","10.1145/3729362","https://doi.org/10.1145/3729362","We present UnitCon, a system for synthesizing targeted unit testsfor runtime exceptions in Java programs. Targeted unit tests aim to reveal a bug at a specific location in the program under test. This capability benefits various tasks in software development, such as patch testing, crash reproduction, or static analysis alarm inspection. However, conventional unit test generation tools are mainly designed for regression tests by maximizing code coverage; hence they are not effective at such target-specific tasks. In this paper, we propose a novel synthesis technique that effectively guides the search for targeted unit tests. The key idea is to use static analysis to prune and prioritize the search space by estimating the semantics of candidate test cases. This allows us to efficiently focus on promising unit tests that are likely to trigger runtime exceptions at the target location. According to our experiments on a suite of Java programs, our approach outperforms the state-of-the-art unit test generation tools. We also applied UnitCon for inspecting static analysis alarms for null pointer exceptions (NPEs) in 51 open-source projects and discovered 21 previously unknown NPE bugs.","2025-06","2025-11-25 22:29:37","2025-11-25 22:29:37","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Software testing; Program synthesis; Program analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WE2JQKDN","conferencePaper","2024","Santos, Shexmo Richarlison Ribeiro dos; Fernandes, Raiane Eunice S.; Santos, Marcos Cesar Barbosa dos; Soares, Michel S.; Rocha, Fabio Gomes; Marczak, Sabrina","Increasing Test Coverage by Automating BDD Tests in Proofs of Concepts (POCs) using LLM","Proceedings of the XXIII Brazilian Symposium on Software Quality","979-8-4007-1777-2","","10.1145/3701625.3701637","https://doi.org/10.1145/3701625.3701637","In today’s landscape, software manufacturers must deliver quickly and with high quality to remain competitive, especially in the cybersecurity sector. Recognizing this need, we have recently implemented several strategies to accelerate our time to market without compromising quality. We introduced the Proof of Concept (POC) and Proof of Value (POV) stages in the Enterprise Architecture team before initiating inception processes for Minimum Viable Products (MVP) and new features. Initially, these proofs focused only on concept validation. However, since 2023, due to the growing need for rapid and high-quality innovation, POCs/POVs have begun to include robust implementations. We adopted the Behaviour-Driven Development (BDD) approach to define user stories and acceptance criteria, which provided a solid evaluation of POC/POV quality and involved the implementation team from the outset. To prevent products from incurring technical debt, we implemented AutoDevSuite, which uses LLM to generate tests based on user stories and acceptance criteria automatically. We used AutoDevSuite in a POC/POV of a cybersecurity product, and the results showed a significant expansion in test coverage, aligned with the acceptance criteria, demonstrating the tool’s effectiveness in automating and improving test quality.","2024","2025-11-25 22:29:37","2025-11-25 22:29:37","","519–525","","","","","","","SBQS '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Software quality; Large Language Model (LLM); Artificial Intelligence (AI); Automatic test code generator; Behavior-Driven Development (BDD)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TVJXGRZJ","conferencePaper","2025","Bose, Dibyendu Brinto","From Prompts to Properties: Rethinking LLM Code Generation with Property-Based Testing","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","10.1145/3696630.3728702","https://doi.org/10.1145/3696630.3728702","Large Language Models (LLMs) have shown promise in automated code generation, but ensuring correctness remains a significant challenge. Traditional unit testing evaluates functional correctness but often fails to capture deeper logical constraints. We apply Property-Based Testing (PBT) as an alternative evaluation strategy to StarCoder and CodeLlama on MBPP and HumanEval. Our results reveal that while pass@k evaluation shows moderate success, PBT exposes additional correctness gaps. A significant portion of generated solutions only partially adhere to correctness properties (30–32%), while 18–23% fail outright. Property extraction is also imperfect, with 9–13% of constraints missing. These findings highlight that unit test-based evaluations may overestimate solution correctness by not capturing fundamental logical errors. Our study demonstrates that combining unit testing with PBT can offer a more comprehensive assessment of generated code correctness, revealing limitations that traditional verification approaches miss.","2025","2025-11-25 22:29:37","2025-11-25 22:29:37","","1660–1665","","","","","","","FSE Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","code generation; large language model(LLM); property-based-testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GGCVMJKL","bookSection","2025","Kong, Qichao; Lv, Zhengwei; Xiong, Yiheng; Wang, Dingchun; Sun, Jingling; Su, Ting; Li, Letao; Yang, Xu; Huo, Gang","ProphetAgent: Automatically Synthesizing GUI Tests from Test Cases in Natural Language for Mobile Apps","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","","https://doi.org/10.1145/3696630.3728543","GUI tests is crucial for ensuring software quality and user satisfaction of mobile apps. In practice, companies often maintain extensive test cases written in natural language. Testers need to convert these test cases into executable scripts for regression and compatibility testing. Requirement changes or version updates often necessitate the addition and modification to these test cases. Thus, when faced with large volumes of test cases and regular updates, this process becomes costly, which is a common challenge across the industry. To address this issue, this paper proposes ProphetAgent that can automatically synthesize executable GUI tests from the test cases written in natural language. ProphetAgent first constructs a Clustered UI Transition Graph (CUTG) enriched with semantic information, then leverages large language models to generate the executable test case based on CUTG and test cases written in natural language. Experiment results show that ProphetAgent achieved a 78.1% success rate across 120 test cases in Douyin, Doubao, and six open-source apps, surpassing existing automated approaches (21.4% for AppAgent and 32.5% for AutoDroid). Additionally, statistical data from ByteDance's testing platform show that ProphetAgent increased testers' efficiency in synthesizing UI tests by 260%.","2025","2025-11-25 22:29:37","2025-11-25 22:29:37","","174–179","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VASH2F7N","conferencePaper","2024","Adejumo, Elijah Kayode; Johnson, Brittany","Towards Leveraging LLMs for Reducing Open Source Onboarding Information Overload","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695286","https://doi.org/10.1145/3691620.3695286","Consistent, diverse, and quality contributions are essential to the sustainability of the open source community. Therefore, it is important that there is infrastructure for effectively onboarding and retaining diverse newcomers to open source software projects. Most often, open source projects rely on onboarding documentation to support newcomers in making their first contributions. Unfortunately, prior studies suggest that information overload from available documentation, along with the predominantly monolingual nature of repositories, can have negative effects on the newcomer experiences and onboarding process. This, coupled with the effort involved in creating and maintaining onboarding documentation, suggest a need for support in creating more accessible documentation. Large language models (LLMs) have shown great potential in providing text transformation support in other domains, and even shown promise in simplifying or generating other kinds of computing artifacts, such as source code and technical documentation. We contend that LLMs can also help make software onboarding documentation more accessible, thereby reducing the potential for information overload. Using ChatGPT (GPT-3.5 Turbo) and Gemini Pro as case studies, we assessed the effectiveness of LLMs for simplifying software onboarding documentation, one method for reducing information overload. We discuss a broader vision for using LLMs to support the creation of more accessible documentation and outline future research directions toward this vision.","2024","2025-11-25 22:29:37","2025-11-25 22:29:37","","2210–2214","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","ChatGPT; generative AI; software; LLMs; documentation; on-boarding; open-source","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3YYWUITF","journalArticle","2025","Li, Fengjie; Jiang, Jiajun; Sun, Jiajun; Zhang, Hongyu","Hybrid Automated Program Repair by Combining Large Language Models and Program Analysis","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3715004","https://doi.org/10.1145/3715004","Automated Program Repair (APR) has garnered significant attention due to its potential to streamline the bug repair process for human developers. Recently, LLM-based APR methods have shown promise in repairing real-world bugs. However, existing APR methods often utilize patches generated by LLMs without further optimization, resulting in reduced effectiveness due to the lack of program-specific knowledge. Furthermore, the evaluations of these APR methods have typically been conducted under the assumption of perfect fault localization, which may not accurately reflect their real-world effectiveness. To address these limitations, this article introduces an innovative APR approach called GiantRepair. Our approach leverages the insight that LLM-generated patches, although not necessarily correct, offer valuable guidance for the patch generation process. Based on this insight, GiantRepair first constructs patch skeletons from LLM-generated patches to confine the patch space, and then generates high-quality patches tailored to specific programs through context-aware patch generation by instantiating the skeletons. To evaluate the performance of our approach, we conduct two large-scale experiments. The results demonstrate that GiantRepair not only effectively repairs more bugs (an average of 27.78% on Defects4J v1.2 and 23.40% on Defects4J v2.0) than using LLM-generated patches directly, but also outperforms state-of-the-art APR methods by repairing at least 42 and 7 more bugs under perfect and automated fault localization scenarios, respectively.","2025-08","2025-11-25 22:29:37","2025-11-25 22:29:37","","","","7","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Program Repair; Program Synthesis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DXGNJ7J9","conferencePaper","2023","Li, Haonan; Hao, Yu; Zhai, Yizhuo; Qian, Zhiyun","Assisting Static Analysis with Large Language Models: A ChatGPT Experiment","Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","979-8-4007-0327-0","","10.1145/3611643.3613078","https://doi.org/10.1145/3611643.3613078","Recent advances of Large Language Models (LLMs), e.g., ChatGPT, exhibited strong capabilities of comprehending and responding to questions across a variety of domains. Surprisingly, ChatGPT even possesses a strong understanding of program code. In this paper, we investigate where and how LLMs can assist static analysis by asking appropriate questions. In particular, we target a specific bug-finding tool, which produces many false positives from the static analysis. In our evaluation, we find that these false positives can be effectively pruned by asking carefully constructed questions about function-level behaviors or function summaries. Specifically, with a pilot study of 20 false positives, we can successfully prune 8 out of 20 based on GPT-3.5, whereas GPT-4 had a near-perfect result of 16 out of 20, where the four failed ones are not currently considered/supported by our questions, e.g., involving concurrency. Additionally, it also identified one false negative case (a missed bug). We find LLMs a promising tool that can enable a more effective and efficient program analysis.","2023","2025-11-25 22:29:37","2025-11-25 22:29:37","","2107–2111","","","","","","","ESEC/FSE 2023","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: San Francisco, CA, USA","","","","large language model; static analysis; bug detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IIYAM45D","journalArticle","2025","Chen, Mengzhuo; Liu, Zhe; Chen, Chunyang; Wang, Junjie; Wu, Boyu; Hu, Jun; Wang, Qing","Standing on the Shoulders of Giants: Bug-Aware Automated GUI Testing via Retrieval Augmentation","Proc. ACM Softw. Eng.","","","10.1145/3715755","https://doi.org/10.1145/3715755","In software development, similar apps often encounter similar bugs due to shared functionalities and implementation methods. However, current automated GUI testing methods mainly focus on generating test scripts to cover more pages by analyzing the internal structure of the app, without targeted exploration of paths that may trigger bugs, resulting in low efficiency in bug discovery. Considering that a large number of bug reports on open source platforms can provide external knowledge for testing, this paper proposes BugHunter, a novel bug-aware automated GUI testing approach that generates exploration paths guided by bug reports from similar apps, utilizing a combination of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG). Instead of focusing solely on coverage, BugHunter dynamically adapts the testing process to target bug paths, thereby increasing bug detection efficiency. BugHunter first builds a high-quality bug knowledge base from historical bug reports. Then it retrieves relevant reports from this large bug knowledge base using a two-stage retrieval process, and generates test paths based on similar apps’ bug reports. BugHunter also introduces a local and global path-planning mechanism to handle differences in functionality and UI design across apps, and the ambiguous behavior or missing steps in the online bug reports. We evaluate BugHunter on 121 bugs across 71 apps and compare its performance against 16 state-of-the-art baselines. BugHunter achieves 60% improvement in bug detection over the best baseline, with comparable or higher coverage against the baselines. Furthermore, BugHunter successfully detects 49 new crash bugs in real-world apps from Google Play, with 33 bugs fixed, 9 confirmed, and 7 pending feedback.","2025-06","2025-11-25 22:29:37","2025-11-25 22:29:37","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large language model; GUI testing; Mobile App","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VHNCFG6H","conferencePaper","2024","Wang, Dingbang; Zhao, Yu; Feng, Sidong; Zhang, Zhaoxu; Halfond, William G. J.; Chen, Chunyang; Sun, Xiaoxia; Shi, Jiangfan; Yu, Tingting","Feedback-Driven Automated Whole Bug Report Reproduction for Android Apps","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680341","https://doi.org/10.1145/3650212.3680341","In software development, bug report reproduction is a challenging task. This paper introduces ReBL, a novel feedback-driven approach that leverages GPT-4, a large-scale language model (LLM), to automatically reproduce Android bug reports. Unlike traditional methods, ReBL bypasses the use of Step to Reproduce (S2R) entities. Instead, it leverages the entire textual bug report and employs innovative prompts to enhance GPT’s contextual reasoning. This approach is more flexible and context-aware than the traditional step-by-step entity matching approach, resulting in improved accuracy and effectiveness. In addition to handling crash reports, ReBL has the capability of handling non-crash functional bug reports. Our evaluation of 96 Android bug reports (73 crash and 23 non-crash) demonstrates that ReBL successfully reproduced 90.63% of these reports, averaging only 74.98 seconds per bug report. Additionally, ReBL outperformed three existing tools in both success rate and speed.","2024","2025-11-25 22:29:37","2025-11-25 22:29:37","","1048–1060","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large Language Model; Prompt Engineering; Android; Automated Bug Reproduction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"METL2ZCH","conferencePaper","2024","Wen, Xin-Cheng; Gao, Cuiyun; Gao, Shuzheng; Xiao, Yang; Lyu, Michael R.","SCALE: Constructing Structured Natural Language Comment Trees for Software Vulnerability Detection","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3652124","https://doi.org/10.1145/3650212.3652124","Recently, there has been a growing interest in automatic software vulnerability detection. Pre-trained model-based approaches have demonstrated superior performance than other Deep Learning (DL)-based approaches in detecting vulnerabilities. However, the existing pre-trained model-based approaches generally employ code sequences as input during prediction, and may ignore vulnerability-related structural information, as reflected in the following two aspects. First, they tend to fail to infer the semantics of the code statements with complex logic such as those containing multiple operators and pointers. Second, they are hard to comprehend various code execution sequences, which is essential for precise vulnerability detection. To mitigate the challenges, we propose a Structured Natural Language Comment tree-based vulnerAbiLity dEtection framework based on the pre-trained models, named . The proposed Structured Natural Language Comment Tree (SCT) integrates the semantics of code statements with code execution sequences based on the Abstract Syntax Trees (ASTs).Specifically, comprises three main modules: (1) Comment Tree Construction, which aims at enhancing the model’s ability to infer the semantics of code statements by first incorporating Large Language Models (LLMs) for comment generation and then adding the comment node to ASTs. (2) Structured Natural Language Comment Tree Construction, which aims at explicitly involving code execution sequence by combining the code syntax templates with the comment tree. (3) SCT-Enhanced Representation, which finally incorporates the constructed SCTs for well capturing vulnerability patterns. Experimental results demonstrate that outperforms the best-performing baseline, including the pre-trained model and LLMs, with improvements of 2.96%, 13.47%, and 3.75% in terms of F1 score on the FFMPeg+Qemu, Reveal, and SVulD datasets, respectively. Furthermore, can be applied to different pre-trained models, such as CodeBERT and UniXcoder, yielding the F1 score performance enhancements ranging from 1.37% to 10.87%.","2024","2025-11-25 22:29:37","2025-11-25 22:29:37","","235–247","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large Language Model; Deep Learning; Vulnerability Detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8YYNRMNA","conferencePaper","2025","Mai, Yubo; Gao, Zhipeng; Hu, Xing; Bao, Lingfeng; Chen, Jingyuan; Sun, Jianling","Code2API: A Tool for Generating Reusable APIs from Stack Overflow Code Snippets","Proceedings of the 34th ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-1474-0","","10.1145/3713081.3731741","https://doi.org/10.1145/3713081.3731741","Nowadays, developers often turn to Stack Overflow for solutions to daily problems, however, these code snippets are partial code that cannot be tested and verified properly. One way to test these code snippets is to transform them into APIs (Application Program Interface) that developers can directly invoked and executed. However, it is often costly and error-prone for developers to manually perform this transformation (referred to as AIPzation task) due to different actions to be taken (e.g., summarizing proper method names, inferring input parameters list and return statements). To help developers quickly reuse code snippets in Stack Overflow, in this paper, we propose Code2API, a Google Chrome extension that uses Large Language Models (LLMs) to automatically perform APIzation of code snippets on Stack Overflow. Code2API guides LLMs through well-designed prompts to generate reusable APIs, using Chain-of-Thought reasoning and few-shot in-context learning to help LLMs understand and solve the APIzation task in a developer-like manner. The evaluation results show that Code2API significantly outperforms the rule-based approach by a large margin. The full paper of this tool has been published in FSE'24 as a research paper [11].Demo video: https://youtu.be/RI-ZpBnNNwQ.Demo website: https://doi.org/10.6084/m9.figshare.24426961.v1.Replication package: https://github.com/qq804020866/Code2API.","2025","2025-11-25 22:29:37","2025-11-25 22:29:37","","71–75","","","","","","","ISSTA Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","large language model; stack overflow; API; code snippets","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"43QJF5G6","conferencePaper","2024","Liu, Zhe; Chen, Chunyang; Wang, Junjie; Chen, Mengzhuo; Wu, Boyu; Che, Xing; Wang, Dandan; Wang, Qing","Make LLM a Testing Expert: Bringing Human-like Interaction to Mobile GUI Testing via Functionality-aware Decisions","Proceedings of the IEEE/ACM 46th International Conference on Software Engineering","979-8-4007-0217-4","","10.1145/3597503.3639180","https://doi.org/10.1145/3597503.3639180","Automated Graphical User Interface (GUI) testing plays a crucial role in ensuring app quality, especially as mobile applications have become an integral part of our daily lives. Despite the growing popularity of learning-based techniques in automated GUI testing due to their ability to generate human-like interactions, they still suffer from several limitations, such as low testing coverage, inadequate generalization capabilities, and heavy reliance on training data. Inspired by the success of Large Language Models (LLMs) like ChatGPT in natural language understanding and question answering, we formulate the mobile GUI testing problem as a Q&amp;A task. We propose GPTDroid, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process. Within this framework, we have also introduced a functionality-aware memory prompting mechanism that equips the LLM with the ability to retain testing knowledge of the whole process and conduct long-term, functionality-based reasoning to guide exploration. We evaluate it on 93 apps from Google Play and demonstrate that it outperforms the best baseline by 32% in activity coverage, and detects 31% more bugs at a faster rate. Moreover, GPTDroid identifies 53 new bugs on Google Play, of which 35 have been confirmed and fixed.","2024","2025-11-25 22:29:37","2025-11-25 22:29:37","","","","","","","","","ICSE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","large language model; automated GUI testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HMT9Z5A3","conferencePaper","2024","Go, Gwihwan; Zhou, Chijin; Zhang, Quan; Zou, Xiazijian; Shi, Heyuan; Jiang, Yu","Towards More Complete Constraints for Deep Learning Library Testing via Complementary Set Guided Refinement","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680364","https://doi.org/10.1145/3650212.3680364","Deep learning library is important in AI systems. Recently, many works have been proposed to ensure its reliability. They often model inputs of tensor operations as constraints to guide the generation of test cases. However, these constraints may narrow the search space, resulting in incomplete testing. This paper introduces a complementary set-guided refinement that can enhance the completeness of constraints. The basic idea is to see if the complementary set of constraints yields valid test cases. If so, the original constraint is incomplete and needs refinement. Based on this idea, we design an automatic constraint refinement tool, DeepConstr, which adopts a genetic algorithm to refine constraints for better completeness. We evaluated it on two DL libraries, PyTorch and TensorFlow. DeepConstr discovered 84 unknown bugs, out of which 72 were confirmed, with 51 fixed. Compared to state-of-the-art fuzzers, DeepConstr increased coverage for 43.44% of operators supported by NNSmith, and 59.16% of operators supported by NeuRI.","2024","2025-11-25 22:29:37","2025-11-25 22:29:37","","1338–1350","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large Language Model; Fuzzing; DL library","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N58WKFW3","conferencePaper","2024","Feng, Sidong; Lu, Haochuan; Jiang, Jianqin; Xiong, Ting; Huang, Likun; Liang, Yinglin; Li, Xiaoqin; Deng, Yuetang; Aleti, Aldeida","Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695260","https://doi.org/10.1145/3691620.3695260","UI automation tests play a crucial role in ensuring the quality of mobile applications. Despite the growing popularity of machine learning techniques to generate these tests, they still face several challenges, such as the mismatch of UI elements. The recent advances in Large Language Models (LLMs) have addressed these issues by leveraging their semantic understanding capabilities. However, a significant gap remains in applying these models to industrial-level app testing, particularly in terms of cost optimization and knowledge limitation. To address this, we introduce CAT to create cost-effective UI automation tests for industry apps by combining machine learning and LLMs with best practices. Given the task description, CAT employs Retrieval Augmented Generation (RAG) to source examples of industrial app usage as the few-shot learning context, assisting LLMs in generating the specific sequence of actions. CAT then employs machine learning techniques, with LLMs serving as a complementary optimizer, to map the target element on the UI screen. Our evaluations on the WeChat testing dataset demonstrate the CAT's performance and cost-effectiveness, achieving 90% UI automation with $0.34 cost, outperforming the state-of-the-art. We have also integrated our approach into the real-world WeChat testing platform, demonstrating its usefulness in detecting 141 bugs and enhancing the developers' testing process.","2024","2025-11-25 22:29:37","2025-11-25 22:47:16","","1973–1978","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","large language model; Optimization; Software testing; Software engineering; Automation; Testing; Computer bugs; cost optimization; retrieval-augmented generation; UI automation test; Mobile applications; Costs; Social networking (online); Message services","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CXRGQM4N","conferencePaper","2024","Phung, Tung; Pădurean, Victor-Alexandru; Singh, Anjali; Brooks, Christopher; Cambronero, José; Gulwani, Sumit; Singla, Adish; Soares, Gustavo","Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4 Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation","Proceedings of the 14th Learning Analytics and Knowledge Conference","979-8-4007-1618-8","","10.1145/3636555.3636846","https://doi.org/10.1145/3636555.3636846","Generative AI and large language models hold great promise in enhancing programming education by automatically generating individualized feedback for students. We investigate the role of generative AI models in providing human tutor-style programming hints to help students resolve errors in their buggy programs. Recent works have benchmarked state-of-the-art models for various feedback generation scenarios; however, their overall quality is still inferior to human tutors and not yet ready for real-world deployment. In this paper, we seek to push the limits of generative AI models toward providing high-quality programming hints and develop a novel technique, GPT4HINTS-GPT3.5VAL. As a first step, our technique leverages GPT-4 as a “tutor” model to generate hints – it boosts the generative quality by using symbolic information of failing test cases and fixes in prompts. As a next step, our technique leverages GPT-3.5, a weaker model, as a “student” model to further validate the hint quality – it performs an automatic quality validation by simulating the potential utility of providing this feedback. We show the efficacy of our technique via extensive evaluation using three real-world datasets of Python programs covering a variety of concepts ranging from basic algorithms to regular expressions and data analysis using pandas library.","2024","2025-11-25 22:29:37","2025-11-25 22:29:37","","12–23","","","","","","","LAK '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Kyoto, Japan","","","","ChatGPT; Generative AI; Feedback Generation; GPT4; Programming Education","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"99LZKDQY","journalArticle","2025","He, Zhuolun; Pu, Yuan; Wu, Haoyuan; Qiu, Tairu; Yu, Bei","Large Language Models for EDA: Future or Mirage?","ACM Trans. Des. Autom. Electron. Syst.","","1084-4309","10.1145/3736167","https://doi.org/10.1145/3736167","In this article, we explore the burgeoning intersection of large language models (LLMs) and electronic design automation (EDA). We critically assess whether LLMs represent a transformative future for EDA or merely a fleeting mirage. By organizing existing research into four critical domains of EDA—code generation, verification and debugging, knowledge representation and retrieval, and optimization/modeling—we provide a comprehensive overview of the current state-of-the-art. The survey concludes with a 5-level roadmap to guide the progressive integration and advancement of LLMs in EDA. Ultimately, this article aims to provide a comprehensive, evidence-based perspective on the role of LLMs in shaping the future of EDA.","2025-10","2025-11-25 22:29:37","2025-11-25 22:29:37","","","","6","30","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J7AB6SI2","conferencePaper","2024","Yin, Xin; Ni, Chao; Wang, Shaohua; Li, Zhenhao; Zeng, Limin; Yang, Xiaohu","ThinkRepair: Self-Directed Automated Program Repair","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680359","https://doi.org/10.1145/3650212.3680359","Though many approaches have been proposed for Automated Program Repair (APR) and indeed achieved remarkable performance, they still have limitations in fixing bugs that require analyzing and reasoning about the logic of the buggy program. Recently, large language models (LLMs) instructed by prompt engineering have attracted much attention for their powerful ability to address many kinds of tasks including bug-fixing. However, the quality of the prompt will highly affect the ability of LLMs and manually constructing high-quality prompts is a costly endeavor. To address this limitation, we propose a self-directed LLM-based automated program repair, ThinkRepair, with two main phases: collection phase and fixing phase. The former phase automatically collects various chains of thoughts that constitute pre-fixed knowledge by instructing LLMs with the Chain-of-Thought (CoT) prompt. The latter phase targets fixing a bug by first selecting examples for few-shot learning and second automatically interacting with LLMs, optionally appending with feedback of testing information. Evaluations on two widely studied datasets (Defects4J and QuixBugs) by comparing ThinkRepair with 12 SOTA APRs indicate the priority of ThinkRepair in fixing bugs. Notably, ThinkRepair fixes 98 bugs and improves baselines by 27%∼344.4% on Defects4J V1.2. On Defects4J V2.0, ThinkRepair fixes 12∼65 more bugs than the SOTA APRs. Additionally, ThinkRepair also makes a considerable improvement on QuixBugs (31 for Java and 21 for Python at most).","2024","2025-11-25 22:29:37","2025-11-25 22:29:37","","1274–1286","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large Language Model; Prompt Engineering; Automated Program Repair","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G73CY2V4","conferencePaper","2025","MacNeil, Stephen; Prather, James; Nabid, Rahad Arman; Gutierrez, Sebastian; Carvalho, Silas; Shrestha, Saimon; Denny, Paul; Reeves, Brent N.; Leinonen, Juho; Rossetti, Rachel Louise","Fostering Responsible AI Use Through Negative Expertise: A Contextualized Autocompletion Quiz","Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 1","979-8-4007-1567-9","","10.1145/3724363.3729067","https://doi.org/10.1145/3724363.3729067","Generative AI tools, like GitHub Copilot, are becoming an industry standard by offering real-time code suggestions that streamline the coding process. Although these systems improve productivity, they also introduce pedagogical challenges. Students may become overly reliant on AI-generated code suggestions, accepting them without critical thought, potentially reducing their ability to engage with the underlying logic of the code. We developed an interactive quiz system within a simulated IDE environment designed to help students think critically about autogenerated code suggestions. Instructors use the tool to create contextualized coding quizzes that present multiple code suggestions at each line. Students must choose the correct option to move on to the next step. Survey responses suggest that this approach could promote critical thinking and scaffold metacognitive skills like planning and reflection. Students reported that the system helped them distinguish between good and bad suggestions. Most students preferred this experience to traditional quizzes or Github Copilot. These findings show the potential to scaffold more critical use of generative AI coding tools.","2025","2025-11-25 22:29:37","2025-11-25 22:29:37","","326–332","","","","","","","ITiCSE 2025","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Nijmegen, Netherlands","","","","generative ai; copilot; auto-completion; negative expertise","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3FWJ7M67","conferencePaper","2024","Mohajer, Mohammad Mahdi; Aleithan, Reem; Harzevili, Nima Shiri; Wei, Moshi; Belle, Alvine Boaye; Pham, Hung Viet; Wang, Song","Effectiveness of ChatGPT for Static Analysis: How Far Are We?","Proceedings of the 1st ACM International Conference on AI-Powered Software","979-8-4007-0685-1","","10.1145/3664646.3664777","https://doi.org/10.1145/3664646.3664777","This paper conducted a novel study to explore the capabilities of ChatGPT, a state-of-the-art LLM, in static analysis tasks such as static bug detection and false positive warning removal. In our evaluation, we focused on two types of typical and critical bugs targeted by static bug detection, i.e., Null Dereference and Resource Leak, as our subjects. We employ Infer, a well-established static analyzer, to aid the gathering of these two bug types from 10 open-source projects. Consequently, our experiment dataset contains 222 instances of Null Dereference bugs and 46 instances of Resource Leak bugs. Our study demonstrates that ChatGPT can achieve remarkable performance in the mentioned static analysis tasks, including bug detection and false-positive warning removal. In static bug detection, ChatGPT achieves accuracy and precision values of up to 68.37% and 63.76% for detecting Null Dereference bugs and 76.95% and 82.73% for detecting Resource Leak bugs, improving the precision of the current leading bug detector, Infer by 12.86% and 43.13% respectively. For removing false-positive warnings, ChatGPT can reach a precision of up to 93.88% for Null Dereference bugs and 63.33% for Resource Leak bugs, surpassing existing state-of-the-art false-positive warning removal tools.","2024","2025-11-25 22:29:37","2025-11-25 22:29:37","","151–160","","","","","","","AIware 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","Large language models; Static analysis; ChatGPT","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TY2WBBRI","conferencePaper","2025","Kerslake, Chris; Denny, Paul; Smith, IV, David H.; Leinonen, Juho; MacNeil, Stephen; Luxton-Reilly, Andrew; Becker, Brett A.","Exploring Student Reactions to LLM-Generated Feedback on Explain in Plain English Problems","Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1","979-8-4007-0531-1","","10.1145/3641554.3701934","https://doi.org/10.1145/3641554.3701934","Code reading and comprehension skills are essential for novices learning programming, and explain-in-plain-English tasks (EiPE) are a well-established approach for assessing these skills. However, manual grading of EiPE tasks is time-consuming and this has limited their use in practice. To address this, we explore an approach where students explain code samples to a large language model (LLM) which generates code based on their explanations. This generated code is then evaluated using test suites, and shown to students along with the test results. We are interested in understanding how automated formative feedback from an LLM guides students' subsequent prompts towards solving EiPE tasks. We analyzed 177 unique attempts on four EiPE exercises from 21 students, looking at what kinds of mistakes they made and how they fixed them. We found that when students made mistakes, they identified and corrected them using either a combination of the LLM-generated code and test case results, or they switched from describing the purpose of the code to describing the sample code line-by-line until the LLM-generated code exactly matched the obfuscated sample code. Our findings suggest both optimism and caution with the use of LLMs for unmonitored formative feedback. We identified false positive and negative cases, helpful variable naming, and clues of direct code recitation by students. For most students, this approach represents an efficient way to demonstrate and assess their code comprehension skills. However, we also found evidence of misconceptions being reinforced, suggesting the need for further work to identify and guide students more effectively.","2025","2025-11-25 22:29:37","2025-11-25 22:29:37","","575–581","","","","","","","SIGCSETS 2025","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Pittsburgh, PA, USA","","","","large language models; llm; eipe; explain in plain english; formative feedback; misconceptions; qualitative analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X8E4GFAR","conferencePaper","2024","Jiang, Shengbei; Zhang, Jiabao; Chen, Wei; Wang, Bo; Zhou, Jianyi; Zhang, Jie","Evaluating Fault Localization and Program Repair Capabilities of Existing Closed-Source General-Purpose LLMs","Proceedings of the 1st International Workshop on Large Language Models for Code","979-8-4007-0579-3","","10.1145/3643795.3648390","https://doi.org/10.1145/3643795.3648390","Automated debugging is an emerging research field that aims to automatically find and repair bugs. In this field, Fault Localization (FL) and Automated Program Repair (APR) gain the most research efforts. Most recently, researchers have adopted pre-trained Large Language Models (LLMs) to facilitate FL and APR and their results are promising. However, the LLMs they used either vanished (such as Codex) or outdated (such as early versions of GPT). In this paper, we evaluate the performance of recent commercial closed-source general-purpose LLMs on FL and APR, i.e., ChatGPT 3.5, ERNIE Bot 3.5, and IFlytek Spark 2.0. We select three popular LLMs and evaluate them on 120 real-world Java bugs from the benchmark Defects4J. For FL and APR, we designed three kinds of prompts for each, considering different kinds of information. The results show that these LLMs could successfully locate 53.3% and correctly fix 12.5% of these bugs.","2024","2025-11-25 22:29:37","2025-11-25 22:47:18","","75–78","","","","","","","LLM4Code '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","large language model; Large language models; Software testing; Software; Software engineering; Chatbots; Large Language Model; Computer bugs; Location awareness; Debugging; Program Repair; Fault Localization; program repair; fault localization; software debugging; Maintenance engineering; Software Debugging; Sparks","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J4UTHBWB","journalArticle","2025","Yuan, Xiaohan; Li, Jinfeng; Wang, Dongxia; Chen, Yuefeng; Mao, Xiaofeng; Huang, Longtao; Chen, Jialuo; Xue, Hui; Liu, Xiaoxia; Wang, Wenhai; Ren, Kui; Wang, Jingyi","S-Eval: Towards Automated and Comprehensive Safety Evaluation for Large Language Models","Proc. ACM Softw. Eng.","","","10.1145/3728971","https://doi.org/10.1145/3728971","Generative large language models (LLMs) have revolutionized natural language processing with their transformative and emergent capabilities. However, recent evidence indicates that LLMs can produce harmful content that violates social norms, raising significant concerns regarding the safety and ethical ramifications of deploying these advanced models. Thus, it is both critical and imperative to perform a rigorous and comprehensive safety evaluation of LLMs before deployment. Despite this need, owing to the extensiveness of LLM generation space, it still lacks a unified and standardized risk taxonomy to systematically reflect the LLM content safety, as well as automated safety assessment techniques to explore the potential risks efficiently. To bridge the striking gap, we propose S-Eval, a novel LLM-based automated Safety Evaluation framework with a newly defined comprehensive risk taxonomy. S-Eval incorporates two key components, i.e., an expert testing LLM Mt and a novel safety critique LLM Mc. The expert testing LLM Mt is responsible for automatically generating test cases in accordance with the proposed risk management (including 8 risk dimensions and a total of 102 subdivided risks). The safety critique LLM Mc can provide quantitative and explainable safety evaluations for better risk awareness of LLMs. In contrast to prior works, S-Eval differs in significant ways: (i) efficient – we construct a multi-dimensional and open-ended benchmark comprising 220,000 test cases across 102 risks utilizing Mt and conduct safety evaluations for 21 influential LLMs via Mc on our benchmark. The entire process is fully automated and requires no human involvement. (ii) effective – extensive validations show S-Eval facilitates a more thorough assessment and better perception of potential LLM risks, and Mc not only accurately quantifies the risks of LLMs but also provides explainable and in-depth insights into their safety, surpassing comparable models such as LLaMA-Guard-2. (iii) adaptive – S-Eval can be flexibly configured and adapted to the rapid evolution of LLMs and accompanying new safety threats, test generation methods and safety critique methods thanks to the LLM-based architecture. We further study the impact of hyper-parameters and language environments on model safety, which may lead to promising directions for future research. S-Eval has been deployed in our industrial partner for the automated safety evaluation of multiple LLMs serving millions of users, demonstrating its effectiveness in real-world scenarios.","2025-06","2025-11-25 22:29:38","2025-11-25 22:29:38","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; Test Generation; Benchmark; Safety Evaluation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ILEWIJV3","conferencePaper","2024","Ran, Dezhi; Wang, Hao; Song, Zihe; Wu, Mengzhou; Cao, Yuan; Zhang, Ying; Yang, Wei; Xie, Tao","Guardian: A Runtime Framework for LLM-Based UI Exploration","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680334","https://doi.org/10.1145/3650212.3680334","Tests for feature-based UI testing have been indispensable for ensuring the quality of mobile applications (apps for short). The high manual labor costs to create such tests have led to a strong interest in automated feature-based UI testing, where an approach automatically explores the App under Test (AUT) to find correct sequences of UI events achieving the target test objective, given only a high-level test objective description. Given that the task of automated feature-based UI testing resembles conventional AI planning problems, large language models (LLMs), known for their effectiveness in AI planning, could be ideal for this task. However, our study reveals that LLMs struggle with following specific instructions for UI testing and replanning based on new information. This limitation results in reduced effectiveness of LLM-driven solutions for automated feature-based UI testing, despite the use of advanced prompting techniques. Toward addressing the preceding limitation, we propose Guardian, a runtime system framework to improve the effectiveness of automated feature-based UI testing by offloading computational tasks from LLMs with two major strategies. First, Guardian refines UI action space that the LLM can plan over, enforcing the instruction following of the LLM by construction. Second, Guardian deliberately checks whether the gradually enriched information invalidates previous planning by the LLM. Guardian removes the invalidated UI actions from the UI action space that the LLM can plan over, restores the state of the AUT to the state before the execution of the invalidated UI actions, and prompts the LLM to re-plan with the new UI action space. We instantiate Guardian with ChatGPT and construct a benchmark named FestiVal with 58 tasks from 23 highly popular apps. Evaluation results on FestiVal show that Guardian achieves 48.3","2024","2025-11-25 22:29:38","2025-11-25 22:29:38","","958–970","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large Language Models; Android Testing; Mobile Testing; Runtime System; Sequential Planning; UI Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U8UBSUTM","conferencePaper","2023","Kang, Sungmin; Yoon, Juyeon; Yoo, Shin","Large Language Models are Few-Shot Testers: Exploring LLM-Based General Bug Reproduction","Proceedings of the 45th International Conference on Software Engineering","978-1-6654-5701-9","","10.1109/ICSE48619.2023.00194","https://doi.org/10.1109/ICSE48619.2023.00194","Many automated test generation techniques have been developed to aid developers with writing tests. To facilitate full automation, most existing techniques aim to either increase coverage, or generate exploratory inputs. However, existing test generation techniques largely fall short of achieving more semantic objectives, such as generating tests to reproduce a given bug report. Reproducing bugs is nonetheless important, as our empirical study shows that the number of tests added in open source repositories due to issues was about 28% of the corresponding project test suite size. Meanwhile, due to the difficulties of transforming the expected program semantics in bug reports into test oracles, existing failure reproduction techniques tend to deal exclusively with program crashes, a small subset of all bug reports. To automate test generation from general bug reports, we propose LIBRO, a framework that uses Large Language Models (LLMs), which have been shown to be capable of performing code-related tasks. Since LLMs themselves cannot execute the target buggy code, we focus on post-processing steps that help us discern when LLMs are effective, and rank the produced tests according to their validity. Our evaluation of LIBRO shows that, on the widely studied Defects4J benchmark, LIBRO can generate failure reproducing test cases for 33% of all studied cases (251 out of 750), while suggesting a bug reproducing test in first place for 149 bugs. To mitigate data contamination (i.e., the possibility of the LLM simply remembering the test code either partially or in whole), we also evaluate LIBRO against 31 bug reports submitted after the collection of the LLM training data terminated: LIBRO produces bug reproducing tests for 32% of the studied bug reports. Overall, our results show LIBRO has the potential to significantly enhance developer efficiency by automatically generating tests from bug reports.","2023","2025-11-25 22:29:38","2025-11-25 22:47:29","","2312–2323","","","","","","","ICSE '23","","","","IEEE Press","Melbourne, Victoria, Australia","","","","","","","","","","","","Semantics; Test pattern generators; Writing; Codes; Computer bugs; Benchmark testing; test generation; natural language processing; software engineering; Training data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FAARUZJN","conferencePaper","2024","Sun, Zhensu; Du, Xiaoning; Yang, Zhou; Li, Li; Lo, David","AI Coders Are among Us: Rethinking Programming Language Grammar towards Efficient Code Generation","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680347","https://doi.org/10.1145/3650212.3680347","Artificial Intelligence (AI) models have emerged as another important audience for programming languages alongside humans and machines, as we enter the era of large language models (LLMs). LLMs can now perform well in coding competitions and even write programs like developers to solve various tasks, including mathematical problems. However, the grammar and layout of current programs are designed to cater the needs of human developers – with many grammar tokens and formatting tokens being used to make the code easier for humans to read. While this is helpful, such a design adds unnecessary computational work for LLMs, as each token they either use or produce consumes computational resources. To improve inference efficiency and reduce computational costs, we propose the concept of AI-oriented grammar.This aims to represent code in a way that better suits the working mechanism of AI models. Code written with AI-oriented grammar discards formats and uses a minimum number of tokens to convey code semantics effectively. To demonstrate the feasibility of this concept, we explore and implement the first AI-oriented grammar for Python, named Simple Python (SimPy). SimPy is crafted by revising the original Python grammar through a series of heuristic rules. Programs written in SimPy maintain identical Abstract Syntax Tree (AST) structures to those in standard Python. This allows for not only execution via a modified AST parser, but also seamless transformation between programs written in Python and SimPy, enabling human developers and LLMs to use Python and SimPy, respectively, when they need to collaborate. We also look into methods to help existing LLMs understand and use SimPy effectively. In the experiments, compared with Python, SimPy enables a reduction in token usage by 13.5% and 10.4% for CodeLlama and GPT-4, respectively, when completing the same set of code-related tasks. Additionally, these models can maintain or even improve their performance when using SimPy instead of Python for these tasks. With these promising results, we call for further contributions to the development of AI-oriented program grammar within our community.","2024","2025-11-25 22:29:38","2025-11-25 22:29:38","","1124–1136","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large Language Model; Code Generation; Programming Language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LIQFQPNG","conferencePaper","2025","Brachman, Michelle; Goldberg, Arielle; Anderson, Andrew; Houde, Stephanie; Muller, Michael; Weisz, Justin D.","Towards Personalized and Contextualized Code Explanations","Adjunct Proceedings of the 33rd ACM Conference on User Modeling, Adaptation and Personalization","979-8-4007-1399-6","","10.1145/3708319.3733681","https://doi.org/10.1145/3708319.3733681","Code understanding is a common and important use case for generative AI code assistance tools. Yet, a user’s background, context, and goals may impact the kinds of code explanations that best fit their needs. Our aim was to understand the kinds of configurations users might want for their code explanations and how those relate to their context. We ran an exploratory study with a medium-fidelity prototype and 10 programmers. Participants valued having configurations and desired automated personalization of code explanations. They found particular merit in being able to configure the structure and detail level in code explanations and felt that their needs might change depending on their prior experience and goals.","2025","2025-11-25 22:29:38","2025-11-25 22:29:38","","120–125","","","","","","","UMAP Adjunct '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Generative AI; Code Explanations; Personalization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RZ8RD8M5","journalArticle","2025","Wu, Weibin; Cao, Yuhang; Yi, Ning; Ou, Rongyi; Zheng, Zibin","Detecting and Reducing the Factual Hallucinations of Large Language Models with Metamorphic Testing","Proc. ACM Softw. Eng.","","","10.1145/3715784","https://doi.org/10.1145/3715784","Question answering (QA) is a fundamental task of large language models (LLMs), which requires LLMs to automatically answer human-posed questions in natural language. However, LLMs are known to distort facts and make non-factual statements (i.e., hallucinations) when dealing with QA tasks, which may affect the deployment of LLMs in real-life situations. In this work, we propose DrHall, a framework for detecting and reducing the factual hallucinations of black-box LLMs with metamorphic testing (MT). We believe that hallucinated answers are unstable. Therefore, when LLMs hallucinate, they are more likely to produce different answers if we use metamorphic testing to make LLMs re-execute the same task with different execution paths, which motivates the design of DrHall. The effectiveness of DrHall is evaluated empirically on three datasets, including a self-built dataset of natural language questions: FactHalluQA, as well as two datasets of programming questions: Refactory and LeetCode. The evaluation results confirm that DrHall can consistently outperform the state-of-the-art baselines, obtaining an average F1 score of over 0.856 for hallucination detection. For hallucination correction, DrHall can also outperform the state-of-the-art baselines, with an average hallucination correction rate of over 53%. We hope that our work can enhance the reliability of LLMs and provide new insights for the research of LLM hallucination mitigation.","2025-06","2025-11-25 22:29:38","2025-11-25 22:29:38","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Metamorphic Testing; Hallucination","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YNHM4L5V","conferencePaper","2024","Abedu, Samuel; Abdellatif, Ahmad; Shihab, Emad","LLM-Based Chatbots for Mining Software Repositories: Challenges and Opportunities","Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering","979-8-4007-1701-7","","10.1145/3661167.3661218","https://doi.org/10.1145/3661167.3661218","Software repositories have a plethora of information about software development, encompassing details such as code contributions, bug reports and code reviews. This rich source of data can be harnessed to enhance not only software quality and development velocity but also to gain insights into team collaboration and inform strategic decision-making throughout the software development lifecycle. Previous studies show that many stakeholders cannot benefit from the project information due to the technical knowledge and expertise required to extract the project data. To lower the barrier to entry by automating the process of extracting and analyzing repository data, we explored the potential of using an LLM to develop a chatbot for answering questions related to software repositories. We evaluated the chatbot on 150 software repository-related questions. We found that the chatbot correctly answered one question. This result prompted us to shift our focus to investigate the challenges in adopting LLMs for the out-of-the-box development of software repository chatbots. We identified five main challenges related to retrieving data, structuring the data, and generating the answer to the user’s query. Among these challenges, the most frequent (83.3%) is the inaccurate retrieval of data to answer questions. In this paper, we share our experience and challenges in developing an LLM-based chatbot to answer software repository-related questions within the SE community. We also provide recommendations on mitigating these challenges. Our findings will serve as a foundation to drive future research aimed at enhancing LLMs for adoption in extracting useful information from software repositories, fostering advancements in natural language understanding, data retrieval, and response generation within the context of software repository-related questions and analytics.","2024","2025-11-25 22:29:38","2025-11-25 22:29:38","","201–210","","","","","","","EASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Salerno, Italy","","","","Large Language Model; Conversational Development Assistant; Software Chatbots","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"86UGC5EY","journalArticle","2025","Straubinger, Philipp; Fulcini, Tommaso; Garaccione, Giacomo; Ardito, Luca; Fraser, Gordon","Gamifying Testing in IntelliJ: A Replicability Study","Proc. ACM Softw. Eng.","","","10.1145/3728983","https://doi.org/10.1145/3728983","Gamification is an emerging technique to enhance motivation and performance in traditionally unengaging tasks like software testing. Previous studies have indicated that gamified systems have the potential to improve software testing processes by providing testers with achievements and feedback. However, further evidence of these benefits across different environments, programming languages, and participant groups is required. This paper aims to replicate and validate the effects of IntelliGame, a gamification plugin for IntelliJ IDEA to engage developers in writing and executing tests. The objective is to generalize the benefits observed in earlier studies to new contexts, i.e., the TypeScript programming language and a larger participant pool. The replicability study consists of a controlled experiment with 174 participants, divided into two groups: one using IntelliGame and one with no gamification plugin. The study employed a two-group experimental design to compare testing behavior, coverage, mutation scores, and participant feedback between the groups. Data was collected through test metrics and participant surveys, and statistical analysis was performed to determine the statistical significance. Participants using IntelliGame showed higher engagement and productivity in testing practices than the control group, evidenced by the creation of more tests, increased frequency of executions, and enhanced utilization of testing tools. This ultimately led to better code implementations, highlighting the effectiveness of gamification in improving functional outcomes and motivating users in their testing endeavors. The replication study confirms that gamification, through IntelliGame, positively impacts software testing behavior and developer engagement in coding tasks. These findings suggest that integrating game elements into the testing environment can be an effective strategy to improve software testing practices.","2025-06","2025-11-25 22:29:38","2025-11-25 22:29:38","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Software Testing; Gamification; IDE; IntelliJ; Replicability Study","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FL2J894E","conferencePaper","2024","Zhang, Yuntong; Ruan, Haifeng; Fan, Zhiyu; Roychoudhury, Abhik","AutoCodeRover: Autonomous Program Improvement","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680384","https://doi.org/10.1145/3650212.3680384","Researchers have made significant progress in automating the software development process in the past decades. Automated techniques for issue summarization, bug reproduction, fault localization, and program repair have been built to ease the workload of developers. Recent progress in Large Language Models (LLMs) has significantly impacted the development process, where developers can use LLM-based programming assistants to achieve automated coding. Nevertheless, software engineering involves the process of program improvement apart from coding, specifically to enable software maintenance (e.g. program repair to fix bugs) and software evolution (e.g. feature additions). In this paper, we propose an automated approach for solving Github issues to autonomously achieve program improvement. In our approach called AutoCodeRover, LLMs are combined with sophisticated code search capabilities, ultimately leading to a program modification or patch. In contrast to recent LLM agent approaches from AI researchers and practitioners, our outlook is more software engineering oriented. We work on a program representation (abstract syntax tree) as opposed to viewing a software project as a mere collection of files. Our code search exploits the program structure in the form of classes/methods to enhance LLM’s understanding of the issue’s root cause, and effectively retrieve a context via iterative search. The use of spectrum-based fault localization using tests, further sharpens the context, as long as a test-suite is available. Experiments on the recently proposed SWE-bench-lite (300 real-life Github issues) show increased efficacy in solving Github issues (19% on SWE-bench-lite), which is higher than the efficacy of the recently reported Swe-agent. Interestingly, our approach resolved 57 GitHub issues in about 4 minutes each (pass@1), whereas developers spent more than 2.68 days on average. In addition, AutoCodeRover achieved this efficacy with significantly lower cost (on average, $0.43 USD), compared to other baselines. We posit that our workflow enables autonomous software engineering, where, in future, auto-generated code from LLMs can be autonomously improved.","2024","2025-11-25 22:29:38","2025-11-25 22:29:38","","1592–1604","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","large language model; automatic program repair; autonomous software engineering; autonomous software improvement","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LDH8KGG4","conferencePaper","2025","Blancaflor, Eric B.; Abaleta, Raphael M.; Achacoso, Luke Martin D.L.; Amper, Alden Christian C.; Ampiloquio, Pfrancis Isaiah R.","Emerging Threat: The Use of AI Voice Cloning Software and Services to Deceive Victims Through Phone Conversations and its Potential Effects on the Filipino Population","Proceeding of the 2024 5th Asia Service Sciences and Software Engineering Conference","979-8-4007-1754-3","","10.1145/3702138.3702145","https://doi.org/10.1145/3702138.3702145","Generative Artificial Intelligence (AI) tools have become increasingly advanced and accessible via the Internet. These advancements and upgraded accessibility of access have resulted in the usage of generative AI in phishing, which has increased the incidences&nbsp;of these attacks. The researchers explore AI voice phishing, or vishing, and its possible implications on the Filipino community by analyzing and reviewing existing literature on AI voice cloning and its application in vishing schemes. The review covers the definition of vishing and AI voice cloning, the methods malicious actors use to clone voices, the Philippines' cybersecurity posture and its current laws on AI vishing, real-life examples of AI vishing, how to protect against it, and the future of AI vishing, as well as the future direction of the study. The researchers ended the study by demanding and advocating additional research on AI detection and recognition, as well as the establishment and stronger implementation of developing legislation in the Philippines and other nations that prohibit the use of generative AI for illegal purposes.","2025","2025-11-25 22:29:38","2025-11-25 22:29:38","","137–146","","","","","","","ASSE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Generative AI; Artificial Intelligence; Cybersecurity; AI Vishing; AI Voice Cloning; Philippines; Voice Phishing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZDBKPEZT","conferencePaper","2024","Esposito, Matteo; Palagiano, Francesco; Lenarduzzi, Valentina; Taibi, Davide","Beyond Words: On Large Language Models Actionability in Mission-Critical Risk Analysis","Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement","979-8-4007-1047-6","","10.1145/3674805.3695401","https://doi.org/10.1145/3674805.3695401","Context. Risk analysis assesses potential risks in specific scenarios. Risk analysis principles are context-less; the same methodology can be applied to a risk connected to health and information technology security. Risk analysis requires a vast knowledge of national and international regulations and standards and is time and effort-intensive. A large language model can quickly summarize information in less time than a human and can be fine-tuned to specific tasks. Aim. Our empirical study aims to investigate the effectiveness of Retrieval-Augmented Generation and fine-tuned LLM in Risk analysis. To our knowledge, no prior study has explored its capabilities in risk analysis. Method. We manually curated 193 unique scenarios leading to 1283 representative samples from over 50 mission-critical analyses archived by the industrial context team in the last five years. We compared the base GPT-3.5 and GPT-4 models versus their Retrieval-Augmented Generation and fine-tuned counterparts. We employ two human experts as competitors of the models and three other three human experts to review the models and the former human expert’s analysis. The reviewers analyzed 5,000 scenario analyses. Results and Conclusions. HEs demonstrated higher accuracy, but LLMs are quicker and more actionable. Moreover, our findings show that RAG-assisted LLMs have the lowest hallucination rates, effectively uncovering hidden risks and complementing human expertise. Thus, the choice of model depends on specific needs, with FTMs for accuracy, RAG for hidden risks discovery, and base models for comprehensiveness and actionability. Therefore, experts can leverage LLMs for an effective complementing companion in risk analysis within a condensed timeframe. They can also save costs by averting unnecessary expenses associated with implementing unwarranted countermeasures.","2024","2025-11-25 22:29:38","2025-11-25 22:29:38","","517–527","","","","","","","ESEM '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Barcelona, Spain","","","","Generative AI; Large Language Model; Explainability; Actionability; Analysis; Fine-Tuning; Human Experts; Management; Retrieval Augmented Generation; Risk; Security; Standards; XAI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JKM9FMCM","conferencePaper","2025","Wang, Huanting; Jacob, Dejice; Kelly, David; Elkhatib, Yehia; Singer, Jeremy; Wang, Zheng","SecureMind: A Framework for Benchmarking Large Language Models in Memory Bug Detection and Repair","Proceedings of the 2025 ACM SIGPLAN International Symposium on Memory Management","979-8-4007-1610-2","","10.1145/3735950.3735954","https://doi.org/10.1145/3735950.3735954","Large language models (LLMs) hold great promise for automating software vulnerability detection and repair, but ensuring their correctness remains a challenge. While recent work has developed benchmarks for evaluating LLMs in bug detection and repair, existing studies rely on hand-crafted datasets that quickly become outdated. Moreover, systematic evaluation of advanced reasoning-based LLMs using chain-of-thought prompting for software security is lacking. We introduce SecureMind, an open-source framework for evaluating LLMs in vulnerability detection and repair, focusing on memory-related vulnerabilities. SecureMind provides a user-friendly Python interface for defining test plans, which automates data retrieval, preparation, and benchmarking across a wide range of metrics. Using SecureMind, we assess 10 representative LLMs, including 7 state-of-the-art reasoning models, on 16K test samples spanning 8 Common Weakness Enumeration (CWE) types related to memory safety violations. Our findings highlight the strengths and limitations of current LLMs in handling memory-related vulnerabilities.","2025","2025-11-25 22:29:38","2025-11-25 22:29:38","","27–40","","","","","","","ISMM '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Seoul, Republic of Korea","","","","Large language models; Bug repair; Software bug detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NNCUB4LB","conferencePaper","2024","Kumar, Atul; Saha, Diptikalyan; Yasue, Toshiaki; Ono, Kohichi; Krishnan, Saravanan; Hans, Sandeep; Satoh, Fumiko; Mitchell, Gerald; Kumar, Sachin","Automated Validation of COBOL to Java Transformation","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695365","https://doi.org/10.1145/3691620.3695365","Recent advances in Large Language Model (LLM) based Generative AI techniques have made it feasible to translate enterpriselevel code from legacy languages such as COBOL to modern languages such as Java or Python. While the results of LLM-based automatic transformation are encouraging, the resulting code cannot be trusted to correctly translate the original code. We propose a framework and a tool to help validate the equivalence of COBOL and translated Java. The results can also help repair the code if there are some issues and provide feedback to the AI model to improve. We have developed a symbolic-execution-based test generation to automatically generate unit tests for the source COBOL programs which also mocks the external resource calls. We generate equivalent JUnit test cases with equivalent mocking as COBOL and run them to check semantic equivalence between original and translated programs. Demo Video: https://youtu.be/aqF_agNP-lU","2024","2025-11-25 22:29:38","2025-11-25 22:46:50","","2415–2418","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","Semantics; Software testing; Test pattern generators; Java; Software; Software engineering; Codes; automatic validation; COBOL to Java; external resource testing; Python; Automatic Validation; External Resource Testing; Mainframes; Structured Query Language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VHBKAYGC","conferencePaper","2025","Yeh, Thomas Y.; Tran, Karena; Gao, Ge; Yu, Tyler; Fong, Wai On; Chen, Tzu-Yi","Bridging Novice Programmers and LLMs with Interactivity","Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1","979-8-4007-0531-1","","10.1145/3641554.3701867","https://doi.org/10.1145/3641554.3701867","While Large Language Models (LLMs) enable experienced programmers to increase their productivity, LLMs' impact on learning and productivity for novices is currently unclear. Recent work showed novice programmers struggle with prompting LLMs for code generation and suggested that the use of LLMs in CS education could exacerbate existing equity issues. Educators are now faced with the difficult question of whether and when to incorporate the use of LLMs into the CS curriculum without adversely impacting student learning and equity. To address these concerns, we study the effects of using an interactive LLM on code generation with novice programmers. We find that using our interactive LLM improves the accuracy of code generation over the baseline LLM. Additionally, after using the interactive LLM, novices write improved prompts even when using the baseline LLM. Based on our findings, we plan to create iGPTs, a set of customized, interactive LLMs spanning CS education learning goals as templates to facilitate LLM integration for improving student learning and retention.","2025","2025-11-25 22:29:38","2025-11-25 22:29:38","","1295–1301","","","","","","","SIGCSETS 2025","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Pittsburgh, PA, USA","","","","generative ai; cs1; llms; novice programmers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UQLDFJKI","conferencePaper","2025","Yu, Junji; Shu, Honglin; Fu, Michael; Wang, Dong; Tantithamthavorn, Chakkrit; Kamei, Yasutaka; Chen, Junjie","A Preliminary Study of Large Language Models for Multilingual Vulnerability Detection","Proceedings of the 34th ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-1474-0","","10.1145/3713081.3731746","https://doi.org/10.1145/3713081.3731746","Deep learning-based approaches, particularly those leveraging pre-trained language models (PLMs), have shown promise in automated software vulnerability detection. However, existing methods are predominantly limited to specific programming languages, restricting their applicability in multilingual settings. Recent advancements in large language models (LLMs) offer language-agnostic capabilities and enhanced semantic understanding, presenting a potential solution to this limitation. While existing studies have explored LLMs for vulnerability detection, their detection performance remains unknown for multilingual vulnerabilities. To address this gap, we conducted a preliminary study to evaluate the effectiveness of PLMs and state-of-the-art LLMs across seven popular programming languages. Our findings reveal that the PLM CodeT5P achieves the best performance in multilingual vulnerability detection, particularly in identifying the most critical vulnerabilities. Based on these results, we further discuss the potential of LLMs in advancing real-world multilingual vulnerability detection. This work represents an initial step toward exploring PLMs and LLMs for cross-language vulnerability detection, offering key insights for future research and practical deployment.","2025","2025-11-25 22:29:38","2025-11-25 22:29:38","","161–168","","","","","","","ISSTA Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","large language model; multilingual vulnerability; vulnerability detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TYEMY7NT","conferencePaper","2024","Yan, Dapeng; Gao, Zhipeng; Liu, Zhiming","A Closer Look at Different Difficulty Levels Code Generation Abilities of ChatGPT","Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering","979-8-3503-2996-4","","10.1109/ASE56229.2023.00096","https://doi.org/10.1109/ASE56229.2023.00096","","2024","2025-11-25 22:29:38","2025-11-25 23:11:13","","1887–1898","","","","","","","ASE '23","","","","IEEE Press","Echternach, Luxembourg","","","","","","","","","","","","large language model; code generation; Chat-GPT; clean code; program competition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P3L6DZG6","conferencePaper","2024","Xu, Junjielong; Cui, Ziang; Zhao, Yuan; Zhang, Xu; He, Shilin; He, Pinjia; Li, Liqun; Kang, Yu; Lin, Qingwei; Dang, Yingnong; Rajmohan, Saravan; Zhang, Dongmei","UniLog: Automatic Logging via LLM and In-Context Learning","Proceedings of the IEEE/ACM 46th International Conference on Software Engineering","979-8-4007-0217-4","","10.1145/3597503.3623326","https://doi.org/10.1145/3597503.3623326","Logging, which aims to determine the position of logging statements, the verbosity levels, and the log messages, is a crucial process for software reliability enhancement. In recent years, numerous automatic logging tools have been designed to assist developers in one of the logging tasks (e.g., providing suggestions on whether to log in try-catch blocks). These tools are useful in certain situations yet cannot provide a comprehensive logging solution in general. Moreover, although recent research has started to explore end-to-end logging, it is still largely constrained by the high cost of fine-tuning, hindering its practical usefulness in software development. To address these problems, this paper proposes UniLog, an automatic logging framework based on the in-context learning (ICL) paradigm of large language models (LLMs). Specifically, UniLog can generate an appropriate logging statement with only a prompt containing five demonstration examples without any model tuning. In addition, UniLog can further enhance its logging ability after warmup with only a few hundred random samples. We evaluated UniLog on a large dataset containing 12,012 code snippets extracted from 1,465 GitHub repositories. The results show that UniLog achieved the state-of-the-art performance in automatic logging: (1) 76.9% accuracy in selecting logging positions, (2) 72.3% accuracy in predicting verbosity levels, and (3) 27.1 BLEU-4 score in generating log messages. Meanwhile, UniLog requires less than 4% of the parameter tuning time needed by fine-tuning the same LLM.","2024","2025-11-25 22:29:38","2025-11-25 22:29:38","","","","","","","","","ICSE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","large language model; in-context learning; logging","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MWHDGLAV","conferencePaper","2025","Choudhuri, Rudrajit; Trinkenreich, Bianca; Pandita, Rahul; Kalliamvakou, Eirini; Steinmacher, Igor; Gerosa, Marco; Sanchez, Christopher; Sarma, Anita","What Guides Our Choices? Modeling Developers' Trust and Behavioral Intentions Towards GenAI","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00087","https://doi.org/10.1109/ICSE55347.2025.00087","Generative AI (genAI) tools, such as ChatGPT or Copilot, are advertised to improve developer productivity and are being integrated into software development. However, misaligned trust, skepticism, and usability concerns can impede the adoption of such tools. Research also indicates that AI can be exclusionary, failing to support diverse users adequately. One such aspect of diversity is cognitive diversity—variations in users' cognitive styles—that leads to divergence in perspectives and interaction styles. When an individual's cognitive style is unsupported, it creates barriers to technology adoption. Therefore, to understand how to effectively integrate genAI tools into software development, it is first important to model what factors affect developers' trust and intentions to adopt genAI tools in practice?We developed a theoretically grounded statistical model to (1) identify factors that influence developers' trust in genAI tools and (2) examine the relationship between developers' trust, cognitive styles, and their intentions to use these tools in their work. We surveyed software developers (N=238) at two major global tech organizations: GitHub Inc. and Microsoft; and employed Partial Least Squares-Structural Equation Modeling (PLS-SEM) to evaluate our model. Our findings reveal that genAI's system/output quality, functional value, and goal maintenance significantly influence developers' trust in these tools. Furthermore, developers' trust and cognitive styles influence their intentions to use these tools in their work. We offer practical suggestions for designing genAI tools for effective use and inclusive user experience.","2025","2025-11-25 22:29:38","2025-11-25 22:29:38","","1691–1703","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","LLM; generative AI; software engineering; behavioral intentions; cognitive styles; PLS-SEM; trust","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G2IQIX9P","conferencePaper","2025","Wang, Wanying; Ma, Zeyu; Zheng, Han; Tan, Xin; Chen, Mingang","Self-Aware Safety Augmentation: Leveraging Internal Semantic Understanding to Enhance Safety in Vision-Language Models","Proceedings of the 33rd ACM International Conference on Multimedia","979-8-4007-2035-2","","10.1145/3746027.3754574","https://doi.org/10.1145/3746027.3754574","Large vision-language models (LVLMs) are vulnerable to harmful input compared to their language-only backbones. We investigated this vulnerability by exploring LVLMs internal dynamics, framing their inherent safety understanding in terms of three key capabilities. Specifically, we define these capabilities as safety perception, semantic understanding, and alignment for linguistic expression, and experimentally pinpointed their primary locations within the model architecture. The results indicate that safety perception often emerges before comprehensive semantic understanding, leading to the reduction in safety. Motivated by these findings, we propose Self-Aware Safety Augmentation (SASA), a technique that projects informative semantic representations from intermediate layers onto earlier safety-oriented layers. This approach leverages the model's inherent semantic understanding to enhance safety recognition without fine-tuning. Then, we employ linear probing to articulate the model's internal semantic comprehension to detect the risk before the generation process. Extensive experiments on various datasets and tasks demonstrate that SASA significantly improves the safety of LVLMs, with minimal impact on the utility.","2025","2025-11-25 22:29:38","2025-11-25 22:29:38","","11199–11208","","","","","","","MM '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Dublin, Ireland","","","","large vision-language models; representation projection; safety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N6FQUGZ7","conferencePaper","2025","Bouzenia, Islem; Devanbu, Premkumar; Pradel, Michael","RepairAgent: An Autonomous, LLM-Based Agent for Program Repair","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00157","https://doi.org/10.1109/ICSE55347.2025.00157","Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. RepairAgent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable RepairAgent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270k tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.","2025","2025-11-25 22:29:38","2025-11-25 22:29:38","","2188–2200","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IXHN3DQN","conferencePaper","2023","Liu, Kaibo; Han, Yudong; Zhang, Jie M.; Chen, Zhenpeng; Sarro, Federica; Harman, Mark; Huang, Gang; Ma, Yun","Who Judges the Judge: An Empirical Study on Online Judge Tests","Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0221-1","","10.1145/3597926.3598060","https://doi.org/10.1145/3597926.3598060","Online Judge platforms play a pivotal role in education, competitive programming, recruitment, career training, and large language model training. They rely on predefined test suites to judge the correctness of submitted solutions. It is therefore important that the solution judgement is reliable and free from potentially misleading false positives (i.e., incorrect solutions that are judged as correct). In this paper, we conduct an empirical study of 939 coding problems with 541,552 solutions, all of which are judged to be correct according to the test suites used by the platform, finding that 43.4% of the problems include false positive solutions (3,440 bugs are revealed in total). We also find that test suites are, nevertheless, of high quality according to widely-studied test effectiveness measurements: 88.2% of false positives have perfect (100%) line coverage, 78.9% have perfect branch coverage, and 32.5% have a perfect mutation score. Our findings indicate that more work is required to weed out false positive solutions and to further improve test suite effectiveness. We have released the detected false positive solutions and the generated test inputs to facilitate future research.","2023","2025-11-25 22:29:38","2025-11-25 22:29:38","","334–346","","","","","","","ISSTA 2023","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Seattle, WA, USA","","","","software testing; Online judge platform; test assessment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BQ6KIICM","conferencePaper","2024","Morales, Sergio; Clarisó, Robert; Cabot, Jordi","A DSL for Testing LLMs for Fairness and Bias","Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems","979-8-4007-0504-5","","10.1145/3640310.3674093","https://doi.org/10.1145/3640310.3674093","Large language models (LLMs) are increasingly integrated into software systems to enhance them with generative AI capabilities. But LLMs may reflect a biased behavior, resulting in systems that could discriminate against gender, age or ethnicity, among other ethical concerns. Society and upcoming regulations will force companies and development teams to ensure their AI-enhanced software is ethically fair. To facilitate such ethical assessment, we propose LangBiTe, a model-driven solution to specify ethical requirements, and customize and automate the testing of ethical biases in LLMs. The evaluation can raise awareness on the biases of the LLM-based components of the system and/or trigger a change in the LLM of choice based on the requirements of that particular application. The model-driven approach makes both the requirements specification and the test generation platform-independent, and provides end-to-end traceability between the requirements and their assessment. We have implemented an open-source tool set, available on GitHub, to support the application of our approach.","2024","2025-11-25 22:29:38","2025-11-25 22:29:38","","203–213","","","","","","","MODELS '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Linz, Austria","","","","Testing; Large Language Models; Bias; Domain-Specific Language; Ethics; Model-Driven Engineering; Red Teaming","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IPC5ZG95","conferencePaper","2025","Qin, Hanbin","To Mock or Not to Mock: Divergence in Mocking Practices between LLM and Developers","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings","979-8-3315-3683-1","","10.1109/ICSE-Companion66252.2025.00077","https://doi.org/10.1109/ICSE-Companion66252.2025.00077","Mock objects are essential for isolating unit tests and reducing dependencies in software testing. However, deciding what to mock requires careful judgment to balance isolation and maintainability. This study evaluates OpenAI's GPT-4o for automating mock decisions by comparing its outputs with developer choices. The findings reveal that while the LLM excels in identifying dependencies, their broader isolation strategy often results in Over-mocking compared to the developers. These insights suggest the potential for LLM-based tools to generate test cases with accurate and well-balanced mocking strategies.","2025","2025-11-25 22:29:38","2025-11-25 22:48:05","","239–240","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","software testing; Software testing; Java; Software engineering; Accuracy; Codes; large language models; test generation; mock testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AF8UM9PT","conferencePaper","2024","Lu, You; Tian, Yifan; Bi, Yuyang; Chen, Bihuan; Peng, Xin","DiaVio: LLM-Empowered Diagnosis of Safety Violations in ADS Simulation Testing","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3652135","https://doi.org/10.1145/3650212.3652135","Simulation testing has been widely adopted by leading companies to ensure the safety of autonomous driving systems (ADSs). Anumber of scenario-based testing approaches have been developed to generate diverse driving scenarios for simulation testing, and demonstrated to be capable of finding safety violations. However, there is no automated way to diagnose whether these violations are caused by the ADS under test and which category these violations belong to. As a result, great effort is required to manually diagnose violations. To bridge this gap, we propose DiaVio to automatically diagnose safety violations in simulation testing by leveraging large language models (LLMs). It is built on top of a new domain specific language (DSL) of crash to align real-world accident reports described in natural language and violation scenarios in simulation testing. DiaVio fine-tunes a base LLM with real-world accident reports to learn diagnosis capability, and uses the fine-tuned LLM to diagnose violation scenarios in simulation testing. Our evaluation has demonstrated the effectiveness and efficiency of DiaVio in violation diagnosis.","2024","2025-11-25 22:29:38","2025-11-25 22:29:38","","376–388","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large Language Models; Automated Driving System; Scenario-based Testing; Violation Diagnosis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KZXT2425","bookSection","2025","Gudipati, Satya Karteek","Chatbot Evaluation Frameworks: From BLEU and F1 to Multi-Dimensional Real-World Benchmarks","Proceedings of the 2025 International Conference on Management Science and Computer Engineering","979-8-4007-1596-9","","","https://doi.org/10.1145/3760023.3760060","In the Natural Language Processing (NLP), BLEU and F1 have served as standard evaluation metrics for a long time and their ability to evaluate modern chatbot systems is becoming less reliable. These metrics are primarily designed to evaluate machine translation and classification of tasks, but they fail to capture the evolving intent alignment, conversational complexity, emotional nuance, and dynamic user interaction patterns that are commonly found in present day's generative AI-powered chatbot systems. Through this paper, while we are critically examining the shortcomings of single-metric evaluations, we also introduce a robust and comprehensive, multi-dimensional benchmarking framework that perfectly suits the contemporary chatbot architectures. Bridging theory and deployment together, our evaluation model incorporates findings from academic research, industry case studies, and hands-on experiments to assess coherence, contextual understanding, goal accuracy, safety compliance, and user satisfaction. By integrating both automatic and human-centric metrics, we subsequently outline a modular evaluation suite. This framework is applied across enterprise-level and open-domain chatbots, and demonstrated improved diagnostic capabilities over traditional methods. By reconciling theory and practice, this paper proposes a more robust and actionable standard for chatbot performance evaluation across diverse domains and use cases.","2025","2025-11-25 22:29:38","2025-11-25 22:29:38","","228–235","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DMADYLSD","conferencePaper","2024","Gao, Xiangshan; Chen, Jialuo; Wang, Jingyi; Shi, Jie; Cheng, Peng; Chen, Jiming","TeDA: A Testing Framework for Data Usage Auditing in Deep Learning Model Development","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680375","https://doi.org/10.1145/3650212.3680375","It is notoriously challenging to audit the potential unauthorized data usage in deep learning (DL) model development lifecycle, i.e., to judge whether certain private user data has been used to train or fine-tune a DL model without authorization. Yet, such data usage auditing is crucial to respond to the urgent requirements of trustworthy Artificial Intelligence (AI) such as data transparency, which are promoted and enforced in recent AI regulation rules or acts like General Data Protection Regulation (GDPR) and EU AI Act. In this work, we propose TeDA, a simple and flexible testing framework for auditing data usage in DL model development process. Given a set of user’s private data to protect (Dp), the intuition of TeDA is to apply membership inference (with good intention) for judging whether the model to audit (Ma) is likely to be trained with Dp. Notably, to significantly expose the usage under membership inference, TeDA applies imperceptible perturbation directed by boundary search to generate a carefully crafted test suite Dt (which we call ‘isotope’) based on Dp. With the test suite, TeDA then adopts membership inference combined with hypothesis testing to decide whether a user’s private data has been used to train Ma with statistical guarantee. We evaluated TeDA through extensive experiments on ranging data volumes across various model architectures for data-sensitive face recognition and medical diagnosis tasks. TeDA demonstrates high feasibility, effectiveness and robustness under various adaptive strategies (e.g., pruning and distillation).","2024","2025-11-25 22:29:39","2025-11-25 22:29:39","","1479–1490","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Test Case; Data Usage Auditing; Isotope; Membership Inference","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5CH8RWME","journalArticle","2025","Feng, Siyuan; Liu, Jiawei; Lai, Ruihang; Ruan, Charlie; Yu, Yong; Zhang, Lingming; Chen, Tianqi","Productively Deploying Emerging Models on Emerging Platforms: A Top-Down Approach for Testing and Debugging","Proc. ACM Softw. Eng.","","","10.1145/3728957","https://doi.org/10.1145/3728957","While existing machine learning (ML) frameworks focus on established platforms, like running CUDA on server-grade GPUs, there have been growing demands to enable emerging AI applications in a broader set of scenarios, such as running Large Language Models (LLMs) within browsers and mobile phones. However, deploying emerging models on new platforms (such as Metal and WebGPU) presents significant software engineering challenges due to rapid model evolution and limited tooling and practices for these platforms. Previous practice for ML model deployment often follows a bottom-up fashion, where engineers first implement individual required operators and then put them together. However, this traditional development approach fails to meet the productivity requirements when deploying emerging ML applications, with the testing and debugging part as a bottleneck. To this end, we introduce TapML, a top-down approach designed to streamline model deployment on diverse platforms. While the traditional bottom-up approach requires crafting manual tests, TapML automatically creates high-quality, realistic test data through operator-wise test carving. Furthermore, TapML uses a migration-based strategy to gradually offload model implementation from the mature source platform to the target platform, minimizing the debugging scope of compound errors. TapML has been used as the default development method in the MLC-LLM project to deploy emerging ML models. Within 2 years, TapML has accelerated the deployment of 105 emerging models in 27 model architectures across 5 emerging platforms. We show that TapML effectively boosts developer productivity while ensuring the quality of deployed models. Furthermore, we summarize comprehensive case studies from our real-world development, offering best practices for developing emerging ML systems.","2025-06","2025-11-25 22:29:39","2025-11-25 22:29:39","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Software Testing; Developer Productivity; Machine Learning Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D7KGMHW8","journalArticle","2025","Xu, Hanxiang; Wang, Shenao; Li, Ningke; Wang, Kailong; Zhao, Yanjie; Chen, Kai; Yu, Ting; Liu, Yang; Wang, Haoyu","Large Language Models for Cyber Security: A Systematic Literature Review","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3769676","https://doi.org/10.1145/3769676","The rapid advancement of Large Language Models (LLMs) has opened up new opportunities for leveraging artificial intelligence in a variety of application domains, including cybersecurity. As the volume and sophistication of cyber threats continue to grow, there is an increasing need for intelligent systems that can automatically detect vulnerabilities, analyze malware, and respond to attacks. In this survey, we conduct a comprehensive review of the literature on the application of LLMs in cybersecurity&nbsp;(LLM4Security). By comprehensively collecting over 40K relevant papers and systematically analyzing 185 papers from top security and software engineering venues, we aim to provide a holistic view of how LLMs are being used to solve diverse problems across the cybersecurity domain.Through our analysis, we identify several key findings. First, we observe that LLMs are being applied to an expanding range of cybersecurity tasks, including vulnerability detection, malware analysis, and network intrusion detection. Second, we analyze application trends of different LLM architectures (such as encoder-only, encoder-decoder, and decoder-only) across security domains. Third, we identify increasingly sophisticated techniques for adapting LLMs to cybersecurity, such as advanced fine-tuning, prompt engineering, and external augmentation strategies. A significant emerging trend is the use of LLM-based autonomous agents, which represent a paradigm shift from single-task execution to orchestrating complex, multi-step security workflows. Furthermore, we find that the datasets used for training and evaluating LLMs are often limited, highlighting the need for more comprehensive datasets and the use of LLMs for data augmentation. Finally, we discuss the main challenges and opportunities for future research, including the need for more interpretable models, addressing the inherent security risks of LLMs, and their potential for proactive defense.Overall, our survey provides a comprehensive overview of the current state-of-the-art in LLM4Security and identifies several promising directions for future research. We believe that the insights and findings presented in this survey will contribute to the growing body of knowledge on the application of LLMs in cybersecurity and provide valuable guidance for researchers and practitioners working in this field.","2025-09","2025-11-25 22:29:39","2025-11-25 22:29:39","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large language model; Cybersecurity; Software security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I6ZMVJVK","conferencePaper","2025","Siam, Md Kamrul; Gu, Huanying; Cheng, Jerry Q.","Programming with AI: Evaluating ChatGPT, Gemini, AlphaCode, and GitHub Copilot for Programmers","Proceedings of the 3rd International Conference on Computing Advancements","979-8-4007-1382-8","","10.1145/3723178.3723224","https://doi.org/10.1145/3723178.3723224","Our everyday lives now heavily rely on artificial intelligence (AI) powered large language models (LLMs). Like regular users, programmers are also benefiting from the newest large language models. In response to the critical role that AI models play in modern software development, this study presents a thorough evaluation of leading programming assistants, including ChatGPT, Gemini (Bard AI), AlphaCode, and GitHub Copilot. The evaluation is based on tasks like natural language processing and code generation accuracy in different programming languages like Java, Python and C++. Based on the results, it has emphasized their strengths and weaknesses and the importance of further modifications to increase the reliability and accuracy of the latest popular models. Although these AI assistants illustrate a high level of progress in language understanding and code generation, along with ethical considerations and responsible usage, they provoke a necessity for discussion. With time, developing more refined AI technology is essential for achieving advanced solutions in various fields, especially with the knowledge of the feature intricacies of these models and their implications. This study offers a comparison of different LLMs and provides essential feedback on the rapidly changing area of AI models. It also emphasizes the need for ethical developmental practices to actualize AI models’ full potential.","2025","2025-11-25 22:29:39","2025-11-25 22:29:39","","346–354","","","","","","","ICCA '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","LLM; ChatGPT; Gemini; code generation; GitHub Copilot; AI model accuracy; AI models; AlphaCode; chatbot; ethical considerations; responsible deployment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TB2AE2LQ","conferencePaper","2024","Huynh, Hieu; Le, Quoc-Tri; Nguyen, Tien N.; Nguyen, Vu","Using LLM for Mining and Testing Constraints in API Testing","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695341","https://doi.org/10.1145/3691620.3695341","Testing Representational State Transfer (REST) APIs is crucial for ensuring the reliability and performance of APIs, which are essential to modern web services. This testing process helps identify and resolve issues related to data exchange and integration with other systems. Among the various API testing techniques, black-box testing relies on the OpenAPI Specification (OAS) to generate test cases and data. However, current API test automation methods are primarily focused on status code [10] and schema validation [1]. Status code validation involves ensuring that each HTTP request returns a response with a status code, a three-digit integer that indicates the outcome of the request. Schema validation verifies the correctness of the response data by comparing it to the schema. This includes checking that all required properties are present and that data types of these properties align with the schema specified.","2024","2025-11-25 22:29:39","2025-11-25 22:48:13","","2486–2487","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","Software testing; Test pattern generators; Software; Software engineering; large language models; Large Language Models; Debugging; Data mining; API testing; API Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MYGZRHCN","journalArticle","2025","Du, Xiaohu; Wen, Ming; Wang, Haoyu; Wei, Zichao; Jin, Hai","Statement-Level Adversarial Attack on Vulnerability Detection Models via Out-of-Distribution Features","Proc. ACM Softw. Eng.","","","10.1145/3729403","https://doi.org/10.1145/3729403","Code vulnerability detection is crucial to ensure software security. Recent advancements, particularly with the emergence of Code Pre-Trained Models (CodePTMs) and Large Language Models (LLMs), have led to significant progress in this area. However, these models are easily susceptible to adversarial attacks, where even slight input modifications can lead the models to generate opposite results. Existing adversarial approaches, such as identifier replacement, code transformation, and dead code insertion, demonstrate promising performance but still face several limitations. First, the perturbations applied to the target code are relatively constrained (e.g., identifier replacement can only be applied to a small subset of tokens within the entire codebase). Second, the design of perturbed tokens lacks specificity in forcing the model to make incorrect predictions (e.g., they are generated by random selection or context-based prediction). Such limitations lead to the inefficiency and ineffectiveness of existing attacks. To address these issues, we propose SLODA (Statement-level OOD Features driven Adversarial Attack), which introduces two types of out-of-distribution (OOD) features: universal features via code deoptimization and label-specific features extracted from existing mispredicted and adversarial examples. These statement-level OOD features not only expand the perturbation scope, but can also significantly reduce the search space due to their inherently adversarial nature. Moreover, since the OOD features are extracted from existing code and the attack considers the context of the target code, they are more difficult to detect. Our extensive experiments across 15 models demonstrate that SLODA surpasses existing five state-of-the-art approaches in terms of the effectiveness, efficiency, and detection resistance. Furthermore, the adversarial examples generated by SLODA also exhibit promising performance to enhance model robustness.","2025-06","2025-11-25 22:29:39","2025-11-25 22:29:39","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Vulnerability Detection; Adversarial Attack","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3AX6MRC8","conferencePaper","2025","Battaglini-Fischer, Sándor; Srinivasan, Nishanthi; Szarvas, Bálint László; Chu, Xiaoyu; Iosup, Alexandru","FAILS: A Framework for Automated Collection and Analysis of LLM Service Incidents","Companion of the 16th ACM/SPEC International Conference on Performance Engineering","979-8-4007-1130-5","","10.1145/3680256.3721320","https://doi.org/10.1145/3680256.3721320","Large Language Model (LLM) services such as ChatGPT, DALL·E, and Cursor have quickly become essential for society, businesses, and individuals, empowering applications such as chatbots, image generation, and code assistance. The complexity of LLM systems makes them prone to failures and affects their reliability and availability, yet their failure patterns are not fully understood, making it an emerging problem. However, there are limited datasets and studies in this area, particularly lacking an open-access tool for analyzing LLM service failures based on incident reports. Addressing these problems, in this work we propose FAILS, the first open-sourced framework for incident reports collection and analysis on different LLM services and providers. FAILS provides comprehensive data collection, analysis, and visualization capabilities, including: (1) It can automatically collect, clean, and update incident data through its data scraper and processing components;(2) It provides 17 types of failure analysis, allowing users to explore temporal trends of incidents, analyze service reliability metrics, such as Mean Time to Recovery (MTTR) and Mean Time Between Failures (MTBF);(3) It leverages advanced LLM tools to assist in data analysis and interpretation, enabling users to gain observations and insights efficiently. All functions are integrated in the backend, allowing users to easily access them through a web-based frontend interface. FAILS supports researchers, engineers, and general users to understand failure patterns and further mitigate operational incidents and outages in LLM services. The framework is publicly available on https://github.com/atlarge-research/FAILS.","2025","2025-11-25 22:29:39","2025-11-25 22:29:39","","187–194","","","","","","","ICPE '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Toronto ON, Canada","","","","llm; failure characterization; failure recovery; incident report; operational data analytics; reliability; system design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N6J5GDYU","conferencePaper","2024","Liu, Zhe; Chen, Chunyang; Wang, Junjie; Chen, Mengzhuo; Wu, Boyu; Huang, Yuekai; Hu, Jun; Wang, Qing","Unblind Text Inputs: Predicting Hint-text of Text Input in Mobile Apps via LLM","Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems","979-8-4007-0330-0","","10.1145/3613904.3642939","https://doi.org/10.1145/3613904.3642939","Mobile apps have become indispensable for accessing and participating in various environments, especially for low-vision users. Users with visual impairments can use screen readers to read the content of each screen and understand the content that needs to be operated. Screen readers need to read the hint-text attribute in the text input component to remind visually impaired users what to fill in. Unfortunately, based on our analysis of 4,501 Android apps with text inputs, over 76% of them are missing hint-text. These issues are mostly caused by developers’ lack of awareness when considering visually impaired individuals. To overcome these challenges, we developed an LLM-based hint-text generation model called HintDroid, which analyzes the GUI information of input components and uses in-context learning to generate the hint-text. To ensure the quality of hint-text generation, we further designed a feedback-based inspection mechanism to further adjust hint-text. The automated experiments demonstrate the high BLEU and a user study further confirms its usefulness. HintDroid can not only help visually impaired individuals, but also help ordinary people understand the requirements of input components. HintDroid demo video: https://youtu.be/FWgfcctRbfI.","2024","2025-11-25 22:29:39","2025-11-25 22:29:39","","","","","","","","","CHI '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Honolulu, HI, USA","","","","Large Language Model; App Accessibility; Mobile App Design; User Interface","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L7U7MXHU","journalArticle","2025","Molina, Facundo; Gorla, Alessandra; d’Amorim, Marcelo","Test Oracle Automation in the Era of LLMs","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3715107","https://doi.org/10.1145/3715107","The effectiveness of a test suite in detecting faults highly depends on the quality of its test oracles. Large Language Models (LLMs) have demonstrated remarkable proficiency in tackling diverse software testing tasks. This article aims to present a roadmap for future research on the use of LLMs for test oracle automation. We discuss the progress made in the field of test oracle automation before the introduction of LLMs, identifying the main limitations and weaknesses of existing techniques. Additionally, we discuss recent studies on the use of LLMs for this task, highlighting the main challenges that arise from their use, e.g., how to assess quality and usefulness of the generated oracles. We conclude with a discussion about the directions and opportunities for future research on LLM-based oracle automation.","2025-05","2025-11-25 22:29:39","2025-11-25 22:29:39","","","","5","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; Test Oracle Problem","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3VVKSI4Z","bookSection","2025","Ma, Lezhi; Liu, Shangqing; Li, Yi; Xie, Xiaofei; Bu, Lei","SpecGen: Automated Generation of Formal Program Specifications via Large Language Models","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00129","In the software development process, formal program specifications play a crucial role in various stages, including requirement analysis, software testing, and verification. However, manually crafting formal program specifications is rather difficult, making the job time-consuming and labor-intensive. Moreover, it is even more challenging to write specifications that correctly and comprehensively describe the semantics of complex programs. To reduce the burden on software developers, automated specification generation methods have emerged. However, existing methods usually rely on predefined templates or grammar, making them struggle to accurately describe the behavior and functionality of complex real-world programs.To tackle this challenge, we introduce SpecGen, a novel technique for formal program specification generation based on Large Language Models (LLMs). Our key insight is to overcome the limitations of existing methods by leveraging the code comprehension capability of LLMs. The process of SpecGen consists of two phases. The first phase employs a conversational approach that guides the LLM in generating appropriate specifications for a given program, aiming to utilize the ability of LLM to generate high-quality specifications. The second phase, designed for where the LLM fails to generate correct specifications, applies four mutation operators to the model-generated specifications and selects verifiable specifications from the mutated ones through a novel heuristic selection strategy by assigning different weights of variants in an efficient manner. We evaluate SpecGen on two datasets, including the SV-COMP Java category benchmark and a manually constructed dataset containing 120 programs. Experimental results demonstrate that SpecGen succeeds in generating verifiable specifications for 279 out of 385 programs, outperforming the existing LLM-based approaches and conventional specification generation tools like Houdini and Daikon. Further investigations on the quality of generated specifications indicate that SpecGen can comprehensively articulate the behaviors of the input program.","2025","2025-11-25 22:29:39","2025-11-25 22:29:39","","16–28","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6K22IXCC","conferencePaper","2024","Jacob, Kerstin","Integrating Mutation Techniques to Keep Specification and Source Code in Sync","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3685556","https://doi.org/10.1145/3650212.3685556","Most software development processes rely on testing for assuring that a software's source code is kept in sync with its specification during software evolution. In testing, the degree of conformance between source code and specification is measured by the test success rate. However, the practical value of this metric depends on the quality of the test suite, i.e., its completeness wrt. the specification and source code. Mutation testing has proved to be effective in assessing test suite completeness; however related work either considers mutating the source code or the specification. Our research will investigate how approaches based on mutating code and specification can be integrated to keep both in sync during software evolution. We aim to contribute (1) a concise method to measuring the degree of conformance between specification and code with each modification, (2) a technique for the identification and visualization of fine-granular trace links between them, and (3) an approach for suggesting updates of specification and code based on detected deviations between them.","2024","2025-11-25 22:29:39","2025-11-25 22:29:39","","1912–1914","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Software Testing; Mutation; Software Evolution; Traceability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VRR3F6FK","journalArticle","2025","Kessel, Marcus; Atkinson, Colin","Morescient GAI for Software Engineering","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3709354","https://doi.org/10.1145/3709354","The ability of Generative AI (GAI) technology to automatically check, synthesize, and modify software engineering artifacts promises to revolutionize all aspects of software engineering. Using GAI for software engineering tasks is consequently one of the most rapidly expanding fields of software engineering research, with over a hundred LLM-based code models having been published since 2021. However, the overwhelming majority of existing code models share a major weakness—they are exclusively trained on the syntactic facet of software, significantly lowering their trustworthiness in tasks dependent on software semantics. To address this problem, a new class of “Morescient” GAI is needed that is “aware” of (i.e., trained on) both the semantic and static facets of software. This, in turn, will require a new generation of software observation platforms capable of generating large quantities of execution observations in a structured and readily analyzable way. In this article, we present a vision and roadmap for how such “Morescient” GAI models can be engineered, evolved, and disseminated according to the principles of open science.","2025-05","2025-11-25 22:29:39","2025-11-25 22:29:39","","","","5","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","generative AI; dataset; analysis; behavior-aware; dynamic; morescience; observation; roadmap; semantics; vision","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J7HJFGPU","conferencePaper","2024","Siddiq, Mohammed Latif; Da Silva Santos, Joanna Cecilia; Tanvir, Ridwanul Hasan; Ulfat, Noshin; Al Rifat, Fahmid; Carvalho Lopes, Vinícius","Using Large Language Models to Generate JUnit Tests: An Empirical Study","Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering","979-8-4007-1701-7","","10.1145/3661167.3661216","https://doi.org/10.1145/3661167.3661216","A code generation model generates code by taking a prompt from a code comment, existing code, or a combination of both. Although code generation models (e.g., GitHub Copilot) are increasingly being adopted in practice, it is unclear whether they can successfully be used for unit test generation without fine-tuning for a strongly typed language like Java. To fill this gap, we investigated how well three models (Codex, GPT-3.5-Turbo, and StarCoder) can generate unit tests. We used two benchmarks (HumanEval and Evosuite SF110) to investigate the effect of context generation on the unit test generation process. We evaluated the models based on compilation rates, test correctness, test coverage, and test smells. We found that the Codex model achieved above 80% coverage for the HumanEval dataset, but no model had more than 2% coverage for the EvoSuite SF110 benchmark. The generated tests also suffered from test smells, such as Duplicated Asserts and Empty Tests.","2024","2025-11-25 22:29:39","2025-11-25 22:29:39","","313–322","","","","","","","EASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Salerno, Italy","","","","large language models; test generation; unit testing; junit; test smells","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z8NZ38HU","conferencePaper","2024","Dingle, Adam; Krulis, Martin","Tackling Students' Coding Assignments with LLMs","Proceedings of the 1st International Workshop on Large Language Models for Code","979-8-4007-0579-3","","10.1145/3643795.3648389","https://doi.org/10.1145/3643795.3648389","State-of-the-art large language models (LLMs) have demonstrated an extraordinary ability to write computer code. This ability can be quite beneficial when integrated into an IDE to assist a programmer with basic coding. On the other hand, it may be misused by computer science students for cheating on coding tests or homework assignments. At present, knowledge about the exact capabilities and limitations of state-of-the-art LLMs is still inadequate. Furthermore, their capabilities have been changing quickly with each new release. In this paper, we present a dataset of 559 programming exercises in 10 programming languages collected from a system for evaluating coding assignments at our university. We have experimented with four well-known LLMs (GPT-3.5, GPT-4, Codey, Code Llama) and asked them to solve these assignments. The evaluation results are intriguing and provide insights into the strengths and weaknesses of the models. In particular, GPT-4 (which performed the best) is currently capable of solving 55% of all our exercises and achieved an average score of 86% on exercises from the introductory programming course (using the best of five generated solutions).","2024","2025-11-25 22:29:39","2025-11-25 22:29:39","","94–101","","","","","","","LLM4Code '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","large language model; LLM; programming; coding; student assignment; teaching","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L25NUXT2","conferencePaper","2023","Nascimento, Nathalia; Alencar, Paulo; Cowan, Donald","Artificial Intelligence vs. Software Engineers: An Empirical Study on Performance and Efficiency using ChatGPT","Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering","","","","","In the realm of Software Engineering (SE), automation has become a tangible reality. Artificial Intelligence (AI) has suc-cessfully addressed challenges in project management, mod-eling, testing, and development. Among the latest innova-tions is ChatGPT, an ML-infused chatbot capable of gen-erating programming codes and software testing strategies. Although there is speculation that AI-based computation can boost productivity and even substitute software engineers in software development, empirical evidence supporting such claims is lacking. Moreover, questions remain about their po-tential to address overlooked evaluation metrics like energy efficiency, vulnerability, fairness (i.e., human bias), and safety. This paper probes into these issues with an empirical study, comparing ChatGPT with both novice and expert program-mers using LeetCode contest problems. The investigation focuses on performance and memory-efficiency, while also acknowledging the need for a broader assessment of non-functional requirements. The results suggest that ChatGPT is better than beginners at solving easy and medium prob-lems, but it is not yet proven to beat expert programmers. This paper posits that a comprehensive comparison of soft-ware engineers and AI-based solutions, considering various evaluation criteria, is pivotal in fostering human-machine collaboration, enhancing the reliability of AI-based meth-ods, and understanding task suitability for humans or AI. Furthermore, it facilitates the effective implementation of co-operative work structures and human-in-the-loop processes.","2023","2025-11-25 22:29:39","2025-11-25 22:29:39","","24–33","","","","","","","CASCON '23","","","","IBM Corp.","USA","","","","","","","","event-place: Las Vegas, NV, USA","","","","Machine Learning; ChatGPT; Software Engineering; AI-based solutions; Performance Evaluation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RUB9Y26K","journalArticle","2024","Hou, Xinyi; Zhao, Yanjie; Liu, Yue; Yang, Zhou; Wang, Kailong; Li, Li; Luo, Xiapu; Lo, David; Grundy, John; Wang, Haoyu","Large Language Models for Software Engineering: A Systematic Literature Review","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3695988","https://doi.org/10.1145/3695988","Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a Systematic Literature Review (SLR) on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We selected and analyzed 395 research articles from January 2017 to January 2024 to answer four key Research Questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, pre-processing, and application, highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and highlighting promising areas for future study. Our artifacts are publicly available at .","2024-12","2025-11-25 22:29:39","2025-11-25 22:29:39","","","","8","33","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Software Engineering; Survey","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9L9DDQIN","journalArticle","2024","Kang, Sungmin; An, Gabin; Yoo, Shin","A Quantitative and Qualitative Evaluation of LLM-Based Explainable Fault Localization","Proc. ACM Softw. Eng.","","","10.1145/3660771","https://doi.org/10.1145/3660771","Fault Localization (FL), in which a developer seeks to identify which part of the code is malfunctioning and needs to be fixed, is a recurring challenge in debugging. To reduce developer burden, many automated FL techniques have been proposed. However, prior work has noted that existing techniques fail to provide rationales for the suggested locations, hindering developer adoption of these techniques. With this in mind, we propose AutoFL, a Large Language Model (LLM)-based FL technique that generates an explanation of the bug along with a suggested fault location. AutoFL prompts an LLM to use function calls to navigate a repository, so that it can effectively localize faults over a large software repository and overcome the limit of the LLM context length. Extensive experiments on 798 real-world bugs in Java and Python reveal AutoFL improves method-level acc@1 by up to 233.3% over baselines. Furthermore, developers were interviewed on their impression of AutoFL-generated explanations, showing that developers generally liked the natural language explanations of AutoFL, and that they preferred reading a few, high-quality explanations instead of many.","2024-07","2025-11-25 22:29:39","2025-11-25 22:29:39","","","","FSE","1","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","fault localization; debugging; language models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NTANL55D","conferencePaper","2024","Mattis, Toni; Krebs, Eva; Rinard, Martin C.; Hirschfeld, Robert","Examples out of Thin Air: AI-Generated Dynamic Context to Assist Program Comprehension by Example","Companion Proceedings of the 8th International Conference on the Art, Science, and Engineering of Programming","979-8-4007-0634-9","","10.1145/3660829.3660845","https://doi.org/10.1145/3660829.3660845","Programmers often benefit from the availability of concrete run-time data alongside abstract source code. However, programmers need to manually exercise the program to reach an interesting state or write code that reproducibly executes a functionality with concrete inputs to be able to observe concrete data. This work aims to automate this process by leveraging generative AI. We present a framework and a preliminary Smalltalk-based prototype allowing programmers to obtain and run examples for the currently viewed source code section from a large language model. Our approach demonstrates how locally hosted LLMs can be fine-tuned and used for such a task with reasonable computational effort while minimizing common problems like hallucinations and out-of-date knowledge. The framework has direct applications in example-based live programming, where it can suggest new examples, and in learning settings where novices need to know how to use certain functionality.","2024","2025-11-25 22:29:39","2025-11-25 22:29:39","","99–107","","","","","","","Programming '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lund, Sweden","","","","large language models; generative ai; example-based programming; live programming; smalltalk","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SUYLHLSR","conferencePaper","2024","Yu, Jiongchi; Xie, Xiaofei; Zhang, Cen; Chen, Sen; Li, Yuekang; Shen, Wenbo","Bugs in Pods: Understanding Bugs in Container Runtime Systems","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680366","https://doi.org/10.1145/3650212.3680366","Container Runtime Systems (CRSs), which form the foundational infrastructure of container clouds, are critically important due to their impact on the quality of container cloud implementations. However, a comprehensive understanding of the quality issues present in CRS implementations remains lacking. To bridge this gap, we conduct the first comprehensive empirical study of CRS bugs. Specifically, we gather 429 bugs from 8,271 commits across dominant CRS projects, including runc, gvisor, containerd, and cri-o. Through manual analysis, we develop taxonomies of CRS bug symptoms and root causes, comprising 16 and 13 categories, respectively. Furthermore, we evaluate the capability of popular testing approaches, including unit testing, integration testing, and fuzz testing in detecting these bugs. The results show that 78.79% of the bugs cannot be detected due to the lack of test drivers, oracles, and effective test cases. Based on the findings of our study, we present implications and future research directions for various stakeholders in the domain of CRSs. We hope that our work can lay the groundwork for future research on CRS bug detection.","2024","2025-11-25 22:29:39","2025-11-25 22:29:39","","1364–1376","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Software Testing; Empirical Study; Container Runtime","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N9WUENRG","conferencePaper","2024","Weiss, Michael","An exploration of pattern mining with ChatGPT","Proceedings of the 29th European Conference on Pattern Languages of Programs, People, and Practices","979-8-4007-1683-6","","10.1145/3698322.3698338","https://doi.org/10.1145/3698322.3698338","This paper takes an exploratory approach to examine the use of ChatGPT for pattern mining. It proposes an eight-step collaborative process that combines human insight with AI capabilities to extract patterns from known uses. The paper offers a practical demonstration of this process by creating a pattern language for integrating Large Language Models (LLMs) with data sources and tools. LLMs, such as ChatGPT, are a new class of AI models that have been trained on large amounts of text, and can create new content, including text, images, or video. The paper also argues for adding affordances of the underlying components as a new element of pattern descriptions. The primary audience of the paper includes pattern writers interested in pattern mining using LLMs.","2024","2025-11-25 22:29:39","2025-11-25 22:29:39","","","","","","","","","EuroPLoP '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","ChatGPT; Large Language Models; LLMs; Affordances; Human-AI collaboration; LLM Integration; Pattern mining","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XRVZF5XB","journalArticle","2025","Zamudio Amaya, José Antonio; Smytzek, Marius; Zeller, Andreas","FANDANGO: Evolving Language-Based Testing","Proc. ACM Softw. Eng.","","","10.1145/3728915","https://doi.org/10.1145/3728915","Language-based fuzzers leverage formal input specifications (languages) to generate arbitrarily large and diverse sets of valid inputs for a program under test. Modern language-based test generators combine grammars and constraints to satisfy syntactic and semantic input constraints. ISLA, the leading input generator in that space, uses symbolic constraint solving to solve input constraints. Using solvers places ISLA among the most precise fuzzers but also makes it slow. In this paper, we explore search-based testing as an alternative to symbolic constraint solving. We employ a genetic algorithm that iteratively generates candidate inputs from an input specification, evaluates them against defined constraints, evolving a population of inputs through syntactically valid mutations and retaining those with superior fitness until the semantic input constraints are met. This evolutionary procedure, analogous to natural genetic evolution, leads to progressively improved inputs that cover both semantics and syntax. This change boosts the efficiency of language-based testing: In our experiments, compared to ISLA, our search-based FANDANGO prototype is faster by one to three orders of magnitude without sacrificing precision. The search-based approach no longer restricts constraints to constraint solvers' (miniature) languages. In FANDANGO, constraints can use the whole Python language and library. This expressiveness gives testers unprecedented flexibility in shaping test inputs. It allows them to state arbitrary goals for test generation: ”Please produce 1,000 valid test inputs where the voltage field follows a Gaussian distribution but never exceeds 20 mV.”","2025-06","2025-11-25 22:29:39","2025-11-25 22:29:39","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","test generation; fuzzing; Language-based testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GL5EFB2M","conferencePaper","2025","Duvvuru, Venkata Sai Aswath; Zhang, Bohan; Vierhauser, Michael; Agrawal, Ankit","LLM-Agents Driven Automated Simulation Testing and Analysis of small Uncrewed Aerial Systems","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00223","https://doi.org/10.1109/ICSE55347.2025.00223","Thorough simulation testing is crucial for validating the correct behavior of small Uncrewed Aerial Systems (sUAS) across multiple scenarios, including adverse weather conditions (such as wind, and fog), diverse settings (hilly terrain, or urban areas), and varying mission profiles (surveillance, tracking). While various sUAS simulation tools exist to support developers, the entire process of creating, executing, and analyzing simulation tests remains a largely manual and cumbersome task. Developers must identify test scenarios, set up the simulation environment, integrate the System under Test (SuT) with simulation tools, formulate mission plans, and collect and analyze results. These labor-intensive tasks limit the ability of developers to conduct exhaustive testing across a wide range of scenarios. To alleviate this problem, in this paper, we propose AutoSimTest, a Large Language Model (LLM)-driven framework, where multiple LLM agents collaborate to support the sUAS simulation testing process. This includes: (1) creating test scenarios that subject the SuT to unique environmental contexts; (2) preparing the simulation environment as per the test scenario; (3) generating diverse sUAS missions for the SuT to execute; and (4) analyzing simulation results and providing an interactive analytics interface. Further, the design of the framework is flexible for creating and testing scenarios for a variety of sUAS use cases, simulation tools, and SuT input requirements. We evaluated our approach by (a) conducting simulation testing of PX4 and ArduPilot flight-controller-based SuTs, (b) analyzing the performance of each agent, and (c) gathering feedback from sUAS developers. Our findings indicate that AutoSimTest significantly improves the efficiency and scope of the sUAS testing process, allowing for more comprehensive and varied scenario evaluations while reducing the manual effort.","2025","2025-11-25 22:29:39","2025-11-25 22:29:39","","385–397","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","AI for SE; simulation testing; sUAS","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8U26KUJD","journalArticle","2024","Saben, Clark; Zeitz, Jessica; Chandrasekar, Prashant","Enabling Blind and Low-Vision (BLV) Developers with LLM-Driven Code Debugging","J. Comput. Sci. Coll.","","1937-4771","","","BLVRUN is a command line shell script designed to offer developers within the blind and low-vision (BLV) community a succinct and insightful overview of traceback errors. Its primary function involves parsing errors and utilizing a refined large language model to generate informative error summaries. In terms of performance, our model rivals that of well-known models like ChatGPT or AI-chatbot plug-ins tailored for specific Integrated Development Environments (IDEs). Importantly, BLV users can seamlessly integrate this tool into their existing development workflows, eliminating the need for any modifications or adaptations to facilitate debugging tasks.","2024-10","2025-11-25 22:29:39","2025-11-25 22:29:39","","204–215","","3","40","","","","","","","","","","","","","","","","","Place: Evansville, IN, USA Publisher: Consortium for Computing Sciences in Colleges","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NUNCVD2F","conferencePaper","2024","de Almeida, Ágatha; Collins, Eliane; Oran, Ana Carolina","AI in Service of Software Quality: How ChatGPT and Personas Are Transforming Exploratory Testing","Proceedings of the XXIII Brazilian Symposium on Software Quality","979-8-4007-1777-2","","10.1145/3701625.3701657","https://doi.org/10.1145/3701625.3701657","Context: Exploratory testing is essential in the software validation process as a way to find unexpected and critical failures in a short time, complementing documented functional test cases. However, creating scenarios to explore the software (such as test charters) can be time-consuming, and depending on the team’s experience, it may lack adequate coverage of functionalities and scenarios that target specific user profiles of the application. Objective: This article investigates how AI, through LLMs (Large Language Models), can assist in creating exploratory test charters that reflect the characteristics and needs of different user personas. Method: To achieve this, an experimental study was conducted where personas were used as input in ChatGPT 3.5 to generate exploratory test charters. The effectiveness of the approach was evaluated by Software Engineering students, who analyzed the performance and usefulness of the generated charters through a questionnaire based on the TAM model, supplemented by qualitative and quantitative analyses. Results: Data analysis indicated positive acceptance of ChatGPT 3.5 by the participants, highlighting its ease of use and perceived usefulness. Conclusion: This study contributes to the field of Software Engineering by demonstrating a practical application of artificial intelligence in the automated generation of test charters. ChatGPT 3.5 has proven to be a promising tool to support the creation of personalized exploratory test charters, contributing to software quality improvement. The integration of artificial intelligence techniques with user-centered design methods can significantly optimize the software testing process.","2024","2025-11-25 22:29:40","2025-11-25 22:29:40","","179–188","","","","","","","SBQS '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","ChatGPT; Artificial Intelligence; Exploratory Testing; Personas; Software Quality","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PTCFHUSV","bookSection","2025","Deljouyi, Amirhossein; Koohestani, Roham; Izadi, Maliheh; Zaidman, Andy","Leveraging Large Language Models for Enhancing the Understandability of Generated Unit Tests","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00032","Automated unit test generators, particularly search-based software testing tools like EvoSuite, are capable of generating tests with high coverage. Although these generators alleviate the burden of writing unit tests, they often pose challenges for software engineers in terms of understanding the generated tests. To address this, we introduce UTGen, which combines search-based software testing and large language models to enhance the understandability of automatically generated test cases. We achieve this enhancement through contextualizing test data, improving identifier naming, and adding descriptive comments. Through a controlled experiment with 32 participants from both academia and industry, we investigate how the understandability of unit tests affects a software engineer's ability to perform bug-fixing tasks. We selected bug-fixing to simulate a real-world scenario that emphasizes the importance of understandable test cases. We observe that participants working on assignments with UTGen test cases fix up to 33% more bugs and use up to 20% less time when compared to baseline test cases. From the post-test questionnaire, we gathered that participants found that enhanced test names, test data, and variable names improved their bug-fixing process.","2025","2025-11-25 22:29:40","2025-11-25 22:29:40","","1449–1461","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z8QM9EF3","journalArticle","2025","Govers, Jarod; Pareek, Saumya; Velloso, Eduardo; Goncalves, Jorge","Feeds of Distrust: Investigating How AI-Powered News Chatbots Shape User Trust and Perceptions","ACM Trans. Interact. Intell. Syst.","","2160-6455","10.1145/3722227","https://doi.org/10.1145/3722227","The start of the 2020s ushered in a new era of Artificial Intelligence through the rise of Generative AI Large Language Models (LLMs) such as Chat-GPT. These AI chatbots offer a form of interactive agency by enabling users to ask questions and query for more information. However, prior research only considers if LLMs have a political bias or agenda, and not how a biased LLM can impact a user's opinion and trust. Our study bridges this gap by investigating a scenario where users read online news articles and then engage with an interactive AI chatbot, where both the news and the AI are biased to hold a particular stance on a news topic. Interestingly, participants were far more likely to adopt the narrative of a biased chatbot over news articles with an opposing stance. Participants were also substantially more inclined to adopt the chatbot's narrative if its stance aligned with the news—all compared to a control news-article only group. Our findings suggest that the very interactive agency offered by an AI chatbot significantly enhances its perceived trust and persuasive ability compared to the ‘static’ articles from established news outlets, raising concerns about the potential for AI-driven indoctrination. We outline the reasons behind this phenomenon and conclude with the implications of biased LLMs for HCI research, as well as the risks of Generative AI undermining democratic integrity through AI-driven Information Warfare.","2025-03","2025-11-25 22:29:40","2025-11-25 22:29:40","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Chatbots; Generative AI; Large Language Models; Bias; Indoctrination; News; Polarisation; Transparency; Trust","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UUDICCJV","conferencePaper","2024","Li, Tsz-On; Zong, Wenxi; Wang, Yibo; Tian, Haoye; Wang, Ying; Cheung, Shing-Chi; Kramer, Jeff","Nuances Are the Key: Unlocking ChatGPT to Find Failure-Inducing Tests with Differential Prompting","Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering","979-8-3503-2996-4","","10.1109/ASE56229.2023.00089","https://doi.org/10.1109/ASE56229.2023.00089","Automated detection of software failures is an important but challenging software engineering task. It involves finding in a vast search space the failure-inducing test cases that contain an input triggering the software fault and an oracle asserting the incorrect execution. We are motivated to study how far this outstanding challenge can be solved by recent advances in large language models (LLMs) such as ChatGPT. However, our study reveals that ChatGPT has a relatively low success rate (28.8%) in finding correct failure-inducing test cases for buggy programs. A possible conjecture is that finding failure-inducing test cases requires analyzing the subtle differences (nuances) between the tokens of a program's correct version and those for its buggy version. When these two versions have similar sets of tokens and attentions, ChatGPT is weak in distinguishing their differences.We find that ChatGPT can successfully generate failure-inducing test cases when it is guided to focus on the nuances. Our solution is inspired by an interesting observation that ChatGPT could infer the intended functionality of buggy code if it is similar to the correct version. Driven by the inspiration, we develop a novel technique, called Differential Prompting, to effectively find failure-inducing test cases with the help of the compilable code synthesized by the inferred intention. Prompts are constructed based on the nuances between the given version and the synthesized code. We evaluate Differential Prompting on Quixbugs (a popular benchmark of buggy programs) and recent programs published at Codeforces (a popular programming contest portal, which is also an official benchmark of ChatGPT). We compare Differential Prompting with two baselines constructed using conventional ChatGPT prompting and Pynguin (the state-of-the-art unit test generation tool for Python programs). Our evaluation results show that for programs of Quixbugs, Differential Prompting can achieve a success rate of 75.0% in finding failure-inducing test cases, outperforming the best baseline by 2.6X. For programs of Codeforces, Differential Prompting's success rate is 66.7%, outperforming the best baseline by 4.0X.","2024","2025-11-25 22:29:40","2025-11-25 22:47:46","","14–26","","","","","","","ASE '23","","","","IEEE Press","Echternach, Luxembourg","","","","","","","","","","","","Test pattern generators; Software; Chatbots; Codes; large language models; Benchmark testing; Programming; failure-inducing test cases; program generation; program intention inference; Task analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T7YMQDPE","journalArticle","2025","Ma, Qianou; Peng, Weirui; Yang, Chenyang; Shen, Hua; Koedinger, Ken; Wu, Tongshuang","What Should We Engineer in Prompts? Training Humans in Requirement-Driven LLM Use","ACM Trans. Comput.-Hum. Interact.","","1073-0516","10.1145/3731756","https://doi.org/10.1145/3731756","Prompting LLMs for complex tasks (e.g., building a trip advisor chatbot) needs humans to clearly articulate customized requirements (e.g., “start the response with a tl;dr”). However, existing prompt engineering instructions often lack focused training on requirement articulation and instead tend to emphasize increasingly automatable strategies (e.g., tricks like adding role-plays and “think step-by-step”). To address the gap, we introduce Requirement-Oriented Prompt Engineering (ROPE), a paradigm that focuses human attention on generating clear, complete requirements during prompting. We implement ROPE through an assessment and training suite that provides deliberate practice with LLM-generated feedback. In a randomized controlled experiment with 30 novices, ROPE significantly outperforms conventional prompt engineering training (20% vs. 1% gains), a gap that automatic prompt optimization cannot close. Furthermore, we demonstrate a direct correlation between the quality of input requirements and LLM outputs. Our work paves the way to empower more end-users to build complex LLM applications.","2025-08","2025-11-25 22:29:40","2025-11-25 22:29:40","","","","4","32","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","LLM; Prompt Engineering; End-User Programming; Human-AI Interaction; Requirement Engineering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8U7P9T3Q","journalArticle","2024","Zuo, Fei; Tompkins, Cody; Qian, Gang; Rhee, Junghwan; Qu, Xianshan; Yang, Bokai","ChatGPT as an Assembly Language Interpreter for Computing Education","J. Comput. Sci. Coll.","","1937-4771","","","Assembly language is a low-level programming language useful for a number of important computing areas, such as hardware and embedded systems programming, computer architecture, reverse engineering, and malware analysis. In recent years, generative AI, enhanced by GPT technology, has been widely adopted in the IT industry as well as computing education. However, little work has been done to investigate the applicability of GPT to teaching assembly language. In this paper, we fill in the gap by providing an empirical study of GPT's ability to interpret assembly instructions. In particular, we manually evaluated GPT-4's per-instruction explanations of code segments for four different computer architectures, namely x86, x86-64, ARM, and AArch64. Our study shows that, while inconsistencies and rare errors do exist, GPT's interpretations are highly accurate in general, demonstrating a great potential for such tools to be applied in pedagogical practices for tutoring assembly language.","2024-10","2025-11-25 22:29:40","2025-11-25 22:29:40","","73–82","","2","40","","","","","","","","","","","","","","","","","Place: Evansville, IN, USA Publisher: Consortium for Computing Sciences in Colleges","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JK79ES7J","conferencePaper","2025","Zhao, Yan; Tang, Chiawei","Towards LLM-Based Automatic Playtest","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","10.1145/3696630.3728700","https://doi.org/10.1145/3696630.3728700","Playtest is the process in which people play a video game for testing. It is critical for the quality assurance of gaming software. Manual playtest is time-consuming and expensive. However, automating this process is challenging, as playtest typically requires for the domain knowledge and problem-solving skills that most conventional testing tools lack. Recent advancements in artificial intelligence (AI) have opened up new possibilities of applying Large Language Models (LLMs) to playtest. However, significant challenges remain: current LLMs cannot visually perceive game environments; most existing research focuses on text-based games or games with robust API; While many non-text games lack APIs to provide textual descriptions of game states, making it almost impossible to naïvely apply LLMs for playtest.This paper introduces Lap, our novel approach of LLM-based Automatic Playtest, which uses ChatGPT to test match-3 games—a category of games where players match three or more identical tiles in a row or column to earn points. Lap encompasses three key phrases: processing of game environments, prompting-based action generation, and action execution. Given a match-3 game, Lap takes a snapshot of the game board and converts it to a numeric matrix; it then prompts ChatGPT-O1-mini API to suggest moves based on that matrix; finally, Lap tentatively applies the suggested moves to earn points and trigger changes in the game board. It repeats the above-mentioned three steps iteratively until timeout.For evaluation, we conducted a case study by applying Lap to an open-source match-3 game—CasseBonbons, and empirically compared Lap with three existing tools. Our results are promising: Lap outperformed existing tools by achieving higher code coverage and triggering more program crashes. Our research will shed light on future research of automatic testing and LLM application.","2025","2025-11-25 22:29:40","2025-11-25 22:29:40","","1645–1651","","","","","","","FSE Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","LLM; research; automated game testing; few-shot prompting","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NY9WBQYM","conferencePaper","2024","Xing, Ye; Huan, Jun; Tok, Wee Hyong; Shen, Cong; Gehrke, Johannes; Lin, Katherine; Guha, Arjun; Tripp, Omer; Ramanathan, Murali Krishna","NL2Code-Reasoning and Planning with LLMs for Code Development","Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining","979-8-4007-0490-1","","10.1145/3637528.3671505","https://doi.org/10.1145/3637528.3671505","There is huge value in making software development more productive with AI. An important component of this vision is the capability to translate natural language to a programming language (""NL2Code"") and thus to significantly accelerate the speed at which code is written.This workshop gathers researchers, practitioners, and users from industry and academia that are working on NL2Code, specifically on the problem of using large language models to convert statements posed in a human language to a formal programming language.","2024","2025-11-25 22:29:40","2025-11-25 22:29:40","","6745–6746","","","","","","","KDD '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Barcelona, Spain","","","","generative ai; copilot; llms; code creation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"58AV2AUC","conferencePaper","2025","Xie, Yuanmin; Xu, Zhenyang; Tian, Yongqiang; Zhou, Min; Zhou, Xintong; Sun, Chengnian","Kitten: A Simple Yet Effective Baseline for Evaluating LLM-Based Compiler Testing Techniques","Proceedings of the 34th ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-1474-0","","10.1145/3713081.3731731","https://doi.org/10.1145/3713081.3731731","Compiler testing is critical and indispensable to improve the correctness of compilers. Spurred by recent advancements in Large Language Models (LLMs), LLM-based compiler testing techniques such as Fuzz4All, have demonstrated their potential in uncovering real bugs in diverse compilers and reducing the required engineering efforts in designing program generators. Given the continuous evolution of LLMs and the emergence of new LLM-based approaches, establishing robust baselines is crucial for rigorous evaluation and driving future advancements in this promising research direction.To this end, we introduce Kitten, a mutation-based, language-agnostic program generator. Kitten leverages a corpus of seed programs, analogous to the training set for LLMs, and utilizes the target language's syntax, akin to the knowledge learned by LLMs. Furthermore, Kitten's mutation operators can generate diverse test programs, demonstrating a behavior analogous to the ability of LLM inference to generate new code.Our evaluations demonstrate that, using existing compiler test suites as seed programs, Kitten outperforms Fuzz4All in terms of code coverage and bug detection capabilities. Within 24 hours, Kitten achieved 48.3%, 9.9%, and 33.8% higher coverage than Fuzz4All on GCC, LLVM, and Rustc, respectively, while identifying an average of 19.3, 20.3, and 15.7 bugs in these compilers across three runs. Over the course of nine months dedicated to Kitten's development and testing, we identified a total of 328 across the compilers GCC, LLVM, Rustc, Solc, JerryScript, scalac, and slang, of which 310 have been confirmed or fixed. We strongly believe that Kitten serves as an effective baseline, enabling the identification of limitations within existing LLM-based approaches and consequently driving advancements in this promising research direction.","2025","2025-11-25 22:29:40","2025-11-25 22:29:40","","21–25","","","","","","","ISSTA Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","benchmarking; compiler testing; language-agnostic code generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7GW2WXH2","conferencePaper","2024","Guo, Xiaoyu; Zhao, Jianjun; Zhao, Pengzhan","On Repairing Quantum Programs Using ChatGPT","Proceedings of the 5th ACM/IEEE International Workshop on Quantum Software Engineering","979-8-4007-0570-0","","10.1145/3643667.3648223","https://doi.org/10.1145/3643667.3648223","Automated Program Repair (APR) is a vital area in software engineering that generates automatic patches for vulnerable programs. While numerous techniques have been proposed for repairing classical programs, quantum programming lacks a comparable automated repair technique. In this initial exploration, we investigate the use of ChatGPT for quantum program repair and evaluate its performance on Bugs4Q, a benchmark suite of quantum program bugs. Our findings demonstrate the feasibility of employing ChatGPT for quantum program repair. Specifically, we assess ChatGPT's ability to address bugs within the Bugs4Q benchmark, revealing its success in repairing 29 out of 38 bugs. This research represents a promising step towards automating the repair process for quantum programs.","2024","2025-11-25 22:29:40","2025-11-25 22:47:47","","9–16","","","","","","","Q-SE 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","Software testing; Computer bugs; Benchmark testing; Debugging; Knowledge engineering; Programming; Automatic Program Repair; automatic program repair; debugging; quantum programming; Training data; Maintenance engineering; Quantum Programming","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CCXG9B56","bookSection","2025","Zhou, Shiyao; Wang, Jincheng; Ye, He; Zhou, Hao; Goues, Claire Le; Luo, Xiapu","LWDIFF: An LLM-Assisted Differential Testing Framework for WebAssembly Runtimes","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00233","WebAssembly (Wasm) runtimes execute Wasm programs, a popular low-level language for efficiently executing high-level languages in browsers, with broad applications across diverse domains. The correctness of those runtimes is critical for both functionality and security of Wasm execution, motivating testing approaches that target Wasm runtimes specifically. However, existing Wasm testing frameworks fail to generate test cases that effectively test all three phases of runtime, i.e., decoding, validation, and execution. To address this research gap, we propose a new differential testing framework for Wasm runtimes, which leverages knowledge from the Wasm language specification that prior techniques overlooked, enhancing comprehensive testing of runtime functionality. Specifically, we first use a large language model to extract that knowledge from the specification. We use that knowledge in the context of multiple novel mutation operators that generate test cases with diverse features to test all three runtime phases. We evaluate LWDiff by applying it to eight Wasm runtimes. Compared with the state-of-the-art Wasm testers, LWDiff achieves the highest branch coverage and identifies the largest number of bugs. In total, LWDiff discovers 31 bugs across eight runtimes, all of which are confirmed, with 25 of them previously undiscovered.","2025","2025-11-25 22:29:40","2025-11-25 22:29:40","","153–164","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HGEQXJ4R","journalArticle","2024","Dilhara, Malinda; Bellur, Abhiram; Bryksin, Timofey; Dig, Danny","Unprecedented Code Change Automation: The Fusion of LLMs and Transformation by Example","Proc. ACM Softw. Eng.","","","10.1145/3643755","https://doi.org/10.1145/3643755","Software developers often repeat the same code changes within a project or across different projects. These repetitive changes are known as “code change patterns” (CPATs). Automating CPATs is crucial to expedite the software development process. While current Transformation by Example (TBE) techniques can automate CPATs, they are limited by the quality and quantity of the provided input examples. Thus, they miss transforming code variations that do not have the exact syntax, data-, or control-flow of the provided input examples, despite being semantically similar. Large Language Models (LLMs), pre-trained on extensive source code datasets, offer a potential solution. Harnessing the capability of LLMs to generate semantically equivalent, yet previously unseen variants of the original CPAT could significantly increase the effectiveness of TBE systems. In this paper, we first discover best practices for harnessing LLMs to generate code variants that meet three criteria: correctness (semantic equivalence to the original CPAT), usefulness (reflecting what developers typically write), and applicability (aligning with the primary intent of the original CPAT). We then implement these practices in our tool PyCraft, which synergistically combines static code analysis, dynamic analysis, and LLM capabilities. By employing chain-of-thought reasoning, PyCraft generates variations of input examples and comprehensive test cases that identify correct variations with an F-measure of 96.6%. Our algorithm uses feedback iteration to expand the original input examples by an average factor of 58x. Using these richly generated examples, we inferred transformation rules and then automated these changes, resulting in an increase of up to 39x, with an average increase of 14x in target codes compared to a previous state-of-the-art tool that relies solely on static analysis. We submitted patches generated by PyCraft to a range of projects, notably esteemed ones like microsoft/DeepSpeed and IBM/inFairness. Their developers accepted and merged 83% the 86 CPAT instances submitted through 44 pull requests. This confirms the usefulness of these changes.","2024-07","2025-11-25 22:29:40","2025-11-25 22:29:40","","","","FSE","1","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Automation; Machine Learning; Generative AI; Large Language Models; Test Case Generation; Code Changes; Code Clone; Program by Example; Python; Transformation by Example","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2ANNDBY7","conferencePaper","2024","Qiu, Ruidi; Zhang, Grace Li; Drechsler, Rolf; Schlichtmann, Ulf; Li, Bing","AutoBench: Automatic Testbench Generation and Evaluation Using LLMs for HDL Design","Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD","979-8-4007-0699-8","","10.1145/3670474.3685956","https://doi.org/10.1145/3670474.3685956","In digital circuit design, testbenches (TBs) constitute the cornerstone of simulation-based hardware verification. Traditional methodologies for testbench generation during simulation-based hardware verification still remain partially manual, resulting in inefficiencies in te sting various sc enarios an d re quiring expensive time from designers. Large Language Models (LLMs) have demonstrated their potential in automating the circuit design flow. However, directly applying LLMs to generate testbenches suffers from a low pass rate. To address this challenge, we introduce Auto-Bench, the first LLM-based testbench generator for digital circuit design, which requires only the description of the design under test (DUT) to automatically generate comprehensive testbenches. In AutoBench, a hybrid testbench structure and a self-checking system are realized using LLMs. To validate the generated test-benches, we also introduce an automated testbench evaluation framework to evaluate the quality of generated testbenches from multiple perspectives. Experimental results demonstrate that Auto-Bench achieves a 57% improvement in the testbench pass@l ratio compared with the baseline that directly generates testbenches using LLMs. For 75 sequential circuits, AutoBench successfully has a 3.36 times testbench pass@l ratio compared with the baseline. The source codes and experimental results are open-sourced at this link: https://github.com/AutoBench/AutoBench. Artifact DOI: 10.5281/zenodo.13325723.","2024","2025-11-25 22:29:40","2025-11-25 22:29:40","","","","","","","","","MLCAD '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Salt Lake City, UT, USA","","","","Large Language Model; Hardware Simulation; Testbench Generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F6R28SQE","journalArticle","2025","Zhong, Renyi; Li, Yichen; Kuang, Jinxi; Gu, Wenwei; Huo, Yintong; Lyu, Michael R.","LogUpdater: Automated Detection and Repair of Specific Defects in Logging Statements","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3731754","https://doi.org/10.1145/3731754","Developers write logging statements to monitor software runtime behaviors and system state. However, poorly constructed or misleading log messages can inadvertently obfuscate actual program execution patterns, thereby impeding effective software maintenance. Existing research on analyzing issues within logging statements is limited, primarily focusing on detecting a singular type of defect and relying on manual intervention for fixes rather than automated solutions.To address the limitation, we initiate a systematic study that pinpoints four specific types of defects in logging statements (i.e., statement code inconsistency, static dynamic inconsistency, temporal relation inconsistency, and readability issues) through the analysis of real-world log-centric changes. We then propose LogUpdater, a two-stage framework for automatically detecting and updating logging statements for these specific defects. In the offline stage, LogUpdater constructs a similarity-based classifier on a set of synthetic defective logging statements to identify specific defect types. During the online testing phase, this classifier first evaluates logging statements in a given code snippet to determine the necessity and type of improvements required. Then, LogUpdater constructs type-aware prompts from historical logging update changes for an LLM-based recommendation framework to suggest updates addressing these specific defects.We evaluate the effectiveness of LogUpdater on a dataset containing real-world logging changes, a synthetic dataset, and a new real-world project dataset. The results indicate that our approach is highly effective in detecting logging defects, achieving an F1 score of 0.625. Additionally, it exhibits significant improvements in suggesting precise static text and dynamic variables, with enhancements of 48.12% and 24.90%, respectively. Furthermore, LogUpdater achieves a 61.49% success rate in recommending correct updates on new real-world projects. We reported 40 problematic logging statements and their fixes to GitHub via pull requests, resulting in 25 changes confirmed and merged across 11 different projects.","2025-04","2025-11-25 22:29:40","2025-11-25 22:29:40","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Logging Practice; Logging Statement","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JC2RCW9S","journalArticle","2025","Luo, Wenqiang; Keung, Jacky; Yang, Boyang; Ye, He; Le Goues, Claire; Bissyandé, Tegawendé F.; Tian, Haoye; Le, Xuan Bach D.","When Fine-Tuning LLMs Meets Data Privacy: An Empirical Study of Federated Learning in LLM-Based Program Repair","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3733599","https://doi.org/10.1145/3733599","Software systems have been evolving rapidly and inevitably introducing bugs at an increasing rate, leading to significant maintenance costs. While large language models (LLMs) have demonstrated remarkable potential in enhancing software development and maintenance practices, particularly in automated program repair (APR), they rely heavily on high-quality code repositories. Most code repositories are proprietary assets that capture the diversity and nuances of real-world industry software practices, which public datasets cannot fully represent. However, obtaining such data from various industries is hindered by data privacy concerns, as companies are reluctant to share their proprietary codebases. There has also been no in-depth investigation of collaborative software development by learning from private and decentralized data while preserving data privacy for program repair.To address the gap, we investigate federated learning as a privacy-preserving method for fine-tuning LLMs on proprietary and decentralized data to boost collaborative software development and maintenance. We use the private industrial dataset TutorCode for fine-tuning and the EvalRepair-Java benchmark for evaluation, and assess whether federated fine-tuning enhances program repair. We then further explore how code heterogeneity (i.e., variations in coding style, complexity, and embedding) and different federated learning algorithms affect bug fixing to provide practical implications for real-world software development collaboration. Our evaluation reveals that federated fine-tuning can significantly enhance program repair, achieving increases of up to 16.67% for Top@10 and 18.44% for Pass@10, even comparable to the bug-fixing capabilities of centralized learning. Moreover, the negligible impact of code heterogeneity implies that industries can effectively collaborate despite diverse data distributions. Different federated algorithms also demonstrate unique strengths across LLMs, suggesting that tailoring the optimization process to specific LLM characteristics can further improve program repair.","2025-05","2025-11-25 22:29:40","2025-11-25 22:29:40","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; Program Repair; Federated Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ERWZW5KL","conferencePaper","2025","Jacobs, Sven; Kempf, Maurice; Kiesler, Natalie","That's Not the Feedback I Need! - Student Engagement with GenAI Feedback in the Tutor Kai","Proceedings of the 2025 Conference on UK and Ireland Computing Education Research","979-8-4007-2078-9","","10.1145/3754508.3754512","https://doi.org/10.1145/3754508.3754512","The potential of Generative AI (GenAI) for generating feedback in computing education has been the subject of numerous studies. However, there is still limited research on how computing students engage with this feedback and to what extent it supports their problem-solving. For this reason, we built a custom web application providing students with Python programming tasks, a code editor, GenAI feedback, and compiler feedback. Via a think-aloud protocol including eye-tracking and a post-interview with 11 undergraduate students, we investigate (1) how much attention the generated feedback received from learners and (2) to what extent the generated feedback is helpful (or not). In addition, students’ attention to GenAI feedback is compared with that towards the compiler feedback. We further investigate differences between students with and without prior programming experience. The findings indicate that GenAI feedback generally receives a lot of visual attention, with inexperienced students spending twice as much fixation time. More experienced students requested GenAI less frequently, and could utilize it better to solve the given problem. It was more challenging for inexperienced students to do so, as they could not always comprehend the GenAI feedback. They often relied solely on the GenAI feedback, while compiler feedback was not read. Understanding students’ attention and perception toward GenAI feedback is crucial for developing educational tools that support student learning.","2025","2025-11-25 22:29:40","2025-11-25 22:29:40","","","","","","","","","UKICER '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Generative AI; Large Language Models; GenAI; Programming Education; Feedback","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4V3TFNNM","bookSection","2025","Cogo, Filipe; Rajbahadur, Gopi Krishnan; Lin, Dayi; Gallaba, Keheliya; Rombaut, Benjamin; Oliva, Gustavo; Lin, Jiahuei (Justina); Vasilevski, Kirill; Hassan, Ahmed E.","A Tutorial on Software Engineering for FMware","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","","https://doi.org/10.1145/3696630.3728621","Foundation Models (FMs) like GPT-4 have given rise to FMware, FM-powered applications representing a new generation of software that is developed under new paradigms. FMware has been widely adopted in both software engineering (SE) research (e.g., test generation) and industrial products (e.g., GitHub copilot), despite the numerous challenges introduced by the stochastic nature of FMs. In our tutorial, we will present the latest research and industry practices in engineering FMware. Our tutorial's perspective is firmly rooted in SE rather than artificial intelligence (AI), ensuring its accessibility to FSE participants.","2025","2025-11-25 22:29:40","2025-11-25 22:29:40","","1231–1233","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2L9JX5RL","conferencePaper","2024","Santos, Sofia; Saraiva, João; Ribeiro, Francisco","Large Language Models in Automated Repair of Haskell Type Errors","Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair","979-8-4007-0577-9","","10.1145/3643788.3648012","https://doi.org/10.1145/3643788.3648012","This paper introduces a new method of Automated Program Repair that relies on a combination of the GPT-4 Large Language Model and automatic type checking of Haskell programs. This method identifies the source of a type error and asks GPT-4 to fix that specific portion of the program. Then, QuickCheck is used to automatically generate a large set of test cases to validate whether the generated repair behaves as the correct solution. Our publicly available experiments revealed a success rate of 88.5% in normal conditions. However, more detailed testing should be performed to more accurately evaluate this form of APR.","2024","2025-11-25 22:29:40","2025-11-25 22:29:40","","42–45","","","","","","","APR '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","large language model; automated program repair; code generation; fault localization; automatic testing; type checking","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HG9UAM35","conferencePaper","2024","Wang, Tuowei; Li, Kun; Hao, Zixu; Bai, Donglin; Ren, Ju; Zhang, Yaoxue; Cao, Ting; Yang, Mao","Long Exposure: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity","Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis","979-8-3503-5291-7","","10.1109/SC41406.2024.00081","https://doi.org/10.1109/SC41406.2024.00081","The adaptation of pre-trained large language models (LLMs) to diverse downstream tasks via fine-tuning is critical for numerous applications. However, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques presents significant challenges in terms of time investments and operational costs. In this paper, we first introduce a nuanced form of sparsity, termed Shadowy Sparsity, which is distinctive in fine-tuning and has not been adequately addressed for acceleration. Under Shadowy Sparsity, we propose Long Exposure1, an efficient system to accelerate PEFT for LLMs. Long Exposure comprises three key components: Shadowy-sparsity Exposer employs a prolonged sensing range to capture more sparsity details under shadowy sparsity; Sequence-oriented Predictor provides efficient yet accurate predictions to handle large sequence inputs and constantly-evolving parameters; and Dynamic-aware Operator facilitates more structured computational patterns and coalesced memory accesses, addressing dynamic sparse operations. Extensive evaluations show that Long Exposure outperforms state-of-the-arts with up to a 2.49× speedup in end-to-end fine-tuning, offering promising advancements in accelerating PEFT for LLMs.","2024","2025-11-25 22:29:40","2025-11-25 22:29:40","","","","","","","","","SC '24","","","","IEEE Press","Atlanta, GA, USA","","","","","","","","","","","","Large Language Model; Fine-tuning; Sparsity","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MLBXSLCN","conferencePaper","2024","Qiu, Yuxin","Full-Stack Collaboration for Robust Heterogeneity-Enabled AI Systems","Companion Proceedings of the 2024 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for Humanity","979-8-4007-1214-2","","10.1145/3689491.3691817","https://doi.org/10.1145/3689491.3691817","In this new era of AI with diverse hardware accelerators such as GPUs and quantum circuits, achieving system-wide robustness requires tackling issues throughout all system layers, spanning from software applications to hardware components. My research is to enhance the robustness of heterogeneity-enabled AI systems by reinventing software testing and analysis techniques via leveraging full-stack insights and advanced AI capabilities. I have completed one research project and have collaborated on a couple of others at the application and language levels. As the next steps, I will explore (1) holistic regression testing to prioritize test inputs associated with system-wide changes and (2) full-stack analysis to optimize computing resource allocation and reduce hardware reliance by analyzing application characteristics and using alternative resources in tandem.","2024","2025-11-25 22:29:41","2025-11-25 22:29:41","","10–12","","","","","","","SPLASH Companion '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Pasadena, CA, USA","","","","software testing; Software engineering; artificial intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UI6S2JNU","conferencePaper","2025","He, Yiling; She, Hongyu; Qian, Xingzhi; Zheng, Xinran; Chen, Zhuo; Qin, Zhan; Cavallaro, Lorenzo","On Benchmarking Code LLMs for Android Malware Analysis","Proceedings of the 34th ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-1474-0","","10.1145/3713081.3731745","https://doi.org/10.1145/3713081.3731745","Large Language Models (LLMs) have demonstrated strong capabilities in various code intelligence tasks. However, their effectiveness for Android malware analysis remains underexplored. Decompiled Android malware code presents unique challenges for analysis, due to the malicious logic being buried within a large number of functions and the frequent lack of meaningful function names.This paper presents Cama, a benchmarking framework designed to systematically evaluate the effectiveness of Code LLMs in Android malware analysis. Cama specifies structured model outputs to support key malware analysis tasks, including malicious function identification and malware purpose summarization. Built on these, it integrates three domain-specific evaluation metrics—consistency, fidelity, and semantic relevance—enabling rigorous stability and effectiveness assessment and cross-model comparison.We construct a benchmark dataset of 118 Android malware samples from 13 families collected in recent years, encompassing over 7.5 million distinct functions, and use Cama to evaluate four popular open-source Code LLMs. Our experiments provide insights into how Code LLMs interpret decompiled code and quantify the sensitivity to function renaming, highlighting both their potential and current limitations in malware analysis.","2025","2025-11-25 22:29:41","2025-11-25 22:29:41","","153–160","","","","","","","ISSTA Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","code LLM; malware analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SK936XYZ","conferencePaper","2024","Liu, Bingchang; Chen, Chaoyu; Gong, Zi; Liao, Cong; Wang, Huan; Lei, Zhichao; Liang, Ming; Chen, Dajun; Shen, Min; Zhou, Hailian; Jiang, Wei; Yu, Hang; Li, Jianguo","MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning","Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining","979-8-4007-0490-1","","10.1145/3637528.3671609","https://doi.org/10.1145/3637528.3671609","Code LLMs have emerged as a specialized research field, with remarkable studies dedicated to enhancing model's coding capabilities through fine-tuning on pre-trained models. Previous fine-tuning approaches were typically tailored to specific downstream tasks or scenarios, which meant separate fine-tuning for each task, requiring extensive training resources and posing challenges in terms of deployment and maintenance. Furthermore, these approaches failed to leverage the inherent interconnectedness among different code-related tasks. To overcome these limitations, we present a multi-task fine-tuning framework, MFTCoder, that enables simultaneous and parallel fine-tuning on multiple tasks. By incorporating various loss functions, we effectively address common challenges in multi-task learning, such as data imbalance, varying difficulty levels, and inconsistent convergence speeds. Extensive experiments have conclusively demonstrated that our multi-task fine-tuning approach outperforms both individual fine-tuning on single tasks and fine-tuning on a mixed ensemble of tasks. Moreover, MFTCoder offers efficient training capabilities, including efficient data tokenization modes and parameter efficient fine-tuning (PEFT) techniques, resulting in significantly improved speed compared to traditional fine-tuning methods. MFTCoder seamlessly integrates with several mainstream open-source LLMs, such as CodeLLama and Qwen. Our MFTCoder fine-tuned CodeFuse-DeepSeek-33B claimed the top spot on the Big Code Models Leaderboard ranked by WinRate as of January 30, 2024. MFTCoder is open-sourced at https://github.com/codefuse-ai/MFTCOder","2024","2025-11-25 22:29:41","2025-11-25 22:29:41","","5430–5441","","","","","","","KDD '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Barcelona, Spain","","","","large language model; code generation; multi-task learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EKAJXNPI","journalArticle","2025","Chen, Chong; Su, Jianzhong; Chen, Jiachi; Wang, Yanlin; Bi, Tingting; Yu, Jianxing; Wang, Yanli; Lin, Xingwei; Chen, Ting; Zheng, Zibin","When ChatGPT Meets Smart Contract Vulnerability Detection: How Far Are We?","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3702973","https://doi.org/10.1145/3702973","With the development of blockchain technology, smart contracts have become an important component of blockchain applications. Despite their crucial role, the development of smart contracts may introduce vulnerabilities and potentially lead to severe consequences, such as financial losses. Meanwhile, large language models, represented by ChatGPT, have gained great attention, showcasing great capabilities in code analysis tasks. In this article, we presented an empirical study to investigate the performance of ChatGPT in identifying smart contract vulnerabilities. Initially, we evaluated ChatGPT’s effectiveness using a publicly available smart contract dataset. Our findings discover that while ChatGPT achieves a high recall rate, its precision in pinpointing smart contract vulnerabilities is limited. Furthermore, ChatGPT’s performance varies when detecting different vulnerability types. We delved into the root causes for the false positives generated by ChatGPT, and categorized them into four groups. Second, by comparing ChatGPT with other state-of-the-art smart contract vulnerability detection tools, we found that ChatGPT’s F-score is lower than others for 3 out of the 7 vulnerabilities. In the case of the remaining 4 vulnerabilities, ChatGPT exhibits a slight advantage over these tools. Finally, we analyzed the limitation of ChatGPT in smart contract vulnerability detection, revealing that the robustness of ChatGPT in this field needs to be improved from two aspects: its uncertainty in answering questions; and the limited length of the detected code. In general, our research provides insights into the strengths and weaknesses of employing large language models, specifically ChatGPT, for the detection of smart contract vulnerabilities.","2025-04","2025-11-25 22:29:41","2025-11-25 22:29:41","","","","4","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","ChatGPT; Large Language Models; Empirical Study; Smart Contracts; Vulnerabilities","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AFW9TSCS","conferencePaper","2025","Weisz, Justin D.; Kumar, Shraddha Vijay; Muller, Michael; Browne, Karen-Ellen; Goldberg, Arielle; Heintze, Katrin Ellice; Bajpai, Shagun","Examining the Use and Impact of an AI Code Assistant on Developer Productivity and Experience in the Enterprise","Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems","979-8-4007-1395-8","","10.1145/3706599.3706670","https://doi.org/10.1145/3706599.3706670","AI assistants are being created to help software engineers conduct a variety of coding-related tasks, such as writing, documenting, and testing code. We describe the use of the watsonx Code Assistant (WCA), an LLM-powered coding assistant deployed internally within IBM. Through surveys of two user cohorts (N=669) and unmoderated usability testing (N=15), we examined developers’ experiences with WCA and its impact on their productivity. We learned about their motivations for using (or not using) WCA, we examined their expectations of its speed and quality, and we identified new considerations regarding ownership of and responsibility for generated code. Our case study characterizes the impact of an LLM-powered assistant on developers’ perceptions of productivity and it shows that although such tools do often provide net productivity increases, these benefits may not always be experienced by all users.","2025","2025-11-25 22:29:41","2025-11-25 22:29:41","","","","","","","","","CHI EA '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","LLM; Generative AI; software engineering; code assistant; productivity","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8DJNH6LU","journalArticle","2025","Mammadov, Tural; Klakow, Dietrich; Koller, Alexander; Zeller, Andreas","Learning Program Behavioral Models from Synthesized Input-Output Pairs","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3748720","https://doi.org/10.1145/3748720","We introduce Modelizer—a novel framework that, given a black-box program, learns a model from its input/output behavior using neural machine translation algorithms. The resulting model mocks the original program: Given an input, the model predicts the output that would have been produced by the program. However, the model is also reversible—that is, the model can predict the input that would have produced a given output. Finally, the model is differentiable and can be efficiently restricted to predict only a certain aspect of the program behavior. Modelizer uses grammars to synthesize and inputs and unsupervised tokenizers to decompose the resulting outputs, allowing it to learn sequence-to-sequence associations between token streams. Other than input grammars, Modelizer only requires the ability to execute the program. The resulting models are small, requiring fewer than 6.3&nbsp;million parameters for languages such as Markdown or HTML; and they are accurate, achieving up to 95.4% accuracy and a BLEU score of&nbsp;0.98 with standard error&nbsp;0.04 in mocking real-world applications. As it learns from and predicts executions rather than code, Modelizer departs from the LLM-centric research trend, opening new opportunities for program-specific models that are fully tuned towards individual programs. Indeed, we foresee several applications of these models, especially as the output of the program can be any aspect of program behavior. Beyond mocking and predicting program behavior, the models can also synthesize inputs that are likely to produce a particular behavior, such as failures or coverage, thus assisting in program understanding and maintenance.","2025-07","2025-11-25 22:29:41","2025-11-25 22:29:41","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Software Testing; Deep Learning; Mocking","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8Y2T8SZC","journalArticle","2025","Patel, Hrishitva","Balancing Act: Automated GenAI Outputs and Their Impact on Social Media Ecosystems","SIGCAS Comput. Soc.","","0095-2737","10.1145/3749886.3749889","https://doi.org/10.1145/3749886.3749889","Generative AI is reshaping social media by automating content creation, personalization, and distribution. While it enhances accessibility and engagement, it also raises concerns around misinformation, bias, and reduced human oversight. A case study using a Twilio-powered WhatsApp bot for NASA's Astronomy Picture of the Day highlights both the potential and the limitations of AI-generated captions. Balancing automation with human judgment is essential to ensure ethical, inclusive, and trustworthy digital ecosystems.","2025-07","2025-11-25 22:29:41","2025-11-25 22:29:41","","8–12","","1","53","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","generative AI; artificial intelligence; accessibility; machine learning; misinformation; content moderation; decision-making; human oversight; social media","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UV6IFMM3","journalArticle","2025","Kong, Jiaolong; Xie, Xiaofei; Liu, Shangqing","Demystifying Memorization in LLM-Based Program Repair via a General Hypothesis Testing Framework","Proc. ACM Softw. Eng.","","","10.1145/3729390","https://doi.org/10.1145/3729390","Large Language Models (LLMs) have achieved remarkable success in various applications, particularly in code-related tasks such as code generation and program repair, setting new performance benchmarks. However, the extensive use of large training corpora raises concerns about whether these achievements stem from genuine understanding or mere memorization of training data—a question often overlooked in current research. This paper aims to study the memorization issue within LLM-based program repair by investigating whether the correct patches generated by LLMs are the result of memorization. The key challenge lies in the absence of ground truth for confirming memorization, leading to various ad-hoc methods designed for its detection. To address this challenge, we first propose a general framework that formalizes memorization detection as a general hypothesis testing problem, where existing approaches can be unified by defining a low-probability event under the null hypothesis that the data is not memorized. The occurrence of such an event leads to the rejection of the null hypothesis, indicating potential memorization. Based on this framework, we design two specific methods (i.e., low-probability events) to detect potential memorization: 1) basic ground-truth matching, and 2) reassessment after substantial code mutation. We investigate the memorization issue in LLM-based program repair using two datasets: Defects4J, a widely used benchmark that is likely included in the training data, and GitBug-Java, a new dataset that is unlikely to be part of the training data. Our findings reveal that a significant portion of correct patches exactly match the ground truths in Defects4J (e.g., 78.83% and 87.42% on GPT-3.5 and CodeLlama-7b, respectively). Moreover, even after significant modifications to the buggy code, where the original repairs should not be generated, a considerable percentage of bugs (e.g., 81.82% on GPT-3.5 and 88.24% on CodeLlama-7b) continue to be fixed exactly as in the original bug fixes, indicating a high likelihood of memorization. Furthermore, we evaluate existing memorization detection methods and demonstrate their ineffectiveness in this context (e.g., most AUROCs are below 0.5). The theoretical analysis under our hypothesis testing framework shows that their defined events may not meet the requirements for being low-probability. The study highlights the critical need for more robust and rigorous evaluations in LLM-based software engineering research, ensuring a clear distinction between true problem-solving capabilities and mere memorization.","2025-06","2025-11-25 22:29:41","2025-11-25 22:29:41","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Program Repair; Code Memorization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VQQ2S9QQ","conferencePaper","2024","de Oliveira Neto, Francisco Gomes","Unveiling Assumptions: Exploring the Decisions of AI Chatbots and Human Testers","Proceedings of the 1st ACM International Conference on AI-Powered Software","979-8-4007-0685-1","","10.1145/3664646.3664762","https://doi.org/10.1145/3664646.3664762","The integration of Large Language Models (LLMs) and chatbots introduces new challenges and opportunities for decision-making in software testing. Decision-making relies on a variety of information, including code, requirements specifications, and other software artifacts that are often unclear or exist solely in the developer’s mind. To fill in the gaps left by unclear information, we often rely on assumptions, intuition, or previous experiences to make decisions. This paper explores the potential of LLM-based chatbots like Bard, Copilot, and ChatGPT, to support software testers in test decisions such as prioritizing test cases effectively. We investigate whether LLM-based chatbots and human testers share similar “assumptions” or intuition in prohibitive testing scenarios where exhaustive execution of test cases is often impractical. Preliminary results from a survey of 127 testers indicate a preference for diverse test scenarios, with a significant majority (96%) favoring dissimilar test sets. Interestingly, two out of four chatbots mirrored this preference, aligning with human intuition, while the others opted for similar test scenarios, chosen by only 3.9% of testers. Our initial insights suggest a promising avenue within the context of enhancing the collaborative dynamics between testers and chatbots.","2024","2025-11-25 22:29:41","2025-11-25 22:29:41","","45–49","","","","","","","AIware 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","Chatbots; Software Testing; Test Prioritization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ARKSESRL","conferencePaper","2024","Lee Solano, Lorenzo; Renzella, Jake; Vassar, Alexandra","DCC Sidekick: Helping Novices Solve Programming Errors Through a Conversational Explanation Interface","Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2","979-8-4007-0424-6","","10.1145/3626253.3635483","https://doi.org/10.1145/3626253.3635483","Students in introductory computing courses often lack the experience required to effectively identify and resolve errors in their code. For such students, Programming Error Messages (PEMs) are often the first indication of an error, and could provide valuable debugging guidance. However, in many cases, such as with standard C compiler implementations, PEMs are largely unsuitable for novices. Confusing, misleading, and filled with terse language and jargon, these messages instead act as an additional source of difficulty.In this paper, we present DCC Sidekick, which integrates the Debugging C Compiler (DCC) with a Large Language Model (LLM) in a web-based dashboard to produce contextual, accurate guidance conducive to student learning. This dashboard is directly accessible from the output of the compiler, and provides a bird's-eye-view of the program source, compiler output, and a conversational AI interface to help unravel cryptic error messages. We aim to deploy DCC Sidekick to a C-based CS1 cohort at a large higher education institution to investigate how novice students utilise the conversational explanation interface during debugging activities. In this work, we present our experience designing and building DCC Sidekick.","2024","2025-11-25 22:29:41","2025-11-25 22:29:41","","1714–1715","","","","","","","SIGCSE 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Portland, OR, USA","","","","generative ai; cs1; ai in education; compiler error messages; error message enhancement","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"79FQDIWA","conferencePaper","2025","Huang, Zijie; Cai, Lizhi; Mao, Xuan; Yangt, Kang","Towards Early Warning and Migration of High-Risk Dormant Open-Source Software Dependencies","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: New Ideas and Emerging Results","979-8-3315-3711-1","","10.1109/ICSE-NIER66352.2025.00030","https://doi.org/10.1109/ICSE-NIER66352.2025.00030","Dormant open-source software (OSS) dependencies are no longer maintained or actively developed, their related code components are more vulnerable and error-prone since they can hardly keep up with evolving software dependents. Presently, their migration remains costly and challenging for practitioners. To tackle such a challenge, we intend to characterize, predict, and automatically migrate high-risk dormant OSS dependencies. Our pilot study of 4,945 Maven dependencies reveals over half of them are dormant, and 12.15% pose a high security risk. These high-risk dependencies can be predicted early based on their version release and usage characteristics. They are rarely migrated by developers, and simple one-to-one API migrations can be achieved with little context using Large Language Models (LLMs). Future research will be conducted on a more complete dataset, incorporate socio-technical features for improved high-risk prediction, and fine-tune a migration code generator.","2025","2025-11-25 22:29:41","2025-11-25 22:29:41","","121–125","","","","","","","ICSE-NIER '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","empirical software engineering; dependency migration; open source sustainability; supply chain security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NU7LFTGD","journalArticle","2025","Li, Jiawei; Gao, Yang; Yang, Yizhe; Bai, Yu; Zhou, Xiaofeng; Li, Yinghao; Sun, Huashan; Liu, Yuhang; Si, Xingpeng; Ye, Yuhao; Wu, Yixiao; Lin, Yiguan; Xu, Bin; Ren, Bowen; Feng, Chong; Huang, Heyan","Fundamental Capabilities and Applications of Large Language Models: A Survey","ACM Comput. Surv.","","0360-0300","10.1145/3735632","https://doi.org/10.1145/3735632","Large Language Models (LLMs) have demonstrated remarkable effectiveness across various domain-specific applications. However, which fundamental capabilities most contribute to their success in different domains remains unclear. This uncertainty complicates LLM evaluation, as existing benchmark-based assessments often fail to capture their real-world performance, where the required capabilities may differ from those measured in the benchmarks. In this survey, we provide a systematic introduction to LLMs’ fundamental capabilities, encompassing their definitions, formation mechanisms, and practical applications. We further explore the relationships among these capabilities and discuss how they collectively support complex problem-solving in domain-specific applications. Building on this foundation, we review recent advances in LLM-driven applications across nine specific domains: medicine, law, computational biology, finance, social sciences and psychology, computer programming and software engineering, robots and agents, AI for disciplines, and creative work. We analyze how specific capabilities are leveraged for each domain to address unique requirements. This perspective enables us to establish connections between these capabilities and domain requirements, and to evaluate the varying importance of different capabilities across different domains. Based on these insights, we propose evaluation strategies tailored to the essential capabilities required in each domain, offering practical guidance for selecting suitable backbone LLMs in real-world applications.","2025-09","2025-11-25 22:29:41","2025-11-25 22:29:41","","","","2","58","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large language model; applications; fundamental capabilities","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PCRY7CGP","journalArticle","2025","Chen, Daihang; Liu, Yonghui; Zhou, Mingyi; Zhao, Yanjie; Wang, Haoyu; Wang, Shuai; Chen, Xiao; Bissyandé, Tegawendé F.; Klein, Jacques; Li, Li","LLM for Mobile: An Initial Roadmap","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3708528","https://doi.org/10.1145/3708528","When mobile meets LLMs, mobile app users deserve to have more intelligent usage experiences. For this to happen, we argue that there is a strong need to apply LLMs for the mobile ecosystem. We therefore provide a research roadmap for guiding our fellow researchers to achieve that as a whole. In this roadmap, we sum up six directions that we believe are urgently required for research to enable native intelligence in mobile devices. In each direction, we further summarize the current research progress and the gaps that still need to be filled by our fellow researchers.","2025-05","2025-11-25 22:29:41","2025-11-25 22:29:41","","","","5","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","LLM; Security; Mobile; On-device model","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8VPRT6PG","conferencePaper","2024","Silva, Luciana Lourdes; Silva, Janio Rosa da; Montandon, Joao Eduardo; Andrade, Marcus; Valente, Marco Tulio","Detecting Code Smells using ChatGPT: Initial Insights","Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement","979-8-4007-1047-6","","10.1145/3674805.3690742","https://doi.org/10.1145/3674805.3690742","This paper presents initial insights into the effectiveness of ChatGPT in detecting code smells in Java projects. We utilize a large dataset comprising four code smells—Blob, Data Class, Feature Envy, and Long Method—classified into three severity levels. To assess ChatGPT’s proficiency, we employ two different prompts: (i) a generic prompt and (ii) a prompt specifying the smells selected for our research. We evaluate ChatGPT’s abilities using metrics such as precision, recall, and F-measure. Our results reveal that the odds of ChatGPT providing a correct outcome with a specific prompt are 2.54 times higher compared to a generic one. Furthermore, ChatGPT is more effective at detecting smells with critical severity (F-measure = 0.52) than those with minor severity (F-measure = 0.43). Finally, we discuss the implications of our findings and suggest future research directions for leveraging large language models to detect code smells.","2024","2025-11-25 22:29:41","2025-11-25 22:29:41","","400–406","","","","","","","ESEM '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Barcelona, Spain","","","","ChatGPT; Large Language Models; Software Quality; Bad Smells; Code Smells; Refactoring Opportunities","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4XF478D3","conferencePaper","2025","Sheng, Junjie; Lin, Yanqiu; Wu, Jiehao; Huang, Yanhong; Shi, Jianqi; Zhang, Min; Wang, Xiangfeng","SolSearch: An LLM-Driven Framework for Efficient SAT-Solving Code Generation","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: New Ideas and Emerging Results","979-8-3315-3711-1","","10.1109/ICSE-NIER66352.2025.00007","https://doi.org/10.1109/ICSE-NIER66352.2025.00007","The Satisfiability (SAT) problem is a core challenge with significant applications in software engineering, including automated testing, configuration management, and program verification. This paper presents SolSearch, a novel framework that harnesses large language models (LLMs) to discover and optimize SAT-solving strategies automatically. Leveraging a curriculum-based, trial-and-error process, SolSearch enables the LLM to iteratively modify and generate SAT solver code, thereby improving solving efficiency and performance. This automated SAT-solving paradigm has the advantage of being plug-and-play, allowing integration with any SAT solver and accelerating the development or design process of new SAT solvers (new methods). Our preliminary experimental results are encouraging by demonstrating that the LLM-powered paradigm improves state-of-the-art SAT solvers on general SAT benchmarks and significantly enhances the performance of the widely used Z3 solver (11% on PAR-2 score). These results highlight the potential for using LLM-driven methods to advance solver adaptability and effectiveness in real-world software engineering challenges. Future research directions are discussed to further refine and validate this approach, offering a promising avenue for integrating AI with traditional software engineering tasks.","2025","2025-11-25 22:29:41","2025-11-25 22:29:41","","6–10","","","","","","","ICSE-NIER '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","code generation; heuristic method; large language models (LLM); SAT solver","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KYHA7LF8","journalArticle","2024","Wang, Wei; Ning, Huilong; Zhang, Gaowei; Liu, Libo; Wang, Yi","Rocks Coding, Not Development: A Human-Centric, Experimental Evaluation of LLM-Supported SE Tasks","Proc. ACM Softw. Eng.","","","10.1145/3643758","https://doi.org/10.1145/3643758","Recently, large language models (LLM) based generative AI has been gaining momentum for their impressive high-quality performances in multiple domains, particularly after the release of the ChatGPT. Many believe that they have the potential to perform general-purpose problem-solving in software development and replace human software developers. Nevertheless, there are in a lack of serious investigation into the capability of these LLM techniques in fulfilling software development tasks. In a controlled 2 × 2 between-subject experiment with 109 participants, we examined whether and to what degree working with ChatGPT was helpful in the coding task and typical software development task and how people work with ChatGPT. We found that while ChatGPT performed well in solving simple coding problems, its performance in supporting typical software development tasks was not that good. We also observed the interactions between participants and ChatGPT and found the relations between the interactions and the outcomes. Our study thus provides first-hand insights into using ChatGPT to fulfill software engineering tasks with real-world developers and motivates the need for novel interaction mechanisms that help developers effectively work with large language models to achieve desired outcomes.","2024-07","2025-11-25 22:29:41","2025-11-25 22:29:41","","","","FSE","1","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","controlled experiment; human-AI collaboration; large langauge models; software development task","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RNMEV3TW","conferencePaper","2025","Ke, Kaiyao","NIODebugger: A Novel Approach to Repair Non-Idempotent-Outcome Tests with LLM-Based Agent","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00226","https://doi.org/10.1109/ICSE55347.2025.00226","Flaky tests, characterized by inconsistent results across repeated executions, present significant challenges in software testing, especially during regression testing. Recently, there has been emerging research interest in non-idempotent-outcome (NIO) flaky tests—tests that pass on the initial run but fail on subsequent executions within the same environment. Despite progress in utilizing Large Language Models (LLMs) to address flaky tests, existing methods have not tackled NIO flaky tests. The limited context window of LLMs restricts their ability to incorporate relevant source code beyond the test method itself, often overlooking crucial information needed to address state pollution, which is the root cause of NIO flakiness.This paper introduces NIODebugger, the first framework to utilize an LLM-based agent to repair flaky tests. NIODebugger features a three-phase design: detection, exploration, and fixing. In the detection phase, dynamic analysis collects stack traces and custom test execution logs from multiple test runs, which helps in understanding accumulative state pollution. During the exploration phase, the LLM-based agent provides instructions for extracting relevant source code associated with test flakiness. In the fixing phase, NIODebugger repairs the tests using the information gathered from the previous phases. NIODebugger can be integrated with multiple LLMs, achieving patching success rates ranging from 11.63% to 58.72%. Its best-performing variant, NIODebugger-GPT-4, successfully generated correct patches for 101 out of 172 previously unknown NIO tests across 20 large-scale open-source projects. We submitted pull requests for all generated patches; 58 have been merged, only 1 was rejected, and the remaining 42 are pending. The Java implementation of NIODebugger is provided as a Maven plugin accessible at https://github.com/kaiyaok2/NIOInspector.","2025","2025-11-25 22:29:41","2025-11-25 22:47:46","","1014–1025","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","Large language models; software testing; Software testing; Java; Software engineering; Source coding; flaky tests; Maintenance engineering; Distance measurement; Feature extraction; llm-based agent; Pollution","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LA974EVY","conferencePaper","2024","Hamza, Muhammad; Siemon, Dominik; Akbar, Muhammad Azeem; Rahman, Tahsinur","Human-AI Collaboration in Software Engineering: Lessons Learned from a Hands-On Workshop","Proceedings of the 7th ACM/IEEE International Workshop on Software-Intensive Business","979-8-4007-0571-7","","10.1145/3643690.3648236","https://doi.org/10.1145/3643690.3648236","This paper investigates the dynamics of human-AI collaboration in software engineering, focusing on the use of ChatGPT. Through a thematic analysis of a hands-on workshop in which 22 professional software engineers collaborated for three hours with ChatGPT, we explore the transition of AI from a mere tool to a collaborative partner. The study identifies key themes such as the evolving nature of human-AI interaction, the capabilities of AI in software engineering tasks, and the challenges and limitations of integrating AI in this domain. The findings show that while AI, particularly ChatGPT, improves the efficiency of code generation and optimization, human oversight remains crucial, especially in areas requiring complex problem-solving and security considerations. This research contributes to the theoretical understanding of human-AI collaboration in software engineering and provides practical insights for effectively integrating AI tools into development processes. It highlights the need for clear role allocation, effective communication, and balanced AI-human collaboration to realize the full potential of AI in software engineering.","2024","2025-11-25 22:29:41","2025-11-25 22:47:26","","7–14","","","","","","","IWSiB '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","Software; Chatbots; ChatGPT; Resource management; generative AI; Generative AI; Conferences; Software Engineering; software engineering; Security; empirical investigation; workshop; Collaboration; Problem-solving; Empirical Investigation; Workshop","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QLAZPUTT","conferencePaper","2024","Aslan Oğuz, Evin; Kuester, Jochen Malte","A Comparative Analysis of ChatGPT-Generated and Human-Written Use Case Descriptions","Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems","979-8-4007-0622-6","","10.1145/3652620.3687800","https://doi.org/10.1145/3652620.3687800","The development of comprehensive use case descriptions is a critical task in software engineering, providing essential insights for requirement analysis and system design. The advent of advanced natural language processing models, such as ChatGPT, has sparked interest in their potential to automate tasks traditionally performed by humans, including the generation of use case descriptions in software engineering. Understanding the capabilities and limitations of ChatGPT in generating use case descriptions is crucial for software engineers. Without a clear understanding of its performance, practitioners may either overestimate its utility, leading to reliance on suboptimal drafts, or underestimate its capabilities, missing opportunities to streamline the drafting process. This paper addresses how well ChatGPT performs in generating use case descriptions, evaluating their quality compared to human-written descriptions. To do so, we employ a structured approach using established quality guidelines and the concept of ""bad smells"" for use case descriptions. Our study presents the first attempt to bridge the knowledge gap by offering a comparative analysis of ChatGPT-generated and human-written use case descriptions. By providing an approach to objectively assess ChatGPT's performance, we highlight its potential and limitations, offering software engineers insights to effectively integrate AI tools into their workflows.","2024","2025-11-25 22:29:41","2025-11-25 22:29:41","","533–540","","","","","","","MODELS Companion '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Linz, Austria","","","","ChatGPT; requirements engineering; quality; use case description","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QKUW2VHR","conferencePaper","2024","Chen, Xi; Liang, Jingsai","Pair Programming with ChatGPT","Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2","979-8-4007-0424-6","","10.1145/3626253.3635600","https://doi.org/10.1145/3626253.3635600","This poster explores the potential of ChatGPT to replace the traditional approach of pair programming in introductory computer science courses. Traditionally, two students collaborate as a driver and a navigator, periodically switching roles. Now, a student can pair up with ChatGPT, which offers an innovative approach to pair programming. This exploratory activity, which emphasizes collaboration and communication, provides step-by-step instructions for effectively interacting with ChatGPT during pair programming.This poster reflects on the advantages and limitations of using ChatGPT in pair programming. The main advantages of using ChatGPT include rapid responses, syntax error-free code generation, and flexibility in handling incomplete pseudocode. The primary limitations include the coding generation style, redundancy in responses, and challenges in understanding the code. Despite the advantages, it may still be valuable to have students work with human partners in certain situations, particularly for learning purposes.This poster proposes that ChatGPT is an invaluable tool for enhancing productivity and emphasizes the importance of becoming proficient in its use during students' college years. It also provides insights into the effective utilization of ChatGPT in pair programming and its preparation for future careers in programming and related fields.","2024","2025-11-25 22:29:41","2025-11-25 22:29:41","","1600–1601","","","","","","","SIGCSE 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Portland, OR, USA","","","","chatgpt; pair programming","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F6KPFSCY","book","2024","","AST '24: Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)","","979-8-4007-0588-5","","","","AST continues to be a venue for researchers and practitioners where they can discuss high quality research contributions on methods for software test automation, and various case studies reporting practices in this field. Indeed, software test automation is a discipline that has produced noteworthy research in the last decade.The special theme of AST 2024 is ""Test automation for and with Generative AI"". This innovative and promising research direction deals with the application of test automation technologies to the testing of Generative AI applications, as well as the adoption of generative AI to facilitate test automation.","2024","2025-11-25 22:29:41","2025-11-25 22:29:41","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RCCAG9ES","conferencePaper","2025","Wang, Lihua; Sun, Jiamou; Jiang, Jiaojiao; Kanhere, Salil S.; Xing, Zhenchang; Jha, Sanjay","Vulnerability Aspects Extraction and Discrepancies Detection across Heterogeneous Threat Intelligence","Proceedings of the Workshop on Privacy in Large Language Models (LLM) and Natural Language Processing (NLP) 2025","979-8-4007-1415-3","","10.1145/3709018.3736330","https://doi.org/10.1145/3709018.3736330","Accurate documentation of security vulnerabilities is essential for effective vulnerability management. However, inconsistencies and discrepancies in aspect-level information across major vulnerability databases like NVD, IBM X-Force, ExploitDB, and Openwall present significant challenges. These challenges arise due to the heterogeneous and unstructured nature of the databases, variations in terminology, differing levels of detail, and the presence of conflicting or incomplete information, which complicate automated extraction and comparison of critical vulnerability aspects. To address these issues, we leveraged large language models (LLMs), specifically GPT-3.5 and GPT-4, to improve the accuracy of extracting seven key vulnerability aspects from 2,000 vulnerability reports and detect semantic discrepancies across diverse databases. Unlike previous traditional NLP models, such as BERT, which struggle with less structured data, LLMs offer a more advanced approach to vulnerability information analysis. Experimental results demonstrate GPT-4’s exceptional performance: an average F1-score of 0.96 in aspect extraction, outperforming the BertGeneration model by up to 26% on less structured databases like Openwall. In semantic similarity analysis, few-shot GPT-4 achieved up to 93% accuracy, significantly surpassing traditional methods like Word Mover’s Distance and Siamese networks. These findings demonstrate the effectiveness of LLMs in extracting vulnerability information and detecting discrepancies, contributing to more robust cybersecurity defences and efficient vulnerability management practices.","2025","2025-11-25 22:29:41","2025-11-25 22:29:41","","13–24","","","","","","","LM-SHIELD '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Large Language Model; Vulnerability aspects extraction; Vulnerability discrepancy","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CGQ3ZIKB","journalArticle","2025","Tu, Zhi; Niu, Liangkun; Fan, Wei; Zhang, Tianyi","Multi-modal Traffic Scenario Generation for Autonomous Driving System Testing","Proc. ACM Softw. Eng.","","","10.1145/3729348","https://doi.org/10.1145/3729348","Autonomous driving systems (ADS) require extensive testing and validation before deployment. However, it is tedious and time-consuming to construct traffic scenarios for ADS testing. In this paper, we propose TrafficComposer, a multi-modal traffic scenario construction approach for ADS testing. TrafficComposer takes as input a natural language (NL) description of a desired traffic scenario and a complementary traffic scene image. Then, it generates the corresponding traffic scenario in a simulator, such as CARLA and LGSVL. Specifically, TrafficComposer integrates high-level dynamic information about the traffic scenario from the NL description and intricate details about the surrounding vehicles, pedestrians, and the road network from the image. The information from the two modalities is complementary to each other and helps generate high-quality traffic scenarios for ADS testing. On a benchmark of 120 traffic scenarios, TrafficComposer achieves 97.0% accuracy, outperforming the best-performing baseline by 7.3%. Both direct testing and fuzz testing experiments on six ADSs prove the bug detection capabilities of the traffic scenarios generated by TrafficComposer. These scenarios can directly discover 37 bugs and help two fuzzing methods find 33%–124% more bugs serving as initial seeds.","2025-06","2025-11-25 22:29:42","2025-11-25 22:29:42","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Software Testing; Autonomous Driving System; Traffic Scenario Generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QPK45DZM","conferencePaper","2024","Eskandani, Nafise; Salvaneschi, Guido","Towards AI for Software Systems","Proceedings of the 1st ACM International Conference on AI-Powered Software","979-8-4007-0685-1","","10.1145/3664646.3664767","https://doi.org/10.1145/3664646.3664767","Generative Artificial Intelligence (GenAI) is being adopted for a number of Software Engineering activities, mostly centering around coding, such as code generation, code comprehension, code reviews, test generation, and bug fixing. Other phases in the Software Engineering process have been less explored. In this paper, we argue that more investigation is needed on the support that GenAI can provide to the design, and operation of software systems, i.e., a number of crucial activities, beyond coding, that are necessary to successfully deliver and maintain software services. These include reasoning about architectural choices and dealing with third-party platforms. We discuss crucial aspects of AI for software systems. taking as a use case Function as a Service (FaaS).We present several challenges, including cold start delays, stateless functions, debugging complexities, and vendor lock-in and explore the potential of GenAI tools to mitigate FaaS challenges. Finally, we outline future research into the application of GenAI tools for the development and deployment of software systems.","2024","2025-11-25 22:29:42","2025-11-25 22:29:42","","79–84","","","","","","","AIware 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","Generative AI; FaaS; Serverless Computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UTSF8C5P","conferencePaper","2024","Harman, Mark","The Role of Software Measurement in Assured LLM-Based Software Engineering","Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering","979-8-4007-1701-7","","10.1145/3661167.3661269","https://doi.org/10.1145/3661167.3661269","Assured Large Language Model Software Engineering (Assured LLMSE) addresses the twin challenges: 1. Ensuring LLM-generated code does not regress the properties of the original code 2. Quantifying the improvement over the original archived by the improve code in a verifiable and measurable way. In so doing, the Assured LLMSE approach tackles the problem of LLMs’ tendency to hallucinate, as well as providing confidence that generated code improves an existing code base. Software testing and measurement play critical roles in this improvement process: testing is the guard against regression, while measurement provides the quantifiable assurance of improvement. Assured LLMSE takes its inspiration from previous work on genetic improvement, for which software measurement also plays a central role. In this keynote we outline the Assured LLMSE approach, highlighting the role of software measurement in the provision of quantifiable, verifiable assurances for code that originates from LLM–based inference. This paper is an outline of the content of the keynote by Mark Harman at the 28th International Conference on Evaluation and Assessment in Software Engineering. This is joint work with Nadia Alshahwan, Andrea Aquino, Jubin Chheda, Anastasia Finegenova, Inna Harper, Mitya Lyubarskiy, Neil Maiden, Alexander Mols, Shubho Sengupta, Rotem Tal, Alexandru Marginean, and Eddy Wang.","2024","2025-11-25 22:29:42","2025-11-25 22:29:42","","4","","","","","","","EASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Salerno, Italy","","","","Large Language Models (LLMs); Automated Code Generation; CodeLlama; Genetic Improvement (GI); Llama; Search Based Software Engineering (SBSE)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UT9UIXBP","conferencePaper","2024","Bo, Lili; Ji, Wangjie; Sun, Xiaobing; Zhang, Ting; Wu, Xiaoxue; Wei, Ying","ChatBR: Automated assessment and improvement of bug report quality using ChatGPT","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695518","https://doi.org/10.1145/3691620.3695518","Bug reports, containing crucial information such as the Observed Behavior (OB), the Expected Behavior (EB), and the Steps to Reproduce (S2R), can help developers localize and fix bugs efficiently. However, due to the increasing complexity of some bugs and the limited experience of some reporters, large numbers of bug reports miss this crucial information. Although machine learning (ML)-based and information retrieval (IR)-based approaches are proposed to detect and supplement the missing information in bug reports, the performance of these approaches depends heavily on the size and quality of bug report datasets.In this paper, we present ChatBR, an approach for automated assessment and improvement of bug report quality using ChatGPT. First, we fine-tune a BERT model using manually annotated bug reports to create a sentence-level multi-label classifier to assess the quality of bug reports by detecting whether existing OB, EB, and S2R. Then, we use ChatGPT in a zero-shot setup to generate missing information (OB, EB, and S2R) to improve the quality of bug reports. Finally, the output of ChatGPT are fed back into the classifier for verification until ChatGPT generates the missing information. Experimental results show that, in the task of detecting missing information in bug reports, ChatBR outperforms the state-of-the-art methods by 25.38%-29.20% in terms of precision. In the task of generating missing information in bug reports, ChatBR can achieve an average of 84.10% in terms of semantic similarity of the generated information and original information across six different projects. Furthermore, ChatBR can generate more than 99.9% of high-quality bug reports (i.e., bug reports that are full of OB, EB, and S2R) within five queries to ChatGPT.","2024","2025-11-25 22:29:42","2025-11-25 22:29:42","","1472–1483","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","ChatGPT; large language models; bug report; pre-trained models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JUXP7W5S","conferencePaper","2023","Wei, Yuxiang; Xia, Chunqiu Steven; Zhang, Lingming","Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair","Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","979-8-4007-0327-0","","10.1145/3611643.3616271","https://doi.org/10.1145/3611643.3616271","During Automated Program Repair (APR), it can be challenging&nbsp;to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models&nbsp;(LLMs) have been shown to be helpful “copilots” in assisting developers with various coding tasks, and have also been directly&nbsp;applied for patch synthesis. However, most LLMs treat programs as&nbsp;sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This&nbsp;results in plenty of statically invalid generated patches, impeding&nbsp;the practicality of the technique. Therefore, we propose Repilot,&nbsp;a framework to further copilot the AI “copilots” (i.e., LLMs) by&nbsp;synthesizing more valid patches during the repair process. Our key&nbsp;insight is that many LLMs produce outputs autoregressively (i.e.,&nbsp;token by token), resembling human writing programs, which can&nbsp;be significantly boosted and guided through a Completion Engine.&nbsp;Repilot synergistically synthesizes a candidate patch through the&nbsp;interaction between an LLM and a Completion Engine, which 1)&nbsp;prunes away infeasible tokens suggested by the LLM and 2) proactively completes the token based on the suggestions provided by the&nbsp;Completion Engine. Our evaluation on a subset of the widely-used&nbsp;Defects4j 1.2 and 2.0 datasets shows that Repilot fixes 66 and 50&nbsp;bugs, respectively, surpassing the best-performing baseline by 14&nbsp;and 16 bugs fixed. More&nbsp;importantly, Repilot is capable of producing more valid and correct patches than the base LLM when given&nbsp;the same generation budget.","2023","2025-11-25 22:29:42","2025-11-25 22:29:42","","172–184","","","","","","","ESEC/FSE 2023","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: San Francisco, CA, USA","","","","Large Language Model; Program Repair; Completion Engine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JAYPG76T","conferencePaper","2024","Hoffmann, Jacob; Frister, Demian","Generating Software Tests for Mobile Applications Using Fine-Tuned Large Language Models","Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)","979-8-4007-0588-5","","10.1145/3644032.3644454","https://doi.org/10.1145/3644032.3644454","Motivation. Software tests are a necessity in the development of software to secure functionality, reliability, and usability [10]; however, these tests are costly and time-consuming [6]. Although tool support for software testing has advanced, there remains considerable potential for enhancement. Many software tests are still devised manually, with the creation of unit tests being particularly laborious. Automating the generation of test cases is promising for streamlining this aspect of software testing [6].Large Language Models (LLMs) have exhibited capabilities in code generation [11, 13–15], test case generation [17], and various other domains [11]. The advancement of model performance of transformer-based LLMs is mainly achieved by expanding the model size in line with an increase in training data size [7, 8]. However, this approach leads to high computational costs which can only be afforded by corporations with significant financial resources. This highlights the need for transformer-based LLMs that perform well on a specific downstream task and are also cost-efficient. Addressing this, we focused on supervised fine-tuning (SFT) of more resource-efficient transformer-based LLMs LLaMA 2 13B, Code Llama 13B, and Mistral 7B for the specific downstream task of generating test cases for mobile applications.Research questions. This work investigated: Does SFT enhance the capabilities of a transformer-based LLM in the specific downstream task of generating test cases for mobile applications while being cost-efficient and runnable on standard consumer hardware? Does the fine-tuned model outperform other state-of-the-art models in the task of test generation for mobile applications?Approach. Our approach is a modification of the ATHENATEST approach [16]. However, our approach focuses on supervised fine-tuning (SFT) on both pre-trained and already fine-tuned transformer-based LLMs for the task of test case generation for mobile applications in Dart.The approach involves three steps, as illustrated in Figure 1. Firstly, a labeled dataset of corresponding input-output pairs (X, Y) was obtained to model the conditional probability P(Y|X; θ) [9, 12]. Dart code and corresponding test files were extracted from open-source GitHub repositories using Google BigQuery. These files were then matched using regular expressions, ensuring that each code file was matched with its corresponding test file based on matching base filenames. The dataset underwent quality filtering and deduplication, resulting in 16,252 input-output pairs, which was then divided into training (90%) and validation (10%) sets. The training set of the dataset consists of a total of 88.5M tokens using the LLaMA tokenizer.Secondly, for SFT on the downstream task of test generation, models were selected based on their code generation capabilities, as indicated by the pass@1 score on the HumanEval [2] and MBPP [1] benchmark, their parameter sizes, and the extent to which they had been trained on Dart data. In model selection, open-source models capable of running on cost-efficient consumer hardware with code generation abilities were primarily chosen.Thirdly, in the SFT process, the test generation task was represented as translation task, in line with ATHENATEST [16]. This is achieved by employing the following structured prompt format for SFT [9]:""prefix_prompt ### Code: code ### Test: test""In this work, there was no prefix prompt used during SFT.Fine-tuning. The fine-tuning was conducted on a single GPU system using Flash Attention 2 [3] and the QLoRA method [4] to reduce memory size and the number of trainable parameters. The fine-tuning process varied in duration up to 32 hours, resulting in total emissions of 13.099 kgCO2eq [5].Experimental Results. The performance of TestGen-Dart models was evaluated for their unit testing capabilities in Dart, in comparison to base models LLaMA 2 13B, Code Llama 13B, and Mistral 7B. The models were loaded in both float16 and 4-bit quantization configurations, and the evaluation involved nine different Dart files, encompassing 42 test cases. The results were obtained in a zero-shot setting using a structured prompt format, as described in the approach section. This included a prefix prompt instructing the models to generate unit tests: ""Generate unit tests in Dart for the following class. The unit test should be structured with the 'test' function, an appropriate description, and an assertion 'expect' within the function to validate the test case."" The generated unit tests were classified into three categories: syntax errors (SE), syntactic correctness (SC), and functional correctness (FC). In a 4-bit quantization configuration, TestGen-Dart_v0.2 enhanced the generation of syntactically correct unit tests by 15.38% and functionally correct unit tests by 16.67%, compared to the underlying base model, Code Llama 13B. Additionally, TestGen-Dart_v0.2 demonstrated superior performance in the 16-bit configuration. This evidenced that supervised fine-tuning (SFT) increases the capability of transformer-based LLMs in a specific downstream task, in this instance, generating test cases for mobile applications, addressing the first research question posed in this work. Additionally, TestGen-Dart_v0.2 outperformed the other state-of-the-art models of interest LLaMA 2 13B and Mistral 7B in that task, addressing the second research question.Conclusion. This work demonstrates that SFT enhances the capability of transformer-based LLMs in generating test cases for mobile applications in Dart. Furthermore, the 13B parameter size of the TestGen-Dart enables it to run locally on standard consumer hardware, potentially making it a cost-efficient and privacy-friendly testing assistant for software developers by avoiding an external server connection to run the model.Outlook. Future work currently in progress may expand this approach to other programming languages and refine TestGen-Dart's performance by using higher-quality fine-tuning data either synthetic or human-annotated. Additionally, the evaluation method may be enhanced by using TestGen-Dart for generating test cases for dummy applications and measuring code coverage.","2024","2025-11-25 22:29:42","2025-11-25 22:29:42","","76–77","","","","","","","AST '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","software testing; large language models; machine learning; mobile testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ILA6JPXH","conferencePaper","2024","Xu, Congying; Chen, Songqiang; Wu, Jiarong; Cheung, Shing-Chi; Terragni, Valerio; Zhu, Hengcheng; Cao, Jialun","MR-Adopt: Automatic Deduction of Input Transformation Function for Metamorphic Testing","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3696020","https://doi.org/10.1145/3691620.3696020","While a recent study reveals that many developer-written test cases can encode a reusable Metamorphic Relation (MR), over 70% of them directly hard-code the source input and follow-up input in the encoded relation. Such encoded MRs, which do not contain an explicit input transformation to transform the source inputs to corresponding follow-up inputs, cannot be reused with new source inputs to enhance test adequacy.In this paper, we propose MR-Adopt (Automatic Deduction Of inPut Transformation) to automatically deduce the input transformation from the hard-coded source and follow-up inputs, aiming to enable the encoded MRs to be reused with new source inputs. With typically only one pair of source and follow-up inputs available in an MR-encoded test case as the example, we leveraged LLMs to understand the intention of the test case and generate additional examples of source-followup input pairs. This helps to guide the generation of input transformations generalizable to multiple source inputs. Besides, to mitigate the issue that LLMs generate erroneous code, we refine LLM-generated transformations by removing MR-irrelevant code elements with data-flow analysis. Finally, we assess candidate transformations based on encoded output relations and select the best transformation as the result. Evaluation results show that MR-Adopt can generate input transformations applicable to all experimental source inputs for 72.00% of encoded MRs, which is 33.33% more than using vanilla GPT-3.5. By incorporating MR-Adopt-generated input transformations, encoded MR-based test cases can effectively enhance the test adequacy, increasing the line coverage and mutation score by 10.62% and 18.91%, respectively.","2024","2025-11-25 22:29:42","2025-11-25 22:47:43","","557–569","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","software testing; Software testing; Software; Software engineering; Codes; large language models; Large Language Models; Debugging; Software Testing; Metamorphic Testing; code generation; Code Generation; metamorphic relation; metamorphic testing; input transformation; Transforms; Input Transformation; Metamorphic Relation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RIIA2ZIE","journalArticle","2025","Wu, Jie JW; Fard, Fatemeh H.","HumanEvalComm: Benchmarking the Communication Competence of Code Generation for LLMs and LLM Agents","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3715109","https://doi.org/10.1145/3715109","Large language models (LLMs) have significantly improved their ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. The most recent trend is using LLM-based agents to iterate the code generation process. Based on the observation that top-level software engineers often ask clarifying questions to reduce Ambiguity in both requirements and coding solutions, we argue that the same should be applied to LLMs for code generation tasks. For this purpose, we define the communication skills of LLMs as “being able to ask clarifying questions when the description of the code generation problem has issues.” In this study, we restrict these issues to three matters from the software requirement engineering field: inconsistent requirements, ambiguous requirements, and incomplete requirements. By asking probing questions about the requirements of problem descriptions before generating the final code, the challenges of programming with LLMs such as unclear intent specification may be alleviated, resulting to a correct code in the initial iterations.In this work, we conducted an empirical study on the benchmark and analysis of the communication skills of LLMs for code generation. We created a new benchmark, HumanEvalComm, by modifying problem descriptions according to three issues mentioned above, Inconsistency, Ambiguity, and Incompleteness. We then experimented on HumanEvalComm with different Code LLMs, and a new LLM agent approach, Code Clarification and Generation Agent (Okanagan), to identify and ask questions in ambiguous parts from code and descriptions for further refining the generated code. In the evaluation, we introduced an LLM-based evaluator and created Communication Rate and Good Question Rate as the evaluation metrics to represent the ratio of questions asked and questions with good quality in responses. We found that more than 60% of responses from Code LLMs still generate code rather than ask questions when the problem descriptions are manually modified according to different clarification categories. The Pass@1 and Test Pass Rate of most Code LLMs drop by 35% — 52% and by 17% — 35%, respectively, with statistical significance in each category for over 75% numbers. Okanagan, as an LLM agent approach that uses LLM such as ChatGPT 3.5, effectively increases the Communication Rate and Good Question Rate by an absolute 58% and 38%, respectively. Thus, Okanagan boosts Pass@1 and Test Pass Rate by an absolute 8% and 7%, respectively, when the problem descriptions are modified based on given clarification categories. This result indicates the potential for achieving more effective communication capability using LLM agent. Our benchmark and full code are publicly available at .","2025-08","2025-11-25 22:29:42","2025-11-25 22:29:42","","","","7","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large language models; artificial intelligence; software requirements; natural language processing; empirical study; code generation; benchmarking; clarifying questions; communication competence; LLM agents","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KK5XHVEL","bookSection","2025","Li, Jiageng; Dong, Zhen; Wang, Chong; You, Haozhen; Zhang, Cen; Liu, Yang; Peng, Xin","LLM Based Input Space Partitioning Testing for Library APIs","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00153","Automated library APIs testing is difficult as it requires exploring a vast space of parameter inputs that may involve objects with complex data types. Existing search based approaches, with limited knowledge of relations between object states and program branches, often suffer from the low efficiency issue, i.e., tending to generate invalid inputs. Symbolic execution based approaches can effectively identify such relations, but fail to scale to large programs.In this work, we present an LLM-based input space partitioning testing approach, LISP, for library APIs. The approach leverages LLMs to understand the code of a library API under test and perform input space partitioning based on its understanding and rich common knowledge. Specifically, we provide the signature and code of the API under test to LLMs, with the expectation of obtaining a text description of each input space partition of the API under test. Then, we generate inputs through employing the generated text description to sample inputs from each partition, ultimately resulting in test suites that systematically explore the program behavior of the API.We evaluate LISP on more than 2,205 library API methods taken from 10 popular open-source Java libraries (e.g., apache/commons-lang with 2.6k stars, guava with 48.8k stars on GitHub). Our experiment results show that LISP is effective in library API testing. It significantly outperforms state-of-the-art tool EvoSuite in terms of edge coverage. On average, LISP achieves 67.82% branch coverage, surpassing EvoSuite by 1.21 times. In total, LISP triggers 404 exceptions or errors in the experiments, and discovers 13 previously unknown vulnerabilities during evaluation, which have been assigned CVE IDs.","2025","2025-11-25 22:29:42","2025-11-25 22:29:42","","1436–1448","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AY9C7IMB","journalArticle","2025","Wang, Guoqing; Sun, Zeyu; Ye, Sixiang; Gong, Zhihao; Chen, Yizhou; Zhao, Yifan; Liang, Qingyuan; Hao, Dan","Do advanced language models eliminate the need for prompt engineering in software engineering?","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3771933","https://doi.org/10.1145/3771933","Large Language Models (LLMs) have significantly advanced software engineering (SE) tasks, with prompt engineering techniques enhancing their performance in code-related areas. However, the rapid development of foundational LLMs such as the non-reasoning models (GPT-4o and Claude 3.5 Sonnet) and the reasoning model o1 raises questions about the continued effectiveness of these prompt engineering techniques. This paper presents an extensive empirical study that reevaluates various prompt engineering techniques within the context of these advanced LLMs. Focusing on five representative SE tasks, i.e., code generation, code translation, program repair, code summarization, and commit message generation, we assess whether prompt engineering techniques still yield improvements with advanced models, the actual effectiveness of reasoning models compared to non-reasoning models, and whether the benefits of using these advanced models justify their increased costs. Our findings reveal that some novel prompt engineering techniques developed for earlier LLMs may provide diminished benefits or even hinder performance when applied to advanced models. In reasoning LLMs, the ability of sophisticated built-in reasoning reduces the impact of complex prompts, sometimes making simple zero-shot prompting more effective in some specific tasks. Prompt strategies that utilize execution feedback or precise task-specific guidance remain effective and are essential for improving performance on complex related-code problems. Furthermore, while reasoning models outperform non-reasoning models in tasks requiring complex reasoning, they offer minimal advantages in tasks that do not need reasoning and may incur unnecessary costs. Based on our study, we provide practical guidance for practitioners on selecting appropriate prompt engineering techniques and foundational LLMs, considering factors such as task requirements, operational costs, and environmental impact. Our work contributes to a deeper understanding of effectively harnessing advanced LLMs in SE tasks, informing future research and application development.","2025-10","2025-11-25 22:29:42","2025-11-25 22:29:42","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Empirical Study; Prompt Engineering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2E8VCR9S","bookSection","2025","Yan, Chuan; Wan, Liuhuo; Guan, Bowei; Yu, Fengqi; Bai, Guangdong","Tracking GPTs Third Party Service: Automation, Analysis, and Insights","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","","https://doi.org/10.1145/3696630.3728711","ChatGPT has quickly advanced from simple natural language processing to tackling more sophisticated and specialized tasks. Drawing inspiration from the success of mobile app ecosystems, OpenAI allows developers to create applications that interact with third-party services, known as GPTs. GPTs can choose to leverage third-party services to integrate with specialized APIs for domain-specific applications. However, the way these disclose privacy setting information limits accessibility and analysis, making it challenging to systematically evaluate the data privacy implications of third-party integrate to GPTs. In order to support academic research on the integration of third-party services in GPTs, we introduce GPTs-ThirdSpy, an automated framework designed to extract GPTs' privacy settings. GPTs-ThirdSpy provides academic researchers with real-time, reliable metadata on third-party services used by GPTs, enabling in-depth analysis of their integration, compliance, and potential security risks. By systematically collecting and structuring this data, GPTs-ThirdSpy facilitates large-scale research on the transparency and regulatory challenges associated with the GPT app ecosystem.","2025","2025-11-25 22:29:42","2025-11-25 22:29:42","","1602–1606","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A8WLW9LI","journalArticle","2025","Gu, Siqi; Fang, Chunrong; Zhang, Quanjun; Chen, Zhenyu","ACTesting: Automated Cross-modal Testing Method of Text-to-Image Software","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3768581","https://doi.org/10.1145/3768581","Recently, creative generative artificial intelligence software has emerged as a pivotal assistant, enabling users to generate content and seek inspiration rapidly. Text-to-Image (T2I) software, one of the most widely used, synthesizes images with text input by engaging in a cross-modal process. However, despite substantial advancements in the T2I engine, T2I software still encounters errors when generating complex or non-realistic scenes, including omitting focal entities, low image realism, and mismatched text-image information. The cross-modal nature of T2I software complicates error detection for traditional testing methods, and the absence of test oracles further exacerbates the complexity of the testing process. To fill this gap, we propose ACTesting, an Automated Cross-modal Testing Method of Text-to-Image Software, the first testing method explicitly designed for T2I software. ACTesting utilizes the metamorphic testing principle to address the oracle problem and identifies cross-modal semantic consistency as its fundamental Metamorphic relation (MR) by employing the Entity-relationship (ER) triples. We design three kinds of mutation operators under the guidance of MR and the adaptability density constraint to construct the new input text. After generating the images based on the text, ACTesting verifies whether MR is satisfied by detecting the ER triples across two modalities to detect the errors of T2I software. In our experiments across five popular T2I software, ACTesting effectively generates error-revealing tests, resulting in a decrease in text-image consistency by up to 20% when compared to the baseline. Additionally, an ablation study demonstrates the efficacy of the proposed mutation operators. The experimental results validate that ACTesting can reliably identify errors within T2I software.","2025-09","2025-11-25 22:29:42","2025-11-25 22:29:42","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Software Testing; Cross-modal; Text-to-Image","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YD3PRPAL","conferencePaper","2025","Shuvo, Uddip Acharjee; Dip, Sajib Acharjee; Vaskar, Nirvar Roy; Al Islam, A. B. M. Alim","Assessing ChatGPT’s Code Generation Capabilities with Short vs Long Context Programming Problems","Proceedings of the 11th International Conference on Networking, Systems, and Security","979-8-4007-1158-9","","10.1145/3704522.3704535","https://doi.org/10.1145/3704522.3704535","This study assesses the code generation capabilities of ChatGPT using competitive programming problems from platforms such as LeetCode, HackerRank, and UVa Online Judge. In a novel approach, we contrast ChatGPT’s performance on concise problems from LeetCode against more complex, narrative-driven problems from Codeforces. Our results reveal significant challenges in addressing the intricate narrative structures of Codeforces, with difficulties in problem recognition and strategic planning in extended contexts. While initial code accuracy for LeetCode problems stands at 72%, it drops to 31% for complex Codeforces problems using Python. Additionally, we explore the impact of targeted instructions aimed at enhancing performance, which increased LeetCode accuracy to 73.53% but saw a decrease in Codeforces performance to 29%. Our analysis further extends across multiple programming languages, examining if iterative prompting and specific feedback can enhance code precision and efficiency. We also delve into ChatGPT’s performance on challenging problems and those released post its training period. This research provides insights into the strengths and weaknesses of AI in code generation and lays groundwork for future developments in AI-driven coding tools.","2025","2025-11-25 22:29:42","2025-11-25 22:29:42","","32–40","","","","","","","NSysS '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","ChatGPT; Prompt Engineering; Code Generation; Performance Evaluation; AI-Assisted Programming; Competitive Programming; Language Models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZH8STRES","conferencePaper","2024","Klemmer, Jan H.; Horstmann, Stefan Albert; Patnaik, Nikhil; Ludden, Cordelia; Burton, Jr., Cordell; Powers, Carson; Massacci, Fabio; Rahman, Akond; Votipka, Daniel; Lipford, Heather Richter; Rashid, Awais; Naiakshina, Alena; Fahl, Sascha","Using AI Assistants in Software Development: A Qualitative Study on Security Practices and Concerns","Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security","979-8-4007-0636-3","","10.1145/3658644.3690283","https://doi.org/10.1145/3658644.3690283","Following the recent release of AI assistants, such as OpenAI's ChatGPT and GitHub Copilot, the software industry quickly utilized these tools for software development tasks, e.g., generating code or consulting AI for advice. While recent research has demonstrated that AI-generated code can contain security issues, how software professionals balance AI assistant usage and security remains unclear. This paper investigates how software professionals use AI assistants in secure software development, what security implications and considerations arise, and what impact they foresee on security in software development. We conducted 27 semi-structured interviews with software professionals, including software engineers, team leads, and security testers. We also reviewed 190 relevant Reddit posts and comments to gain insights into the current discourse surrounding AI assistants for software development. Our analysis of the interviews and Reddit posts finds that, despite many security and quality concerns, participants widely use AI assistants for security-critical tasks, e.g., code generation, threat modeling, and vulnerability detection. Participants' overall mistrust leads to checking AI suggestions in similar ways to human code. However, they expect improvements and, therefore, a heavier use of AI for security tasks in the future. We conclude with recommendations for software professionals to critically check AI suggestions, for AI creators to improve suggestion security and capabilities for ethical security tasks, and for academic researchers to consider general-purpose AI in software development.","2024","2025-11-25 22:29:42","2025-11-25 22:29:42","","2726–2740","","","","","","","CCS '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Salt Lake City, UT, USA","","","","large language models; llm; generative ai; software development; ai assistants; interviews; software security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3CYXS4LP","journalArticle","2025","Wang, Ziliang; Li, Ge; Li, Jia; Li, Jia; Yan, Meng; Xiong, Yingfei; Jin, Zhi","M2CVD: Enhancing Vulnerability Understanding through Multi-Model Collaboration for Code Vulnerability Detection","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3771923","https://doi.org/10.1145/3771923","Large Language Models (LLMs) have strong capabilities in code comprehension, but fine-tuning costs and semantic alignment issues limit their project-specific optimization; conversely, fine-tuned models such as CodeBERT are easy to fine-tune, but it is often difficult to learn vulnerability semantics from complex code languages. To address these challenges, this paper introduces the Multi-Model Collaborative Vulnerability Detection approach (M2CVD) that leverages the strong capability of analyzing vulnerability semantics from LLMs to improve the detection accuracy of fine-tuned models. M2CVD employs a novel collaborative process: first enhancing the quality of vulnerability description produced by LLMs through the understanding of project code by fine-tuned models, and then using these improved vulnerability descriptions to boost the detection accuracy of fine-tuned models. M2CVD include three main phases: 1) Initial Vulnerability Detection: The initial vulnerability detection is conducted by fine-tuning a detection model (e.g., CodeBERT) and interacting with an LLM (e.g., ChatGPT) respectively. The vulnerability description will be generated by the LLM when the code is detected vulnerable by the LLM. 2) Vulnerability Description Refinement: By informing the LLM of the vulnerability assessment results of the detection model, we refine the vulnerability description by interacting with the LLM. Such refinement can enhance LLM’s vulnerability understanding in specific projects, effectively bridging the previously mentioned alignment gap; 3) Integrated Vulnerability Detection: M2CVD integrates code fragment and the refined vulnerability descriptions inferred to form synthetic data. Then, the synthetic data is used to fine-tune a validation model, optimize the defect feature learning efficiency of the model, and improve the detection accuracy. We demonstrated M2CVD’s effectiveness on two real-world datasets, where M2CVD significantly outperformed the baseline. In addition, we demonstrate that the M2CVD collaborative method can extend to other different LLMs and fine-tuned models to improve their accuracy in vulnerability detection tasks.","2025-10","2025-11-25 22:29:42","2025-11-25 22:29:42","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large language model; Model collaboration; Pre-trained models; Vulnerability detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5F3Z6LXK","conferencePaper","2025","Kong, Fanshuang; Zhang, Richong; Guo, Xiaohui; Chen, Junfan; Wang, Ziqiao","Preserving Label Correlation for Multi-label Text Classification by Prototypical Regularizations","Proceedings of the ACM on Web Conference 2025","979-8-4007-1274-6","","10.1145/3696410.3714797","https://doi.org/10.1145/3696410.3714797","Multi-label text classification (MLTC) assigns multiple labels to a sentence, with the key challenge being capturing label correlations. Existing models prioritize leveraging correlations but often overlook overfitting, while plug-and-play regularization methods fail to preserve correlations effectively. In this paper, we distinguish two types of label correlations: explicit co-occurring correlations and implicit semantic correlations, and propose regularizations on prototypical label embeddings for correlation preservation. Specifically, we first generate the prototypical embedding of multiple co-occurred labels as an intermediate. We then apply a prototypical regularization on the distance between the sentence embedding and corresponding prototypical embedding to alleviate the over-alignment issue caused by binary cross entropy loss and facilitate explicit correlation preservation. We finally extend the vanilla Mixup, which solely mixes multi-hot labels, on prototypical embedding mixing to promote implicit correlation preservation. Empirical studies show the effectiveness of our regularization methods.","2025","2025-11-25 22:29:42","2025-11-25 22:29:42","","3300–3310","","","","","","","WWW '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sydney NSW, Australia","","","","mixup; multi-label text classification; prototypical label","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NKZURFGB","journalArticle","2024","Dong, Yihong; Jiang, Xue; Jin, Zhi; Li, Ge","Self-Collaboration Code Generation via ChatGPT","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3672459","https://doi.org/10.1145/3672459","Although large language models (LLMs) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing LLMs, exemplified by ChatGPT. Specifically, through role instructions, (1) Multiple LLM agents act as distinct “experts,” each responsible for a specific subtask within a complex task; (2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other’s work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three LLM roles (i.e., analyst, coder, and tester) responsible for software development’s analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9–47.1% Pass@1 compared to the base LLM agent. Moreover, we showcase that self-collaboration could potentially enable LLMs to efficiently handle complex repository-level tasks that are not readily solved by the single LLM agent.","2024-09","2025-11-25 22:29:42","2025-11-25 22:29:42","","","","7","33","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","large language models; software development; Code generation; multi-agent collaboration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XEN4CJ54","conferencePaper","2025","Hossain, Soneya Binta; Dwyer, Matthew B.","TOGLL: Correct and Strong Test Oracle Generation with LLMs","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00098","https://doi.org/10.1109/ICSE55347.2025.00098","Test oracles play a crucial role in software testing, enabling effective bug detection. Despite initial promise, neural methods for automated test oracle generation often result in a large number of false positives and weaker test oracles. While LLMs have shown impressive effectiveness in various software engineering tasks, including code generation, test case creation, and bug fixing, there remains a notable absence of large-scale studies exploring their effectiveness in test oracle generation. The question of whether LLMs can address the challenges in effective oracle generation is both compelling and requires thorough investigation.In this research, we present the first comprehensive study to investigate the capabilities of LLMs in generating correct, diverse, and strong test oracles capable of effectively identifying a large number of unique bugs. To this end, we fine-tuned seven code LLMs using six distinct prompts on a large dataset consisting of 110 Java projects. Utilizing the most effective fine-tuned LLM and prompt pair, we introduce TOGLL, a novel LLM-based method for test oracle generation. To investigate the generalizability of TOGLL, we conduct studies on 25 unseen large-scale Java projects. Besides assessing the correctness, we also assess the diversity and strength of the generated oracles. We compare the results against EvoSuite and the state-of-the-art neural method, TOGA. Our findings reveal that TOGLL can produce 3.8 times more correct assertion oracles and 4.9 times more exception oracles than TOGA. Regarding bug detection effectiveness, TOGLL can detect 1,023 unique mutants that EvoSuite cannot, which is ten times more than what TOGA can detect. Additionally, TOGLL significantly outperforms TOGA in detecting real bugs from the Defects4J dataset.","2025","2025-11-25 22:29:42","2025-11-25 22:48:06","","1475–1487","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","Large language models; Software testing; Java; Software engineering; Accuracy; Codes; Computer bugs; llms; bug detection effectiveness; evosuite; test oracle generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QAWYLQL6","conferencePaper","2025","Clear, Tony; Cajander, Åsa; Clear, Alison; McDermott, Roger; Daniels, Mats; Divitini, Monica; Forshaw, Matthew; Humble, Niklas; Kasinidou, Maria; Kleanthous, Styliani; Kultur, Can; Parvini, Ghazaleh; Polash, Mohammad; Zhu, Tingting","AI Integration in the IT Professional Workplace: A Scoping Review and Interview Study with Implications for Education and Professional Competencies","2024 Working Group Reports on Innovation and Technology in Computer Science Education","979-8-4007-1208-1","","10.1145/3689187.3709607","https://doi.org/10.1145/3689187.3709607","As Artificial Intelligence (AI) continues transforming workplaces globally, particularly within the Information Technology (IT) industry, understanding its impact on IT professionals and computing curricula is crucial. This research builds on joint work from two countries, addressing concerns about AI's increasing influence in IT sector workplaces and its implications for tertiary education. The study focuses on AI technologies such as generative AI (GenAI) and large language models (LLMs). It examines how they are perceived and adopted and their effects on workplace dynamics, task allocation, and human-system interaction.IT professionals, noted as early adopters of AI, offer valuable insights into the interplay between AI and work engagement, highlighting the significant competencies required for digital workplaces. This study employs a dual-method approach, combining a systematic and multi-vocal literature review and qualitative research methods. These included a thematic analysis of a set of 47 interviews conducted between March and May of 2024 with IT professionals in two countries (New Zealand and Sweden). The research aimed to understand the implications for computing students, education curricula, and the assessment of emerging professional competencies.The literature review found insufficient evidence addressing comprehensive AI practice methodologies, highlighting the need to both develop and regulate professional competencies for effective AI integration. Key interview findings revealed diverse levels of GenAI adoption, ranging from individual experimentation to institutional integration. Participants generally expressed positive attitudes toward the technology and were actively pursuing self-learning despite some concerns. The themes emerging from the interviews included AI's role in augmenting human tasks, privacy and security concerns, productivity enhancements, legal and ethical challenges, and the evolving need for new competencies in the workplace.The study underscores the critical role of competency frameworks in guiding professional development and ensuring preparedness for an AI-driven environment. Additionally, it highlights the need for educational institutions to adapt curricula to address these emerging demands effectively","2025","2025-11-25 22:29:42","2025-11-25 22:29:42","","34–67","","","","","","","ITiCSE 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Milan, Italy","","","","large language models; artificial intelligence; generative ai; computing competencies; computing curricula; it profession","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9PKG2KVY","conferencePaper","2024","Fei, Haoxiang; Zhang, Yu; Zhang, Hongbo; Wang, Yanlin; Liu, Qing","MoonBit: Explore the Design of an AI-Friendly Programming Language","Proceedings of the 1st International Workshop on Large Language Models for Code","979-8-4007-0579-3","","10.1145/3643795.3648376","https://doi.org/10.1145/3643795.3648376","MoonBit, a new general-purpose programming language designed for cloud and edge computing, was initiated in late 2022, coinciding with the announcement of ChatGPT. Language models like GPT, capable of producing practical programs, are revolutionizing the way we write programs and interact with computers. However, significant challenges persist, such as the models' inability to understand the global context of a whole project with its dependencies, the need for human verification and correction of generated code, and the lack of assurance in meeting basic requirements like syntactic correctness.In this paper, we explore the design of the MoonBit language highlighting its AI integration, emphasizing the synergy between traditional code intelligence and large language model capabilities. We also introduce a real-time, semantics-based sampler to guide the inference process of language models. This approach ensures the generated programs are both syntactically correct and free from obvious semantic flaws, such as type errors. Crucially, this has been achieved with minimal impact on overall performance. Our evaluation demonstrates a notable improvement in code quality, achieved without sacrificing the models' responsiveness.","2024","2025-11-25 22:29:42","2025-11-25 22:29:42","","79–83","","","","","","","LLM4Code '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","large language model; static analysis; program synthesize","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DU2K83LN","conferencePaper","2025","Ganiyu, Abiodun; Gajjar, Pranshav; Shah, Vijay K","DEMO: AI5GTest: LLM based Automation for 5G O-RAN Testing","18th ACM Conference on Security and Privacy in Wireless and Mobile Networks","979-8-4007-1530-3","","10.1145/3734477.3736154","https://doi.org/10.1145/3734477.3736154","The transition to Open Radio Access Networks (O-RAN) introduces testing challenges due to multi-vendor interoperability requirements, with existing manual frameworks being error-prone and unscalable. To address this, we propose AI5GTest, an AI-driven framework that automates O-RAN component testing using cooperative Large Language Models (LLMs). Gen-LLM generates test flows from O-RAN/3GPP specifications, Val-LLM validates signaling compliance, and Debug-LLM diagnoses failures. A human-in-the-loop mechanism ensures transparency by verifying specifications before validation. Evaluated on 24 test cases with the srsRAN 5G stack, AI5GTest reduces test execution time significantly compared to manual methods while maintaining high accuracy, demonstrating scalable, trustworthy automation for O-RAN ecosystems.","2025","2025-11-25 22:29:42","2025-11-25 22:29:42","","298–299","","","","","","","WiSec 2025","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Arlington, VA, USA","","","","llm; automated testing; o-ran","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"58CDFFHN","journalArticle","2025","Chen, Xiang; Gao, Chaoyang; Chen, Chunyang; Zhang, Guangbei; Liu, Yong","An Empirical Study on Challenges for LLM Application Developers","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3715007","https://doi.org/10.1145/3715007","In recent years, large language models (LLMs) have seen rapid advancements, significantly impacting various fields such as computer vision, natural language processing, and software engineering. These LLMs, exemplified by OpenAI's ChatGPT, have revolutionized the way we approach language understanding and generation tasks. However, in contrast to traditional software development practices, LLM development introduces new challenges for AI developers in design, implementation, and deployment. These challenges span different areas (such as prompts, APIs, and plugins), requiring developers to navigate unique methodologies and considerations specific to LLM application development.Despite the profound influence of LLMs, to the best of our knowledge, these challenges have not been thoroughly investigated in previous empirical studies. To fill this gap, we present the first comprehensive study on understanding the challenges faced by LLM developers. Specifically, we crawl and analyze 29,057 relevant questions from a popular OpenAI developer forum. We first examine their popularity and difficulty. After manually analyzing 2,364 sampled questions, we construct a taxonomy of challenges faced by LLM developers. Based on this taxonomy, we summarize a set of findings and actionable implications for LLM-related stakeholders, including developers and providers (especially the OpenAI organization).","2025-08","2025-11-25 22:29:42","2025-11-25 22:29:42","","","","7","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Empirical Study; Prompt Engineering; Development Challenges; LLM Developer; Mining Software Repository","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"74PGUSIG","journalArticle","2025","Tang, Lingxiao; Liu, Jiakun; Liu, Zhongxin; Yang, Xiaohu; Bao, Lingfeng","LLM4SZZ: Enhancing SZZ Algorithm with Context-Enhanced Assessment on Large Language Models","Proc. ACM Softw. Eng.","","","10.1145/3728885","https://doi.org/10.1145/3728885","The SZZ algorithm is the dominant technique for identifying bug-inducing commits and serves as a foundation for many software engineering studies, such as bug prediction and static code analysis, thereby enhancing software quality and facilitating better maintenance practices. Researchers have proposed many variants to enhance the SZZalgorithm’s performance since its introduction. The majority of them rely on static techniques or heuristic assumptions, making them easy to implement, but their performance improvements are often limited. Recently, a deep learning-based SZZ algorithm has been introduced to enhance the original SZZ algorithm. However, it requires complex preprocessing and is restricted to a single programming language. Additionally, while it enhances precision, it sacrifices recall. Furthermore, most of variants overlook crucial information, such as commit messages and patch context, and are limited to bug-fixing commits involving deleted lines. The emergence of large language models (LLMs) offers an opportunity to address these drawbacks. In this study, we investigate the strengths and limitations of LLMs and propose LLM4SZZ, which employs two approaches (i.e., rank-based identification and context-enhanced identification) to handle different types of bug-fixing commits. We determine which approach to adopt based on the LLM’s ability to comprehend the bug and identify whether the bug is present in a commit. The context-enhanced identification provides the LLM with more context and requires it to find the bug-inducing commit among a set of candidate commits. In rank-based identification, we ask the LLM to select buggy statements from the bug-fixing commit and rank them based on their relevance to the root cause. Experimental results show that LLM4SZZ outperforms all baselines across three datasets, improving F1-score by 6.9% to 16.0% without significantly sacrificing recall. Additionally, LLM4SZZ can identify many bug-inducing commits that the baselines fail to detect, accounting for 7.8%, 7.4% and 2.5% of the total bug-inducing commits across three datasets, respectively.","2025-06","2025-11-25 22:29:42","2025-11-25 22:29:42","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","large language model; SZZ Algorithm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RIKIFMB3","journalArticle","2025","Li, Qingyuan; Yin, Zhixin; Yang, Yaopeng; Li, Chuanyi; Shen, Zongwen; Ge, Jidong; Zhong, Wenkang; Luo, Bin; Ng, Vincent","IMPACT: Identifying and Classifying Multiple Sourced and Categorized Self-Admitted Technical Debts","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3747180","https://doi.org/10.1145/3747180","Self-Admitted Technical Debt (SATD) refers to suboptimal solutions deliberately introduced to accelerate the software development process, often at the expense of software maintainability and sustainability. Therefore, timely identification and repayment of the SATD is critical for the software system. As exploration deepens, it is found that effectively prioritizing the repayment of SATD with more significant impacts on software quality requires not only identifying SATD but also further classifying it. However, existing SATD identification and classification approaches face the following challenges: (1) SATDs originate from diverse sources. Code comments are a widespread source, but recent research has revealed that SATDs can originate from other sources, such as pull requests, issues, and commit messages. Nonetheless, existing approaches primarily target code comments, lacking the capability to analyze SATDs from other sources effectively. (2) SATDs fall into diverse categories. Nonetheless, existing SATD classification approaches fail to address all SATD categories comprehensively and show inadequate performance. (3) Imbalance of existing SATD datasets. Real-world SATD data is scarce, making dataset collection challenging. Moreover, SATD distribution across different sources is uneven, further complicating the construction of high-quality datasets. To alleviate these challenges, this paper presents an SATD identification and classification framework named IMPACT. First, IMPACT employs ChatGPT to construct an augmented dataset. Subsequently, it utilizes a pipeline with two fine-tuned language models of different parameter sizes to identify and classify SATD separately. To evaluate the effectiveness of IMPACT, we compare it with three state-of-the-art SATD classification methods and its two foundation models. Experimental results demonstrate that IMPACT outperforms state-of-the-art methods by a large margin, even surpasses its foundation model GLM-4-9B-Chat. It achieves the optimal average F1 score of 0.697 on the source of pull requests, the most challenging data source. Moreover, experiments on the cross-project test set show that IMPACT demonstrates strong generalizability on unseen project data.","2025-07","2025-11-25 22:29:43","2025-11-25 22:29:43","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Data Augmentation; Mixture of Experts; SATD Identification and Classification; Self-Admitted Technical Debt","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"37R8RQ3Q","conferencePaper","2025","Alrabie, Lina; Andolina, Salvatore","Towards Human-Centered RAG: A Study of AI-Supported Testing Practices in Italian Public Administration","Proceedings of the 16th Biannual Conference of the Italian SIGCHI Chapter","979-8-4007-2102-1","","10.1145/3750069.3750103","https://doi.org/10.1145/3750069.3750103","This study examines the integration of a Retrieval-Augmented Generation (RAG)–based AI testing assistant within an Italian public administration, focusing on its practical benefits and challenges. Qualitative feedback from 15 software testers reveals that while the assistant improves testing efficiency and supports knowledge retrieval, challenges regarding accuracy, consistency, and transparency reduce user trust and effectiveness. Participants highlighted the need for enhanced data curation, improved interaction, and finer-grained control over the system, indicating areas for future improvements to the assistant and its surrounding workflows. The findings emphasize the importance of adopting a human-centered approach in the design and integration of AI solutions in the public sector, offering valuable insights for the ongoing development of human-centered RAG research.","2025","2025-11-25 22:29:43","2025-11-25 22:29:43","","","","","","","","","CHItaly '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Software testing; LLM; Artificial intelligence; Large Language Models; Retrieval Augmented Generation; HCAI; Human-centered AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VVSV4ND2","journalArticle","2025","Zheng, Mingwei; Xie, Danning; Shi, Qingkai; Wang, Chengpeng; Zhang, Xiangyu","Validating Network Protocol Parsers with Traceable RFC Document Interpretation","Proc. ACM Softw. Eng.","","","10.1145/3728955","https://doi.org/10.1145/3728955","Validating the correctness of network protocol implementations is highly challenging due to the oracle and traceability problems. The former determines when a protocol implementation can be considered buggy, especially when the bugs do not cause any observable symptoms. The latter allows developers to understand how an implementation violates the protocol specification, thereby facilitating bug fixes. Unlike existing works that rarely take both problems into account, this work considers both and provides an effective solution using recent advances in large language models (LLMs). Our key observation is that network protocols are often released with structured specification documents, a.k.a. RFC documents, which can be systematically translated to formal protocol message specifications via LLMs. Such specifications, which may contain errors due to the hallucination of LLMs, are used as a quasi-oracle to validate protocol parsers, while the validation results in return gradually refine the oracle. Since the oracle is derived from the document, any bugs we find in a protocol implementation can be traced back to the document, thus addressing the traceability problem. We have extensively evaluated our approach using nine network protocols and their implementations written in C, Python, and Go. The results show that our approach outperforms the state-of-the-art and has detected 69 bugs, with 36 confirmed. The project also demonstrates the potential for fully automating software validation based on natural language specifications, a process previously considered predominantly manual due to the need to understand specification documents and derive expected outputs for test inputs.","2025-06","2025-11-25 22:29:43","2025-11-25 22:29:43","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large language model; Traceability; Network protocol parsers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EE6NDCIU","journalArticle","2025","Bouzenia, Islem; Pradel, Michael","You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary Projects","Proc. ACM Softw. Eng.","","","10.1145/3728922","https://doi.org/10.1145/3728922","The ability to execute the test suite of a project is essential in many scenarios, e.g., to assess code quality and code coverage, to validate code changes made by developers or automated tools, and to ensure compatibility with dependencies. Despite its importance, executing the test suite of a project can be challenging in practice because different projects use different programming languages, software ecosystems, build systems, testing frameworks, and other tools. These challenges make it difficult to create a reliable, universal test execution method that works across different projects. This paper presents ExecutionAgent, an automated technique that prepares scripts for building an arbitrary project from source code and running its test cases. Inspired by the way a human developer would address this task, our approach is a large language model (LLM)-based agent that autonomously executes commands and interacts with the host system. The agent uses meta-prompting to gather guidelines on the latest technologies related to the given project, and it iteratively refines its process based on feedback from the previous steps. Our evaluation applies ExecutionAgent to 50 open-source projects that use 14 different programming languages and many different build and testing tools. The approach successfully executes the test suites of 33/50 projects, while matching the test results of ground truth test suite executions with a deviation of only 7.5%. These results improve over the best previously available technique by 6.6x. The costs imposed by the approach are reasonable, with an execution time of 74 minutes and LLM costs of USD&nbsp;0.16, on average per project. We envision ExecutionAgent to serve as a valuable tool for developers, automated programming tools, and researchers that need to execute tests across a wide variety of projects.","2025-06","2025-11-25 22:29:43","2025-11-25 22:29:43","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","LLM agents; autonomous software development; project setup automation; test suite execution","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KKMYIBR8","bookSection","2025","Larsen, Knud Ronau; Edvall, Magnus; Ho-Quang, Truong; Dobslaw, Felix; Jolak, Rodi","An LLM Assistant for Software Project Onboarding","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","","https://doi.org/10.1145/3696630.3728719","New developers commonly join software teams for projects in the advanced stages of development. They frequently face barriers, especially in software comprehension, that impede their ability to make early contributions. To aid newcomers in understanding software projects, we have developed an LLM-based tool called SPAC-B that can answer software project-specific questions. In this study, a case study is conducted to investigate the accuracy of SPAC-B's answers to common developer questions when resolving issues on two open-source projects regarding relevance, completeness, and correctness. On a Likert scale (1–5), answers from SPAC-B scored a mean of 4.6 in relevance and 4.3 in completeness and correctness. An experiment with ten software developers is performed to further examine SPAC-B's ability to help newcomers formulate plans for resolving real-life open-source issues. Results show that the participants could make a better implementation plan with the use of SPAC-B and 8/10 participants found the tool to be helpful.","2025","2025-11-25 22:29:43","2025-11-25 22:29:43","","1345–1352","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6XQYBAEN","conferencePaper","2025","Kim, Myeongsoo; Stennett, Tyler; Sinha, Saurabh; Orso, Alessandro","A Multi-Agent Approach for REST API Testing with Semantic Graphs and LLM-Driven Inputs","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00179","https://doi.org/10.1109/ICSE55347.2025.00179","As modern web services increasingly rely on REST APIs, their thorough testing has become crucial. Furthermore, the advent of REST API documentation languages, such as the OpenAPI Specification, has led to the emergence of many black-box REST API testing tools. However, these tools often focus on individual test elements in isolation (e.g., APIs, parameters, values), resulting in lower coverage and less effectiveness in fault detection. To address these limitations, we present AutoRestTest, the first black-box tool to adopt a dependency-embedded multi-agent approach for REST API testing that integrates multi-agent reinforcement learning (MARL) with a semantic property dependency graph (SPDG) and Large Language Models (LLMs). Our approach treats REST API testing as a separable problem, where four agents—API, dependency, parameter, and value agents—collaborate to optimize API exploration. LLMs handle domain-specific value generation, the SPDG model simplifies the search space for dependencies using a similarity score between API operations, and MARL dynamically optimizes the agents' behavior. Our evaluation of AutoRestTest on 12 real-world REST services shows that it outperforms the four leading black-box REST API testing tools, including those assisted by RESTGPT (which generates realistic test inputs using LLMs), in terms of code coverage, operation coverage, and fault detection. Notably, AutoRestTest is the only tool able to trigger an internal server error in the Spotify service. Our ablation study illustrates that each component of AutoRestTest—the SPDG, the LLM, and the agent-learning mechanism—contributes to its overall effectiveness.","2025","2025-11-25 22:29:43","2025-11-25 22:46:39","","1409–1421","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","Large language models; Semantics; Software engineering; Testing; Fault detection; automated REST API testing; multi-agent reinforcement learning for testing; Automated REST API Testing; Reinforcement learning; Documentation; Closed box; Web services; Servers; Multi-Agent Reinforcement Learning for Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S3KS3ETM","conferencePaper","2024","Lin, Dayi; Cogo, Filipe Roseiro; Rajbahadur, Gopi Krishnan; Hassan, Ahmed E.","Technical Brief on Software Engineering for FMware","Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings","979-8-4007-0502-1","","10.1145/3639478.3643062","https://doi.org/10.1145/3639478.3643062","Foundation Models (FM) like GPT-4 have given rise to FMware, FM-powered applications, which represent a new generation of software that is developed with new roles, assets, and paradigms. FMware has been widely adopted in both software engineering (SE) research (e.g., test generation) and industrial products (e.g., GitHub copilot), despite the numerous challenges introduced by the stochastic nature of FMs. Such challenges jeopardize the quality and trustworthiness of FMware. In our technical brief, we will present the latest research and industrial practices in engineering FMware, and discuss the SE challenges and opportunities facing both researchers and practitioners in the FMware era.The brief is unique in that it is presented from an SE point of view, not an AI point-of-view ensuring that attendees are not bogged into complex mathematical and AI details unless they are essential for contextualizing the SE challenges and opportunities.","2024","2025-11-25 22:29:43","2025-11-25 22:47:57","","431–433","","","","","","","ICSE-Companion '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","Test pattern generators; Software; Software engineering; Artificial intelligence; FMware; foundation model; software engineering for FMware; Foundation Model; Software engineering for FMware; Mathematical models; Frequency modulation; Stochastic processes","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZXTZCR7S","conferencePaper","2024","Feng, Sidong; Chen, Chunyang","Prompting Is All You Need: Automated Android Bug Replay with Large Language Models","Proceedings of the IEEE/ACM 46th International Conference on Software Engineering","979-8-4007-0217-4","","10.1145/3597503.3608137","https://doi.org/10.1145/3597503.3608137","Bug reports are vital for software maintenance that allow users to inform developers of the problems encountered while using the software. As such, researchers have committed considerable resources toward automating bug replay to expedite the process of software maintenance. Nonetheless, the success of current automated approaches is largely dictated by the characteristics and quality of bug reports, as they are constrained by the limitations of manually-crafted patterns and pre-defined vocabulary lists. Inspired by the success of Large Language Models (LLMs) in natural language understanding, we propose AdbGPT, a new lightweight approach to automatically reproduce the bugs from bug reports through prompt engineering, without any training and hard-coding effort. AdbGPT leverages few-shot learning and chain-of-thought reasoning to elicit human knowledge and logical reasoning from LLMs to accomplish the bug replay in a manner similar to a developer. Our evaluations demonstrate the effectiveness and efficiency of our AdbGPT to reproduce 81.3% of bug reports in 253.6 seconds, outperforming the state-of-the-art baselines and ablation studies. We also conduct a small-scale user study to confirm the usefulness of AdbGPT in enhancing developers' bug replay capabilities.","2024","2025-11-25 22:29:43","2025-11-25 22:29:43","","","","","","","","","ICSE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","large language model; prompt engineering; automated bug replay","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2EU3B9WZ","conferencePaper","2025","Ma, Xiaoyue; Chen, Junming; Luo, Lannan; Zeng, Qiang","LLM-Assisted IoT Testing: Finding Conformance Bugs in Matter SDKs","Proceedings of the 31st Annual International Conference on Mobile Computing and Networking","979-8-4007-1129-9","","10.1145/3680207.3765257","https://doi.org/10.1145/3680207.3765257","Matter is an IoT standard endorsed by hundreds of companies, designed to ensure interoperability between devices from various vendors. The Matter Software Development Kit (SDK) serves as the foundation for developing Matter devices, making bug discovery in Matter SDKs crucial. Given the extensive specification and the rapid evolution of Matter—five versions released in just two and a half years—the need for automated solutions is increasingly urgent. In this paper, we present MatterGuard, the first automated system for identifying bugs in Matter SDKs that violate the specification. Unlike traditional SDK testing approaches, which typically integrate testing code with the SDK code, Matter-Guard decouples the two, allowing the testing code to be reused across SDK versions. Furthermore, MatterGuard leverages a large language model to analyze the Matter specification and uses the extracted knowledge to guide the bug discovery process. In our evaluation across all five SDK versions, MatterGuard uncovers 109 bugs, demonstrating the effectiveness and scalability of our approach.","2025","2025-11-25 22:29:43","2025-11-25 22:29:43","","953–968","","","","","","","ACM MOBICOM '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Kerry Hotel, Hong Kong, Hong Kong, China","","","","program analysis; bug discovery; IoT security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WBU2IUPB","conferencePaper","2025","Zhu, Jiajun; Cheng, Xinyu; Luo, Zhongsu; Zhou, Yunfan; Shu, Xinhuan; Weng, Di; Wu, Yingcai","ViseGPT: Towards Better Alignment of LLM-generated Data Wrangling Scripts and User Prompts","Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology","979-8-4007-2037-6","","10.1145/3746059.3747689","https://doi.org/10.1145/3746059.3747689","Large language models (LLMs) enable the rapid generation of data-wrangling scripts based on natural language instructions, but these scripts may not fully adhere to user-specified requirements, necessitating careful inspection and iterative refinement. Existing approaches primarily assist users in understanding script logic and spotting potential issues themselves, rather than providing direct validation of correctness. To enhance debugging efficiency and optimize the user experience, we develop ViseGPT, a tool that automatically extracts constraints from user prompts to generate comprehensive test cases for verifying script reliability. The test results are then transformed into a tailored Gantt chart, allowing users to intuitively assess alignment with semantic requirements and iteratively refine their scripts. Our design decisions are informed by a formative study (N=8) that explores user practices and challenges. We further evaluate the effectiveness and usability of ViseGPT through a user study (N=18). Results indicate that ViseGPT significantly improves debugging efficiency for LLM-generated data-wrangling scripts, enhances users’ ability to detect and correct issues, and streamlines the workflow experience.","2025","2025-11-25 22:29:43","2025-11-25 22:29:43","","","","","","","","","UIST '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Human-AI Interaction; Data Wrangling Scripts; Debugging Support","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LCC7BVGG","journalArticle","2025","Terragni, Valerio; Vella, Annie; Roop, Partha; Blincoe, Kelly","The Future of AI-Driven Software Engineering","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3715003","https://doi.org/10.1145/3715003","A paradigm shift is underway in Software Engineering, with AI systems such as LLMs playing an increasingly important role in boosting software development productivity. This trend is anticipated to persist. In the next years, we expect a growing symbiotic partnership between human software developers and AI. The Software Engineering research community cannot afford to overlook this trend; we must address the key research challenges posed by the integration of AI into the software development process. In this article, we present our vision of the future of software development in an AI-driven world and explore the key challenges that our research community should address to realize this vision.","2025-05","2025-11-25 22:29:43","2025-11-25 22:29:43","","","","5","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Machine Learning; Large Language Models; Software Testing; Requirements Engineering; Software Engineering; Artificial Intelligence; APIs; Libraries","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JEX6DFUH","conferencePaper","2024","Alshahwan, Nadia; Chheda, Jubin; Finogenova, Anastasia; Gokkaya, Beliz; Harman, Mark; Harper, Inna; Marginean, Alexandru; Sengupta, Shubho; Wang, Eddy","Automated Unit Test Improvement using Large Language Models at Meta","Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering","979-8-4007-0658-5","","10.1145/3663529.3663839","https://doi.org/10.1145/3663529.3663839","This paper describes Meta’s TestGen-LLM tool, which uses LLMs to automatically improve existing human-written tests. TestGen-LLM verifies that its generated test classes successfully clear a set of filters that assure measurable improvement over the original test suite, thereby eliminating problems due to LLM hallucination. We describe the deployment of TestGen-LLM at Meta test-a-thons for the Instagram and Facebook platforms. In an evaluation on Reels and Stories products for Instagram, 75% of TestGen-LLM’s test cases built correctly, 57% passed reliably, and 25% increased coverage. During Meta’s Instagram and Facebook test-a-thons, it improved 11.5% of all classes to which it was applied, with 73% of its recommendations being accepted for production deployment by Meta software engineers. We believe this is the first report on industrial scale deployment of LLM-generated code backed by such assurances of code improvement.","2024","2025-11-25 22:29:43","2025-11-25 22:29:43","","185–196","","","","","","","FSE 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","Large Language Models; Automated Test Generation; LLMs; Unit Testing; Genetic Improvement","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N2E5ULDR","conferencePaper","2024","Pang, Ashley; Vahid, Frank","ChatGPT and Cheat Detection in CS1 Using a Program Autograding System","Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1","979-8-4007-0600-4","","10.1145/3649217.3653558","https://doi.org/10.1145/3649217.3653558","We experimented with ChatGPT's ability to write programs in a CS1 class, and the ability of a popular tool to auto-detect ChatGPT-written programs. We found ChatGPT was proficient at generating correct programs from a mere copy-paste of the English programming assignment specifications. However, running ChatGPT for 10 programming assignments and acting as 20 different students, and using zyBook's APEX beta tool for academic integrity, we found: (1) ChatGPT-generated programs tend to use a programming style departing from the style taught in the textbook or by the instructor, and these ""style anomalies"" were automatically detected. (2) Although ChatGPT may for the same assignment generate a few different program solutions for different students, ChatGPT often generates highly-similar programs for different students, so if enough students in a class (e.g., 5 or more) use ChatGPT, their programs will likely be flagged by a similarity checker. (3) If students are required to do all programming in the autograder's IDE, then a student using ChatGPT ends up showing very little time relative to classmates, which is automatically flagged. (4) Manually, we observed that if a student consistently uses ChatGPT to submit programs, the programming style may vary across programs, something normal students don't do; automation of style inconsistency detection was recently added to APEX. In short, while there will no doubt be an arms race between AI-generated programs and automatic detection of AI-generated programs, currently students using ChatGPT for multiple CS1 programs can be detected by automated tools such as zyBooks' APEX.","2024","2025-11-25 22:29:43","2025-11-25 22:29:43","","367–373","","","","","","","ITiCSE 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Milan, Italy","","","","ChatGPT; large language models; CS1; academic integrity; cheat detection; plagiarism; similarity checking; style anomalies","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JW5NFEZB","conferencePaper","2024","Firouzi, Ehsan; Ghafari, Mohammad; Ebrahimi, Mike","ChatGPT’s Potential in Cryptography Misuse Detection: A Comparative Analysis with Static Analysis Tools","Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement","979-8-4007-1047-6","","10.1145/3674805.3695408","https://doi.org/10.1145/3674805.3695408","The correct adoption of cryptography APIs is challenging for mainstream developers, often resulting in widespread API misuse. Meanwhile, cryptography misuse detectors have demonstrated inconsistent performance and remain largely inaccessible to most developers. We investigated the extent to which ChatGPT can detect cryptography misuses and compared its performance with that of the state-of-the-art static analysis tools. Our investigation, mainly based on the CryptoAPI-Bench benchmark, demonstrated that ChatGPT is effective in identifying cryptography API misuses, and with the use of prompt engineering, it can even outperform leading static cryptography misuse detectors.","2024","2025-11-25 22:29:43","2025-11-25 22:29:43","","582–588","","","","","","","ESEM '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Barcelona, Spain","","","","ChatGPT; security; Java cryptography; static program analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PVU8MHEY","conferencePaper","2025","Fayaz, Avid; Glassey, Richard; Baltatzis, Alexander","Generating Personalized Assignments with Students in the Loop","Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 1","979-8-4007-1567-9","","10.1145/3724363.3729070","https://doi.org/10.1145/3724363.3729070","We present a system that enables students to generate their own programming assignments that are both personalized to their interests and aligned with the course objectives. A group of twelve undergraduate students randomly selected from an introductory programming course ( 200) received training and access to the system for six weeks. Students were free to use the system to generate assignments or take regular assignments instead. Weekly student feedback was used to refine the system through three iterations, and a focus group with three students was held after the course to gather opinions on the experience. Our findings are complicated and cautionary: students appreciated generating their own personalized assignments with feedback on demand. However, the cognitive load of having to generate and decide which assignment to complete, along with the variation of difficulty and alignment with learning objectives were noted as problematic and discouraging. Perhaps the most negative but heartwarming result is that students missed the community aspect of independently solving and collectively discussing solutions to a common assignment.","2025","2025-11-25 22:29:43","2025-11-25 22:29:43","","305–311","","","","","","","ITiCSE 2025","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Nijmegen, Netherlands","","","","generative ai; cs1; assignments; automation; feedback","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6ZX2R5S9","conferencePaper","2024","Tufano, Rosalia; Mastropaolo, Antonio; Pepe, Federica; Dabic, Ozren; Di Penta, Massimiliano; Bavota, Gabriele","Unveiling ChatGPT's Usage in Open Source Projects: A Mining-based Study","Proceedings of the 21st International Conference on Mining Software Repositories","979-8-4007-0587-8","","10.1145/3643991.3644918","https://doi.org/10.1145/3643991.3644918","Large Language Models (LLMs) have gained significant attention in the software engineering community. Nowadays developers have the possibility to exploit these models through industrial-grade tools providing a handy interface toward LLMs, such as OpenAI's ChatGPT. While the potential of LLMs in assisting developers across several tasks has been documented in the literature, there is a lack of empirical evidence mapping the actual usage of LLMs in software projects. In this work, we aim at filling such a gap. First, we mine 1,501 commits, pull requests (PRs), and issues from open-source projects by matching regular expressions likely to indicate the usage of ChatGPT to accomplish the task. Then, we manually analyze these instances, discarding false positives (i.e., instances in which ChatGPT was mentioned but not actually used) and categorizing the task automated in the 467 true positive instances (165 commits, 159 PRs, 143 issues). This resulted in a taxonomy of 45 tasks which developers automate via ChatGPT. The taxonomy, accompanied with representative examples, provides (i) developers with valuable insights on how to exploit LLMs in their workflow and (ii) researchers with a clear overview of tasks that, according to developers, could benefit from automated solutions.","2024","2025-11-25 22:29:43","2025-11-25 22:29:43","","571–583","","","","","","","MSR '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","ChatGPT; empirical study","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W4R3QALA","conferencePaper","2024","Rao, Nikitha; Jain, Kush; Alon, Uri; Goues, Claire Le; Hellendoorn, Vincent J.","CAT-LM Training Language Models on Aligned Code and Tests","Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering","979-8-3503-2996-4","","10.1109/ASE56229.2023.00193","https://doi.org/10.1109/ASE56229.2023.00193","Testing is an integral but often neglected part of the software development process. Classical test generation tools such as EvoSuite generate behavioral test suites by optimizing for coverage, but tend to produce tests that are hard to understand. Language models trained on code can generate code that is highly similar to that written by humans, but current models are trained to generate each file separately, as is standard practice in natural language processing, and thus fail to consider the code-under-test context when producing a test file. In this work, we propose the Aligned Code And Tests Language Model (CATLM), a GPT-style language model with 2.7 Billion parameters, trained on a corpus of Python and Java projects. We utilize a novel pretraining signal that explicitly considers the mapping between code and test files when available. We also drastically increase the maximum sequence length of inputs to 8,192 tokens, 4x more than typical code generation models, to ensure that the code context is available to the model when generating test code. We analyze its usefulness for realistic applications, showing that sampling with filtering (e.g., by compilability, coverage) allows it to efficiently produce tests that achieve coverage similar to ones written by developers while resembling their writing style. By utilizing the code context, CAT-LM generates more valid tests than even much larger language models trained with more data (CodeGen 16B and StarCoder) and substantially outperforms a recent test-specific model (TeCo) at test completion. Overall, our work highlights the importance of incorporating software-specific insights when training language models for code and paves the way to more powerful automated test generation.","2024","2025-11-25 22:29:43","2025-11-25 22:29:43","","409–420","","","","","","","ASE '23","","","","IEEE Press","Echternach, Luxembourg","","","","","","","","","","","","large language models; test generation; code-test alignment; test completion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RB9KREPE","conferencePaper","2025","Zhong, Renyi","Towards Quality Assurance of Natural Language in Code","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings","979-8-3315-3683-1","","10.1109/ICSE-Companion66252.2025.00056","https://doi.org/10.1109/ICSE-Companion66252.2025.00056","The quality of code in software development is critical to ensuring robust, functional, and maintainable applications. While static analysis techniques can improve code quality, they often overlook the natural language (NL) components of code, such as comments and logging statements, which play an essential role in program development and maintenance. In this thesis, we aim to address quality assurance for these NL elements through two primary objectives. First, we plan to propose a new benchmark to facilitate consistent comparisons among existing methods. Second, we tackle logging statement quality by identifying a broader range of defect types and introducing an automated detection and repair tool. This work addresses critical limitations in current research, which often focuses on single defect types and lacks repair functionality, by enabling a comprehensive and practical approach to NL quality in code. Our contributions provide a foundation for ongoing improvements in code maintainability and contribute to a more reliable software development lifecycle.","2025","2025-11-25 22:29:43","2025-11-25 22:29:43","","187–189","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","large language model; quality assurance; code comment; logging statement","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SQNY5D9M","conferencePaper","2023","Ahmad, Aakash; Waseem, Muhammad; Liang, Peng; Fahmideh, Mahdi; Aktar, Mst Shamima; Mikkonen, Tommi","Towards Human-Bot Collaborative Software Architecting with ChatGPT","Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering","979-8-4007-0044-6","","10.1145/3593434.3593468","https://doi.org/10.1145/3593434.3593468","Architecting software-intensive systems can be a complex process. It deals with the daunting tasks of unifying stakeholders’ perspectives, designers’ intellect, tool-based automation, pattern-driven reuse, and so on, to sketch a blueprint that guides software implementation and evaluation. Despite its benefits, architecture-centric software engineering (ACSE) suffers from a multitude of challenges. ACSE challenges could stem from a lack of standardized processes, socio-technical limitations, and scarcity of human expertise etc. that can impede the development of existing and emergent classes of software. Software Development Bots (DevBots) trained on large language models can help synergise architects’ knowledge with artificially intelligent decision support to enable rapid architecting in a human-bot collaborative ACSE. An emerging solution to enable this collaboration is ChatGPT, a disruptive technology not primarily introduced for software engineering, but is capable of articulating and refining architectural artifacts based on natural language processing. We detail a case study that involves collaboration between a novice software architect and ChatGPT to architect a service-based software. Future research focuses on harnessing empirical evidence about architects’ productivity and explores socio-technical aspects of architecting with ChatGPT to tackle challenges of ACSE.","2023","2025-11-25 22:29:43","2025-11-25 22:29:43","","279–285","","","","","","","EASE '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Oulu, Finland","","","","ChatGPT; Large Language Models; DevBots; Software Architecture","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FTEKESHJ","conferencePaper","2024","Ma, Wanqin; Yang, Chenyang; Kästner, Christian","(Why) Is My Prompt Getting Worse? Rethinking Regression Testing for Evolving LLM APIs","Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI","979-8-4007-0591-5","","10.1145/3644815.3644950","https://doi.org/10.1145/3644815.3644950","Large Language Models (LLMs) are increasingly integrated into software applications. Downstream application developers often access LLMs through APIs provided as a service. However, LLM APIs are often updated silently and scheduled to be deprecated, forcing users to continuously adapt to evolving models. This can cause performance regression and affect prompt design choices, as evidenced by our case study on toxicity detection. Based on our case study, we emphasize the need for and re-examine the concept of regression testing for evolving LLM APIs. We argue that regression testing LLMs requires fundamental changes to traditional testing approaches, due to different correctness notions, prompting brittleness, and non-determinism in LLM APIs.","2024","2025-11-25 22:29:43","2025-11-25 22:48:16","","166–171","","","","","","","CAIN '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","Software testing; Software; Software engineering; Adaptation models; Debugging; Large Language Models (LLM); large language models (LLM); regression testing; Toxicology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TD8XTUHE","conferencePaper","2024","Sun, Maolin; Yang, Yibiao; Wang, Yang; Wen, Ming; Jia, Haoxiang; Zhou, Yuming","SMT Solver Validation Empowered by Large Pre-Trained Language Models","Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering","979-8-3503-2996-4","","10.1109/ASE56229.2023.00180","https://doi.org/10.1109/ASE56229.2023.00180","SMT solvers are utilized to check the satisfiability of logic formulas and have been applied in various crucial domains, including software verification, test case generation, and program synthesis. However, bugs hidden in SMT solvers can lead to severe consequences, causing erroneous results in these domains. Therefore, ensuring the reliability and robustness of SMT solvers is of critical importance. Despite several testing approaches proposed for SMT solvers, generating effective test formulas to comprehensively test SMT solvers remains a challenge. To address this challenge, in this study, we propose to port large language models (LLMs) to generate SMT formulas for fuzzing solvers. Specifically, the study presents a novel retrain-finetune pipeline to unleash the potential of language models to generate effective SMT formulas and improve their generation performance through data augmentation. We implemented our approach as a practical fuzzing tool, named LaST, and then extensively tested the state-of-the-art SMT solvers, namely Z3, cvc5, and Bitwuzla. To date, LaST has successfully uncovered 65 genuine bugs for the solvers, of which 45 have been fixed by the developers.","2024","2025-11-25 22:29:43","2025-11-25 22:47:52","","1288–1300","","","","","","","ASE '23","","","","IEEE Press","Echternach, Luxembourg","","","","","","","","","","","","large language model; Software; Adaptation models; Computer bugs; Fuzzing; fuzzing; data augmentation; retrain-finetune; SMT solver; Data models; Pipelines; Data augmentation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S6RBA9YU","conferencePaper","2024","Qiu, Ketai","Autonomic Testing: Testing with Scenarios from Production","Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings","979-8-4007-0502-1","","10.1145/3639478.3639802","https://doi.org/10.1145/3639478.3639802","My PhD addresses the problem of detecting field failures with a new approach to test software systems under conditions that emerge only in production. Ex-vivo approaches detect field failures by executing the software system in the testbed with data extracted from the production environment. In-vivo approaches execute the available test suites in the production environment. We will define autonomic testing that detects conditions that emerge only in production scenarios, generates test cases for the new conditions, and executes the generated test cases in the new scenarios, to detect failures before they occur in production.","2024","2025-11-25 22:29:43","2025-11-25 22:29:43","","156–158","","","","","","","ICSE-Companion '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","test generation; autonomic testing; failure detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZR9QLH4R","journalArticle","2024","Khojah, Ranim; Mohamad, Mazen; Leitner, Philipp; de Oliveira Neto, Francisco Gomes","Beyond Code Generation: An Observational Study of ChatGPT Usage in Software Engineering Practice","Proc. ACM Softw. Eng.","","","10.1145/3660788","https://doi.org/10.1145/3660788","Large Language Models (LLMs) are frequently discussed in academia and the general public as support tools for virtually any use case that relies on the production of text, including software engineering. Currently, there is much debate, but little empirical evidence, regarding the practical usefulness of LLM-based tools such as ChatGPT for engineers in industry. We conduct an observational study of 24 professional software engineers who have been using ChatGPT over a period of one week in their jobs, and qualitatively analyse their dialogues with the chatbot as well as their overall experience (as captured by an exit survey). We find that rather than expecting ChatGPT to generate ready-to-use software artifacts (e.g., code), practitioners more often use ChatGPT to receive guidance on how to solve their tasks or learn about a topic in more abstract terms. We also propose a theoretical framework for how the (i) purpose of the interaction, (ii) internal factors (e.g., the user's personality), and (iii) external factors (e.g., company policy) together shape the experience (in terms of perceived usefulness and trust). We envision that our framework can be used by future research to further the academic discussion on LLM usage by software engineering practitioners, and to serve as a reference point for the design of future empirical LLM research in this domain.","2024-07","2025-11-25 22:29:43","2025-11-25 22:29:43","","","","FSE","1","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Chatbots; Large Language Models (LLMs); Software Development Bots","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TPUSQFB6","bookSection","2025","Baresi, Luciano; Hu, Davide Yi Xian; Stocco, Andrea; Tonella, Paolo","Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00206","Simulation-based testing is widely used to assess the reliability of Autonomous Driving Systems (ADS), but its effectiveness is limited by the operational design domain (ODD) conditions available in such simulators. To address this limitation, in this work, we explore the integration of generative artificial intelligence techniques with physics-based simulators to enhance ADS system-level testing. Our study evaluates the effectiveness and computational overhead of three generative strategies based on diffusion models, namely instruction-editing, inpainting, and inpainting with refinement. Specifically, we assess these techniques' capabilities to produce augmented simulator-generated images of driving scenarios representing new ODDs. We employ a novel automated detector for invalid inputs based on semantic segmentation to ensure semantic preservation and realism of the neural generated images. We then performed system-level testing to evaluate the ability of the ADS to generalize to newly synthesized ODDs. Our findings show that diffusion models help to increase the coverage of ODD for system-level ADS testing. Our automated semantic validator achieved a percentage of false positives as low as 3%, retaining the correctness and quality of the images generated for testing. Our approach successfully identified new ADS system failures before real-world testing.","2025","2025-11-25 22:29:43","2025-11-25 22:29:43","","398–410","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TME2WWU2","conferencePaper","2024","Guerino, Lucca Renato; Kuroishi, Pedro Henrique; Paiva, Ana Cristina Ramada; Vincenzi, Auri Marcelo Rizzo","Static and Dynamic Comparison of Mutation Testing Tools for Python","Proceedings of the XXIII Brazilian Symposium on Software Quality","979-8-4007-1777-2","","10.1145/3701625.3701659","https://doi.org/10.1145/3701625.3701659","Context: Mutation testing is a rigorous approach for assessing the quality of test suites by injecting faults (i.e., mutants) into software under test. Tools, such as CosmicRay and Mutpy, are examples of Mutation Testing tools for Python software programs. Problem: With different Python mutation testing tools, comparative analysis is lacking to evaluate their effectiveness in different usage scenarios. Furthermore, the evolution of these tools makes continuous evaluation of their functionalities and characteristics necessary. Method: In this work, we evaluate (statically and dynamically) four Python mutation testing tools, namely CosmicRay, MutPy, MutMut, and Mutatest. In static evaluation, we introduce a comparison framework, adapted from one previously applied to Java tools, and collected information from tool documentation and developer surveys. For dynamic evaluation, we use tests built based on those produced by Pynguin, which are improved through the application of Large Language Models (LLMs) and manual analyses. Then, the adequate test suites were cross-tested among different tools to evaluate their effectiveness in killing mutants each other. Results: Our findings reveal that CosmicRay offers superior functionalities and customization options for mutant generation compared to its counterparts. Although CosmicRay’s performance was slightly lower than MutPy in the dynamic tests, its recent updates and active community support highlight its potential for future enhancements. Cross-examination of the test suites further shows that mutation scores varied narrowly among tools, with a slight emphasis on MutPy as the most effective mutant fault model.","2024","2025-11-25 22:29:43","2025-11-25 22:29:43","","199–209","","","","","","","SBQS '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Software Testing; Mutation Testing; Automated Test Generation; Coverage Testing; Experimental Software Engineering; Python Mutation Tools; Testing Tools","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8G2L67LI","conferencePaper","2024","Song, Somin; Kundu, Ashish; Tak, Byungchul","POSTER: Seccomp profiling with Dynamic Analysis via ChatGPT-assisted Test Code Generation","Proceedings of the 19th ACM Asia Conference on Computer and Communications Security","979-8-4007-0482-6","","10.1145/3634737.3659426","https://doi.org/10.1145/3634737.3659426","The effectiveness of Seccomp kernel feature depends on how tightly and accurately the necessary system calls are specified in the seccomp policy. Static code analysis may miss out or over-approximate required system calls. With dynamic analysis, it is difficult to cover all possible execution paths. In this work, we aim to advance the state-of-the-art dynamic analysis approach by enabling it to increase the coverage of the target application's functionalities. Our approach takes as input the application's online documentation and leverages ChatGPT to generate a large number of test codes for functionalities in the documentation. This automated process eliminates the barrier to manually writing a large number of test codes for conducting dynamic analysis. Through our preliminary evaluation, we confirmed that ChatGPT can be used effectively to automatically generate a large number of test codes. Also, we observed early evidence that the seccomp policy generated from running the test codes could be more sound than the ones generated by static analysis.","2024","2025-11-25 22:29:43","2025-11-25 22:29:43","","1928–1930","","","","","","","ASIA CCS '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Singapore, Singapore","","","","static analysis; ChatGPT; dynamic analysis; seccomp; test code","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6FQR5TDV","conferencePaper","2025","Dickmann, Dennis; Offenhäuser, Philipp; Saxena, Rishabh; Markomanolis, George; Rigazzi, Alessandro; Keller, Patrick; Kayabay, Kerem; Hoppe, Dennis","Evaluating AMD Instinct™ MI300A APU: Performance Insights on LLM Training via Knowledge Distillation","Proceedings of the Cray User Group","979-8-4007-1327-9","","10.1145/3757348.3757361","https://doi.org/10.1145/3757348.3757361","","2025","2025-11-25 22:29:44","2025-11-25 22:29:44","","115–126","","","","","","","CUG '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","LLM; AI; Deep Learning; Computer Science; HPC; MI300A; ML","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7CDN68QK","conferencePaper","2024","Li, Dong; Yan, Meng; Zhang, Yaosheng; Liu, Zhongxin; Liu, Chao; Zhang, Xiaohong; Chen, Ting; Lo, David","CoSec: On-the-Fly Security Hardening of Code LLMs via Supervised Co-decoding","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680371","https://doi.org/10.1145/3650212.3680371","Large Language Models (LLMs) specialized in code have shown exceptional proficiency across various programming-related tasks, particularly code generation. Nonetheless, due to its nature of pretraining on massive uncritically filtered data, prior studies have shown that code LLMs are prone to generate code with potential vulnerabilities. Existing approaches to mitigate this risk involve crafting data without vulnerability and subsequently retraining or fine-tuning the model. As the number of parameters exceeds a billion, the computation and data demands of the above approaches will be enormous. Moreover, an increasing number of code LLMs tend to be distributed as services, where the internal representation is not accessible, and the API is the only way to reach the LLM, making the prior mitigation strategies non-applicable. To cope with this, we propose CoSec, an on-the-fly Security hardening method of code LLMs based on security model-guided Co-decoding, to reduce the likelihood of code LLMs to generate code containing vulnerabilities. Our key idea is to train a separate but much smaller security model to co-decode with a target code LLM. Since the trained secure model has higher confidence for secure tokens, it guides the generation of the target base model towards more secure code generation. By adjusting the probability distributions of tokens during each step of the decoding process, our approach effectively influences the tendencies of generation without accessing the internal parameters of the target code LLM. We have conducted extensive experiments across various parameters in multiple code LLMs (i.e., CodeGen, StarCoder, and DeepSeek-Coder), and the results show that our approach is effective in security hardening. Specifically, our approach improves the average security ratio of six base models by 5.02%-37.14%, while maintaining the functional correctness of the target model.","2024","2025-11-25 22:29:44","2025-11-25 22:29:44","","1428–1439","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large Language Models; Code Generation; Software Security; AI Safety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HZL8E63H","conferencePaper","2024","Amirizaniani, Maryam; Martin, Elias; Roosta, Tanya; Chadha, Aman; Shah, Chirag","AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe Approach","Proceedings of the 33rd ACM International Conference on Information and Knowledge Management","979-8-4007-0436-9","","10.1145/3627673.3679222","https://doi.org/10.1145/3627673.3679222","As Large Language Models (LLMs) are integrated into various sectors, ensuring their reliability and safety is crucial. This necessitates rigorous probing and auditing to maintain their effectiveness and trustworthiness in practical applications. Subjecting LLMs to varied iterations of a single query can unveil potential inconsistencies in their knowledge base or functional capacity. However, a tool for performing such audits with a easy to execute workflow, and low technical threshold is lacking. In this demo, we introduce ""AuditLLM,"" a novel tool designed to audit the performance of various LLMs in a methodical way. AuditLLM's primary function is to audit a given LLM by deploying multiple probes derived from a single question, thus detecting any inconsistencies in the model's comprehension or performance. A robust, reliable, and consistent LLM is expected to generate semantically similar responses to variably phrased versions of the same question. Building on this premise, AuditLLM generates easily interpretable results that reflect the LLM's consistency based on a single input question provided by the user. A certain level of inconsistency has been shown to be an indicator of potential bias, hallucinations, and other issues. One could then use the output of AuditLLM to further investigate issues with the aforementioned LLM. To facilitate demonstration and practical uses, AuditLLM offers two key modes: (1) Live mode which allows instant auditing of LLMs by analyzing responses to real-time queries; and (2) Batch mode which facilitates comprehensive LLM auditing by processing multiple queries at once for in-depth analysis. This tool is beneficial for both researchers and general users, as it enhances our understanding of LLMs' capabilities in generating responses, using a standardized auditing platform.","2024","2025-11-25 22:29:44","2025-11-25 22:29:44","","5174–5179","","","","","","","CIKM '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Boise, ID, USA","","","","large language model; auditing LLMs tools","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BQS78LME","journalArticle","2025","Zhong, Renyi; Li, Yichen; Yu, Guangba; Gu, Wenwei; Kuang, Jinxi; Huo, Yintong; Lyu, Michael R.","Larger Is Not Always Better: Exploring Small Open-source Language Models in Logging Statement Generation","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3773287","https://doi.org/10.1145/3773287","Developers use logging statements to create logs that document system behavior and aid in software maintenance. As such, high-quality logging is essential for effective maintenance; however, manual logging often leads to errors and inconsistency. Recent methods emphasize using large language models (LLMs) for automated logging statement generation, but these present privacy and resource issues, hindering their suitability for enterprise use. This paper presents the first large-scale empirical study evaluating small open-source language models (SOLMs) for automated logging statement generation. We evaluate four prominent SOLMs using various prompt strategies and parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA) and Retrieval-Augmented Generation (RAG). Our results show that fine-tuned SOLMs with LoRA and RAG prompts, particularly Qwen2.5-coder-14B, outperform existing tools and LLM baselines (e.g., Claude3.7 sonnet and GPT4o) in predicting logging locations and generating high-quality statements, with robust generalization across diverse repositories. These findings highlight SOLMs as a privacy-preserving, efficient alternative for automated logging.","2025-10","2025-11-25 22:29:44","2025-11-25 22:29:44","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Logging Practice; Logging Statement; Logging Text; Software Logging","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5TU77CRQ","journalArticle","2025","Chen, Yujia; Ye, Yang; Li, Zhongqi; Ma, Yuchi; Gao, Cuiyun","Smaller but Better: Self-Paced Knowledge Distillation for Lightweight yet Effective LCMs","Proc. ACM Softw. Eng.","","","10.1145/3729405","https://doi.org/10.1145/3729405","Large code models (LCMs) have remarkably advanced the field of code generation. Despite their impressive capabilities, they still face practical deployment issues, such as high inference costs, limited accessibility of proprietary LCMs, and adaptability issues of ultra-large LCMs. These issues highlight the critical need for more accessible, lightweight yet effective LCMs. Knowledge distillation (KD) offers a promising solution, which transfers the programming capabilities of larger, advanced LCMs (Teacher) to smaller, less powerful LCMs (Student). However, existing KD methods for code intelligence often lack consideration of fault domain knowledge and rely on static seed knowledge, leading to degraded programming capabilities of student models. In this paper, we propose a novel Self-Paced knOwledge DistillAtion framework, named SODA, aiming at developing lightweight yet effective student LCMs via adaptively transferring the programming capabilities from advanced teacher LCMs. SODA consists of three stages in one cycle: (1) Correct-and-Fault Knowledge Delivery stage aims at improving the student model’s capability to recognize errors while ensuring its basic programming skill during the knowledge transferring, which involves correctness-aware supervised learning and fault-aware contrastive learning methods. (2) Multi-View Feedback stage aims at measuring the quality of results generated by the student model from two views, including model-based and static tool-based measurement, for identifying the difficult questions; (3) Feedback-based Knowledge Update stage aims at updating the student model adaptively by generating new questions at different difficulty levels, in which the difficulty levels are categorized based on the feedback in the second stage. By performing the distillation process iteratively, the student model is continuously refined through learning more advanced programming skills from the teacher model. We compare SODA with four state-of-the-art KD approaches on three widely-used code generation benchmarks with different programming languages. Experimental results show that SODA improves the student model by 65.96% in terms of average Pass@1, outperforming the best baseline PERsD by 29.85%. Based on the proposed SODA framework, we develop SodaCoder, a series of lightweight yet effective LCMs with ∼7B parameters, which outperform 15 LCMs with less than or equal to 16B parameters. Notably, SodaCoder-DS-6.7B, built on DeepseekCoder-6.7B, even surpasses the prominent ChatGPT on average Pass@1 across seven programming languages (66.4 vs. 61.3).","2025-06","2025-11-25 22:29:44","2025-11-25 22:29:44","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Code Generation; Knowledge Distillation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VMZK97AB","conferencePaper","2024","Guo, Lianghong; Wang, Yanlin; Shi, Ensheng; Zhong, Wanjun; Zhang, Hongyu; Chen, Jiachi; Zhang, Ruikai; Ma, Yuchi; Zheng, Zibin","When to Stop? Towards Efficient Code Generation in LLMs with Excess Token Prevention","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680343","https://doi.org/10.1145/3650212.3680343","Code generation aims to automatically generate code snippets that meet given natural language requirements and plays an important role in software development. Although Code LLMs have shown excellent performance in this domain, their long generation time poses a signification limitation in practice use. In this paper, we first conduct an in-depth preliminary study with different Code LLMs on code generation task and identify a significant efficiency issue, i.e., continual generation of excess tokens. It harms the developer productivity and leads to huge computational wastes. To address it, we introduce CodeFast, an inference acceleration approach for Code LLMs on code generation. The key idea of CodeFast is to terminate the inference process in time when unnecessary excess tokens are detected. First, we propose an automatic data construction framework to obtain training data. Then, we train a unified lightweight model GenGuard applicable to multiple programming languages to predict whether to terminate inference at the current step. Finally, we enhance Code LLM with GenGuard to accelerate its inference in code generation task. We conduct extensive experiments with CodeFast on five representative Code LLMs across four widely used code generation datasets. Experimental results show that (1) CodeFast can significantly improve the inference speed of various Code LLMs in code generation, ranging form 34% to 452%, without compromising the quality of generated code. (2) CodeFast is stable across different parameter settings and can generalize to untrained datasets. Our code and data are available at https://github.com/DeepSoftwareAnalytics/CodeFast.","2024","2025-11-25 22:29:44","2025-11-25 22:29:44","","1073–1085","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Machine learning for analysis; Testing and development processes","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q5AYW94B","conferencePaper","2025","Cheng, Zhuo; Zou, Zhou; Huang, Qing; Xing, Zhenchang; Zhang, Wei; Wang, Shaochen; Yi, Xueting; Jin, Huan; Liu, Zhiping; Lu, Zhaojin","DesDD: A Design-Enabled Framework with Dual-Layer Debugging for LLM-based Iterative API Orchestrating","Proceedings of the 16th International Conference on Internetware","979-8-4007-1926-4","","10.1145/3755881.3755911","https://doi.org/10.1145/3755881.3755911","In contemporary Software Engineering (SE), coordinated API calls are necessary to perform complex data retrieval operations as well as tasks. Large Language Models (LLMs) offer highly potential capabilities for natural language parsing and automation of tasks, which sparked research into integrating APIs orchestration with LLMs. Nonetheless, while existing LLM-based frameworks have developed considerably, yet they experience challenges in tackling complex tasks which tend to involve iterative, step-by-step problem-solving. Current frameworks lack structured guidance, relying on LLMs’ own capabilities, resulting in blind iterations, inefficient error correction, and inefficient token utilization. This work introduces DesDD (Design-enabled framework with Dual-layer Debugging), a structured framework for LLM-driven iterative API orchestration. By applying software engineering design principles, we organize API orchestration workflows into distinct design and coding phases. Our dual-layer debugging mechanism detects and corrects errors in both phases, making the orchestration process more reliable and efficient. DesDD provides a structured design-first, then-code pathway, enabling LLMs to solve tasks iteratively through a well-defined, stepwise approach that systematically guide each problem-solving stage. The built-in dual-layer debugging component provides hierarchical error detection in both the design and coding aspects, allowing targeted corrections. Through comprehensive testing, we found that DesDD performs better than existing frameworks in orchestration efficiency and accuracy while using significantly fewer tokens. DesDD offers an effective LLM-based solution for API orchestration in complex tasks, e.g., chemical continuous flow process control. Its use of SE design principles ensures wide applicability, making it a promising approach for LLM-driven automated task resolution across diverse scenarios.","2025","2025-11-25 22:29:44","2025-11-25 22:29:44","","402–412","","","","","","","Internetware '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","large language models (LLMs); API Orchestrating; chemical continuous flow; dual-layer debugging; iterative problem-solving; SE-inspired framework; token utilization efficiency","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FERUTUQ5","conferencePaper","2024","Aranda, Manoel; Oliveira, Naelson; Soares, Elvys; Ribeiro, Márcio; Romão, Davi; Patriota, Ullyanne; Gheyi, Rohit; Souza, Emerson; Machado, Ivan","A Catalog of Transformations to Remove Smells From Natural Language Tests","Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering","979-8-4007-1701-7","","10.1145/3661167.3661225","https://doi.org/10.1145/3661167.3661225","Test smells can pose difficulties during testing activities, such as poor maintainability, non-deterministic behavior, and incomplete verification. Existing research has extensively addressed test smells in automated software tests but little attention has been given to smells in natural language tests. While some research has identified and catalogued such smells, there is a lack of systematic approaches for their removal. Consequently, there is also a lack of tools to automatically identify and remove natural language test smells. This paper introduces a catalog of transformations designed to remove seven natural language test smells and a companion tool implemented using Natural Language Processing (NLP) techniques. Our work aims to enhance the quality and reliability of natural language tests during software development. The research employs a two-fold empirical strategy to evaluate its contributions. First, a survey involving 15 software testing professionals assesses the acceptance and usefulness of the catalog’s transformations. Second, an empirical study evaluates our tool to remove natural language test smells by analyzing a sample of real-practice tests from the Ubuntu OS. The results indicate that software testing professionals find the transformations valuable. Additionally, the automated tool demonstrates a good level of precision, as evidenced by a F-Measure rate of 83.70%.","2024","2025-11-25 22:29:44","2025-11-25 22:29:44","","7–16","","","","","","","EASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Salerno, Italy","","","","Software Testing; Natural Language Test; Test Smells","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VZ7TNWJN","journalArticle","2025","Zhang, Chenbo; Xu, Wenying; Liu, Jinbu; Zhang, Lu; Liu, Guiyang; Guan, Jihong; Zhou, Qi; Zhou, Shuigeng","LogBase: A Large-Scale Benchmark for Semantic Log Parsing","Proc. ACM Softw. Eng.","","","10.1145/3728969","https://doi.org/10.1145/3728969","Logs generated by large-scale software systems contain a huge amount of useful information. As the first step of automated log analysis, log parsing has been extensively studied. General log parsing techniques focus on identifying static templates from raw logs, but overlook the more important semantics implied in dynamic log parameters. With the popularity of Artificial Intelligence for IT Operations (AIOps), traditional log parsing methods no longer meet the requirements of various downstream tasks. Researchers are now exploring the next generation of log parsing techniques, i.e., semantic log parsing, to identify both log templates and semantics in log parameters. However, the absence of semantic annotations in existing datasets hinders the training and evaluation of semantic log parsers, thereby stalling the progress of semantic log parsing. To fill this gap and advance the field of semantic log parsing, we construct LogBase, the first semantic log parsing benchmark dataset. LogBase consists of logs from 130 popular open-source projects, containing 85,300 semantically annotated log templates, surpassing existing datasets in both log source diversity and template richness. To build Logbase, we develop the framework GenLog for constructing semantic log parsing datasets. GenLog mines log template-parameter-context triplets from popular open-source repositories on GitHub, and uses chain-of-thought (CoT) techniques with large language models (LLMs) to generate high-quality logs. Meanwhile, GenLog employs human feedback to improve the quality of the generated data and ensure its reliability. GenLog is highly automated and cost-effective, enabling researchers to easily and efficiently construct semantic log parsing datasets. Furthermore, we also design a set of comprehensive evaluation metrics for LogBase, including general log parser metrics and the metrics specifically for semantic log parsers and LLM-based parsers. With LogBase, we extensively evaluate 15 existing log parsers, revealing their true performance in complex scenarios. We believe that this work provides researchers with valuable data, reliable tools, and insightful findings to support and guide the future research of semantic log parsing.","2025-06","2025-11-25 22:29:44","2025-11-25 22:29:44","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large language model; Benchmark; AIOps; Semantic log parsing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"76GVGMGN","conferencePaper","2024","Champa, Arifa Islam; Rabbi, Md Fazle; Nachuma, Costain; Zibran, Minhaz F.","ChatGPT in Action: Analyzing Its Use in Software Development","Proceedings of the 21st International Conference on Mining Software Repositories","979-8-4007-0587-8","","10.1145/3643991.3645077","https://doi.org/10.1145/3643991.3645077","The emergence of AI tools such as ChatGPT is being used to assist with software development, but little is known of how developers utilize these tools as well as the capabilities of these tools in software engineering tasks. Using the DevGPT dataset, we conduct quantitative analyses of the tasks developers seek assistance from ChatGPT and how effectively ChatGPT addresses them. We also examine the impact of initial prompt quality on conversation length. The findings reveal where ChatGPT is most and least suited to assist in the identified 12 software development tasks. The insights from this research would guide the software developers, researchers, and AI tool providers in optimizing these tools for more effective programming aid.","2024","2025-11-25 22:29:44","2025-11-25 22:29:44","","182–186","","","","","","","MSR '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","ChatGPT conversation; prompt quality; software development tasks; task efficiency","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GAJTSYMQ","conferencePaper","2024","Maia, Caio Jordan de Lima; Aguiar, Yuska Paola Costa","AI-Driven Acceptance Testing: first insights exploring the educational potential for test analysts","Proceedings of the XXIII Brazilian Symposium on Software Quality","979-8-4007-1777-2","","10.1145/3701625.3701691","https://doi.org/10.1145/3701625.3701691","This article examines the potential of AI-based natural language processing models, such as ChatGPT and Bard, as educational tools in the context of acceptance test case creation. The principal findings indicate that test cases manually generated by two QA professionals (one junior and one senior) exhibited reduced coverage and accuracy in comparison to those generated by the AIs, which, while expeditious, also demonstrated limitations. The findings indicate that AI can significantly enhance the learning process for less experienced QA professionals by providing clear and structured examples. Furthermore, collaboration between AI tools and experienced professionals optimizes the process, offering greater efficiency and test coverage. The study underscores the importance of integrating technological advancements into educational practices to better prepare future software quality professionals.","2024","2025-11-25 22:29:44","2025-11-25 22:29:44","","665–672","","","","","","","SBQS '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","ChatGPT; Acceptance Test; Bard; Generative AI-Supported Education; Quality Assurance; Software Quality Education; Test Plan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A2C4JR76","conferencePaper","2024","Cogo, Filipe Roseiro; Rajbahadur, Gopi Krishnan; Lin, Dayi; Hassan, Ahmed E.","A Tutorial on Software Engineering for FMware","Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering","979-8-4007-0658-5","","10.1145/3663529.3663820","https://doi.org/10.1145/3663529.3663820","Foundation Models (FMs) like GPT-4 have given rise to FMware, FM-powered applications representing a new generation of software that is developed with new roles, assets, and paradigms. FMware has been widely adopted in both software engineering (SE) research (e.g., test generation) and industrial products (e.g., GitHub copilot), despite the numerous challenges introduced by the stochastic nature of FMs. In our tutorial, we will present the latest research and industrial practices in engineering FMware, along with a hands-on session to acquaint attendees with core tools and techniques to build FMware. Our tutorial's perspective is firmly rooted in SE rather than artificial intelligence (AI), ensuring that participants are spared from delving into mathematical and AI-related intricacies unless they are crucial for introducing SE challenges and opportunities.","2024","2025-11-25 22:29:44","2025-11-25 22:29:44","","710–712","","","","","","","FSE 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","FMware; Foundation Model; Software engineering for FMware","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KZBTXQRU","conferencePaper","2025","Mohammadi, Mahmoud; Li, Yipeng; Lo, Jane; Yip, Wendy","Evaluation and Benchmarking of LLM Agents: A Survey","Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2","979-8-4007-1454-2","","10.1145/3711896.3736570","https://doi.org/10.1145/3711896.3736570","The rise of LLM-based agents has opened new frontiers in AI applications, yet evaluating these agents remains a complex and underdeveloped area. This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives-what to evaluate, such as agent behavior, capabilities, reliability, and safety-and (2) evaluation process-how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling. In addition to taxonomy, we highlight enterprise-specific challenges, such as role-based access to data, the need for reliability guarantees, dynamic and long-horizon interactions, and compliance, which are often overlooked in current research. We also identify the future research directions, including holistic, more realistic, and scalable evaluation. This work aims to bring clarity to the fragmented landscape of agent evaluation and provide a framework for systematic assessment, enabling researchers and practitioners to evaluate LLM agents for real-world deployment.","2025","2025-11-25 22:29:44","2025-11-25 22:29:44","","6129–6139","","","","","","","KDD '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Toronto ON, Canada","","","","safety; agent behavior; agent evaluation; benchmarks; enterprise ai; evaluation taxonomy; llm agents","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EALERP4V","journalArticle","2025","Chevrot, Antoine; Vernotte, Alexandre; Falleri, Jean-Rémy; Blanc, Xavier; Legeard, Bruno; Cretin, Aymeric","Are Autonomous Web Agents Good Testers?","Proc. ACM Softw. Eng.","","","10.1145/3728879","https://doi.org/10.1145/3728879","Despite advances in automated testing, manual testing remains prevalent due to the high maintenance demands associated with test script fragility—scripts often break with minor changes in application structure. Recent developments in Large Language Models (LLMs) offer a potential alternative by powering Autonomous Web Agents (AWAs) that can autonomously interact with applications. These agents may serve as Autonomous Test Agents (ATAs), potentially reducing the need for maintenance-heavy automated scripts by utilising natural language instructions similar to those used by human testers. This paper investigates the feasibility of adapting AWAs for natural language test case execution and how to evaluate them. We contribute with (1) a benchmark of three offline web applications, and a suite of 113 manual test cases, split between passing and failing cases, to evaluate and compare ATAs performance, (2) SeeAct-ATA and pinATA, two open-source ATA implementations capable of executing test steps, verifying assertions and giving verdicts, and (3) comparative experiments using our benchmark that quantifies our ATAs effectiveness. Finally we also proceed to a qualitative evaluation to identify the limitations of PinATA, our best performing implementation. Our findings reveal that our simple implementation, SeeAct-ATA, does not perform well compared to our more advanced PinATA implementation when executing test cases (50% performance improvement). However, while PinATA obtains around 60% of correct verdict and up to a promising 94% specificity, we identify several limitations that need to be addressed to develop more resilient and reliable ATAs, paving the way for robust, low maintenance test automation.","2025-06","2025-11-25 22:29:44","2025-11-25 22:29:44","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Benchmark; Autonomous Tester Agent; Web Automation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JW2QN4IK","conferencePaper","2025","McDanel, Bradley; Novak, Ed","Designing LLM-Resistant Programming Assignments: Insights and Strategies for CS Educators","Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1","979-8-4007-0531-1","","10.1145/3641554.3701872","https://doi.org/10.1145/3641554.3701872","The rapid advancement of Large Language Models (LLMs) like ChatGPT has raised concerns among computer science educators about how programming assignments should be adapted. This paper explores the capabilities of LLMs (GPT-3.5, GPT-4, and Claude Sonnet) in solving complete, multi-part CS homework assignments from the SIGCSE Nifty Assignments list. Through qualitative and quantitative analysis, we found that LLM performance varied significantly across different assignments and models, with Claude Sonnet consistently outperforming the others. The presence of starter code and test cases improved performance for advanced LLMs, while certain assignments, particularly those involving visual elements, proved challenging for all models. LLMs often disregarded assignment requirements, produced subtly incorrect code, and struggled with context-specific tasks. Based on these findings, we propose strategies for designing LLM-resistant assignments. Our work provides insights for instructors to evaluate and adapt their assignments in the age of AI, balancing the potential benefits of LLMs as learning tools with the need to ensure genuine student engagement and learning.","2025","2025-11-25 22:29:44","2025-11-25 22:29:44","","756–762","","","","","","","SIGCSETS 2025","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Pittsburgh, PA, USA","","","","ai-resistant assignments; assignment design; cs education; llm code generation; programming pedagogy","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5RX79NCP","conferencePaper","2024","Hora, Andre","Predicting Test Results without Execution","Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering","979-8-4007-0658-5","","10.1145/3663529.3663794","https://doi.org/10.1145/3663529.3663794","As software systems grow, test suites may become complex, making it challenging to run the tests frequently and locally. Recently, Large Language Models (LLMs) have been adopted in multiple software engineering tasks. It has demonstrated great results in code generation, however, it is not yet clear whether these models understand code execution. Particularly, it is unclear whether LLMs can be used to predict test results, and, potentially, overcome the issues of running real-world tests. To shed some light on this problem, in this paper, we explore the capability of LLMs to predict test results without execution. We evaluate the performance of the state-of-the-art GPT-4 in predicting the execution of 200 test cases of the Python Standard Library. Among these 200 test cases, 100 are passing and 100 are failing ones. Overall, we find that GPT-4 has a precision of 88.8%, recall of 71%, and accuracy of 81% in the test result prediction. However, the results vary depending on the test complexity: GPT-4 presented better precision and recall when predicting simpler tests (93.2% and 82%) than complex ones (83.3% and 60%). We also find differences among the analyzed test suites, with the precision ranging from 77.8% to 94.7% and recall between 60% and 90%. Our findings suggest that GPT-4 still needs significant progress in predicting test results.","2024","2025-11-25 22:29:44","2025-11-25 22:29:44","","542–546","","","","","","","FSE 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","software testing; large language models; GPT-4; LLMs","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ATIS2GUG","conferencePaper","2025","Guo, Keying; Zhao, Peiling; Cao, Yuzhong","A compatibility test method for Android applications based on multimodal large models in crowdsourcing scenarios","Proceedings of the 2025 5th International Conference on Computer Network Security and Software Engineering","979-8-4007-1361-3","","10.1145/3732365.3732402","https://doi.org/10.1145/3732365.3732402","With the popularity of smartphones, the Android operating system has taken a significant share of the global market, prompting researchers to explore automated testing techniques to cope with the diversity and complexity of operating systems. This paper proposes a compatibility testing method for Android applications based on multi-modal large model, which combines Android automated testing tools and multi-modal large models to achieve simultaneous testing of multiple mobile phones. The multi-modal large model named STLLM proposed in this paper can automatically generate software test scripts based on Android system, support simultaneous testing on multiple devices, solve the compatibility problems and problems caused by manual coding in software testing, and improve the efficiency and accuracy of testing. Experimental results show that the proposed method can effectively improve test coverage, reduce test cost, and save a lot of time and energy for developers.","2025","2025-11-25 22:29:44","2025-11-25 22:29:44","","215–224","","","","","","","CNSSE '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Software testing; Android; Automated testing; Compatibility testing; Multimodal large models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7AGPV2HC","conferencePaper","2025","Arora, Utkarsh; Garg, Anupam; Gupta, Aryan; Jain, Samyak; Mehta, Ronit; Oberoi, Rupin; Prachi; Raina, Aryaman; Saini, Manav; Sharma, Sachin; Singh, Jaskaran; Tyagi, Sarthak; Kumar, Dhruv","Analyzing LLM Usage in an Advanced Computing Class in India","Proceedings of the 27th Australasian Computing Education Conference","979-8-4007-1425-2","","10.1145/3716640.3716657","https://doi.org/10.1145/3716640.3716657","This study examines the use of large language models (LLMs) by undergraduate and graduate students for programming assignments in advanced computing classes. Unlike existing research, which primarily focuses on introductory classes and lacks in-depth analysis of actual student-LLM interactions, our work fills this gap. We conducted a comprehensive analysis involving 411 students from a Distributed Systems class at an Indian university, where they completed three programming assignments and shared their experiences through Google Form surveys and interviews.Our findings reveal that students leveraged LLMs for a variety of tasks, including code generation, debugging, conceptual inquiries, and test case creation. They employed a spectrum of prompting strategies, ranging from basic contextual prompts to advanced techniques like chain-of-thought prompting and iterative refinement. While students generally viewed LLMs as beneficial for enhancing productivity and learning, we noted a concerning trend of over-reliance, with many students submitting entire assignment descriptions to obtain complete solutions. Given the increasing use of LLMs in the software industry, our study highlights the need to update undergraduate curricula to include training on effective prompting strategies and to raise awareness about the benefits and potential drawbacks of LLM usage in academic settings.","2025","2025-11-25 22:29:44","2025-11-25 22:29:44","","154–163","","","","","","","ACE '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Large Language Models; Computing Education; User Study","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JRIKTGYS","conferencePaper","2024","Hou, Zongzhi; Zhao, Ruohan; Li, Zhongyang; Wang, Zheng; Wu, Yizhen; Gou, Junwei; Zhu, Zhifeng","A Method for Efficient Structured Data Generation with Large Language Models","Proceedings of the 2nd Workshop on Large Generative Models Meet Multimodal Applications","979-8-4007-1193-0","","10.1145/3688866.3689127","https://doi.org/10.1145/3688866.3689127","With the rapid development of large language model technology, we find ourselves at an interesting juncture regarding the importance of data. The textual data samples from these large unsupervised models are often of poor quality, which in turn produces substandard results. Implicitly, this means that the model struggles to learn the exact underlying structure of the data distribution without supervision, which can manifest as output lacking fidelity and relevance to real data distributions. In order to overcome some of these limitations in data-driven text generation tasks, this paper presents a Efficient Data Generation System (EDGS) for multimodal structured data generation. EDGS achieves this by using clustering abstraction to process various data input types through templates, thereby enabling quick data generation and reducing resource consumption. EDGS is robust, ensuring stable and reliable performance across different conditions and is more cost-effective in terms of token usage. Traditional methods handle data on a per-instance basis without using templates, but for EDGS, by abstracting and clustering different input types, it efficiently generates data from large models. Its adaptability makes it ideal for multi-modal data generation tasks since it relies on the fundamental functions of general large-scale language models. A query-answer bidirectional generation mechanism is used to rapidly amplify data with the system's help.","2024","2025-11-25 22:29:44","2025-11-25 22:29:44","","36–44","","","","","","","LGM3A '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Melbourne VIC, Australia","","","","large language model; artificial intelligence; data generation; multi-modality","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RCSBHZ74","bookSection","2025","He, Xuan; Li, Dong; Wen, Hao; Zhu, Yueheng; Liu, Chao; Yan, Meng; Zhang, Hongyu","CoSEFA: An LLM-Based Programming Assistant for Secure Code Generation via Supervised Co-Decoding","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","","https://doi.org/10.1145/3696630.3728609","Programming assistants based on Large Language Models (LLMs) assist developers in code generation and improve efficiency. These LLM-based programming assistants are prone to generate code with vulnerabilities. To mitigate security risk, existing approaches retrain an LLM with crafted data without vulnerabilities. However, retraining requires enormous computational costs and the accessible LLMs' internal representation, often unavailable for LLMs deployed as distributed online services. Our previous work, CoSec, proposed a supervised co-decoding approach using base LLMs alongside a small-scale security-focused LLM fine-tuned with code changes before and after vulnerability fixing.In this paper, we present CoSEFA, a programming assistant with CoSec as its core component for secure code hardening and error fixing. The front-end is a Visual Studio Code IDE extension with a dialogue interaction interface. The back-end handles users' request for code generation, test case generation, etc., providing secure code generation and addressing three types of code generation errors. Experimental results showed that CoSEFA improved the generation security by 9.34% and functionality correctness by 11.2%, compared with the base LLM. Our user study showed that experienced developers rated CoSEFA as more useful than Copilot for security-related programming tasks, highlighting the importance of secure code generation.Demo Source Code: https://github.com/TPA115K31/CoSEFADemo Video: https://youtu.be/LO_JH1eQBbY","2025","2025-11-25 22:29:44","2025-11-25 22:29:44","","1198–1202","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V7TG6H46","bookSection","2025","Macedo, Marcos; Tian, Yuan; Nie, Pengyu; Cogo, Filipe R.; Adams, Bram","InterTrans: Leveraging Transitive Intermediate Translations to Enhance LLM-Based Code Translation","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00236","Code translation aims to convert a program from one programming language (PL) to another. This long-standing software engineering task is crucial for modernizing legacy systems, ensuring cross-platform compatibility, enhancing performance, and more. However, automating this process remains challenging due to many syntactic and semantic differences between PLs. Recent studies show that even advanced techniques such as large language models (LLMs), especially open-source LLMs, still struggle with the task.Currently, code LLMs are trained with source code from multiple programming languages, thus presenting multilingual capabilities. In this paper, we investigate whether such capabilities can be harnessed to enhance code translation. To achieve this goal, we introduce InterTrans, an LLM-based automated code translation approach that, in contrast to existing approaches, leverages intermediate translations to bridge the syntactic and semantic gaps between source and target PLs. InterTrans contains two stages. It first utilizes a novel Tree of Code Translation (ToCT) algorithm to plan transitive intermediate translation sequences between a given source and target PL, then validates them in a specific order. We evaluate InterTrans with three open LLMs on three benchmarks (i.e., CodeNet, HumanEval-X, and TransCoder) involving six PLs. Results show an absolute improvement of 18.3% to 43.3% in Computation Accuracy (CA) for InterTrans over Direct Translation with 10 attempts. The best-performing variant of InterTrans (with the Magicoder LLM) achieved an average CA of 87.3%-95.4% on three benchmarks.","2025","2025-11-25 22:29:44","2025-11-25 22:29:44","","1153–1164","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MXU3HEKD","conferencePaper","2025","Petingola, Mitchell; Zhang, Yue; Yan, Yan; Lin, Wenjun","Integrating Ethical AI Tools into Educational Practices for Enhancing Academic Integrity","Proceedings of the 7th ACM Conference on Conversational User Interfaces","979-8-4007-1527-3","","10.1145/3719160.3737626","https://doi.org/10.1145/3719160.3737626","As large language models (LLMs) become increasingly integrated into educational settings, concerns about academic integrity, ethical usage, and student engagement are becoming more prominent. While these AI tools can effectively provide personalized learning experiences and support diverse student needs, they also risk overreliance and promote unethical academic practices if used without appropriate safeguards. This paper presents a novel approach that integrates an LLM-based assistant directly into a learning management system (LMS) with carefully designed constraints to encourage active learning, reduce misuse, and preserve academic integrity. We establish core design principles to address the challenges associated with LLMs in education and provide a detailed description of our system’s architecture. Additionally, we conduct a pilot study to assess the tool’s impact on student learning and gather feedback for further improvements. A prototype of the tool is publicly available on Github.","2025","2025-11-25 22:29:44","2025-11-25 22:29:44","","","","","","","","","CUI '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","large language model; artificial intelligence; academic integrity; education","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5L6KFXAI","conferencePaper","2025","Quan, Lili; Xie, Xiaofei; Guo, Qianyu; Jiang, Lingxiao; Chen, Sen; Wang, Junjie; Li, Xiaohong","TensorJSFuzz: Effective Testing of Web-Based Deep Learning Frameworks via Input-Constraint Extraction","Proceedings of the ACM on Web Conference 2025","979-8-4007-1274-6","","10.1145/3696410.3714649","https://doi.org/10.1145/3696410.3714649","As web applications grow in popularity, developers are increasingly integrating deep learning (DL) models into these environments. Web-based DL frameworks (e.g., TensorFlow.js) are essential for building and deploying such applications. Therefore, ensuring the quality of these frameworks is critical. While extensive testing efforts have been made for native DL frameworks such as TensorFlow and PyTorch, web-based DL frameworks have not yet undergone systematic testing. A key challenge is generating syntactically and semantically valid inputs while designing effective test oracles for web environments. To address this, we introduce TensorJSFuzz, a novel method for testing web-based DL frameworks. To ensure input quality, TensorJSFuzz extracts constraints directly from the source code of DL operators. By leveraging Large Language Models (e.g., ChatGPT) to understand the code and extract input constraints, TensorJSFuzz performs type-aware random generation coupled with dependency-aware refinement to create high-quality test inputs. These inputs are then subjected to differential testing across various backends, including CPU, TensorFlow, Wasm, and WebGL. Our experimental results show that TensorJSFuzz outperforms all baselines in generating valid inputs and identifying bugs. In particular, TensorJSFuzz successfully detected 92 bugs, with 30 already confirmed or fixed by developers, demonstrating its effectiveness in improving the robustness of web-based DL frameworks.","2025","2025-11-25 22:29:44","2025-11-25 22:29:44","","3405–3414","","","","","","","WWW '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sydney NSW, Australia","","","","large language model; fuzzing; web-based deep learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IYIHF4HV","conferencePaper","2024","Ding, Hao; Fan, Ziwei; Guehring, Ingo; Gupta, Gaurav; Ha, Wooseok; Huan, Jun; Liu, Linbo; Omidvar-Tehrani, Behrooz; Wang, Shiqi; Zhou, Hao","Reasoning and Planning with Large Language Models in Code Development","Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining","979-8-4007-0490-1","","10.1145/3637528.3671452","https://doi.org/10.1145/3637528.3671452","Large Language Models (LLMs) are revolutionizing the field of code development by leveraging their deep understanding of code patterns, syntax, and semantics to assist developers in various tasks, from code generation and testing to code understanding and documentation. In this survey, accompanying our proposed lecture-style tutorial for KDD 2024, we explore the multifaceted impact of LLMs on the code development, delving into techniques for generating a high-quality code, creating comprehensive test cases, automatically generating documentation, and engaging in an interactive code reasoning. Throughout the survey, we highlight some crucial components surrounding LLMs, including pre-training, fine-tuning, prompt engineering, iterative refinement, agent planning, and hallucination mitigation. We put forward that such ingredients are essential to harness the full potential of these powerful AI models in revolutionizing software engineering and paving the way for a more efficient, effective, and innovative future in code development.","2024","2025-11-25 22:29:44","2025-11-25 22:29:44","","6480–6490","","","","","","","KDD '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Barcelona, Spain","","","","large language model; code generation; application modernization; code development; code migration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QDBKDZ6R","conferencePaper","2024","Molina, Facundo; Copia, Juan Manuel; Gorla, Alessandra","FixCheck: A Tool for Improving Patch Correctness Analysis","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3685308","https://doi.org/10.1145/3650212.3685308","Patch correctness assessment aims at effectively detecting overfitted patches, i.e., patches that causes all tests to pass but do not actually fix the bug. Although several automated techniques for assessing patch correctness have been proposed, these techniques typically yield a binary result (correct/incorrect) without providing any additional information explaining the rationale behind the decision of classifying a patch as correct or incorrect. This tool demo paper presents FixCheck, a tool based on static analysis, random testing and Large Language Models (LLMs), that seeks to improve the patch correctness analysis process by providing fault-revealing tests for potentially incorrect patches. To this end, FixCheck first employs static analysis and random testing to generate a comprehensive set of test cases that are similar to the original failing test case. Then, FixCheck relies on LLMs to derive meaningful assertions for each new test case. Finally, FixCheck executes the generated tests, and those that fail are selected and prioritized based on their likelihood of revealing a defect in the patch.","2024","2025-11-25 22:29:45","2025-11-25 22:29:45","","1856–1860","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large Language Models; Dynamic Analysis; Patch Correctness Assessment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DUNEX8V8","conferencePaper","2023","Hertling, Sven; Paulheim, Heiko","OLaLa: Ontology Matching with Large Language Models","Proceedings of the 12th Knowledge Capture Conference 2023","979-8-4007-0141-2","","10.1145/3587259.3627571","https://doi.org/10.1145/3587259.3627571","Ontology (and more generally: Knowledge Graph) Matching is a challenging task where information in natural language is one of the most important signals to process. With the rise of Large Language Models, it is possible to incorporate this knowledge in a better way into the matching pipeline. A number of decisions still need to be taken, e.g., how to generate a prompt that is useful to the model, how information in the KG can be formulated in prompts, which Large Language Model to choose, how to provide existing correspondences to the model, how to generate candidates, etc. In this paper, we present a prototype that explores these questions by applying zero-shot and few-shot prompting with multiple open Large Language Models to different tasks of the Ontology Alignment Evaluation Initiative (OAEI). We show that with only a handful of examples and a well-designed prompt, it is possible to achieve results that are en par with supervised matching systems which use a much larger portion of the ground truth.","2023","2025-11-25 22:29:45","2025-11-25 22:29:45","","131–139","","","","","","","K-CAP '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Pensacola, FL, USA","","","","Large Language Model; Entity Resolution; Ontology Matching","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FVQJCJE8","journalArticle","2025","Blocklove, Jason; Thakur, Shailja; Tan, Benjamin; Pearce, Hammond; Garg, Siddharth; Karri, Ramesh","Automatically Improving LLM-based Verilog Generation using EDA Tool Feedback","ACM Trans. Des. Autom. Electron. Syst.","","1084-4309","10.1145/3723876","https://doi.org/10.1145/3723876","Traditionally, digital hardware designs are written in the Verilog hardware description language (HDL) and debugged manually by engineers. This can be time-consuming and error-prone for complex designs. Large Language Models (LLMs) are emerging as a potential tool to help generate fully functioning HDL code, but most works have focused on generation in the single-shot capacity: i.e., run and evaluate, a process that does not leverage debugging and, as such, does not adequately reflect a realistic development process. In this work, we evaluate the ability of LLMs to leverage feedback from electronic design automation (EDA) tools to fix mistakes in their own generated Verilog. To accomplish this, we present an open-source, highly customizable framework, AutoChip, which combines conversational LLMs with the output from Verilog compilers and simulations to iteratively generate and repair Verilog. To determine the success of these LLMs we leverage the VerilogEval benchmark set. We evaluate four state-of-the-art conversational LLMs, focusing on readily accessible commercial models. EDA tool feedback proved to be consistently more effective than zero-shot prompting only with GPT-4o, the most computationally complex model we evaluated. In the best case, we observed a 5.8% increase in the number of successful designs with a 34.2% decrease in cost over the best zero-shot results. Mixing smaller models with this larger model at the end of the feedback iterations resulted in equally as much success as with GPT-4o using feedback, but incurred 41.9% lower cost (corresponding to an overall decrease in cost over zero-shot by 89.6%).","2025-10","2025-11-25 22:29:45","2025-11-25 22:29:45","","","","6","30","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","large language models; Verilog; automation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IQRSKQ7X","conferencePaper","2025","Blasquez, Isabelle","Developing Critical Thinking with AI Coding Assistants: An Educational Experience focusing on Testing and Legacy Code","Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 1","979-8-4007-1567-9","","10.1145/3724363.3729050","https://doi.org/10.1145/3724363.3729050","The rise of AI coding assistants, like GitHub Copilot, is transforming software development. These tools promise productivity gains and support for various tasks, from code generation to explain legacy systems. However, their integration into education raises pedagogical challenges: How can we use their potential without compromising students' autonomy and mastery of fundamental concepts? How can we stay critical of their limitations?This paper explores these questions through a structured educational experience. It is based on a guided tutorial designed to confront students with the assistants' limitations. A simultaneous questionnaire is provided to allow students to take the necessary time to thoroughly analyze the assistant's responses. The tutorial has three stages. It introduces students to real-world scenarios of increasing complexity: getting started, implementing business rules, and working on a legacy project.This approach helps develop skills such as critical thinking, prompt refinement, and error correction. The results also show that well-supervised use of coding assistants can enhance teaching in testing and working with legacy code. They help students overcome initial roadblocks and encourage them to think about best practices. Furthermore, students themselves highlight the importance of supervising these tools to maintain their autonomy.","2025","2025-11-25 22:29:45","2025-11-25 22:29:45","","500–506","","","","","","","ITiCSE 2025","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Nijmegen, Netherlands","","","","generative ai; software engineering; critical thinking; software quality","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M4AS8KGI","journalArticle","2025","Leite Ramalho, Neilson Carlos; Amario de Souza, Higor; Lordello Chaim, Marcos","Testing and Debugging Quantum Programs: The Road to 2030","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3715106","https://doi.org/10.1145/3715106","Quantum computing has existed in the theoretical realm for several decades. Recently, quantum computing has re-emerged as a promising technology to solve problems that a classical computer could take hundreds of years to solve. However, there are challenges and opportunities for academics and practitioners regarding software engineering practices for testing and debugging quantum programs. This article presents a roadmap for addressing these challenges, pointing out the existing gaps in the literature and suggesting research directions. We discuss the limitations caused by noise, the no-cloning theorem, the lack of a standard architecture for quantum computers, among others. Regarding testing, we highlight gaps and opportunities related to transpilation, mutation analysis, input states with hybrid interfaces, program analysis, and coverage. For debugging, we present the current strategies, including classical techniques applied to quantum programs, quantum-specific assertions, and quantum-related bug patterns. We introduce a conceptual model to illustrate concepts regarding the testing and debugging of quantum programs and the relationship between them. Those concepts are used to identify and discuss research challenges to cope with quantum programs through 2030, focusing on the interfaces between classical and quantum computing and on creating testing and debugging techniques that take advantage of the unique quantum computing characteristics.","2025-05","2025-11-25 22:29:45","2025-11-25 22:29:45","","","","5","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Quantum Software Debugging; Quantum Software Engineering; Quantum Software Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JTM7PZX8","conferencePaper","2023","Lau, Sam; Guo, Philip","From ""Ban It Till We Understand It"" to ""Resistance is Futile"": How University Programming Instructors Plan to Adapt as More Students Use AI Code Generation and Explanation Tools such as ChatGPT and GitHub Copilot","Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 1","978-1-4503-9976-0","","10.1145/3568813.3600138","https://doi.org/10.1145/3568813.3600138","Over the past year (2022–2023), recently-released AI tools such as ChatGPT and GitHub Copilot have gained significant attention from computing educators. Both researchers and practitioners have discovered that these tools can generate correct solutions to a variety of introductory programming assignments and accurately explain the contents of code. Given their current capabilities and likely advances in the coming years, how do university instructors plan to adapt their courses to ensure that students still learn well? To gather a diverse sample of perspectives, we interviewed 20 introductory programming instructors (9 women + 11 men) across 9 countries (Australia, Botswana, Canada, Chile, China, Rwanda, Spain, Switzerland, United States) spanning all 6 populated continents. To our knowledge, this is the first empirical study to gather instructor perspectives about how they plan to adapt to these AI coding tools that more students will likely have access to in the future. We found that, in the short-term, many planned to take immediate measures to discourage AI-assisted cheating. Then opinions diverged about how to work with AI coding tools longer-term, with one side wanting to ban them and continue teaching programming fundamentals, and the other side wanting to integrate them into courses to prepare students for future jobs. Our study findings capture a rare snapshot in time in early 2023 as computing instructors are just starting to form opinions about this fast-growing phenomenon but have not yet converged to any consensus about best practices. Using these findings as inspiration, we synthesized a diverse set of open research questions regarding how to develop, deploy, and evaluate AI coding tools for computing education.","2023","2025-11-25 22:29:45","2025-11-25 22:29:45","","106–121","","","","","","","ICER '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Chicago, IL, USA","","","","LLM; ChatGPT; Copilot; AI coding tools; instructor perspectives","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MJLA8XGE","conferencePaper","2025","Xu, Hanxiang; Ma, Wei; Zhou, Ting; Zhao, Yanjie; Chen, Kai; Hu, Qiang; Liu, Yang; Wang, Haoyu","CKGFuzzer: LLM-Based Fuzz Driver Generation Enhanced By Code Knowledge Graph","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings","979-8-3315-3683-1","","10.1109/ICSE-Companion66252.2025.00079","https://doi.org/10.1109/ICSE-Companion66252.2025.00079","In recent years, the programming capabilities of large language models (LLMs) have garnered significant attention. Fuzz testing, a highly effective technique, plays a key role in enhancing software reliability and detecting vulnerabilities. However, traditional fuzz testing tools rely on manually crafted fuzz drivers, which can limit both testing efficiency and effectiveness. To address this challenge, we propose an automated fuzz testing method driven by a code knowledge graph and powered by an LLM-based intelligent agent system, referred to as CKGFuzzer. We approach fuzz driver creation as a code generation task, leveraging the knowledge graph of the code repository to automate the generation process within the fuzzing loop, while continuously refining both the fuzz driver and input seeds. The code knowledge graph is constructed through interprocedural program analysis, where each node in the graph represents a code entity, such as a function or a file. The knowledge graph-enhanced CKGFuzzer not only effectively resolves compilation errors in fuzz drivers and generates input seeds tailored to specific API usage scenarios, but also analyzes fuzz driver crash reports, assisting developers in improving code quality. By querying the knowledge graph of the code repository and learning from API usage scenarios, we can better identify testing targets and understand the specific purpose of each fuzz driver. We evaluated our approach using eight open-source software projects. The experimental results indicate that CKGFuzzer achieved an average improvement of 8.73% in code coverage compared to state-of-the-art techniques. Additionally, CKGFuzzer reduced the manual review workload in crash case analysis by 84.4% and successfully detected 11 real bugs (including nine previously unreported bugs) across the tested libraries. Our research enhances the overall performance of fuzz testing by refining fuzz driver generation strategies and input seed analysis, offering a more effective solution for vulnerability remediation and software quality improvement.","2025","2025-11-25 22:29:45","2025-11-25 22:29:45","","243–254","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IGZBHMX5","conferencePaper","2025","Roy, Nimisha; Horielko, Oleksandr; Omojokun, Olufisayo","Integrating AI Tools in Advanced Computer Science Curricula: A Case Study of Course Redesign","Proceedings of the ACM Global on Computing Education Conference 2025 Vol 1","979-8-4007-1929-5","","10.1145/3736181.3747130","https://doi.org/10.1145/3736181.3747130","Integrating artificial intelligence (AI) tools into software engineering (SE) has transformed industry workflows, presenting new opportunities and challenges for computing education. This experience report details the systematic redesign of two advanced undergraduate courses-Capstone (CS30) and Software Engineering (CS31)-to incorporate AI tools into software design, implementation, and testing phases while preserving core engineering rigor. The redesign was structured to ensure that AI tools enhanced rather than replaced students' problem-solving and SE skills, reinforcing critical engagement with AI-generated outputs. Unlike introductory courses, where AI may overshadow fundamental learning, this intervention introduced AI at a stage where students had already developed programming and software engineering competencies. The curriculum emphasized project-based learning, iterative refinement, debugging strategies, and structured AI tool comparisons, encouraging students to critically assess AI-assisted workflows against traditional manual development through assignments and reflections. While AI tools improved efficiency in repetitive tasks, challenges such as hallucinations, biases, and prompt engineering needs highlighted the importance of scaffolding AI-assisted learning. This report presents lessons learned, key challenges, and best practices for integrating AI into computing curricula, ensuring students balance AI automation with rigorous problem-solving and preparation for industry workflows.","2025","2025-11-25 22:29:45","2025-11-25 22:29:45","","92–98","","","","","","","CompEd 2025","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Gaborone, Botswana","","","","generative ai; software engineering; hallucination; productivity; capstone; project-based learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z3VCYT3U","conferencePaper","2025","Hebbar, Sannidhi V; Harini S, Sasmita; Kumar, Viraj","Refuting LLM-generated Code with Reactive Task Comprehension","Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 1","979-8-4007-1567-9","","10.1145/3724363.3729100","https://doi.org/10.1145/3724363.3729100","Large Language Models (LLMs) for code generation have improved to the point where they are being integrated into professional software development workflows. Since these models occasionally generate buggy code, it is important for students to develop the ability to refute such code (typically, by identifying a counterexample input on which the code fails to perform the desired task). To create counterexamples manually, prior work has suggested code comprehension and task comprehension as two necessary skills. In this paper, we anticipate advances in software development tools and consider a limited form of the latter skill – reactive task comprehension – where students only need to correctly state the code's desired behavior on inputs suggested by an automated system. We make two contributions. First, we demonstrate the feasibility of such a system based on existing LLMs and code coverage tools. Second, we show that reactive task comprehension is surprisingly effective in refuting LLM-generated buggy Python functions in the HumanEval+ dataset. Bearing in mind that students are likely to have access to increasingly sophisticated code generation models and assistive systems, we discuss the implications of our findings for introductory programming education.","2025","2025-11-25 22:29:45","2025-11-25 22:29:45","","30–36","","","","","","","ITiCSE 2025","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Nijmegen, Netherlands","","","","code comprehension; cs0/cs1; refute problems; task comprehension","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2NLK3FL8","conferencePaper","2024","Chen, Yang","Flakiness Repair in the Era of Large Language Models","Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings","979-8-4007-0502-1","","10.1145/3639478.3641227","https://doi.org/10.1145/3639478.3641227","Flaky tests can non-deterministically pass or fail regardless of any change to the code, which negatively impacts the effectiveness of the regression testing. Prior repair techniques for flaky tests mainly leverage program analysis techniques to mitigate test flakiness, which only focus on Order-Dependent (OD) and Implementation-Dependent (ID) flakiness with known flakiness patterns and root causes. In this paper, we propose an approach to repair flaky tests with the power of Large Language Models (LLMs). Our approach successfully repaired 79% of OD tests and 58% of ID tests in an extensive evaluation using 666 flaky tests from 222 projects. We submitted pull requests to fix 61 flaky tests; at the time of submission, 19 tests have already been accepted. However, we observed that currently LLMs are ineffective in adequately repairing Non-Order-Dependent (NOD) flaky tests by analyzing 118 of such tests from 11 projects.","2024","2025-11-25 22:29:45","2025-11-25 22:29:45","","441–443","","","","","","","ICSE-Companion '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","software testing; large language models; test flakiness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KFEN4BKR","conferencePaper","2024","Lyu, Yunlong; Xie, Yuxuan; Chen, Peng; Chen, Hao","Prompt Fuzzing for Fuzz Driver Generation","Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security","979-8-4007-0636-3","","10.1145/3658644.3670396","https://doi.org/10.1145/3658644.3670396","Crafting high-quality fuzz drivers not only is time-consuming but also requires a deep understanding of the library. However, the state-of-the-art automatic fuzz driver generation techniques fall short of expectations. While fuzz drivers derived from consumer code can reach deep states, they have limited coverage. Conversely, interpretative fuzzing can explore most API calls but requires numerous attempts within a large search space. We propose PromptFuzz, a coverage-guided fuzzer for prompt fuzzing that iteratively generates fuzz drivers to explore undiscovered library code. To explore API usage in fuzz drivers during prompt fuzzing, we propose several key techniques: instructive program generation, erroneous program validation, coverage-guided prompt mutation, and constrained fuzzer scheduling. We implemented PromptFuzz and evaluated it on 14 real-world libraries. Compared with OSS-Fuzz and Hopper (the state-of-the-art fuzz driver generation tool), fuzz drivers generated by PromptFuzz achieved 1.61 and 1.63 times higher branch coverage than those by OSS-Fuzz and Hopper, respectively. Moreover, the fuzz drivers generated by PromptFuzz detected 33 genuine, new bugs out of a total of 49 crashes, out of which 30 bugs have been confirmed by their respective communities.","2024","2025-11-25 22:29:45","2025-11-25 22:29:45","","3793–3807","","","","","","","CCS '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Salt Lake City, UT, USA","","","","automated test generation; fuzzing; vulnerability detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZS6UQ8SH","journalArticle","2024","Xie, Xiaoyuan; Jin, Shuo; Chen, Songqiang; Cheung, Shing-Chi","Word Closure-Based Metamorphic Testing for Machine Translation","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3675396","https://doi.org/10.1145/3675396","With the wide application of machine translation, the testing of Machine Translation Systems (MTSs) has attracted much attention. Recent works apply Metamorphic Testing (MT) to address the oracle problem in MTS testing. Existing MT methods for MTS generally follow the workflow of input transformation and output relation comparison, which generates a follow-up input sentence by mutating the source input and compares the source and follow-up output translations to detect translation errors, respectively. These methods use various input transformations to generate the test case pairs and have successfully triggered numerous translation errors. However, they have limitations in performing fine-grained and rigorous output relation comparison and thus may report many false alarms and miss many true errors. In this article, we propose a word closure-based output comparison method to address the limitations of the existing MTS MT methods. We first propose word closure as a new comparison unit, where each closure includes a group of correlated input and output words in the test case pair. Word closures suggest the linkages between the appropriate fragment in the source output translation and its counterpart in the follow-up output for comparison. Next, we compare the semantics on the level of word closure to identify the translation errors. In this way, we perform a fine-grained and rigorous semantic comparison for the outputs and thus realize more effective violation identification. We evaluate our method with the test cases generated by five existing input transformations and the translation outputs from three popular MTSs. Results show that our method significantly outperforms the existing works in violation identification by improving the precision and recall and achieving an average increase of 29.9% in F1 score. It also helps to increase the F1 score of translation error localization by 35.9%.","2024-11","2025-11-25 22:29:45","2025-11-25 22:29:45","","","","8","33","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","deep learning testing; metamorphic testing; Machine translation; word closure","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A56I6RDQ","conferencePaper","2025","Shao, Yuchen; Huang, Yuheng; Shen, Jiawei; Ma, Lei; Su, Ting; Wan, Chengcheng","Are LLMs Correctly Integrated into Software Systems?","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00204","https://doi.org/10.1109/ICSE55347.2025.00204","Large language models (LLMs) provide effective solutions in various application scenarios, with the support of retrieval-augmented generation (RAG). However, developers face challenges in integrating LLM and RAG into software systems, due to lacking interface specifications, various requirements from software context, and complicated system management. In this paper, we have conducted a comprehensive study of 100 open-source applications that incorporate LLMs with RAG support, and identified 18 defect patterns. Our study reveals that 77% of these applications contain more than three types of integration defects that degrade software functionality, efficiency, and security. Guided by our study, we propose systematic guidelines for resolving these defects in software life cycle. We also construct an open-source defect library Hydrangea [1].","2025","2025-11-25 22:29:45","2025-11-25 22:29:45","","1178–1190","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","LLM; empirical software engineering; defects","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VKA5GEYH","conferencePaper","2023","Qureshi, Basit","ChatGPT in Computer Science Curriculum Assessment: An analysis of Its Successes and Shortcomings","Proceedings of the 2023 9th International Conference on E-Society, e-Learning and e-Technologies","979-8-4007-0041-5","","10.1145/3613944.3613946","https://doi.org/10.1145/3613944.3613946","The application of Artificial intelligence for teaching and learning in the academic sphere is a trending subject of interest in computing education. ChatGPT, as an AI-based tool, provides various advantages, such as heightened student involvement, cooperation, accessibility, and availability. This paper addresses the prospects and obstacles associated with utilizing ChatGPT as a tool for learning and assessment in undergraduate Computer Science curriculum in particular to teaching and learning fundamental programming courses. Students having completed the course work for a Data Structures and Algorithms (a sophomore-level course) participated in this study. Two groups of students were given programming challenges to solve within a short period of time. The control group (group A) had access to textbooks and notes of programming courses, however, no Internet access was provided. Group B students were given access to ChatGPT and were encouraged to use it to help solve the programming challenges. The challenge was conducted in a computer lab environment using Programming Contest Control (PC2) environment which is widely used in ACM International Collegiate Programming Contest (ICPC). Each team of students addresses the problem by writing executable code that satisfies a certain number of test cases. Student teams were scored based on their performance in terms of the number of successfully passed test cases. Results show that students using ChatGPT had an advantage in terms of earned scores, however, there were inconsistencies and inaccuracies in the submitted code consequently affecting the overall performance. After a thorough analysis, the paper’s findings indicate that incorporating AI in higher education brings about various opportunities and challenges. Nonetheless, universities can efficiently manage these apprehensions by adopting a proactive and ethical stance toward the implementation of such tools.","2023","2025-11-25 22:29:45","2025-11-25 22:29:45","","7–13","","","","","","","ICSLT '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Portsmouth, United Kingdom","","","","ChatGPT; Academic assessment; Data Structures and Algorithms; programming concepts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T43V9X6L","conferencePaper","2025","Guo, Qi; Xie, Xiaofei; Liu, Shangqing; Hu, Ming; Li, Xiaohong; Bu, Lei","Intention is All You Need: Refining Your Code from Your Intention","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00191","https://doi.org/10.1109/ICSE55347.2025.00191","Code refinement aims to enhance existing code by addressing issues, refactoring, and optimizing to improve quality and meet specific requirements. As software projects scale in size and complexity, the traditional iterative exchange between reviewers and developers becomes increasingly burdensome. While recent deep learning techniques have been explored to accelerate this process, their performance remains limited, primarily due to challenges in accurately understanding reviewers' intents.This paper proposes an intention-based code refinement technique that enhances the conventional comment-to-code process by explicitly extracting reviewer intentions from the comments. Our approach consists of two key phases: Intention Extraction and Intention Guided Revision Generation. Intention Extraction categorizes comments using predefined templates, while Intention Guided Revision Generation employs large language models (LLMs) to generate revised code based on these defined intentions. Three categories with eight subcategories are designed for comment transformation, which is followed by a hybrid approach that combines rule-based and LLM-based classifiers for accurate classification. Extensive experiments with five LLMs (GPT4o, GPT3.5, DeepSeekV2, DeepSeek7B, CodeQwen7B) under different prompting settings demonstrate that our approach achieves 79% accuracy in intention extraction and up to 66% in code refinement generation. Our results highlight the potential of our approach in enhancing data quality and improving the efficiency of code refinement.","2025","2025-11-25 22:29:45","2025-11-25 22:29:45","","1127–1139","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","large language model; code refinement; intention-based generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QYUSADJQ","conferencePaper","2023","Chen, Le; Ding, Xianzhong; Emani, Murali; Vanderbruggen, Tristan; Lin, Pei-Hung; Liao, Chunhua","Data Race Detection Using Large Language Models","Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis","979-8-4007-0785-8","","10.1145/3624062.3624088","https://doi.org/10.1145/3624062.3624088","Large language models (LLMs) are demonstrating significant promise as an alternate strategy to facilitate analyses and optimizations of high-performance computing programs, circumventing the need for resource-intensive manual tool creation. In this paper, we explore a novel LLM-based data race detection approach combining prompting engineering and fine-tuning techniques. We create a dedicated dataset named DRB-ML, which is derived from DataRaceBench, with fine-grain labels showing the presence of data race pairs and their associated variables, line numbers, and read/write information. DRB-ML is then used to evaluate representative LLMs and fine-tune open-source ones. Our experiment shows that LLMs can be a viable approach to data race detection. However, they still cannot compete with traditional data race detection tools when we need detailed information about variable pairs causing data races.","2023","2025-11-25 22:29:45","2025-11-25 22:29:45","","215–223","","","","","","","SC-W '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Denver, CO, USA","","","","large language model; OpenMP; data race detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TNSHCU6R","conferencePaper","2025","Zhang, Hao; Yu, Dongjun; Zhang, Lei; Rong, Guoping; Yu, Yongda; Shen, Haifeng; Zhang, He; Shao, Dong; Kuang, Hongyu","AUCAD: Automated Construction of Alignment Dataset from Log-Related Issues for Enhancing LLM-based Log Generation","Proceedings of the 16th International Conference on Internetware","979-8-4007-1926-4","","10.1145/3755881.3755889","https://doi.org/10.1145/3755881.3755889","Log statements have become an integral part of modern software systems. Prior research efforts have focused on supporting the decisions of placing log statements, such as where/what to log. With the increasing adoption of Large Language Models (LLMs) for code-related tasks such as code completion or generation, automated approaches for generating log statements have gained much momentum. However, the performance of these approaches still has a long way to go. This paper explores enhancing the performance of LLM-based solutions for automated log statement generation by post-training LLMs with a purpose-built dataset. Thus the primary contribution is a novel approach called AUCAD, which automatically constructs such a dataset with information extracting from log-related issues. Researchers have long noticed that a significant portion of the issues in the open-source community are related to log statements. However, distilling this portion of data requires manual efforts, which is labor-intensive and costly, rendering it impractical. Utilizing our approach, we automatically extract log-related issues from 1,537 entries of log data across 88 projects and identify 808 code snippets (i.e., methods) with retrievable source code both before and after modification of each issue (including log statements) to construct a dataset. Each entry in the dataset consists of a data pair representing high-quality and problematic log statements, respectively. With this dataset, we proceed to post-train multiple LLMs (primarily from the Llama series) for automated log statement generation. Both human and experimental evaluations indicate that these models significantly outperform existing LLM-based solutions, thereby validating the efficacy of our method for constructing a post-training dataset to enhance LLM-based log statement generation.","2025","2025-11-25 22:29:45","2025-11-25 22:29:45","","413–425","","","","","","","Internetware '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","LLM; Alignment; Log Statement Generation; Log-related Issues","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NDDGC7YA","journalArticle","2025","Tang, Mingxin; Chen, Wei; Wu, Lizhou; Huang, Libo; Zeng, Kun","ChatDSE: A Zero-Shot Microarchitecture Design Space Explorer Powered by GPT4.0","ACM Trans. Des. Autom. Electron. Syst.","","1084-4309","10.1145/3735640","https://doi.org/10.1145/3735640","Design Space Exploration (DSE) aims at identifying Pareto optimal synthesis configurations. Previous works require microarchitecture samples with key labels, including power and clock cycles, to train their models. However, as the chip design space expands rapidly, the cost of sampling the design space has significantly increased, due to the growing number of samples and time-consuming Very Large Scale Integration (VLSI) implementation flow. Recent advancements in Large Language Models (LLMs) have demonstrated their remarkable power in zero-shot learning tasks, presenting an innovative strategy for accomplishing DSE. Hence, this article presents ChatDSE, a zero-shot framework for DSE that is powered by the advanced capabilities of the LLM GPT4.0. Firstly, this framework analyzes the nature of the target microarchitecture and generates a corresponding system context to provide the prior knowledge of the microarchitecture. Secondly, a proposed sampling algorithm, PriorDC, identifies the most representative samples with pseudo labels. One of these samples is chosen as a baseline, whose power and clock cycles labels are set as 1, and the remaining sample labels are obtained by chatting with GPT4.0. Finally, ChatDSE engages in a dialogue with GPT4.0 to estimate the power and clock cycles of designs within the space, ultimately identifying the Pareto optimal design set. In the DSE for the RISC-V Berkeley Out-of-Order Machine (BOOM), experimental results show that ChatDSE is capable of identifying optimal designs and accelerates the exploration process by 574 times when compared to the state-of-the-art DSE methodologies.","2025-07","2025-11-25 22:29:45","2025-11-25 22:29:45","","","","4","30","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","large language model; BOOM microarchitecture; Design space exploration; pareto optimization; zero-shot","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"64RWV7KT","conferencePaper","2024","Mathews, Noble Saji; Nagappan, Meiyappan","Test-Driven Development and LLM-based Code Generation","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695527","https://doi.org/10.1145/3691620.3695527","Recent Large Language Models (LLMs) have demonstrated significant capabilities in generating code snippets directly from problem statements. This increasingly automated process mirrors traditional human-led software development, where code is often written in response to a requirement. Historically, Test-Driven Development (TDD) has proven its merit, requiring developers to write tests before the functional code, ensuring alignment with the initial problem statements. Applying TDD principles to LLM-based code generation offers one distinct benefit: it enables developers to verify the correctness of generated code against predefined tests. This paper investigates if and how TDD can be incorporated into AI-assisted code-generation processes. We experimentally evaluate our hypothesis that providing LLMs like GPT-4 and Llama 3 with tests in addition to the problem statements enhances code generation outcomes. We experimented with established function-level code generation benchmarks such as MBPP and HumanEval. Our results consistently demonstrate that including test cases leads to higher success in solving programming challenges. We assert that TDD is a promising paradigm for helping ensure that the code generated by LLMs effectively captures the requirements.","2024","2025-11-25 22:29:45","2025-11-25 22:47:59","","1583–1594","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","Large language models; LLM; Software; Software engineering; Accuracy; Software development management; Codes; Testing; Benchmark testing; Software Engineering; software engineering; testing; code generation; Code Generation; Programming; TDD; Computational modeling; Mirrors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LV8BKKJ9","conferencePaper","2025","Guo, Siyuan; Liu, Huiwu; Chen, Xiaolong; Xie, Yuming; Zhang, Liang; Han, Tao; Chen, Hechang; Chang, Yi; Wang, Jun","Optimizing Case-Based Reasoning System for Functional Test Script Generation with Large Language Models","Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2","979-8-4007-1454-2","","10.1145/3711896.3737254","https://doi.org/10.1145/3711896.3737254","In this work, we explore the potential of large language models (LLMs) for generating functional test scripts, which necessitates understanding the dynamically evolving code structure of the target software. To achieve this, we propose a case-based reasoning (CBR) system utilizing a 4R cycle (i.e., retrieve, reuse, revise, and retain), which maintains and leverages a case bank of test intent descriptions and corresponding test scripts to facilitate LLMs for test script generation. To improve user experience further, we introduce Re4, an optimization method for the CBR system, comprising reranking-based retrieval finetuning and reinforced reuse finetuning. Specifically, we first identify positive examples with high semantic and script similarity, providing reliable pseudo-labels for finetuning the retriever model without costly labeling. Then, we apply supervised finetuning, followed by a reinforcement learning finetuning stage, to align LLMs with our production scenarios, ensuring the faithful reuse of retrieved cases. Extensive experimental results on two product development units from Huawei Datacom demonstrate the superiority of the proposed CBR+Re4. Notably, we also show that the proposed Re4 method can help alleviate the repetitive generation issues with LLMs.","2025","2025-11-25 22:29:45","2025-11-25 22:29:45","","4487–4498","","","","","","","KDD '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Toronto ON, Canada","","","","large language model; reinforcement learning; case-based reasoning; functional testing; test script generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BWEZ2RS5","conferencePaper","2025","Wang, Yutong; Rubio-González, Cindy","LLM4FP: LLM-Based Program Generation for Triggering Floating-Point Inconsistencies Across Compilers","Proceedings of the SC '25 Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis","979-8-4007-1871-7","","10.1145/3731599.3767362","https://doi.org/10.1145/3731599.3767362","Floating-point inconsistencies across compilers can undermine the reliability of numerical software. We present LLM4FP, the first framework that uses Large Language Models (LLMs) to generate floating-point programs specifically designed to trigger such inconsistencies. LLM4FP combines Grammar-Based Generation and Feedback-Based Mutation to produce diverse and valid programs. We evaluate LLM4FP across multiple compilers and optimization levels, measuring inconsistency rate, time cost, and program diversity. LLM4FP detects over twice as many inconsistencies compared to the state-of-the-art tool, Varity. Notably, most of the inconsistencies involve real-valued differences, rather than extreme values like NaN or infinities. LLM4FP also uncovers inconsistencies across a wider range of optimization levels, and finds the most mismatches between host and device compilers. These results show that LLM-guided program generation improves the detection of numerical inconsistencies. In practice, numerical software and HPC developers can use LLM4FP to compare compilers and select those that provide more accurate and consistent floating-point behavior, while compiler developers can use it to identify and address subtle consistency issues in their implementations.","2025","2025-11-25 22:29:45","2025-11-25 22:29:45","","225–234","","","","","","","SC Workshops '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Large Language Models; differential testing; floating-point arithmetic; random testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KFCHAA95","conferencePaper","2023","Gu, Qiuhan","LLM-Based Code Generation Method for Golang Compiler Testing","Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","979-8-4007-0327-0","","10.1145/3611643.3617850","https://doi.org/10.1145/3611643.3617850","Modern optimizing compilers are among the most complex software systems humans build. One way to identify subtle compiler bugs is fuzzing. Both the quantity and the quality of testcases are crucial to the performance of fuzzing. Traditional testcase-generation methods, such as Csmith and YARPGen, have been proven successful at discovering compiler bugs. However, such generated testcases have limited coverage and quantity. In this paper, we present a code generation method for compiler testing based on LLM to maximize the quality and quantity of the generated code. In particular, to avoid undefined behavior and syntax errors in generated testcases, we design a filter strategy to clean the source code, preparing a high-quality dataset for the model training. Besides, we present a seed schedule strategy to improve code generation. We apply the method to test the Golang compiler and the result shows that our pipeline outperforms previous methods both qualitatively and quantitatively. It produces testcases with an average coverage of 3.38%, in contrast to the testcases generated by GoFuzz, which have an average coverage of 0.44%. Moreover, among all the generated testcases, only 2.79% exhibited syntax errors, and none displayed undefined behavior.","2023","2025-11-25 22:29:45","2025-11-25 22:29:45","","2201–2203","","","","","","","ESEC/FSE 2023","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: San Francisco, CA, USA","","","","Code generation; Compiler testing; Go language; Large model","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JBZ52NTJ","journalArticle","2025","Weißl, Oliver; Abdellatif, Amr; Chen, Xingcheng; Merabishvili, Giorgi; Riccio, Vincenzo; Kacianka, Severin; Stocco, Andrea","Targeted Deep Learning System Boundary Testing","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3771557","https://doi.org/10.1145/3771557","Evaluating the behavioral boundaries of deep learning (DL) systems is crucial for understanding their reliability across diverse, unseen inputs. Existing solutions fall short as they rely on untargeted, random perturbations with limited controlled input variations. In this work, we introduce Mimicry, a novel black-box test generator for fine-grained, targeted exploration of DL system boundaries. Mimicry performs boundary testing by leveraging the probabilistic nature of DL outputs to identify promising directions for exploration. By using style-based GANs to disentangle inputs into content and style components, Mimicry generates boundary test inputs by mimicking features from both source and target classes. We evaluated Mimicry’s effectiveness in generating boundary inputs for five DL image classification systems, comparing it to two baselines from the literature. Our results show that Mimicry consistently identifies inputs up to (25times) closer to the theoretical decision boundary, outperforming the baselines with statistical significance. Moreover, it generates semantically meaningful boundary test cases that reveal new functional misbehaviors, while the baselines mostly produce corrupted or invalid inputs. Thanks to its enhanced control over latent space manipulations, Mimicry remains effective as dataset complexity grows, resulting in a up to (36%) higher validity rate and competitive diversity, as supported by a comprehensive human assessment.","2025-10","2025-11-25 22:29:45","2025-11-25 22:29:45","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","generative AI; boundary testing; DL testing; search-based optimization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QB2PTSYC","conferencePaper","2024","Koziolek, Heiko; Grüner, Sten; Hark, Rhaban; Ashiwal, Virendra; Linsbauer, Sofia; Eskandani, Nafise","LLM-based and Retrieval-Augmented Control Code Generation","Proceedings of the 1st International Workshop on Large Language Models for Code","979-8-4007-0579-3","","10.1145/3643795.3648384","https://doi.org/10.1145/3643795.3648384","Control code is designed and implemented for industrial automation applications that manage power plants, petrochemical processes, or steel production. Popular large language models (LLM) can synthesize low-level control code in the Structured Text programming notation according to the standard IEC 61131-3, but are not aware of proprietary control code function block libraries, which are often used in practice. To automate control logic implementation tasks, we proposed a retrieval-augmented control code generation method that can integrate such function blocks into the generated code. With this method control engineers can benefit from the code generation capabilities of LLMs, re-use proprietary and well-tested function blocks, and speed up typical programming tasks significantly. We have evaluated the method using a prototypical implementation based on GPT-4, LangChain, Open-PLC, and the open-source OSCAT function block library. In several spot sample tests, we successfully generated IEC 61131-3 ST code that integrated the desired function blocks, could be compiled, and validated through simulations.","2024","2025-11-25 22:29:45","2025-11-25 22:29:45","","22–29","","","","","","","LLM4Code '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","ChatGPT; large language models; GPT-4; code generation; DCS; IEC 61131-3; industrial automation; PLC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MDRFJ8RZ","conferencePaper","2024","Canizares, Pablo C.; Ávila, Daniel; Perez-Soler, Sara; Guerra, Esther; De Lara, Juan","Coverage-based Strategies for the Automated Synthesis of Test Scenarios for Conversational Agents","Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)","979-8-4007-0588-5","","10.1145/3644032.3644456","https://doi.org/10.1145/3644032.3644456","Conversational agents - or chatbots - are increasingly used as the user interface to many software services. While open-domain chatbots like ChatGPT excel in their ability to chat about any topic, task-oriented conversational agents are designed to perform goal-oriented tasks (e.g., booking or shopping) guided by a dialogue-based user interaction, which is explicitly designed. Like any kind of software system, task-oriented conversational agents need to be properly tested to ensure their quality. For this purpose, some tools permit defining and executing conversation test cases. However, there are currently no established means to assess the coverage of the design of a task-oriented agent by a test suite, or mechanisms to automate quality test case generation ensuring the agent coverage.To attack this problem, we propose test coverage criteria for task-oriented conversational agents, and define coverage-based strategies to synthesise test scenarios, some oriented to test case reduction. We provide an implementation of the criteria and the strategies that is independent of the agent development platform. Finally, we report on their evaluation on open-source Dialogflow and Rasa agents, and a comparison against a state-of-the-art testing tool. The experiment shows benefits in terms of test generation correctness, increased coverage and reduced testing time.","2024","2025-11-25 22:29:45","2025-11-25 22:47:14","","23–33","","","","","","","AST '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","Software testing; Automation; Software systems; Chatbots; Testing; Debugging; testing; task-oriented conversational agents; test suite generation; Oral communication; Natural languages; Task-oriented conversational agents; Test suite generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A8Q4K46L","conferencePaper","2025","Sollenberger, Zachariah; Patel, Jay; Munley, Christian; Jarmusch, Aaron; Chandrasekaran, Sunita","LLM4VV: Exploring LLM-as-a-Judge for Validation and Verification Testsuites","Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis","979-8-3503-5554-3","","10.1109/SCW63240.2024.00238","https://doi.org/10.1109/SCW63240.2024.00238","Large Language Models (LLM) continue to improve and are revolutionizing the landscape of software development. These large models have demonstrated potential to generate, debug, test, analyze, document, and even translate code. Thus they are a valuable tool in the software development cycle. If used correctly, such tools can often accelerate the development cycle. Though the tools are powerful and new, the community is cautious of training using biased or sensitive data, which can lead to biased, dangerous, or incorrect outputs along with the inadvertent release of confidential information. Additionally, the carbon footprints and the un-explainability of these ""black box"" models continue to raise questions about the reliability of LLMs.With these opportunities and these challenges ahead, this paper explores the idea of ""judging"" LLM-generated code to better understand and ""open up"" the un-explainable ""black box"" models used by LLMs. We probe into the black box of one such LLM that has generated the best compiler tests for the directive-based programming models OpenMP and OpenACC in our earlier research. We challenge DeepSeek's deepseek-coder-33B-instruct model with intentionally-erroneous code, and we also define relevant metrics and adopt an agent-based approach to evaluate the LLM and assess its capabilities as an LLM-as-a-judge. We also develop a pipeline-based approach to streamline the entire workflow. Finally, we make use of all of these strategies together to develop a more reliable method for automatically validating LLM-generated compiler tests. Based on our results, utilizing an agent-based prompting approach and setting up a validation pipeline structure drastically increased the quality of deepseek-coder-33B-instruct evaluation of tests which are used to validate compiler implementations of directive-based parallel programming models.","2025","2025-11-25 22:29:46","2025-11-25 22:29:46","","1885–1893","","","","","","","SC-W '24","","","","IEEE Press","Atlanta, GA, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5BTEHGVH","conferencePaper","2025","Chen, Xin; Jiang, Zhijie; Guo, Yong; Jia, Zhouyang; Zheng, Si; Zhang, Yuanliang; Li, Shanshan","MetaCoder: Generating Code from Multiple Perspectives","Proceedings of the 16th International Conference on Internetware","979-8-4007-1926-4","","10.1145/3755881.3755884","https://doi.org/10.1145/3755881.3755884","Large Language Models (LLMs) have already demonstrated excellent performance in code generation tasks. However, their proficiency varies considerably among different programming languages, performing well in languages like Python, but struggling with languages such as C++ and Java. This discrepancy limits their utility in scenarios requiring multi-language support. Existing methods aimed at enhancing the code generation capabilities of LLMs typically emphasize general performance improvements while overlooking discrepancies between languages, resulting in suboptimal outcomes for less proficient languages.To address this challenge, we propose MetaCoder. Given a task description, MetaCoder first generates code in high-proficiency language, and then summarizes the code. Finally, MetaCoder generates target code using the task description, generated code, and summary. Additionally, MetaCoder detects and corrects syntax errors in the target code. We evaluate MetaCoder on HumanEval-x, and compared with Zero-Shot, the Pass@1 in generating C++ and Java code has improved by up to 13.09% and 16.98%, respectively.","2025","2025-11-25 22:29:46","2025-11-25 22:29:46","","437–448","","","","","","","Internetware '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Large Language Model; Code Generation; Multi-language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HYJD4VCM","journalArticle","2024","Deb, Sourav; Jain, Kush; van Tonder, Rijnard; Le Goues, Claire; Groce, Alex","Syntax Is All You Need: A Universal-Language Approach to Mutant Generation","Proc. ACM Softw. Eng.","","","10.1145/3643756","https://doi.org/10.1145/3643756","While mutation testing has been a topic of academic interest for decades, it is only recently that “real-world” developers, including industry leaders such as Google and Meta, have adopted mutation testing. We propose a new approach to the development of mutation testing tools, and in particular the core challenge of generating mutants. Current practice tends towards two limited approaches to mutation generation: mutants are either (1) generated at the bytecode/IR level, and thus neither human readable nor adaptable to source-level features of languages or projects, or (2) generated at the source level by language-specific tools that are hard to write and maintain, and in fact are often abandoned by both developers and users. We propose instead that source-level mutation generation is a special case of program transformation in general, and that adopting this approach allows for a single tool that can effectively generate source-level mutants for essentially any programming language. Furthermore, by using parser parser combinators many of the seeming limitations of an any-language approach can be overcome, without the need to parse specific languages. We compare this new approach to mutation to existing tools, and demonstrate the advantages of using parser parser combinators to improve on a regular-expression based approach to generation. Finally, we show that our approach can provide effective mutant generation even for a language for which it lacks any language-specific operators, and that is not very similar in syntax to any language it has been applied to previously.","2024-07","2025-11-25 22:29:46","2025-11-25 22:29:46","","","","FSE","1","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Software Testing; Mutants; Mutation Generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MMU386J9","conferencePaper","2025","Yang, Can; Pereira Nunes, Bernardo; Rodríguez Méndez, Sergio","LLM as Auto-Prompt Engineer: Automated NER Prompt Optimisation","Companion Proceedings of the ACM on Web Conference 2025","979-8-4007-1331-6","","10.1145/3701716.3717818","https://doi.org/10.1145/3701716.3717818","The emergence of Large Language Models (LLMs) has revolutionised natural language processing capabilities. However, despite these advances, effectively optimising prompts for knowledge extraction tasks like Named Entity Recognition (NER) remains challenging. This paper presents a zero-shot automated prompt engineering approach that decomposes the NER task into two phases: entity boundary detection and entity classification. Our method incorporates structured task analysis, automated prompt generation, test case generation, and iterative optimisation, requiring no labelled training examples. This decomposition allows for more precise entity recognition while maintaining the efficiency. Through experimentation on the CoNLL-2003 dataset using standard exact-match evaluation metrics, our approach demonstrates improvements over unified methods, achieving a 75.39% F1 score compared to baseline approaches (72.90%). The key contributions include: (1) A structured pipeline for zero-shot automated prompt engineering in NER tasks that addresses the challenges of prompt design and optimisation; (2) A two-phase approach to NER tasks that separates boundary detection from entity classification; and (3) Experimental results demonstrating the effectiveness of our approach compared to existing zero-shot approaches in NER tasks.","2025","2025-11-25 22:29:46","2025-11-25 22:29:46","","2574–2578","","","","","","","WWW '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sydney NSW, Australia","","","","large language models; automated prompt engineering; named entity recognition; prompt optimisation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YPPNWJFD","conferencePaper","2024","Zeng, Zhengran; Wang, Yidong; Xie, Rui; Ye, Wei; Zhang, Shikun","CoderUJB: An Executable and Unified Java Benchmark for Practical Programming Scenarios","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3652115","https://doi.org/10.1145/3650212.3652115","In the evolving landscape of large language models (LLMs) tailored for software engineering, the need for benchmarks that accurately reflect real-world development scenarios is paramount. Current benchmarks are either too simplistic or fail to capture the multi-tasking nature of software development. To address this, we introduce CoderUJB, a new benchmark designed to evaluate LLMs across diverse Java programming tasks that are executable and reflective of actual development scenarios, acknowledging Java's prevalence in real-world software production. CoderUJB comprises 2,239 programming questions derived from 17 real open-source Java projects and spans five practical programming tasks. Our empirical study on this benchmark investigates the coding abilities of various open-source and closed-source LLMs, examining the effects of continued pre-training in specific programming languages code and instruction fine-tuning on their performance. The findings indicate that while LLMs exhibit strong potential, challenges remain, particularly in non-functional code generation (e.g., test generation and defect detection). Importantly, our results advise caution in the specific programming languages continued pre-training and instruction fine-tuning, as these techniques could hinder model performance on certain tasks, suggesting the need for more nuanced strategies. CoderUJB thus marks a significant step towards more realistic evaluations of programming capabilities in LLMs, and our study provides valuable insights for the future development of these models in software engineering.","2024","2025-11-25 22:29:46","2025-11-25 22:29:46","","124–136","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large Language Models; Code Generation; Benchmark","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7A5BYWGY","conferencePaper","2025","Luo, Gang; Han, Julien; Ceker, Hayreddin; Bouyarmane, Karim","Using Large Language Models to Improve Product Information in E-commerce Catalogs","Proceedings of the 34th ACM International Conference on Information and Knowledge Management","979-8-4007-2040-6","","10.1145/3746252.3761437","https://doi.org/10.1145/3746252.3761437","To give customers good experience, an e-commerce retailer needs high-quality product information in its catalog. Yet, the raw product information often lacks sufficient quality. For a large catalog that can contain billions of products, manually fixing this information is highly labor-intensive. To address this issue, we propose using the tool use functionality of large language models to automatically improve product information. In this talk, we show why existing data cleaning methods are not well suited for this task and how we designed our automated system to improve product information. When evaluated on a random sample of products from an e-commerce catalog, our system improved product information completeness by 78% with no major drop in information accuracy.","2025","2025-11-25 22:29:46","2025-11-25 22:29:46","","6873–6874","","","","","","","CIKM '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Seoul, Republic of Korea","","","","large language model; e-commerce; product information; tool use","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HQ338EZG","conferencePaper","2024","Mattis, Toni; Böhme, Lukas; Krebs, Eva; Rinard, Martin C.; Hirschfeld, Robert","Faster Feedback with AI? A Test Prioritization Study","Companion Proceedings of the 8th International Conference on the Art, Science, and Engineering of Programming","979-8-4007-0634-9","","10.1145/3660829.3660837","https://doi.org/10.1145/3660829.3660837","Feedback during programming is desirable, but its usefulness depends on immediacy and relevance to the task. Unit and regression testing are practices to ensure programmers can obtain feedback on their changes; however, running a large test suite is rarely fast, and only a few results are relevant. Identifying tests relevant to a change can help programmers in two ways: upcoming issues can be detected earlier during programming, and relevant tests can serve as examples to help programmers understand the code they are editing. In this work, we describe an approach to evaluate how well large language models (LLMs) and embedding models can judge the relevance of a test to a change. We construct a dataset by applying faulty variations of real-world code changes and measuring whether the model could nominate the failing tests beforehand. We found that, while embedding models perform best on such a task, even simple information retrieval models are surprisingly competitive. In contrast, pre-trained LLMs are of limited use as they focus on confounding aspects like coding styles. We argue that the high computational cost of AI models is not always justified, and tool developers should also consider non-AI models for code-related retrieval and recommendation tasks. Lastly, we generalize from unit tests to live examples and outline how our approach can benefit live programming environments.","2024","2025-11-25 22:29:46","2025-11-25 22:29:46","","32–40","","","","","","","Programming '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lund, Sweden","","","","large language models; generative ai; testing; embedding models; test prioritization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AMGAL2MZ","conferencePaper","2025","Zhang, Haonan","On the Brittleness of Legacy Web UI Testing: A Pragmatic Perspective","Proceedings of the 34th ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-1474-0","","10.1145/3713081.3731742","https://doi.org/10.1145/3713081.3731742","Automated web UI testing addresses the labor- and time-intensive demands of manual testing. Despite its importance, numerous barriers impede the continuous automation of end-to-end web UI testing: brittle legacy tests, suboptimal automation frameworks, and web app design unsuited for effective UI testing. While many studies investigate these challenges, most overlook the root causes of brittleness from a pragmatic perspective. This dissertation first reassesses these causes by examining the brittleness of testing real-world web apps. Building on that, we extend existing frameworks to mitigate flakiness stemming from impractical automation frameworks. We also transform traditional web UI testing by decoupling test cases from web applications, using natural language as an intermediary. Finally, we introduce a pragmatic benchmark to steer research toward more realistic web applications. Through these studies, we aim to shed light on current challenges and encourage a more pragmatic perspective on UI testing.","2025","2025-11-25 22:29:46","2025-11-25 22:29:46","","76–79","","","","","","","ISSTA Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","large language models; automated web GUI testing; GUI testing brittleness; pragmatic GUI tests","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LNYF5FXW","conferencePaper","2025","Li, Fengjie; Jiang, Jiajun; Sun, Jiajun; Zhang, Hongyu","Evaluating the Generalizability of LLMs in Automated Program Repair","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: New Ideas and Emerging Results","979-8-3315-3711-1","","10.1109/ICSE-NIER66352.2025.00024","https://doi.org/10.1109/ICSE-NIER66352.2025.00024","LLM-based automated program repair methods have attracted significant attention for their state-of-the-art performance. However, they were primarily evaluated on a few well-known datasets like Defects4J, raising questions about their effectiveness on new datasets. In this study, we evaluate 11 top-performing LLMs on Defects4J-Trans, a new dataset derived from transforming Defects4J while maintaining the original semantics. Results from experiments on both Defects4J and Defects4J-Trans show that all studied LLMs have limited generalizability in APR tasks, with the average number of correct and plausible patches decreasing by 49.48% and 42.90%, respectively, on Defects4J-Trans. Further investigation into incorporating additional repair-relevant information in repair prompts reveals that, although this information significantly enhances the LLMs' capabilities (increasing the number of correct and plausible patches by up to 136.67% and 121.82%, respectively), performance still falls short of their original results. This indicates that prompt engineering alone is insufficient to substantially enhance LLMs' repair capabilities. Based on our study, we also offer several recommendations for future research.","2025","2025-11-25 22:29:46","2025-11-25 22:29:46","","91–95","","","","","","","ICSE-NIER '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","LLM; program repair; generalizability of LLM","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W69PW8K7","journalArticle","2025","Yu, Shengcheng; Fang, Chunrong; Tuo, Ziyuan; Zhang, Quanjun; Chen, Chunyang; Chen, Zhenyu; Su, Zhendong","Vision-Based Mobile App GUI Testing: A Survey","ACM Comput. Surv.","","0360-0300","10.1145/3773027","https://doi.org/10.1145/3773027","Graphical User Interface (GUI) has become one of the most significant parts of mobile applications (apps). It is a direct bridge between mobile apps and end users, which directly affects the end user’s experience. Neglecting GUI quality can undermine the value and effectiveness of the entire mobile app solution. Significant research efforts have been devoted to GUI testing, one effective method to ensure mobile app quality. By conducting rigorous GUI testing, developers can ensure that the visual and interactive elements of the mobile apps not only meet functional requirements but also provide a seamless and user-friendly experience. However, traditional solutions, relying on the source code or layout files, have met challenges in both effectiveness and efficiency due to the gap between what is obtained and what app GUI actually presents. Vision-based mobile app GUI testing approaches emerged with the development of computer vision technologies and have achieved promising progress. In this survey paper, we provide a comprehensive investigation of the state-of-the-art techniques on 271 papers, among which 92 are vision-based studies. This survey covers different topics of GUI testing, like GUI test generation, GUI test record &amp; replay, GUI testing framework, etc. In particular, we highlight the emerging role of vision-based techniques and analyze how they reshape traditional approaches to mobile app GUI testing. Based on the investigation of existing studies, we outline the challenges and opportunities of (vision-based) mobile app GUI testing and propose promising research directions with the combination of emerging techniques.","2025-10","2025-11-25 22:29:46","2025-11-25 22:29:46","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","GUI Testing; Mobile App Testing; GUI Image Understanding","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"788I9IFG","bookSection","2025","Ouyang, Shuyin; Zhang, Jie M.; Sun, Zeyu; Penuela, Albert Merono","Knowledge-Enhanced Program Repair for Data Science Code","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00246","This paper introduces DSrepair, a knowledge-enhanced program repair approach designed to repair the buggy code generated by LLMs in the data science domain. DSrepair uses knowledge graph based RAG for API knowledge retrieval and bug knowledge enrichment to construct repair prompts for LLMs. Specifically, to enable knowledge graph-based API retrieval, we construct DS-KG (Data Science Knowledge Graph) for widely used data science libraries. For bug knowledge enrichment, we employ an abstract syntax tree (AST) to localize errors at the AST node level. We evaluate DSrepair's effectiveness against five state-of-the-art LLM-based repair baselines using four advanced LLMs on the DS-1000 dataset. The results show that DSrepair outperforms all five baselines. Specifically, when compared to the second-best baseline, DSrepair achieves substantial improvements, fixing 44.4%, 14.2%, 20.6%, and 32.1% more buggy code snippets for each of the four evaluated LLMs, respectively. Additionally, it achieves greater efficiency, reducing the number of tokens required per code task by 17.49%, 34.24%, 24.71%, and 17.59%, respectively.","2025","2025-11-25 22:29:46","2025-11-25 22:29:46","","898–910","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V6V7WY6J","journalArticle","2025","Davis, Matthew C.; Choi, Sangheon; Wei, Amy; Estep, Sam; Myers, Brad A.; Sunshine, Joshua","TestLoop: A Process Model Describing Human-in-the-Loop Software Test Suite Generation","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3765754","https://doi.org/10.1145/3765754","There is substantial diversity among testing tools used by software engineers. For example, fuzzers may target crashes and security vulnerabilities while Test sUite Generators (TUGs) may create high-coverage test suites. In the research community, test generation tools are primarily evaluated using metrics like bugs identified or code coverage. However, achieving good values for these metrics does not necessarily imply that these tools help software engineers efficiently develop effective test suites. To understand the test suite generation process, we performed a secondary analysis of recordings from a previously-published user study in which 28 professional software engineers used two tools to generate test suites for three programs with each tool. From these 168 recordings ( (28 userstimes 2 toolstimes 3 programs/tool) ), we extracted a process model of test suite generation called TestLoop that builds upon prior work and systematizes a user’s test suite generation process for a single function into 7 steps. We then used TestLoop’s steps to describe 8 prior and 10 new recordings of users generating test suites using the Jest, Hypothesis, and NaNofuzz test generation tools. Our results showed that TestLoop can be used to help answer previously hard-to-answer questions about how users interact with test suite generation tools and to identify ways that tools might be improved.","2025-09","2025-11-25 22:29:46","2025-11-25 22:29:46","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CTBKM7AK","conferencePaper","2024","Buchmann, Thomas","Prompting Bidirectional Model Transformations - The Good, The Bad and The Ugly","Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems","979-8-4007-0622-6","","10.1145/3652620.3687802","https://doi.org/10.1145/3652620.3687802","This paper investigates the comparative effectiveness of model-to-model transformations generated by an LLM based upon user prompts versus those created with dedicated model transformation languages, using a standard benchmark. The emergence of Generative AI offers a novel approach, allowing developers to specify transformations in natural language rather than learning specialized languages. However, our findings suggest that, in its current state, generative AI does not yet pose a threat to dedicated model transformation languages. While AI-assisted approaches promise to provide flexibility and accessibility, dedicated model transformation languages still offer structured advantages crucial for complex transformations, especially when bidirectionality and incrementality are mandatory requirements. This research contributes to the ongoing discourse on the role of AI in software engineering, highlighting its potential and current limitations in enhancing model transformation processes.","2024","2025-11-25 22:29:46","2025-11-25 22:29:46","","550–555","","","","","","","MODELS Companion '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Linz, Austria","","","","LLM; AI; Bx; MDE; modeling; modeltransformation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MRGQUVM9","journalArticle","2024","Sharpe, James S.; Dougherty, Ryan E.; Smith, Sarah J.","Can ChatGPT Pass a CS1 Python Course?","J. Comput. Sci. Coll.","","1937-4771","","","In this paper we determine whether an LLM-ChatGPT in this case-can successfully complete the assignments in our CS1 course as if it were a ""real"" student. Our study contains a two-stage approach, involving reprompts to the LLM in the cases of either not successfully completing the assignment, or using concepts that are more advanced than are taught in our course. We find that LLMs can in fact can either perfectly solve, or almost perfectly solve, every assignment in our CS1 course.","2024-04","2025-11-25 22:29:46","2025-11-25 22:29:46","","128–142","","8","39","","","","","","","","","","","","","","","","","Place: Evansville, IN, USA Publisher: Consortium for Computing Sciences in Colleges","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4NCYZDNM","journalArticle","2024","Zhang, Jialu; Cambronero, José Pablo; Gulwani, Sumit; Le, Vu; Piskac, Ruzica; Soares, Gustavo; Verbruggen, Gust","PyDex: Repairing Bugs in Introductory Python Assignments using LLMs","Proc. ACM Program. Lang.","","","10.1145/3649850","https://doi.org/10.1145/3649850","Students often make mistakes in their introductory programming assignments as part of their learning process. Unfortunately, providing custom repairs for these mistakes can require a substantial amount of time and effort from class instructors. Automated program repair (APR) techniques can be used to synthesize such fixes. Prior work has explored the use of symbolic and neural techniques for APR in the education domain. Both types of approaches require either substantial engineering efforts or large amounts of data and training. We propose to use a large language model trained on code, such as Codex (a version of GPT), to build an APR system – PyDex – for introductory Python programming assignments. Our system can fix both syntactic and semantic mistakes by combining multi-modal prompts, iterative querying, test-case-based selection of few-shots, and program chunking. We evaluate PyDex on 286 real student programs and compare to three baselines, including one that combines a state-of-the-art Python syntax repair engine, BIFI, and a state-of-the-art Python semantic repair engine for student assignments, Refactory. We find that PyDex can fix more programs and produce smaller patches on average.","2024-04","2025-11-25 22:29:46","2025-11-25 22:29:46","","","","OOPSLA1","8","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","large language models; automated program repair; AI for programming education","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"556W8PET","conferencePaper","2023","Kim, Myeongsoo; Corradini, Davide; Sinha, Saurabh; Orso, Alessandro; Pasqua, Michele; Tzoref-Brill, Rachel; Ceccato, Mariano","Enhancing REST API Testing with NLP Techniques","Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0221-1","","10.1145/3597926.3598131","https://doi.org/10.1145/3597926.3598131","RESTful services are commonly documented using OpenAPI specifications. Although numerous automated testing techniques have been proposed that leverage the machine-readable part of these specifications to guide test generation, their human-readable part has been mostly neglected. This is a missed opportunity, as natural language descriptions in the specifications often contain relevant information, including example values and inter-parameter dependencies, that can be used to improve test generation. In this spirit, we propose NLPtoREST, an automated approach that applies natural language processing techniques to assist REST API testing. Given an API and its specification, NLPtoREST extracts additional OpenAPI rules from the human-readable part of the specification. It then enhances the original specification by adding these rules to it. Testing tools can transparently use the enhanced specification to perform better test case generation. Because rule extraction can be inaccurate, due to either the intrinsic ambiguity of natural language or mismatches between documentation and implementation, NLPtoREST also incorporates a validation step aimed at eliminating spurious rules. We performed studies to assess the effectiveness of our rule extraction and validation approach, and the impact of enhanced specifications on the performance of eight state-of-the-art REST API testing tools. Our results are encouraging and show that NLPtoREST can extract many relevant rules with high accuracy, which can in turn significantly improve testing tools’ performance.","2023","2025-11-25 22:29:46","2025-11-25 22:29:46","","1232–1243","","","","","","","ISSTA 2023","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Seattle, WA, USA","","","","Automated REST API Testing; Natural Language Processing for Testing; OpenAPI Specification Analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7VVTE5CU","conferencePaper","2025","Li, Yuxuan; Shirado, Hirokazu; Das, Sauvik","Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases in Language Models","Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency","979-8-4007-1482-5","","10.1145/3715275.3732212","https://doi.org/10.1145/3715275.3732212","While advances in fairness and alignment have helped mitigate overt biases exhibited by large language models (LLMs) when explicitly prompted, we hypothesize that these models may still exhibit implicit biases when simulating human behavior. To test this hypothesis, we propose a technique to systematically uncover such biases across a broad range of sociodemographic categories by assessing decision-making disparities among agents with LLM-generated, sociodemographically-informed personas. Using our technique, we tested six LLMs across three sociodemographic groups and four decision-making scenarios. Our results show that state-of-the-art LLMs exhibit significant sociodemographic disparities in nearly all simulations, with more advanced models exhibiting greater implicit biases despite reducing explicit biases. Furthermore, when comparing our findings to real-world disparities reported in empirical studies, we find that the biases we uncovered are directionally aligned but markedly amplified. This directional alignment highlights the utility of our technique in uncovering systematic biases in LLMs rather than random variations; moreover, the presence and amplification of implicit biases emphasizes the need for novel strategies to address these biases.","2025","2025-11-25 22:29:46","2025-11-25 22:29:46","","3303–3325","","","","","","","FAccT '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","large language model; bias; language agent; social simulation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3ZM78HSQ","book","2025","","LM-SHIELD '25: Proceedings of the Workshop on Privacy in Large Language Models (LLM) and Natural Language Processing (NLP) 2025","","979-8-4007-1415-3","","","","","2025","2025-11-25 22:29:46","2025-11-25 22:29:46","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U6M6L7T7","bookSection","2025","Gao, Yanjie; Luo, Jiyu; Lin, Haoxiang; Zhang, Hongyu; Wu, Ming; Yang, Mao","dl²: Detecting Communication Deadlocks in Deep Learning Jobs","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","","https://doi.org/10.1145/3696630.3728529","In recent years, deep learning has seen widespread adoption across various domains, giving rise to large-scale models such as large language models. Training these models, particularly in distributed environments, presents substantial computational and communication challenges. A critical issue is the communication deadlock—a state in which processes become indefinitely stalled while awaiting network messages from others, which leads to resource wastage and reduced productivity. Current approaches to deadlock handling are either unsuitable for deep learning due to its unique hybrid programming paradigm or limit optimization opportunities. This paper presents dl2, a novel dynamic analysis tool designed to detect communication deadlocks in deep learning jobs. dl2 models the runtime trace of a job as an execution graph, detects unmatched communications, and constructs a wait-for graph to identify deadlock cycles. dl2 can also handle nondeterministic communication behaviors, providing replay and diagnostic support for root cause analysis. We evaluate dl2 using PyTorch with a combination of synthetic test cases and real-world deep learning workloads. The experimental results show that dl2 successfully detects all communication deadlocks, achieving 100% precision and recall, which highlights its effectiveness.","2025","2025-11-25 22:29:46","2025-11-25 22:29:46","","27–38","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"25M6W3X6","conferencePaper","2024","Ouyang, Yicheng; Yang, Jun; Zhang, Lingming","Benchmarking Automated Program Repair: An Extensive Study on Both Real-World and Artificial Bugs","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3652140","https://doi.org/10.1145/3650212.3652140","As bugs are inevitable and prevalent in real-world programs, many Automated Program Repair (APR) techniques have been proposed to generate patches for them. However, due to the lack of a standard for evaluating APR techniques, prior works tend to use different settings and benchmarks in evaluation, threatening the trustworthiness of the evaluation results. Additionally, they typically only adopt plausibility and genuineness as evaluation metrics, which may potentially mask some underlying issues in APR techniques. To overcome these issues, in this paper, we conduct an extensive and multi-dimensional evaluation of nine learning-based and three traditional state-of-the-art APR techniques under the same environment and settings. We employ the widely studied Defects4J V2.0.0 benchmark and a newly constructed large-scale mutation-based benchmark named MuBench, derived from Defects4J and including 1,700 artificial bugs generated by various mutators, to uncover potential limitations in these APR techniques. We also apply multi-dimensional metrics, including compilability/plausibility/genuineness metrics, as well as SYE (SYntactic Equivalence) and TCE (Trivial Compiler Equivalence) metrics, to thoroughly analyze the 1,814,652 generated patches. This paper presents noteworthy findings from the extensive evaluation: Firstly, Large Language Model (LLM) based APR demonstrates less susceptibility to overfitting on the Defects4J V1.2.0 dataset and fixes the most number of bugs. Secondly, the study suggests a promising future for combining traditional and learning-based APR techniques, as they exhibit complementary advantages in fixing different types of bugs. Additionally, this work highlights the necessity for further enhancing patch compilability of learning-based APR techniques, despite the presence of various existing strategies attempting to improve it. The study also reveals other guidelines for enhancing APR techniques, including the need for handling unresolvable symbol compilability issues and reducing duplicate/no-op patch generation. Finally, our study uncovers seven implementation issues in the studied techniques, with five of them confirmed and fixed by the corresponding authors.","2024","2025-11-25 22:29:46","2025-11-25 22:29:46","","440–452","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Empirical assessment; Mutation testing; Program repair","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JP6MWKPH","conferencePaper","2025","P?durean, Victor-Alexandru; Denny, Paul; Singla, Adish","BugSpotter: Automated Generation of Code Debugging Exercises","Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1","979-8-4007-0531-1","","10.1145/3641554.3701974","https://doi.org/10.1145/3641554.3701974","Debugging is an essential skill when learning to program, yet its instruction and emphasis often vary widely across introductory courses. In the era of code-generating large language models (LLMs), the ability for students to reason about code and identify errors is increasingly important. However, students frequently resort to trial-and-error methods to resolve bugs without fully understanding the underlying issues. Developing the ability to identify and hypothesize the cause of bugs is crucial but can be time-consuming to teach effectively through traditional means. This paper introduces BugSpotter, an innovative tool that leverages an LLM to generate buggy code from a problem description and verify the synthesized bugs via a test suite. Students interact with BugSpotter by designing failing test cases, where the buggy code's output differs from the expected result as defined by the problem specification. This not only provides opportunities for students to enhance their debugging skills, but also to practice reading and understanding problem specifications. We deployed BugSpotter in a large classroom setting and compared the debugging exercises it generated to exercises hand-crafted by an instructor for the same problems. We found that the LLM-generated exercises produced by BugSpotter varied in difficulty and were well-matched to the problem specifications. Importantly, the LLM-generated exercises were comparable to those manually created by instructors with respect to student performance, suggesting that BugSpotter could be an effective and efficient aid for learning debugging.","2025","2025-11-25 22:29:46","2025-11-25 22:29:46","","896–902","","","","","","","SIGCSETS 2025","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Pittsburgh, PA, USA","","","","test cases; generative ai; llms; debugging; bugspotter; exercise generation; programming education","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LE539RG8","conferencePaper","2025","Sagtani, Hitesh; Mehrotra, Rishabh; Liu, Beyang","Improving FIM Code Completions via Context &amp; Curriculum Based Learning","Proceedings of the Eighteenth ACM International Conference on Web Search and Data Mining","979-8-4007-1329-3","","10.1145/3701551.3703563","https://doi.org/10.1145/3701551.3703563","Fill-in-the-Middle (FIM) models play a vital role in code completion tasks, leveraging both prefix and suffix context to provide more accurate and contextually relevant suggestions. This paper presents approaches to improve FIM code completion while addressing the challenge of maintaining low latency for real-time coding assistance. We enhance FIM code completion by incorporating context and curriculum examples in the training process. We identify patterns where completion suggestions fail more frequently, revealing complexities that smaller language models struggle with. To address these challenges, we develop a curriculum dataset by extracting hard-to-complete patterns from code repositories and generate context examples using semantic and static analysis tools (e.g. TSC compiler). We fine-tune various sized models, including StarCoder and DeepSeek, on this enhanced dataset. Our evaluation encompasses three key dimensions: the Santa Coder FIM task, the Amazon CCEval benchmark, and a new Multi-Line Infilling evaluation benchmark derived from SWE-bench. Comprehensive ablation studies across multiple model sizes reveal that while all fine-tuned models show improvements, the performance gains are more pronounced for smaller parameter models and that incorporating difficult-to-complete examples as part of curriculum learning improves completion performance. This finding is particularly sig- nificant given the latency constraints of code completion tasks. While larger models like GPT and Claude perform well in multi- line completions but are prohibitively challenging to use given high latency, and our fine-tuned models achieve a balance between per- formance and latency. Finally, we validate our approach through online A/B testing, demonstrating tangible improvements in Completion Acceptance Rate (CAR) and Completion Persistence Rate (CPR), with zero latency impact.","2025","2025-11-25 22:29:46","2025-11-25 22:29:46","","801–810","","","","","","","WSDM '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Hannover, Germany","","","","large language model; a/b-testing; code completions","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4GW79528","conferencePaper","2025","O'Brien, Gabrielle","How Scientists Use Large Language Models to Program","Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems","979-8-4007-1394-1","","10.1145/3706598.3713668","https://doi.org/10.1145/3706598.3713668","Scientists across disciplines write code for critical activities like data collection and generation, statistical modeling, and visualization. As large language models that can generate code have become widely available, scientists may increasingly use these models during research software development. We investigate the characteristics of scientists who are early-adopters of code generating models and conduct interviews with scientists at a public, research-focused university. Through interviews and reviews of user interaction logs, we see that scientists often use code generating models as an information retrieval tool for navigating unfamiliar programming languages and libraries. We present findings about their verification strategies and discuss potential vulnerabilities that may emerge from code generation practices unknowingly influencing the parameters of scientific analyses.","2025","2025-11-25 22:29:46","2025-11-25 22:29:46","","","","","","","","","CHI '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","generative AI; Copilot; Code assistant; data analysis; data science; program synthesis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9IEWF9TQ","bookSection","2025","Ronanki, Krishna","Facilitating Trustworthy Human-Agent Collaboration in LLM-based Multi-Agent System oriented Software Engineering","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","","https://doi.org/10.1145/3696630.3728717","Multi-agent autonomous systems (MAS) are better at addressing challenges that spans across multiple domains than singular autonomous agents. This holds true within the field of software engineering (SE) as well. The state-of-the-art research on MAS within SE focuses on integrating LLMs at the core of autonomous agents to create LLM-based multi-agent autonomous (LMA) systems. However, the introduction of LMA systems into SE brings a plethora of challenges. One of the major challenges is the strategic allocation of tasks between humans and the LMA system in a trustworthy manner. To address this challenge, a RACI-based framework is proposed in this work in progress article, along with implementation guidelines and an example implementation of the framework. The proposed framework can facilitate efficient collaboration, ensure accountability, and mitigate potential risks associated with LLM-driven automation while aligning with the Trustworthy AI guidelines. The future steps for this work delineating the planned empirical validation method are also presented.","2025","2025-11-25 22:29:46","2025-11-25 22:29:46","","1333–1337","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2SXJPK2M","conferencePaper","2024","Zhang, Huan; Cheng, Wei; Wu, Yuhan; Hu, Wei","A Pair Programming Framework for Code Generation via Multi-Plan Exploration and Feedback-Driven Refinement","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695506","https://doi.org/10.1145/3691620.3695506","Large language models (LLMs) have achieved impressive performance on code generation. Although prior studies enhanced LLMs with prompting techniques and code refinement, they still struggle with complex programming problems due to rigid solution plans. In this paper, we draw on pair programming practices to propose PairCoder, a novel LLM-based framework for code generation. PairCoder incorporates two collaborative LLM agents, namely a Navigator agent for high-level planning and a Driver agent for specific implementation. The Navigator is responsible for proposing promising solution plans, selecting the current optimal plan, and directing the next iteration round based on execution feedback. The Driver follows the guidance of Navigator to undertake initial code generation, code testing, and refinement. This interleaved and iterative workflow involves multi-plan exploration and feedback-based refinement, which mimics the collaboration of pair programmers. We evaluate PairCoder with both open-source and closed-source LLMs on various code generation benchmarks. Extensive experimental results demonstrate the superior accuracy of PairCoder, achieving relative pass@1 improvements of 12.00%–162.43% compared to prompting LLMs directly.","2024","2025-11-25 22:29:46","2025-11-25 22:29:46","","1319–1331","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","large language model; code generation; pair programming; agent","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HWBWA85T","journalArticle","2024","Tambon, Florian; Khomh, Foutse; Antoniol, Giuliano","GIST: Generated Inputs Sets Transferability in Deep Learning","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3672457","https://doi.org/10.1145/3672457","To foster the verifiability and testability of deep neural networks (DNN), an increasing number of methods for test case generation techniques are being developed.When confronted with testing DNN models, the user can apply any existing test generation technique. However, it needs to do so for each technique and each DNN model under test, which can be expensive. Therefore, a paradigm shift could benefit this testing process: rather than regenerating the test set independently for each DNN model under test, we could transfer from existing DNN models.This article introduces Generated Inputs Sets Transferability (GIST), a novel approach for the efficient transfer of test sets. Given a property selected by a user (e.g., neurons covered, faults), GIST enables the selection of good test sets from the point of view of this property among available test sets. This allows the user to recover similar properties on the transferred test sets as he would have obtained by generating the test set from scratch with a test cases generation technique. Experimental results show that GIST can select effective test sets for the given property to transfer. Moreover, GIST scales better than reapplying test case generation techniques from scratch on DNN models under test.","2024-12","2025-11-25 22:29:46","2025-11-25 22:29:46","","","","8","33","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","testing; deep learning; DNN; Test sets generation; transferability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DWTRNIYT","conferencePaper","2025","Bassner, Patrick; Lottner, Anna; Krusche, Stephan","Towards Understanding the Impact of Context-Aware AI Tutors and General-Purpose AI Chatbots on Student Learning","Proceedings of the 25th Koli Calling International Conference on Computing Education Research","979-8-4007-1599-0","","10.1145/3769994.3770025","https://doi.org/10.1145/3769994.3770025","Large programming courses face critical challenges in providing personalized support at scale. Educators have responded by developing specialized AI tutors, yet empirical research comparing their effectiveness to general-purpose tools and traditional support remains scarce. We conducted an exploratory randomized, between-subjects mixed-methods study (N=33) in which students implemented the Burrows–Wheeler Transform under one of three conditions: a context-aware AI tutor (Iris), a general-purpose chatbot (ChatGPT), or no AI support.Quantitative analyses detected no significant differences in learning gains, completion time, or code accuracy, while qualitative interviews unveiled key insights: time pressure dominated tool selection, with students prioritizing efficiency over learning under stress; context-aware guidance was universally appreciated; students showed polarized scaffolding preferences; and ChatGPT users sought external verification more than Iris users. Over-reliance concerns were prevalent, with ChatGPT users expressing stronger concerns about this issue post-intervention.These findings indicate that tool design and situational context strongly shape AI adoption even when performance metrics converge, underscoring the need for larger comparative trials to determine which learners benefit from different AI supports and how to integrate them responsibly.","2025","2025-11-25 22:29:46","2025-11-25 22:29:46","","","","","","","","","Koli Calling '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","ChatGPT; Generative AI; Large Language Models; CS1; Education Technology; Interactive Learning; Programming Exercises","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MU3LVRTA","conferencePaper","2024","Dhulipala, Hridya; Yadavally, Aashish; Nguyen, Tien N.","Planning to Guide LLM for Code Coverage Prediction","Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering","979-8-4007-0609-7","","10.1145/3650105.3652292","https://doi.org/10.1145/3650105.3652292","Code coverage serves as a crucial metric to assess testing effectiveness, measuring the degree to which a test suite exercises different facets of the code, such as statements, branches, or paths. Despite its significance, coverage profilers necessitate access to the entire codebase, constraining their usefulness in situations where the code is incomplete or execution is not feasible, and even cost-prohibitive. In this paper, we present CodePilot, a plan-based prompting approach grounded in program semantics, which collaborates with a Large Language Model (LLM) to enhance code coverage prediction. To address the intricacies of predicting code coverage, CodePilot employs planning by discerning various types of statements in an execution flow. Planning empowers GPT to autonomously generate plans based on guided examples, and then CodePilot prompts the GPT model to predict code coverage (Action) based on the plan it generated (Reasoning). Our experiments evaluating CodePilot demonstrate high accuracy, achieving up to 55% in exact-match and 89% in statement-match. It performs relatively better than the baselines, achieving up to 33% and 19% relatively higher in those metrics. We also showed that due to highly accurate plans (90%), GPT model predicts better code coverage. Moreover, we show CodePilot's utility in correctly predicting the least covered statements.","2024","2025-11-25 22:29:46","2025-11-25 22:29:46","","24–34","","","","","","","FORGE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","large language models; AI4SE; code coverage analysis; planning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3CKI9XPC","conferencePaper","2025","Fang, Yi; Fan, Dongzhe; Ding, Sirui; Liu, Ninghao; Tan, Qiaoyu","UniGLM: Training One Unified Language Model for Text-Attributed Graphs Embedding","Proceedings of the Eighteenth ACM International Conference on Web Search and Data Mining","979-8-4007-1329-3","","10.1145/3701551.3703586","https://doi.org/10.1145/3701551.3703586","Representation learning on text-attributed graphs (TAGs), where nodes are associated with textual descriptions, is crucial for textual and relational knowledge systems, such as social media and recommendation scenarios. However, state-of-the-art embedding methods for TAGs primarily focus on fine-tuning pre-trained language models (PLMs) using structure-aware training objectives. While effective, these methods are tailored for individual TAG and cannot generalize across various graph scenarios. Given the shared textual space, leveraging multiple TAGs for joint fine-tuning, aligning text and graph structure from different aspects, would be more beneficial. Therefore, we propose the Unified Graph Language Model (UniGLM), a novel foundation model pretrained over multiple TAGs from a variety of domains, which can generalize well to both in-domain and cross-domain graph scenarios. Specifically, UniGLM fine-tunes well-established PLMs (e.g., Sentence-BERT) using a domain-aware contrastive learning objective that unifies structure heterogeneity and node statistics across various domains with an adaptive and learnable positive sample selection scheme. Additionally, a lazy updating module is introduced to speed up training by reducing repetitive encoding of positive samples. Extensive datasets across multiple domains, downstream tasks (node classification and link prediction), and a spectrum of graph backbones (supervised and self-supervised graph models) are conducted to compare UniGLM with state-of-the-art baselines. Our empirical observations suggest that UniGLM can generate informative representations for cross-domain graphs observed in the training. More importantly, UniGLM also exhibits competitive transfer ability in encoding unseen TAGs that are not used for training. This study provides deep insights into how to adapt PLMs to graph data and demonstrates the potential of building foundation model for graph representation learning.","2025","2025-11-25 22:29:47","2025-11-25 22:29:47","","973–981","","","","","","","WSDM '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Hannover, Germany","","","","graph embedding foundation model; graph representation learning; textual-attributed graphs","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5KBYC4QW","conferencePaper","2024","Cao, Jialun; Chen, Zhiyong; Wu, Jiarong; Cheung, Shing-Chi; Xu, Chang","JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating Large Language Models","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695470","https://doi.org/10.1145/3691620.3695470","Code generation benchmarks such as HumanEval are widely adopted to evaluate LLMs' capabilities. However, after consolidating the latest 24 benchmarks, we noticed three significant imbalances. First, imbalanced programming language. 95.8% of benchmarks involve Python, while only 5 benchmarks involve Java, resulting in an insufficient understanding of LLMs' capability to generate Java code. Second, imbalanced code granularity. Function-/statement-level benchmarks account for over 83.3% of benchmarks. Only a mere handful extends to class-/project-levels, and all are limited to Python. Third, lacking advanced features. Existing benchmarks primarily assess basic coding skills (e.g., variables, operators, and control structures), while overlooking advanced Object-Oriented Programming (OOP) features (i.e., encapsulation, inheritance, and polymorphism). Considering the prevalence of these advanced features in real-world Java project development, constructing benchmarks to test LLMs on handling OOP features is necessary.To fill these gaps, we propose JavaBench, a project-level Java benchmark that exercises OOP features. It comprises four Java projects with 389 methods in 106 Java classes. The test coverage is up to 92%, and JavaBench is attested by 282 undergraduate students, reaching a 90.93/100 average score (i.e., pass rate against the test suite), ensuring the quality of documentation, code skeleton, and tests. To better evaluate LLM's capability against JavaBench, we introduce a systematic evaluation design covering three context settings and five synthesis strategies at two granularities using three hierarchical metrics. Our extensive experiment yields several interesting findings. First, we noticed that regarding project-level Java programming, LLMs are far behind undergraduate students (no project can be correctly completed by any studied LLMs, and at most 48.24% Pass@5 in a more relaxed evaluation). Second, using method signature as prompt context may strike an ideal balance for project-level code generation. JavaBench is publicly available at https://github.com/java-bench/JavaBench. We also release a leaderboard and invite model developers to participate and test their models against JavaBench at https://java-bench.github.io/leaderboard.html.","2024","2025-11-25 22:29:47","2025-11-25 22:29:47","","870–882","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","large language model; program synthesis; object-oriented programming","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZGIRQFEW","conferencePaper","2025","Chomątek, Łukasz; Papuga, Jakub; Nowak, Przemyslaw; Poniszewska-Marańda, Aneta","Decoding CI/CD Practices in Open-Source Projects with LLM Insights","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","10.1145/3696630.3728699","https://doi.org/10.1145/3696630.3728699","The analysis of continuous integration and continuous deployment (CI/CD) pipeline configurations offers critical insights into modern software development practices. This paper leverages advanced large language models (LLMs) — DeepSeek-V2 and GPT-4o — to examine pipeline definition files from 28,770 popular GitHub repositories, uncovering adoption trends across CI/CD practices. Our LLM-driven approach reveals near-universal implementation of build (80.0%) and test (80.2%) stages, contrasted by lower adoption of specialized practices like static application security testing (SAST) (12.1%), containerization (17.9%), and cloud deployment (4.2%). System-specific patterns emerge, with Travis CI leading in build automation (99.2%) and GitHub Actions excelling in linting (44.9%) and SAST (16.5%), reflecting distinct system strengths. By harnessing LLMs to parse and interpret complex pipeline files at scale, this study not only maps the CI/CD landscape but also demonstrates the transformative potential of artificial intelligence in software engineering research. We propose future work to refine LLM-based tools for pipeline analysis and address adoption gaps, aiming to enhance CI/CD accessibility and effectiveness across diverse projects.","2025","2025-11-25 22:29:47","2025-11-25 22:29:47","","1638–1644","","","","","","","FSE Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","large language models; code repositories; continuous deployment; continuous integration; experience","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GZWWT2RM","conferencePaper","2024","Wang, Luqiao; Wang, Qiangqiang; Wang, Jiaqi; Zhao, Yutong; Wei, Minjie; Quan, Zhou; Cui, Di; Li, Qingshan","HECS: A Hypergraph Learning-Based System for Detecting Extract Class Refactoring Opportunities","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3685307","https://doi.org/10.1145/3650212.3685307","HECS is an advanced tool designed for Extract Class refactoring by leveraging hypergraph learning to model complex dependencies within large classes. Unlike traditional tools that rely on direct one-to-one dependency graphs, HECS uses intra-class dependency hypergraphs to capture one-to-many relationships. This allows HECS to provide more accurate and relevant refactoring suggestions. The tool constructs hypergraphs for each target class, attributes nodes using a pre-trained code model, and trains an enhanced hypergraph neural network. Coupled with a large language model, HECS delivers practical refactoring suggestions. In evaluations on large-scale and real-world datasets, HECS achieved a 38.5% increase in precision, 9.7% in recall, and 44.4% in f1-measure compared to JDeodorant, SSECS, and LLMRefactor. These improvements make HECS a valuable tool for developers, offering practical insights and enhancing existing refactoring techniques.","2024","2025-11-25 22:29:47","2025-11-25 22:29:47","","1851–1855","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Extract Class Refactoring; Hypergraph Neural Network","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JQC96HI3","conferencePaper","2025","Gebreegziabher, Simret Araya; Chiang, Charles; Wang, Zichu; Ashktorab, Zahra; Brachman, Michelle; Geyer, Werner; Li, Toby Jia-Jun; Gómez-Zará, Diego","MetricMate: An Interactive Tool for Generating Evaluation Criteria for LLM-as-a-Judge Workflow","Proceedings of the 4th Annual Symposium on Human-Computer Interaction for Work","979-8-4007-1384-2","","10.1145/3729176.3729199","https://doi.org/10.1145/3729176.3729199","The rise of the use of Large Language Models (LLMs) in work has driven the need for robust evaluation methods that align model behavior with human values and preferences. LLM-as-a-judge approaches have emerged as a scalable solution, leveraging LLMs to evaluate generated outputs based on flexible user-defined criteria. However, users often struggle to articulate clear evaluation criteria. In addition, human preferences and criteria definitions evolve, and predefined templates fail to account for context-specific nuances. To address these challenges, we present MetricMate, an interactive tool that supports users in defining and calibrating evaluation criteria for LLM-as-a-judge systems. MetricMate introduces hierarchical criteria definitions and curated examples of success and failure to promote human-AI criteria negotiation and alignment. Additionally, MetricMate learns from users’ interactions with data by enabling users to group data to identify patterns and provide context-specific criteria.","2025","2025-11-25 22:29:47","2025-11-25 22:29:47","","","","","","","","","CHIWORK '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Large Language Models; Evaluation Methods; Human AI Interaction; LLM-as-a-Judge","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U8IEGG7K","conferencePaper","2024","Taylor, Andrew; Vassar, Alexandra; Renzella, Jake; Pearce, Hammond","dcc –help: Transforming the Role of the Compiler by Generating Context-Aware Error Explanations with Large Language Models","Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1","979-8-4007-0423-9","","10.1145/3626252.3630822","https://doi.org/10.1145/3626252.3630822","In the challenging field of introductory programming, high enrolments and failure rates drive us to explore tools and systems to enhance student outcomes, especially automated tools that scale to large cohorts. This paper presents and evaluates the dcc –help tool, an integration of a Large Language Model (LLM) into the Debugging C Compiler (DCC) to generate unique, novice-focused explanations tailored to each error. dcc –help prompts an LLM with contextual information of compile- and run-time error occurrences, including the source code, error location and standard compiler error message. The LLM is instructed to generate novice-focused, actionable error explanations and guidance, designed to help students understand and resolve problems without providing solutions. dcc –help was deployed to our CS1 and CS2 courses, with 2,565 students using the tool over 64,000 times in ten weeks. We analysed a subset of these error/explanation pairs to evaluate their properties, including conceptual correctness, relevancy, and overall quality. We found that the LLM-generated explanations were conceptually accurate in 90% of compile-time and 75% of run-time cases, but often disregarded the instruction not to provide solutions in code. Our findings, observations and reflections following deployment indicate that dcc –help provides novel opportunities for scaffolding students' introduction to programming.","2024","2025-11-25 22:29:47","2025-11-25 22:29:47","","1314–1320","","","","","","","SIGCSE 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Portland, OR, USA","","","","large language models; generative ai; cs1; debugging; ai in education; compiler error messages; error message enhancement; ai in cs1; programming error messages","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VHRFV4XG","conferencePaper","2024","Gordon, Colin S.","Mocking Temporal Logic","Proceedings of the 2024 ACM SIGPLAN International Symposium on SPLASH-E","979-8-4007-1216-6","","10.1145/3689493.3689980","https://doi.org/10.1145/3689493.3689980","Temporal logics cover important classes of system specifications dealing with system behavior over time. Despite the prevalence of long-running systems that accept repeated input and output, and thus the clear relevance of temporal specifications to training software engineers, temporal logics are rarely taught to undergraduates. We motivate and describe an approach to teaching temporal specifications and temporal reasoning indirectly through teaching students about mocking dependencies, which is widely used in software testing of large systems (and therefore of more obvious relevance to students), less notationally intimidating to students, and still teaches similar reasoning principles. We report on 7 years of experience using this indirect approach to behavioral specifications in a software quality course.","2024","2025-11-25 22:29:47","2025-11-25 22:29:47","","98–109","","","","","","","SPLASH-E '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Pasadena, CA, USA","","","","software testing; software engineering education; software specification; temporal logic","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ETFZMN7F","journalArticle","2025","Jorgensen, Steven; Nadizar, Giorgia; Pietropolli, Gloria; Manzoni, Luca; Medvet, Eric; O'Reilly, Una-May; Hemberg, Erik","Policy Search through Genetic Programming and LLM-assisted Curriculum Learning","ACM Trans. Evol. Learn. Optim.","","2688-299X","10.1145/3772718","https://doi.org/10.1145/3772718","Curriculum learning (CL) consists in using a diverse set of user-provided test cases, with varying levels of difficulty and organized in a suitable progression, for learning a policy. The quality of test cases is important to allow optimization techniques as genetic programming (GP) to solve policy search problems. In this work, we evaluate large language models (LLMs) as providers of test cases for GP-based policy search. We consider two policy search tasks, a single-player and a multi-player game, and four LLMs differing in complexity and specialization, which we prompt in order to generate suitable test cases for the two games. We experimentally assess the intrinsic quality of LLM-generated test cases and their utility when inserted in a curriculum consumed by a GP optimization. We evaluate the robustness of the approach with respect to the way cases are scheduled in curricula and with respect to the policy representation, for which we use both graphs and linear programs evolved by GP. We observe that the effectiveness of LLM-assisted CL depends on both the choice of LLM and the design of the prompting and scheduling strategies. These findings highlight important considerations for leveraging LLMs in automated curriculum design for GP-based optimization.","2025-10","2025-11-25 22:29:47","2025-11-25 22:29:47","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large language models; Curriculum learning; Graph-based GP","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EZAI8F5M","conferencePaper","2024","van Schaik, Tempest A.; Pugh, Brittany","A Field Guide to Automatic Evaluation of LLM-Generated Summaries","Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval","979-8-4007-0431-4","","10.1145/3626772.3661346","https://doi.org/10.1145/3626772.3661346","Large Language models (LLMs) are rapidly being adopted for tasks such as text summarization, in a wide range of industries. This has driven the need for scalable, automatic, reliable, and cost-effective methods to evaluate the quality of LLM-generated text. What is meant by evaluating an LLM is not yet well defined and there are widely different expectations about what kind of information evaluation will produce. Evaluation methods that were developed for traditional Natural Language Processing (NLP) tasks (before the rise of LLMs) remain applicable but are not sufficient for capturing high-level semantic qualities of summaries. Emerging evaluation methods that use LLMs to evaluate LLM-output, appear to be powerful but lacking in reliability. New elements of LLM generated text that were not an element of previous NLP tasks, such as the artifacts of hallucination, need to be considered. We outline the different types of LLM evaluation currently used in the literature but focus on offline, system-level evaluation of the text generated by LLMs. Evaluating LLM-generated summaries is a complex and fast-evolving area, and we propose strategies for applying evaluation methods to avoid common pitfalls. Despite having promising strategies for evaluating LLM summaries, we highlight some open challenges that remain.","2024","2025-11-25 22:29:47","2025-11-25 22:29:47","","2832–2836","","","","","","","SIGIR '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Washington DC, USA","","","","llms; evaluation metrics; offline evaluation; summarization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A993DK2Z","conferencePaper","2025","Huq, Syed Fatiul; Tafreshipour, Mahan; Kalcevich, Kate; Malek, Sam","Automated Generation of Accessibility Test Reports from Recorded User Transcripts","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00043","https://doi.org/10.1109/ICSE55347.2025.00043","Testing for accessibility is a significant step when developing software, as it ensures that all users, including those with disabilities, can effectively engage with web and mobile applications. While automated tools exist to detect accessibility issues in software, none are as comprehensive and effective as the process of user testing, where testers with various disabilities evaluate the application for accessibility and usability issues. However, user testing is not popular with software developers as it requires conducting lengthy interviews with users and later parsing through large recordings to derive the issues to fix. In this paper, we explore how large language models (LLMs) like GPT 4.0, which have shown promising results in context comprehension and semantic text generation, can mitigate this issue and streamline the user testing process. Our solution, called Recall, takes in auto-generated transcripts from user testing video recordings and extracts the accessibility and usability issues mentioned by the tester. Our systematic prompt engineering determines the optimal configuration of input, instruction, context and demonstrations for best results. We evaluate Recall's effectiveness on 36 user testing sessions across three applications. Based on the findings, we investigate the strengths and weaknesses of using LLMs in this space.","2025","2025-11-25 22:29:47","2025-11-25 22:29:47","","204–216","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","large language models; crowd-sourced software testing; software accessibility","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HC2WZXAZ","conferencePaper","2024","Li, Wanpeng; Guo, Yuejun","Poster: Automated Dependency Mapping for Web API Security Testing Using Large Language Models","Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security","979-8-4007-0636-3","","10.1145/3658644.3691377","https://doi.org/10.1145/3658644.3691377","Dependency extraction is crucial in web API security testing, as it helps identify the required API sequences to exploit a vulnerability. Traditional methods are generally rule-based and require extensive manual analysis of API specification documents by domain experts to formulate appropriate rules. This manual process is not only time-consuming and labor-intensive but also prone to missing dependencies and inaccuracies, which can compromise the effectiveness of security testing. In this paper, we explore the potential of large language models (LLMs) to automate dependency mapping in web APIs. By leveraging the capabilities of advanced LLMs such as GPT-3.5, Mistral-7B-Instruct, and Llama-3-8B-Instruct, which include understanding and generating natural language, we aim to streamline the dependency mapping process, reducing the need for manual analysis and enhancing accuracy. Our preliminary experiments demonstrate that this approach can effectively build dependency mappings, offering a a promising alternative to traditional rule-based approaches.","2024","2025-11-25 22:29:47","2025-11-25 22:29:47","","5024–5026","","","","","","","CCS '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Salt Lake City, UT, USA","","","","large language model; api security testing; dependency mapping","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SGBYSWFA","journalArticle","2025","Zhang, Xiaoyu; Zhai, Juan; Ma, Shiqing; Wang, Shiwei; Shen, Chao","Citadel: Context Similarity Based Deep Learning Framework Bug Finding","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3773993","https://doi.org/10.1145/3773993","With the application of deep learning technology, tools of DL framework testing are in high demand. Existing DL framework testing tools have limited coverage of bug types. For example, they lack the capability of effectively finding performance bugs, which are critical for DL models regarding performance, economics, and the environment. Moreover, existing tools are inefficient, generating hundreds of test cases with few trigger bugs. In this paper, we propose Citadel, a method that accelerates bug finding in terms of efficiency and effectiveness. We observe that many DL framework bugs are similar due to the similarity of operators and algorithms belonging to the same family. Orthogonal to existing bug-finding tools, Citadel aims to find new bugs that are similar to reported ones that have known test oracles. Citadel defines context similarity to measure the similarity of DL framework API pairs and automatically generates test cases with oracles for APIs that are similar to the problematic APIs in existing bug reports. Citadel effectively detects 58 and 66 API bugs on PyTorch and TensorFlow (excluding those rejected by developers or duplicates of prior reports), many of which, e.g.,, 13 performance bugs, cannot be detected by existing tools. Moreover, 35.40% of test cases generated by Citadel can trigger bugs significantly transcending the state-of-the-art method (3.90%).","2025-10","2025-11-25 22:29:47","2025-11-25 22:29:47","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Software Testing; Deep Learning Library; Deep Learning Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WPIVLDGE","conferencePaper","2024","Macedo, Marcos; Tian, Yuan; Cogo, Filipe; Adams, Bram","Exploring the Impact of the Output Format on the Evaluation of Large Language Models for Code Translation","Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering","979-8-4007-0609-7","","10.1145/3650105.3652301","https://doi.org/10.1145/3650105.3652301","Code translation between programming languages is a long-existing and critical task in software engineering, facilitating the modernization of legacy systems, ensuring cross-platform compatibility, and enhancing software performance. With the recent advances in large language models (LLMs) and their applications to code translation, there is an increasing need for comprehensive evaluation of these models. In this study, we empirically analyze the generated outputs of eleven popular instruct-tuned LLMs with parameters ranging from 1B up to 46.7B on 3,820 translation pairs across five languages, including C, C++, Go, Java, and Python. Our analysis found that between 26.4% and 73.7% of code translations produced by our evaluated LLMs necessitate post-processing, as these translations often include a mix of code, quotes, and text rather than being purely source code. Overlooking the output format of these models can inadvertently lead to underestimation of their actual performance. This is particularly evident when evaluating them with execution-based metrics such as Computational Accuracy (CA). Our results demonstrate that a strategic combination of prompt engineering and regular expression can effectively extract the source code from the model generation output. In particular, our method can help eleven selected models achieve an average Code Extraction Success Rate (CSR) of 92.73%. Our findings shed light on and motivate future research to conduct more reliable benchmarks of LLMs for code translation.","2024","2025-11-25 22:29:47","2025-11-25 22:29:47","","57–68","","","","","","","FORGE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","large language model; LLM; software engineering; empirical study; code translation; benchmarking; case study; evaluation; output format","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XCKJ2LUW","conferencePaper","2025","Jacobs, Sven; Peters, Henning; Jaschke, Steffen; Kiesler, Natalie","Unlimited Practice Opportunities: Automated Generation of Comprehensive, Personalized Programming Tasks","Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 1","979-8-4007-1567-9","","10.1145/3724363.3729089","https://doi.org/10.1145/3724363.3729089","Generative artificial intelligence (GenAI) offers new possibilities for generating personalized programming exercises, addressing the need for individual practice. However, the task quality along with the student perspective on such generated tasks remains largely unexplored. Therefore, this paper introduces and evaluates a new feature of the so-called Tutor Kai for generating comprehensive programming tasks, including problem descriptions, code skeletons, unit tests, and model solutions. The presented system allows students to freely choose programming concepts and contextual themes for their tasks. To evaluate the system, we conducted a two-phase mixed-methods study comprising (1) an expert rating of 200 automatically generated programming tasks w.r.t. task quality, and (2) a study with 26 computer science students who solved and rated the personalized programming tasks. Results show that experts classified 89.5% of the generated tasks as functional and 92.5% as solvable. However, the system's rate for implementing all requested programming concepts decreased from 94% for single-concept tasks to 40% for tasks addressing three concepts. The student evaluation further revealed high satisfaction with the personalization. Students also reported perceived benefits for learning. The results imply that the new feature has the potential to offer students individual tasks aligned with their context and need for exercise. Tool developers, educators, and, above all, students can benefit from these insights and the system itself.","2025","2025-11-25 22:29:47","2025-11-25 22:29:47","","319–325","","","","","","","ITiCSE 2025","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Nijmegen, Netherlands","","","","large language models; generative ai; context personalization; programming education; programming exercises","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G4ASXNHU","conferencePaper","2025","Newsham, Lewis; Prince, Daniel","Personality-Driven Decision Making in LLM-Based Autonomous Agents","Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems","979-8-4007-1426-9","","","","The embedding of Large Language Models (LLMs) into autonomous agents is a rapidly developing field which enables dynamic, configurable behaviours without the need for extensive domain-specific training. In our previous work, we introduced SANDMAN, a Deceptive Agent architecture leveraging the Five-Factor OCEAN personality model, demonstrating that personality induction significantly influences agent task planning. Building on these findings, this study presents a novel method for measuring and evaluating how induced personality traits affect task selection processes-specifically planning, scheduling, and decision-making-in LLM-based agents. Our results reveal distinct task-selection patterns aligned with induced OCEAN attributes, underscoring the feasibility of designing highly plausible Deceptive Agents for proactive cyber defense strategies.","2025","2025-11-25 22:29:47","2025-11-25 22:29:47","","1538–1547","","","","","","","AAMAS '25","","","","International Foundation for Autonomous Agents and Multiagent Systems","Richland, SC","","","","","","","","event-place: Detroit, MI, USA","","","","large language models; decision-making; planning; autonomous agents; language agents; personality induction; task selection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"73F2VZZ6","journalArticle","2024","Xin, Qi; Wu, Haojun; Tang, Jinran; Liu, Xinyu; Reiss, Steven P.; Xuan, Jifeng","Detecting, Creating, Repairing, and Understanding Indivisible Multi-Hunk Bugs","Proc. ACM Softw. Eng.","","","10.1145/3660828","https://doi.org/10.1145/3660828","This paper presents our approach proposed to detect and create indivisible multi-hunk bugs, an evaluation of existing repair techniques based on these bugs, and a study of the patches of these bugs constructed by the developers and existing tools. Multi-hunk bug repair aims to deal with complex bugs by fixing multiple locations of the program. Previous research on multi-hunk bug repair is severely misguided, as the evaluation of previous techniques is predominantly based on the Defects4J dataset containing a great deal of divisible multi-hunk bugs. A divisible multi-hunk bug is essentially a combination of multiple bugs triggering different failures and is uncommon while debugging, as the developer typically deals with one failure at a time. To address this problem and provide a better basis for multi-hunk bug repair, we propose an enumeration-based approach IBugFinder, which given a bug dataset can automatically detect divisible and indivisible bugs in the dataset and further isolate the divisible bugs into new indivisible bugs. We applied IBugFinder to 281 multi-hunk bugs from the Defects4J dataset. IBugFinder identified 139 divisible bugs and created 249 new bugs among which 105 are multi-hunk. We evaluated existing repair techniques with the indivisible multi-hunk bugs detected and created by IBugFinder and found that these techniques repaired only a small number of bugs suggesting weak multi-hunk repair abilities. We further studied the patches of indivisible multi-hunk bugs constructed by the developers and the various tools with a focus on understanding the relationships of the partial patches made at different locations. The study has led to the identification of 8 partial patch relationships, which suggest different strategies for multi-hunk patch generation and provide important implication for multi-hunk bug repair.","2024-07","2025-11-25 22:29:47","2025-11-25 22:29:47","","","","FSE","1","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Automated program repair; indivisible multi-hunk bugs; partial patch relationship","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M5EZ2386","conferencePaper","2025","Liang, Ming; Zhang, Qingyu; Zuo, Zhipeng; Zheng, Shaoqiang; Chen, Dajun; Jiang, Wei; Li, Yong","FuseApplyBench: Multilingual Benchmark for Trustworthy Code Edit Applying Task","Proceedings of the 34th ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-1474-0","","10.1145/3713081.3732929","https://doi.org/10.1145/3713081.3732929","With the rise of Language Models (LMs) and Large Language Models (LLMs), their potential for code editing (CE) has gained attention. An approach is to have LLMs generate draft code modifications, which are then refined by smaller LMs in further Code Editing Apply (CEA). However, CEA is error-prone, and existing benchmarks do not systematically evaluate LLM performance in handling these issues. We introduce FuseApplyBench, a benchmark designed to evaluate LLM performance across three major error types in CEA tasks. Atop FuseApplyBench's pipeline, we collect datasets to perform fine-tuning, enhancing code modifications' reliability (denoted as FuseApply). We benchmark FuseApply, four widely used open source LLMs, and Kortix-FastApply on FuseApplyBench. Results show that FuseApply significantly improves trustworthiness and accuracy metrics, while other models demonstrate weaker performance, highlighting opportunities for advancing LLM in CE.","2025","2025-11-25 22:29:47","2025-11-25 22:29:47","","183–189","","","","","","","ISSTA Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","code editing; code language model; trustworthy LLM","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"75XBM328","conferencePaper","2024","Hartman, Jan; Sagtani, Hitesh; Tibshirani, Julie; Mehrotra, Rishabh","AI-assisted Coding with Cody: Lessons from Context Retrieval and Evaluation for Code Recommendations","Proceedings of the 18th ACM Conference on Recommender Systems","979-8-4007-0505-2","","10.1145/3640457.3688060","https://doi.org/10.1145/3640457.3688060","In this work, we discuss a recently popular type of recommender system: an LLM-based coding assistant. Connecting the task of providing code recommendations in multiple formats to traditional RecSys challenges, we outline several similarities and differences due to domain specifics. We emphasize the importance of providing relevant context to an LLM for this use case and discuss lessons learned from context enhancements &amp; offline and online evaluation of such AI-assisted coding systems.","2024","2025-11-25 22:29:47","2025-11-25 22:29:47","","748–750","","","","","","","RecSys '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Bari, Italy","","","","large language model; code generation; evaluation; coding assistant; context window","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D2PDTSYE","conferencePaper","2024","Dong, Jinhao; Sun, Jun; Lin, Yun; Zhang, Yedi; Ma, Murong; Dong, Jin Song; Hao, Dan","Revisiting the Conflict-Resolving Problem from a Semantic Perspective","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3694993","https://doi.org/10.1145/3691620.3694993","Collaborative software development significantly enhances development productivity by enabling multiple contributors to work concurrently on different branches. Despite these advantages, such collaboration often increases the likelihood of causing conflicts. Resolving these conflicts brings huge challenges, primarily due to the necessity of comprehending the differences between conflicting versions. Researchers have explored various automatic conflict resolution techniques, including unstructured, structured, and learning-based approaches. However, these techniques are mostly heuristic-based or black-box in nature, which means they do not attempt to solve the root cause of the conflicts, i.e., the existence of different program behaviors exhibited by the conflicting versions.In this work, we propose sMerge, a novel conflict resolution approach based on the semantics of program behaviors. We first give the formal definition of the merge conflict problem as well as the specific conditions under which conflicts happen and the criteria employed to select certain version as the resolution. Based on the definition, we propose to resolve the conflicts from the perspective of program behaviors. In particular, we argue that the key to resolving conflicts is identifying different program behaviors, and thus can be solved through targeted test generation. We conduct an extensive evaluation of sMerge using a comprehensive dataset of conflicts sourced from various projects. Our results show that sMerge can effectively solve the merge problem by employing different test generation techniques, including search-based, GPT-based, and manual testing. We remark that sMerge provides a way to understand the program behavior differences through testing, which not only allows us to solve the merge problem soundly but also enables the detection of incorrect ground truths provided by developers, thereby enhancing the reliability of the merge process.","2024","2025-11-25 22:29:47","2025-11-25 22:29:47","","141–152","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","behavior-based conflict resolving; targeted test generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QWJWK5CA","journalArticle","2024","Yu, Shengcheng; Fang, Chunrong; Li, Xin; Ling, Yuchen; Chen, Zhenyu; Su, Zhendong","Effective, Platform-Independent GUI Testing via Image Embedding and Reinforcement Learning","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3674728","https://doi.org/10.1145/3674728","Software applications (apps) have been playing an increasingly important role in various aspects of society. In particular, mobile apps and web apps are the most prevalent among all applications and are widely used in various industries as well as in people’s daily lives. To help ensure mobile and web app quality, many approaches have been introduced to improve app GUI testing via automated exploration, including random testing, model-based testing, learning-based testing, and so on. Despite the extensive effort, existing approaches are still limited in reaching high code coverage, constructing high-quality models, and being generally applicable. Reinforcement learning-based approaches, as a group of representative and advanced approaches for automated GUI exploration testing, are faced with difficult challenges, including effective app state abstraction, reward function design, and so on. Moreover, they heavily depend on the specific execution platforms (i.e., Android or Web), thus leading to poor generalizability and being unable to adapt to different platforms.This work specifically tackles these challenges based on the high-level observation that apps from distinct platforms share commonalities in GUI design. Indeed, we propose PIRLTest, an effective platform-independent approach for app testing. Specifically, PIRLTest utilizes computer vision and reinforcement learning techniques in a novel, synergistic manner for automated testing. It extracts the GUI widgets from GUI pages and characterizes the corresponding GUI layouts, embedding the GUI pages as states. The app GUI state combines the macroscopic perspective (app GUI layout) and the microscopic perspective (app GUI widget) and attaches the critical semantic information from GUI images. This enables PIRLTest to be platform-independent and makes the testing approach generally applicable on different platforms. PIRLTest explores apps with the guidance of a curiosity-driven strategy, which uses a Q-network to estimate the values of specific state-action pairs to encourage more exploration in uncovered pages without platform dependency. The exploration will be assigned with rewards for all actions, which are designed considering both the app GUI states and the concrete widgets, to help the framework explore more uncovered pages. We conduct an empirical study on 20 mobile apps and 5 web apps, and the results show that PIRLTest is zero-cost when being adapted to different platforms, and can perform better than the baselines, covering 6.3–41.4% more code on mobile apps and 1.5–51.1% more code on web apps. PIRLTest is capable of detecting 128 unique bugs on mobile and web apps, including 100 bugs that cannot be detected by the baselines.","2024-09","2025-11-25 22:29:47","2025-11-25 22:29:47","","","","7","33","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Software testing; reinforcement learning; GUI image understanding; platform-independent testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R9TV9C5S","journalArticle","2024","Pternea, Moschoula; Singh, Prerna; Chakraborty, Abir; Oruganti, Yagna; Milletari, Mirco; Bapat, Sayli; Jiang, Kebei","The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models","J. Artif. Int. Res.","","1076-9757","10.1613/jair.1.15960","https://doi.org/10.1613/jair.1.15960","In this work, we review research studies that combine Reinforcement Learning (RL) and Large Language Models (LLMs), two areas that owe their momentum to the development of Deep Neural Networks (DNNs). We propose a novel taxonomy of three main classes based on the way that the two model types interact with each other. The first class, RL4LLM, includes studies where RL is leveraged to improve the performance of LLMs on tasks related to Natural Language Processing (NLP). RL4LLM is divided into two sub-categories depending on whether RL is used to directly fine-tune an existing LLM or to improve the prompt of the LLM. In the second class, LLM4RL, an LLM assists the training of an RL model that performs a task that is not inherently related to natural language. We further break down LLM4RL based on the component of the RL training framework that the LLM assists or replaces, namely reward shaping, goal generation, and policy function. Finally, in the third class, RL+LLM, an LLM and an RL agent are embedded in a common planning framework without either of them contributing to training or fine-tuning of the other. We further branch this class to distinguish between studies with and without natural language feedback. We use this taxonomy to explore the motivations behind the synergy of LLMs and RL and explain the reasons for its success, while pinpointing potential shortcomings and areas where further research is needed, as well as alternative methodologies that serve the same goal.","2024-09","2025-11-25 22:29:47","2025-11-25 22:29:47","","","","","80","","","","","","","","","","","","","","","","","Place: El Segundo, CA, USA Publisher: AI Access Foundation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N65IHYHM","journalArticle","2024","Shiri Harzevili, Nima; Mohajer, Mohammad Mahdi; Wei, Moshi; Pham, Hung Viet; Wang, Song","History-Driven Fuzzing for Deep Learning Libraries","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3688838","https://doi.org/10.1145/3688838","Recently, many Deep Learning (DL) fuzzers have been proposed for API-level testing of DL libraries. However, they either perform unguided input generation (e.g., not considering the relationship between API arguments when generating inputs) or only support a limited set of corner-case test inputs. Furthermore, many developer APIs crucial for library development remain untested, as they are typically not well documented and lack clear usage guidelines, unlike end-user APIs. This makes them a more challenging target for automated testing. To fill this gap, we propose a novel fuzzer named Orion, which combines guided test input generation and corner-case test input generation based on a set of fuzzing heuristic rules constructed from historical data known to trigger critical issues in the underlying implementation of DL APIs. To extract the fuzzing heuristic rules, we first conduct an empirical study on the root cause analysis of 376 vulnerabilities in two of the most popular DL libraries, PyTorch and TensorFlow. We then construct the fuzzing heuristic rules based on the root causes of the extracted historical vulnerabilities. Using these fuzzing heuristic rules, Orion generates corner-case test inputs for API-level fuzzing. In addition, we extend the seed collection of existing studies to include test inputs for developer APIs. Our evaluation shows that Orion reports 135 vulnerabilities in the latest releases of TensorFlow and PyTorch, 76 of which were confirmed by the library developers. Among the 76 confirmed vulnerabilities, 69 were previously unknown, and 7 have already been fixed. The rest are awaiting further confirmation. For end-user APIs, Orion detected 45.58% and 90% more vulnerabilities in TensorFlow and PyTorch, respectively, compared to the state-of-the-art conventional fuzzer, DeepRel. When compared to the state-of-the-art LLM-based DL fuzzer, AtlasFuz, and Orion detected 13.63% more vulnerabilities in TensorFlow and 18.42% more vulnerabilities in PyTorch. Regarding developer APIs, Orion stands out by detecting 117% more vulnerabilities in TensorFlow and 100% more vulnerabilities in PyTorch compared to the most relevant fuzzer designed for developer APIs, such as FreeFuzz.","2024-12","2025-11-25 22:29:47","2025-11-25 22:29:47","","","","1","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","test generation; deep learning; Fuzz testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TIHH5VQD","conferencePaper","2024","Lazuka, Malgorzata; Anghel, Andreea; Parnell, Thomas","LLM-Pilot: Characterize and Optimize Performance of your LLM Inference Services","Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis","979-8-3503-5291-7","","10.1109/SC41406.2024.00022","https://doi.org/10.1109/SC41406.2024.00022","As Large Language Models (LLMs) are rapidly growing in popularity, LLM inference services must be able to serve requests from thousands of users while satisfying performance requirements. The performance of an LLM inference service is largely determined by the hardware onto which it is deployed, but understanding of which hardware will deliver on performance requirements remains challenging. In this work we present LLM-Pilot - a first-of-its-kind system for characterizing and predicting performance of LLM inference services. LLM-Pilot performs benchmarking of LLM inference services, under a realistic workload, across a variety of GPUs, and optimizes the service configuration for each considered GPU to maximize performance. Finally, using this characterization data, LLM-Pilot learns a predictive model, which can be used to recommend the most cost-effective hardware for a previously unseen LLM. Compared to existing methods, LLM-Pilot can deliver on performance requirements 33% more frequently, whilst reducing costs by 60% on average.","2024","2025-11-25 22:29:47","2025-11-25 22:29:47","","","","","","","","","SC '24","","","","IEEE Press","Atlanta, GA, USA","","","","","","","","","","","","large language models; benchmarking; inference services; performance; prediction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"36GIJDNQ","journalArticle","2025","Xie, Linna; Li, Zhong; Pei, Yu; Wen, Zhongzhen; Liu, Kui; Zhang, Tian; Li, Xuandong","PReMM: LLM-Based Program Repair for Multi-method Bugs via Divide and Conquer","Proc. ACM Program. Lang.","","","10.1145/3763097","https://doi.org/10.1145/3763097","Large-language models (LLMs) have been leveraged to enhance the capability of automated program repair techniques in recent research. While existing LLM-based program repair techniques compared favorably to other techniques based on heuristics, constraint-solving, and learning in producing high-quality patches, they mainly target bugs that can be corrected by changing a single faulty method, which greatly limits the effectiveness of such techniques in repairing bugs that demand patches spanning across multiple methods. In this work, we propose the PReMM technique to effectively propose patches changing multiple methods. PReMM builds on three core component techniques: the faulty method clustering technique to partition the faulty methods into clusters based on the dependence relationship among them, enabling a divide-and-conquer strategy for the repairing task; the fault context extraction technique to gather extra information about the fault context which can be utilized to better guide the diagnosis of the fault and the generation of correct patches; the dual-agent-based patch generation technique that employs two LLM-based agents with different roles to analyze the fault more precisely and generate patches of higher-quality. We have implemented the PReMM technique into a tool with the same name and applied the tool to repair real-world bugs from datasets Defects4J V1.2 and V2.0. PReMM produced correct patches for 307 bugs in total. Compared with ThinkRepair, the state-of-the-art LLM-based program repair technique, PReMM correctly repaired 102 more bugs, achieving an improvement of 49.8%.","2025-10","2025-11-25 22:29:47","2025-11-25 22:29:47","","","","OOPSLA2","9","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; Automated Program Repair; Context-Aware Repair; Divide and Conquer; Multi-method Bugs","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3V58PYED","conferencePaper","2024","Sallou, June; Durieux, Thomas; Panichella, Annibale","Breaking the Silence: the Threats of Using LLMs in Software Engineering","Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results","979-8-4007-0500-7","","10.1145/3639476.3639764","https://doi.org/10.1145/3639476.3639764","Large Language Models (LLMs) have gained considerable traction within the Software Engineering (SE) community, impacting various SE tasks from code completion to test generation, from program repair to code summarization. Despite their promise, researchers must still be careful as numerous intricate factors can influence the outcomes of experiments involving LLMs. This paper initiates an open discussion on potential threats to the validity of LLM-based research including issues such as closed-source models, possible data leakage between LLM training data and research evaluation, and the reproducibility of LLM-based findings. In response, this paper proposes a set of guidelines tailored for SE researchers and Language Model (LM) providers to mitigate these concerns. The implications of the guidelines are illustrated using existing good practices followed by LLM providers and a practical example for SE researchers in the context of test case generation.","2024","2025-11-25 22:29:47","2025-11-25 22:46:52","","102–106","","","","","","","ICSE-NIER'24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","Large language models; Test pattern generators; Software engineering; Codes; Large Language Models; Software Engineering; Evaluation; Guidelines; Data models; Training data; Maintenance engineering; Reproducibility of results","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"69QJPGT3","journalArticle","2025","She, Xinyu; Liu, Yue; Zhao, Yanjie; He, Yiling; Li, Li; Tantithamthavorn, Chakkrit; Qin, Zhan; Wang, Haoyu","Pitfalls in Language Models for Code Intelligence: A Taxonomy and Survey","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3748647","https://doi.org/10.1145/3748647","Modern language models (LMs) have been successfully employed in source code generation and understanding, leading to a significant increase in research focused on learning-based code intelligence, such as automated bug repair, and test case generation. Despite their great potential, language models for code intelligence (LM4Code) are susceptible to potential pitfalls, which hinder realistic performance and further impact their reliability and applicability in real-world deployment. Such challenges drive the need for a comprehensive understanding - not just identifying these issues but delving into their possible implications and existing solutions to build more reliable language models tailored to code intelligence. Based on a well-defined systematic research approach, we conducted an extensive literature review to uncover the pitfalls inherent in LM4Code. Finally, 121 primary studies from top-tier venues have been identified. After carefully examining these studies, we designed a taxonomy of pitfalls in LM4Code research and conducted a systematic study to summarize the issues, current solutions, implications, and challenges of different pitfalls for LM4Code systems. We developed a comprehensive classification scheme that dissects pitfalls across four crucial aspects: data collection and labeling, system design and learning, performance evaluation, and deployment and maintenance. Through this study, we aim to provide a roadmap for researchers and practitioners, facilitating their understanding and utilization of LM4Code in reliable and trustworthy ways.","2025-07","2025-11-25 22:29:47","2025-11-25 22:29:47","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Software engineering; Code generation; Code intelligence; Language models for Code; Trustworthiness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AMWNGNTV","conferencePaper","2024","Wang, Guanyu; Li, Yuekang; Liu, Yi; Deng, Gelei; Li, Tianlin; Xu, Guosheng; Liu, Yang; Wang, Haoyu; Wang, Kailong","MeTMaP: Metamorphic Testing for Detecting False Vector Matching Problems in LLM Augmented Generation","Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering","979-8-4007-0609-7","","10.1145/3650105.3652297","https://doi.org/10.1145/3650105.3652297","Augmented generation techniques such as Retrieval-Augmented Generation (RAG) and Cache-Augmented Generation (CAG) have revolutionized the field by enhancing large language model (LLM) outputs with external knowledge and cached information. However, the integration of vector databases, which serve as a backbone for these augmentations, introduces critical challenges, particularly in ensuring accurate vector matching. False vector matching in these databases can significantly compromise the integrity and reliability of LLM outputs, leading to misinformation or erroneous responses. Despite the crucial impact of these issues, there is a notable research gap in methods to effectively detect and address false vector matches in LLM-augmented generation.This paper presents MeTMaP, a metamorphic testing framework developed to identify false vector matching in LLM-augmented generation systems. We derive eight metamorphic relations (MRs) from six NLP datasets, which form our method's core, based on the idea that semantically similar texts should match and dissimilar ones should not. MeTMaP uses these MRs to create sentence triplets for testing, simulating real-world matching scenarios. Our evaluation of MeTMaP over 203 vector matching configurations, involving 29 embedding models and 7 distance metrics, uncovers significant inaccuracies. The results, showing a maximum accuracy of only 41.51% on our tests compared to the original datasets, emphasize the widespread issue of false matches in vector matching methods and the critical need for effective detection and mitigation in LLM-augmented applications.","2024","2025-11-25 22:29:48","2025-11-25 22:47:42","","12–23","","","","","","","FORGE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","Semantics; Accuracy; Metamorphic Testing; metamorphic testing; augmented generation; vector matching; Databases; Prevention and mitigation; Vectors; Reliability engineering; Augmented Generation; Terminology; Vector Matching","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RLERCAIQ","conferencePaper","2024","Qu, Muzi; Liu, Jie; Kang, Liangyi; Wang, Shuai; Ye, Dan; Huang, Tao","Dynamic Scoring Code Token Tree: A Novel Decoding Strategy for Generating High-Performance Code","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695505","https://doi.org/10.1145/3691620.3695505","Within the realms of scientific computing, large-scale data processing, and artificial intelligence-powered computation, disparities in performance, which originate from differing code implementations, directly influence the practicality of the code. Although existing works tried to utilize code knowledge to enhance the execution performance of codes generated by large language models, they neglect code evaluation outcomes which directly refer to the code execution details, resulting in inefficient computation. To address this issue, we propose DSCT-Decode, an innovative adaptive decoding strategy for large language models, that employs a data structure named 'Code Token Tree' (CTT), which guides token selection based on code evaluation outcomes. DSCT-Decode assesses generated code across three dimensions—correctness, performance, and similarity—and utilizes a dynamic penalty-based boundary intersection method to compute multi-objective scores, which are then used to adjust the scores of nodes in the CTT during backpropagation. By maintaining a balance between exploration, through token selection probabilities, and exploitation, through multi-objective scoring, DSCT-Decode effectively navigates the code space to swiftly identify high-performance code solutions. To substantiate our framework, we developed a new benchmark, big-DS-1000, which is an extension of DS-1000. This benchmark is the first of its kind to specifically evaluate code generation methods based on execution performance. Comparative evaluations with leading large language models, such as CodeLlama and GPT-4, show that our framework achieves an average performance enhancement of nearly 30%. Furthermore, 30% of the codes exhibited a performance improvement of more than 20%, underscoring the effectiveness and potential of our framework for practical applications.","2024","2025-11-25 22:29:48","2025-11-25 22:29:48","","1308–1318","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","large language model; code generation; performance optimization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DR4I74IP","conferencePaper","2024","Lin, Bo; Wang, Shangwen; Wen, Ming; Chen, Liqian; Mao, Xiaoguang","One Size Does Not Fit All: Multi-granularity Patch Generation for Better Automated Program Repair","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680381","https://doi.org/10.1145/3650212.3680381","Automated program repair aims to automate bug correction and alleviate the burden of manual debugging, which plays a crucial role in software development and maintenance. Recent studies reveal that learning-based approaches have outperformed conventional APR techniques (e.g., search-based APR). Existing learning-based APR techniques mainly center on treating program repair either as a translation task or a cloze task. The former primarily emphasizes statement-level repair, while the latter concentrates on token-level repair, as per our observations. In practice, however, patches may manifest at various repair granularity, including statement, expression, or token levels. Consequently, merely generating patches from a single granularity would be ineffective to tackle real-world defects. Motivated by this observation, we propose Mulpor, a multi-granularity patch generation approach designed to address the diverse nature of real-world bugs. Mulpor comprises three components: statement-level, expression-level, and token-level generator, each is pre-trained to generate correct patches at its respective granularity. The approach involves generating candidate patches from various granularities, followed by a re-ranking process based on a heuristic to prioritize patches. Experimental results on the Defects4J dataset demonstrate that Mulpor correctly repair 92 bugs on Defects4J-v1.2, which achieves 27.0% (20 bugs) and 12.2% (10 bugs) improvement over the previous state-of-the-art NMT-style Rap-Gen and Cloze-style GAMMA. We also studied the generalizability of Mulpor in repairing vulnerabilities, revealing a notable 51% increase in the number of correctly-fixed patches compared with state-of-the-art vulnerability repair approaches. This paper underscores the importance of considering multiple granularities in program repair techniques for a comprehensive strategy to address the diverse nature of real-world software defects. Mulpor, as proposed herein, exhibits promising results in achieving effective and diverse bug fixes across various program repair scenarios.","2024","2025-11-25 22:29:48","2025-11-25 22:29:48","","1554–1566","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Automated Program Repair; Deep Learning; Pre-Training","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W9EE6MBB","journalArticle","2025","Dong, Jinhao; Sun, Jun; Zhang, Wenjie; Dong, Jin Song; Hao, Dan","ConTested: Consistency-Aided Tested Code Generation with LLM","Proc. ACM Softw. Eng.","","","10.1145/3728902","https://doi.org/10.1145/3728902","Recent advancements in large language models (LLMs) have significantly improved code generation, which generates code snippets automatically based on natural language requirements. Despite achieving state-of-the-art performance, LLMs often struggle to generate accurate and reliable code, requiring developers to spend substantial effort debugging and evaluating the generated output. Researchers have proposed leveraging Consistency to select code that passes more tests (inter-consistency) and demonstrates consistent behavior across more counterparts (intra-consistency). However, since the tests themselves are also generated by LLMs, relying on majority voting based on incorrect tests leads to unreliable results. To address this, we propose a lightweight interaction framework that incorporates user feedback to effectively guide consistency. Our results demonstrate that, with minimal human effort, performance can be significantly improved. In each iteration, we introduce a rank-correct-fix co-evolution process between code and tests. This process iteratively enhances the quality of both, making the consistency voting between code and tests more reliable. We evaluate ConTested through extensive experiments, demonstrating its effectiveness across multiple LLMs, including GPT-3.5 and GPT-4o. Our results show improvements of 32.9% over GPT-3.5 and 16.97% over GPT-4o. Additionally, ConTested achieves an 11.1% improvement over the SOTA post-processing technique, MPSC. This improvement is achieved with only a 4-round interaction with users, requiring minimal user effort. A user study further confirms the feasibility and cost-effectiveness of ConTested, highlighting its ability to enhance code generation without introducing substantial overhead.","2025-06","2025-11-25 22:29:48","2025-11-25 22:29:48","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Code Generation; Iterative Interaction; Self-Consistency","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PP6K4YV8","conferencePaper","2024","Alshahwan, Nadia; Harman, Mark; Harper, Inna; Marginean, Alexandru; Sengupta, Shubho; Wang, Eddy","Assured Offline LLM-Based Software Engineering","Proceedings of the ACM/IEEE 2nd International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering","979-8-4007-0564-9","","10.1145/3643661.3643953","https://doi.org/10.1145/3643661.3643953","In this paper we address the following question: How can we use Large Language Models (LLMs) to improve code independently of a human, while ensuring that the improved code(1) does not regress the properties of the original code ?(2) improves the original in a verifiable and measurable way ?To address this question, we advocate Assured LLM-Based Software Engineering; a generate-and-test approach, inspired by Genetic Improvement. Assured LLMSE applies a series of semantic filters that discard code that fails to meet these twin guarantees. This overcomes the potential problem of LLM's propensity to hallucinate. It allows us to generate code using LLMs, independently of any human. The human plays the role only of final code reviewer, as they would do with code generated by other human engineers.This paper is an outline of the content of the keynote by Mark Harman at the International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering, Monday 15th April 2024, Lisbon, Portugal.","2024","2025-11-25 22:29:48","2025-11-25 22:29:48","","7–12","","","","","","","InteNSE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","large language models (LLMs); automated code generation; codellama; genetic improvement (GI); llama; search based software engineering (SBSE)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T8V2J3SL","conferencePaper","2025","Simoni, Marco; Saracino, Andrea; P, Vinod; Conti, Mauro","MoRSE: Bridging the Gap in Cybersecurity Expertise with Retrieval Augmented Generation","Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing","979-8-4007-0629-5","","10.1145/3672608.3707898","https://doi.org/10.1145/3672608.3707898","In this paper, we introduce MoRSE (Mixture of RAGs Security Experts), the first specialised AI chatbot for cybersecurity. MoRSE aims to provide comprehensive and complete knowledge about cybersecurity. MoRSE uses two RAG (Retrieval Augmented Generation) systems designed to retrieve and organize information from multidimensional cybersecurity contexts. MoRSE differs from traditional RAGs by using parallel retrievers that work together to retrieve semantically related information in different formats and structures. Unlike traditional Large Language Models (LLMs) that rely on Parametric Knowledge Bases, MoRSE retrieves relevant documents from Non-Parametric Knowledge Bases in response to user queries. Subsequently, MoRSE uses this information to generate accurate answers. In addition, MoRSE benefits from real-time updates to its knowledge bases, enabling continuous knowledge enrichment without retraining. We have evaluated the effectiveness of MoRSE against other state-of-the-art LLMs, evaluating the system on 600 cybersecurity specific questions. The experimental evaluation has shown that the improvement in terms of relevance and correctness of the answer is more than 10% compared to known solutions such as GPT-4 and Mixtral 7x8.","2025","2025-11-25 22:29:48","2025-11-25 22:29:48","","1213–1222","","","","","","","SAC '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Catania International Airport, Catania, Italy","","","","large language model; cyber threat intelligence; cybersecurity; retrieval augmented generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5H868VC6","conferencePaper","2025","Kuang, Hongyu; Zhang, Ning; Gao, Hui; Zhou, Xin; Assuncao, Wesley K. G.; Ma, Xiaoxing; Shao, Dong; Rong, Guoping; Zhang, He","Brevity is the Soul of Wit: Condensing Code Changes to Improve Commit Message Generation","Proceedings of the 16th International Conference on Internetware","979-8-4007-1926-4","","10.1145/3755881.3755977","https://doi.org/10.1145/3755881.3755977","Commit messages are valuable resources for describing why code changes are committed to repositories in version control systems (e.g., Git). They effectively help developers understand code changes and better perform software maintenance tasks. Unfortunately, developers often neglect to write high-quality commit messages in practice. Therefore, a growing body of work is proposed to generate commit messages automatically. These works all demonstrated that how to organize and represent code changes is vital in generating good commit messages, including the use of fine-grained graphs or embeddings to better represent code changes. In this study, we choose an alternative way to condense code changes before generation, i.e., proposing brief yet concise text templates consisting of the following three parts: (1) summarized code changes, (2) elicited comments, and (3) emphasized code identifiers. Specifically, we first condense code changes by using our proposed templates with the help of a heuristic-based tool named ChangeScribe, and then fine-tune CodeLlama-7B on the pairs of our proposed templates and corresponding commit messages. Our proposed templates better utilize pre-trained language models, while being naturally brief and readable to complement generated commit messages for developers. Our evaluation based on a widely used dataset showed that our approach can outperform six baselines in terms of BLEU-Norm, METEOR, and ROUGE-L, with average improvements of 51.7%, 78.7%, and 62.5%, respectively. The ablation study and human evaluation also provide further insights into the effectiveness of our approach.","2025","2025-11-25 22:29:48","2025-11-25 22:29:48","","389–401","","","","","","","Internetware '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Large Language Model; Code Change Representation; Commit Message Generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"46DFI9GA","conferencePaper","2025","Zi, Yangtian; Li, Luisa; Guha, Arjun; Anderson, Carolyn; Feldman, Molly Q","“I Would Have Written My Code Differently': Beginners Struggle to Understand LLM-Generated Code","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","10.1145/3696630.3731663","https://doi.org/10.1145/3696630.3731663","Large language models (LLMs) are being increasingly adopted for programming work. Prior work shows that while LLMs accelerate task completion for professional programmers, beginning programmers struggle to prompt models effectively. However, prompting is just half of the code generation process– when code is generated, it must be read, evaluated, and integrated (or rejected). How accessible are these tasks for beginning programmers?This paper measures how well beginners comprehend LLM-generated code and explores the challenges students face in judging code correctness. We compare how well students understand natural language descriptions of functions and LLM-generated implementations, studying 32 CS1 students on 160 task instances. Our results show a low per-task success rate of 32.5%, with indiscriminate struggles across demographic populations. Key challenges include barriers for non-native English speakers, unfamiliarity with Python syntax, and automation bias. Our findings highlight the barrier that code comprehension presents to beginning programmers seeking to write code with LLMs.","2025","2025-11-25 22:29:48","2025-11-25 22:29:48","","1479–1488","","","","","","","FSE Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","large language models; CS1; computer science education; code comprehension","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3AREJCR2","conferencePaper","2024","Li, Jialong; Zhang, Mingyue; Li, Nianyu; Weyns, Danny; Jin, Zhi; Tei, Kenji","Exploring the Potential of Large Language Models in Self-adaptive Systems","Proceedings of the 19th International Symposium on Software Engineering for Adaptive and Self-Managing Systems","979-8-4007-0585-4","","10.1145/3643915.3644088","https://doi.org/10.1145/3643915.3644088","Large Language Models (LLMs), with their abilities in knowledge acquisition and reasoning, can potentially enhance the various aspects of Self-adaptive Systems (SAS). Yet, the potential of LLMs in SAS remains largely unexplored and ambiguous, due to the lack of literature from flagship conferences or journals in the field, such as SEAMS and TAAS. The interdisciplinary nature of SAS suggests that drawing and integrating ideas from related fields, such as software engineering and autonomous agents, could unveil innovative research directions for LLMs within SAS. To this end, this paper reports the results of a literature review of studies in relevant fields, summarizes and classifies the studies relevant to SAS, and outlines their potential to specific aspects of SAS. Literature classification: www.github.com/545659928/LLM4SAS","2024","2025-11-25 22:29:48","2025-11-25 22:29:48","","77–83","","","","","","","SEAMS '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, AA, Portugal","","","","large language model; survey; self-adaptive systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B7P4XN3K","conferencePaper","2024","Ahmed, Toufique; Devanbu, Premkumar","Better patching using LLM prompting, via Self-Consistency","Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering","979-8-3503-2996-4","","10.1109/ASE56229.2023.00065","https://doi.org/10.1109/ASE56229.2023.00065","Large Language models (LLMs) can be induced to solve non-trivial problems with ""few-shot"" prompts including illustrative problem-solution examples. Now if the few-shots also include ""chain of thought"" (CoT) explanations, which are of the form problem-explanation-solution, LLMs will generate a ""explained"" solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] (S-C) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct!Unfortunately, the use of this highly-performant S-C (or even CoT) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the S-C approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots. We achieve state-of-the art results, beating previous approaches to prompting-based program repair, on the MODIT dataset; we also find evidence suggesting that the correct commit messages are helping the LLM learn to produce better patches.","2024","2025-11-25 22:29:48","2025-11-25 22:29:48","","1742–1746","","","","","","","ASE '23","","","","IEEE Press","Echternach, Luxembourg","","","","","","","","","","","","program repair; LLMS; self-consistency","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CWCQBBJW","conferencePaper","2024","Serato, Jay Vince Donoso; Sta. Romana, Cherry Lyn","Development and Utilization of AI-Enabled Automatic Programming Problem Generator Using the CodeRunner Plugin of Moodle","Proceedings of the 2024 10th International Conference on Education and Training Technologies","979-8-4007-1789-5","","10.1145/3661904.3661921","https://doi.org/10.1145/3661904.3661921","Programming instructors provide various kinds of problems that suit their current topics of programming. With the use of Learning Management Systems (LMS) such as Moodle, teachers can create and store their problems in problem banks using a question plugin CodeRunner. However creating and validating programming problems takes significant time and creative effort. With instructors dealing multiple programming languages, it would need mastery not only to solve the problem but also to use a specific language. This paper presents an automation of the programming problem generation using AI. This is effectively an improvement of the CodeRunner plugin of Moodle that allows programming instructors to generate problem descriptions, answers, and testcases of different programming topics in various programming languages with a few clicks. To address the issue of difficulty control, an additional feature is placed to generate an easier or harder problem than what is currently generated. The evaluation of the improved tool showed that the problems generated with AI are similar, correct, and practical that matches the human-generated problems.","2024","2025-11-25 22:29:48","2025-11-25 22:29:48","","147–152","","","","","","","ICETT '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Macau, China","","","","Generative AI; Automatic question generation; Programming learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E5WZHPPM","conferencePaper","2024","Sun, Yuxuan","Automated Generation and Compilation of Fuzz Driver Based on Large Language Models","Proceedings of the 2024 9th International Conference on Cyber Security and Information Engineering","979-8-4007-1813-7","","10.1145/3689236.3689272","https://doi.org/10.1145/3689236.3689272","Fuzz drivers are essential components of library fuzz testing, yet automatically generating correct and robust fuzz drivers and executing them is challenging. The predominant approach to fuzz testing libraries and their function interfaces involves security experts manually writing test drivers for fuzz testers. In contrast, generation based on LLMs (Large Language Models) is a promising direction, as it can operate with lower requirements on consumer programs, utilizes API (Application Programming Interface) usage information across multiple dimensions, and generates user-friendly output code. However, there is currently no fully automated method for driver generation and compilation. To address this issue, this paper has designed an automated method for generating and compiling drivers, and has evaluated the quality of the drivers it produces. Evaluation results indicate that drivers generated by large language models perform nearly as well in coverage as those written manually, and the generated compilation commands achieve an accuracy of 75%.","2024","2025-11-25 22:29:48","2025-11-25 22:29:48","","461–468","","","","","","","ICCSIE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Large Language Model; Fuzz Driver Generation; Fuzz Testing; Vulnerability Detection; Driver Compilation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VTDFRC97","conferencePaper","2023","Wan, Yuxuan; Wang, Wenxuan; He, Pinjia; Gu, Jiazhen; Bai, Haonan; Lyu, Michael R.","BiasAsker: Measuring the Bias in Conversational AI System","Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","979-8-4007-0327-0","","10.1145/3611643.3616310","https://doi.org/10.1145/3611643.3616310","Powered by advanced Artificial Intelligence (AI) techniques, conversational AI systems, such as ChatGPT, and digital assistants like Siri, have been widely deployed in daily life. However, such systems may still produce content containing biases and stereotypes, causing potential social problems. Due to modern AI techniques’ data-driven, black-box nature, comprehensively identifying and measuring biases in conversational systems remains challenging. Particularly, it is hard to generate inputs that can comprehensively trigger potential bias due to the lack of data containing both social groups and biased properties. In addition, modern conversational systems can produce diverse responses (e.g., chatting and explanation), which makes existing bias detection methods based solely on sentiment and toxicity hardly being adopted. In this paper, we propose BiasAsker, an automated framework to identify and measure social bias in conversational AI systems. To obtain social groups and biased properties, we construct a comprehensive social bias dataset containing a total of 841 groups and 5,021 biased properties. Given the dataset, BiasAsker automatically generates questions and adopts a novel method based on existence measurement to identify two types of biases (i.e., absolute bias and related bias) in conversational systems. Extensive experiments on eight commercial systems and two famous research models, such as ChatGPT and GPT-3, show that 32.83% of the questions generated by BiasAsker can trigger biased behaviors in these widely deployed conversational systems. All the code, data, and experimental results have been released to facilitate future research.","2023","2025-11-25 22:29:48","2025-11-25 22:29:48","","515–527","","","","","","","ESEC/FSE 2023","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: San Francisco, CA, USA","","","","Software testing; conversational models; social bias","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T96YZLGR","conferencePaper","2023","Liu, Zhe; Chen, Chunyang; Wang, Junjie; Che, Xing; Huang, Yuekai; Hu, Jun; Wang, Qing","Fill in the Blank: Context-Aware Automated Text Input Generation for Mobile GUI Testing","Proceedings of the 45th International Conference on Software Engineering","978-1-6654-5701-9","","10.1109/ICSE48619.2023.00119","https://doi.org/10.1109/ICSE48619.2023.00119","Automated GUI testing is widely used to help ensure the quality of mobile apps. However, many GUIs require appropriate text inputs to proceed to the next page, which remains a prominent obstacle for testing coverage. Considering the diversity and semantic requirement of valid inputs (e.g., flight departure, movie name), it is challenging to automate the text input generation. Inspired by the fact that the pre-trained Large Language Model (LLM) has made outstanding progress in text generation, we propose an approach named QTypist based on LLM for intelligently generating semantic input text according to the GUI context. To boost the performance of LLM in the mobile testing scenario, we develop a prompt-based data construction and tuning method which automatically extracts the prompts and answers for model tuning. We evaluate QTypist on 106 apps from Google Play, and the result shows that the passing rate of QTypist is 87%, which is 93% higher than the best baseline. We also integrate QTypist with the automated GUI testing tools and it can cover 42% more app activities, 52% more pages, and subsequently help reveal 122% more bugs compared with the raw tool.","2023","2025-11-25 22:29:48","2025-11-25 22:29:48","","1355–1367","","","","","","","ICSE '23","","","","IEEE Press","Melbourne, Victoria, Australia","","","","","","","","","","","","large language model; GUI testing; android app; prompt-tuning; text input generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U4HZRWUR","conferencePaper","2024","Li, Ziyu; Shin, Donghwan","Mutation-based Consistency Testing for Evaluating the Code Understanding Capability of LLMs","Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI","979-8-4007-0591-5","","10.1145/3644815.3644946","https://doi.org/10.1145/3644815.3644946","Large Language Models (LLMs) have shown remarkable capabilities in processing both natural and programming languages, which have enabled various applications in software engineering, such as requirement engineering, code generation, and software testing. However, existing code generation benchmarks do not necessarily assess the code understanding performance of LLMs, especially for the subtle inconsistencies that may arise between code and its semantics described in natural language.In this paper, we propose a novel method, called Mutation-based Consistency Testing (MCT), to systematically assess the code understanding performance of LLMs, particularly focusing on subtle differences between code and its descriptions, by introducing code mutations to existing code generation datasets. Code mutations are small changes that alter the semantics of the original code, creating a mismatch with the natural language description. MCT uses different types of code mutations, such as operator replacement and statement deletion, to generate inconsistent code-description pairs. MCT then uses these pairs to test the ability of LLMs to detect the inconsistencies correctly.We conduct a case study on the two popular LLMs, GPT-3.5 and GPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, which consists of 164 programming problems written in six programming languages (Python, C++, Java, Go, JavaScript, and Rust). The results show that the LLMs have significant variations in their code understanding performance and that they have different strengths and weaknesses depending on the mutation type and language. We further explain conditions under which the LLMs result in correct answers using input characteristics (e.g., number of tokens) and investigate to what extent the test results can be improved using one-shot prompts (i.e., providing an additional example). Our MCT method and the case study results provide valuable implications for future research and development of LLM-based software engineering.","2024","2025-11-25 22:29:48","2025-11-25 22:47:44","","150–159","","","","","","","CAIN '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","Analytical models; Semantics; Software testing; Codes; large language models; Large Language Models; Benchmark testing; Software Engineering; software engineering; mutation analysis; Training; Sensitivity; Mutation Analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6KCEB4A3","book","2024","","SBFT '24: Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing","","979-8-4007-0562-5","","","","Welcome to the 17th edition of the International Workshop on Search-Based and Fuzz Testing (SBFT), formerly the International Workshop on Search-Based Software Testing. Search- Based Software Testing (SBST) applies search-based optimization algorithms to address various problems in software testing. The research in this area has proposed various SBST approaches that achieve different testing goals (e.g., structural, functional, non-functional, and state-based properties) across a range of application domains (e.g., traditional, web, enterprise, mobile applications, and Cyber-physical systems). Fuzz Testing also seeks automation to generate efficient tests that uncover issues in the systems under test (SUT). Fuzz Testing is usually applied at the system level and aims to generate unexpected inputs that would result in crashes of the SUT.The research endeavours in SBST and Fuzz Testing tackle similar testing problems and propose techniques grounded in similar principles (e.g., driving the test generation process by the achieved coverage). The recognition of this similarity has led to a decision to rename the workshop to Search-Based and Fuzz Testing starting in 2023. The primary objective of this workshop is to provide a platform for uniting together researchers and industrial practitioners from SBST, Fuzzing, and the wider Software Engineering community to exchange experiences and explore directions for future research on software testing automation. A second objective is to promote using search and fuzzing techniques to combine testing with other areas of software engineering.","2024","2025-11-25 22:29:48","2025-11-25 22:29:48","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YN5FRYCJ","conferencePaper","2025","Mondesire, Sean; Nsiye, Emmanuel; Soykan, Bulent; Martin, Glenn","Automating HPC Software Compilation, Deployment, and Error Resolution through an LLM-based Multi-Agent System","Practice and Experience in Advanced Research Computing 2025: The Power of Collaboration","979-8-4007-1398-9","","10.1145/3708035.3736023","https://doi.org/10.1145/3708035.3736023","High-performance computing (HPC) systems rely on complex software configurations that are traditionally managed through manual processes, leading to inefficiencies and increased risk of errors. In this paper, we present an LLM-based multi-agent system designed to automate the compilation, deployment, and error resolution of HPC software. Our approach leverages state-of-the-art language models to generate, refine, and iteratively improve build scripts, thereby reducing reliance on static documentation and manual intervention. Extensive experiments on a benchmark cluster demonstrate a 97% success rate in autonomously building over 200 commonly used HPC software packages. Moreover, our agents effectively identify and rectify build and test failures and provide detailed descriptions with actionable recommendations when manual intervention is required. This advancement significantly reduces administrative overhead, enabling HPC administrators to reallocate resources to other critical tasks and paving the way for more resilient and efficient system operations.","2025","2025-11-25 22:29:48","2025-11-25 22:29:48","","","","","","","","","PEARC '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Large Language Models; High-performance Computing; Multi-Agent Systems; Software Automation; Software Building","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FRX42X2R","journalArticle","2025","Coppola, Riccardo; Ardito, Luca; Leotta, Maurizio","Gamify 2024: Gamification in Software Development,Verification, and Validation","SIGSOFT Softw. Eng. Notes","","0163-5948","10.1145/3743095.3743104","https://doi.org/10.1145/3743095.3743104","In this paper we report the outcomes of the 3rd edition of the International Workshop on Gamification in Software Development, Verification, and Validation (Gamify 2024) which was held as part of the ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA), in Vienna, Austria, September 17, 2024.","2025-07","2025-11-25 22:29:48","2025-11-25 22:29:48","","45–47","","3","50","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A735GREC","conferencePaper","2025","Protogeros, Ioannis; Vanbever, Laurent","Continual Benchmarking of LLM-Based Systems on Networking Operations","Proceedings of the ACM SIGCOMM 2025 Posters and Demos","979-8-4007-2026-0","","10.1145/3744969.3748425","https://doi.org/10.1145/3744969.3748425","The inherent complexity of operating modern network infrastructures has led to growing interest in using Large Language Models (LLMs) to support network operators, particularly in the area of Incident Management (IM). Yet, the absence of standardized benchmarks for evaluating such systems poses challenges in tracking progress, comparing approaches, and uncovering their limitations. As LLM-based tools become widespread, there is a clear need for a comprehensive benchmarking suite that reflects the diversity and complexity of operational tasks encountered in real-world networks.This poster outlines our vision for designing such a modular benchmarking suite. We describe an approach for generating operational tasks of varying complexity and discuss how to evaluate LLMs on these tasks and assess system-level performance. As a preliminary evaluation, we benchmark three LLMs — GPT-4.1, Gemini 2.5-Pro, and Claude 3.7 Sonnet — across over 100 test cases and two pipeline variants.","2025","2025-11-25 22:29:48","2025-11-25 22:29:48","","70–72","","","","","","","ACM SIGCOMM Posters and Demos '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Coimbra, Portugal","","","","Large Language Models; Benchmarking; Incident Management; Network Management","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KRME6U89","conferencePaper","2025","Cummins, Chris; Seeker, Volker; Grubisic, Dejan; Roziere, Baptiste; Gehring, Jonas; Synnaeve, Gabriel; Leather, Hugh","LLM Compiler: Foundation Language Models for Compiler Optimization","Proceedings of the 34th ACM SIGPLAN International Conference on Compiler Construction","979-8-4007-1407-8","","10.1145/3708493.3712691","https://doi.org/10.1145/3708493.3712691","Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of software engineering and coding tasks. However, their application in the domain of code and compiler optimization remains underexplored. Training LLMs is resource-intensive, requiring substantial GPU hours and extensive data collection, which can be prohibitive. To address this gap, we introduce LLM&nbsp;Compiler, a suite of robust, openly available, pre-trained models specifically designed for compiler tasks. Built on the foundation of Code&nbsp;Llama, LLM&nbsp;Compiler enhances the understanding of compiler intermediate representations (IRs), assembly language, and optimization techniques. The models have been trained on a vast corpus of 546 billion tokens of LLVM-IR and assembly code and have undergone instruction fine-tuning to interpret compiler behavior. To demonstrate the utility of these research tools, we also present fine-tuned versions of the models with enhanced capabilities in optimizing code size and disassembling from x86_64 and ARM assembly back into LLVM-IR. These achieve 77% of the optimising potential of an autotuning search, and 45% disassembly round trip (14% exact match). LLM&nbsp;Compiler is released under a bespoke commercial license to allow wide reuse and is available in two sizes: 7 billion and 13 billion parameters. Our aim is to provide scalable, cost-effective foundational models for further research and development in compiler optimization by both academic researchers and industry practitioners. Since we released LLM&nbsp;Compiler the community has quantized, repackaged, and downloaded the models over 250k times.","2025","2025-11-25 22:29:48","2025-11-25 22:29:48","","141–153","","","","","","","CC '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Las Vegas, NV, USA","","","","Large Language Models; Code Optimization; Compiler Optimization; LLVM-IR; Pre-trained Models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7RXLJB3G","conferencePaper","2025","Geng, Minghong","Hierarchical Frameworks for Scaling-up Multi-agent Coordination","Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems","979-8-4007-1426-9","","","","Multi-agent reinforcement learning has emerged as a powerful framework for developing collaborative behaviors in autonomous systems. However, existing MARL methods often struggle with scalability in terms of both the number of agents and decision-making horizons. My research focuses on developing hierarchical approaches to scale up MARL systems through two complementary directions: structural scaling by increasing the number of coordinated agents and temporal scaling by extending planning horizons. My initial work introduced HiSOMA, a hierarchical framework integrating self-organizing neural networks with MARL for long-horizon planning, and MOSMAC, a benchmark for evaluating MARL methods on multi-objective MARL scenarios. Building on these foundations, my recent work studies L2M2, a novel framework that leverages large language models for high-level planning in hierarchical multi-agent systems. My ongoing research explores complex bimanual control tasks, specifically investigating multi-agent approaches for coordinated dual-hand manipulation.","2025","2025-11-25 22:29:48","2025-11-25 22:29:48","","2932–2934","","","","","","","AAMAS '25","","","","International Foundation for Autonomous Agents and Multiagent Systems","Richland, SC","","","","","","","","event-place: Detroit, MI, USA","","","","large language model; benchmark; hierarchical multi-agent system; multi-agent reinforcement learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MWEVFEYL","conferencePaper","2025","Birillo, Anastasiia; Keuning, Hieke; Migut, Gosia; Dzialets, Katsiaryna; Golubev, Yaroslav","Creating in-IDE Programming Courses","Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2","979-8-4007-0532-8","","10.1145/3641555.3704762","https://doi.org/10.1145/3641555.3704762","The in-IDE learning format represents a novel way of teaching programming to students entirely within an industry-grade IDE, allowing them to learn both the language and the necessary tooling at the same time. In this tutorial, we will teach the audience everything they need to know to create in-IDE courses and analyze how the students are working in them. In the first part of the tutorial, the audience will get to know the JetBrains Academy plugin that allows creating courses for IntelliJ-based IDEs such as IntelliJ IDEA and PyCharm. The participants will develop their own simple courses with theory, programming tasks, and quizzes, as well as employ some LLM-based features like automatic test generation. In the second part, we will learn how to use another plugin to collect code snapshots and the usage of IDE features of students when they are solving the tasks. Finally, the participants will solve tasks in their own course while using the data gathering plugin, and we will show them how to process and analyze the collected data. As the outcome of the tutorial, the audience will know how to create in-IDE courses, track the students' performance and analyze it, and will already have their own simple course and a dataset that can be expanded or used for further research.","2025","2025-11-25 22:29:48","2025-11-25 22:29:48","","1767","","","","","","","SIGCSETS 2025","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Pittsburgh, PA, USA","","","","generative AI; LLMs; programming education; programming exercises; activity tracking; course creation; In-IDE learning; JetBrains academy; MOOCs","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"87LCFVA8","conferencePaper","2025","Pathak, Aditya; Gandhi, Rachit; Uttam, Vaibhav; Ramamoorthy, Arnav; Ghosh, Pratyush; Jindal, Aaryan Raj; Verma, Shreyash; Mittal, Aditya; Ased, Aashna; Khatri, Chirag; Nakka, Yashwanth; Devansh; Challa, Jagat Sesh; Kumar, Dhruv","Rubric Is All You Need: Improving LLM-Based Code Evaluation With Question-Specific Rubrics","Proceedings of the 2025 ACM Conference on International Computing Education Research V.1","979-8-4007-1340-8","","10.1145/3702652.3744220","https://doi.org/10.1145/3702652.3744220","Since the emergence of Large Language Models (LLMs) popularized by the release of GPT-3 and ChatGPT, LLMs have shown remarkable promise in programming-related tasks. While code generation using LLMs has become a popular field of research, code evaluation using LLMs remains under-explored. In this paper, we focus on LLM-based code evaluation and attempt to fill in the existing gaps. We propose multi-agentic novel approaches using question-specific rubrics tailored to the problem statement, arguing that these perform better for logical assessment than the existing approaches that use question-agnostic rubrics. To address the lack of suitable evaluation datasets, we introduce two datasets: a Data Structures and Algorithms dataset containing 150 student submissions from a popular Data Structures and Algorithms practice website, and an Object Oriented Programming dataset comprising 80 student submissions from undergraduate computer science courses. In addition to using standard metrics (Spearman Correlation, Cohen’s Kappa), we additionally propose a new metric called as Leniency, which quantifies evaluation strictness relative to expert assessment. Our comprehensive analysis demonstrates that question-specific rubrics significantly enhance logical assessment of code in educational settings, providing better feedback aligned with instructional goals beyond mere syntactic correctness.","2025","2025-11-25 22:29:48","2025-11-25 22:29:48","","181–195","","","","","","","ICER '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Large Language Models; Code Assessment and Grading","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TJDL3IV4","journalArticle","2025","Robinson, Diana; Cabrera, Christian; Gordon, Andrew D.; Lawrence, Neil D.; Mennen, Lars","Requirements Are All You Need: The Final Frontier for End-User Software Engineering","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3708524","https://doi.org/10.1145/3708524","What if end-users could own the software development lifecycle from conception to deployment using only requirements expressed in language, images, video or audio? We explore this idea, building on the capabilities that Generative AI brings to software generation and maintenance techniques. How could designing software in this way better serve end-users? What are the implications of this process for the future of end-user software engineering and the software development lifecycle? We discuss the research needed to bridge the gap between where we are today and these imagined systems of the future.","2025-05","2025-11-25 22:29:48","2025-11-25 22:29:48","","","","5","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; End-User Programming; End-User Software Engineering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I5JX4WXD","conferencePaper","2024","Cui, Di; Wang, Qiangqiang; Zhao, Yutong; Wang, Jiaqi; Wei, Minjie; Hu, Jingzhao; Wang, Luqiao; Li, Qingshan","One-to-One or One-to-Many? Suggesting Extract Class Refactoring Opportunities with Intra-class Dependency Hypergraph Neural Network","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680379","https://doi.org/10.1145/3650212.3680379","Excessively large classes that encapsulate multiple responsibilities are challenging to comprehend and maintain. Addressing this issue, several Extract Class refactoring tools have been proposed, employing a two-phase process: identifying suitable fields or methods for extraction, and implementing the mechanics of refactoring. These tools traditionally generate an intra-class dependency graph to analyze the class structure, applying hard-coded rules based on this graph to unearth refactoring opportunities. Yet, the graph-based approach predominantly illuminates direct, “one-to-one” relationship between pairwise entities. Such a perspective is restrictive as it overlooks the complex, “one-to-many” dependencies among multiple entities that are prevalent in real-world classes. This narrow focus can lead to refactoring suggestions that may diverge from developers’ actual needs, given their multifaceted nature. To bridge this gap, our paper leverages the concept of intra-class dependency hypergraph to model one-to-many dependency relationship and proposes a hypergraph learning-based approach to suggest Extract Class refactoring opportunities named HECS. For each target class, we first construct its intra-class dependency hypergraph and assign attributes to nodes with a pre-trained code model. All the attributed hypergraphs are fed into an enhanced hypergraph neural network for training. Utilizing this trained neural network alongside a large language model (LLM), we construct a refactoring suggestion system. We trained HECS on a large-scale dataset and evaluated it on two real-world datasets. The results show that demonstrates an increase of 38.5% in precision, 9.7% in recall, and 44.4% in f1-measure compared to 3 state-of-the-art refactoring tools including JDeodorant, SSECS, and LLMRefactor, which is more useful for 64% of participants. The results also unveil practical suggestions and new insights that benefit existing extract-related refactoring techniques.","2024","2025-11-25 22:29:48","2025-11-25 22:29:48","","1529–1540","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Extract Class Refactoring; Hypergraph Neural Network","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QKCQWQGT","journalArticle","2025","Su, Yanqi; Xing, Zhenchang; Wang, Chong; Chen, Chunyang; Xu, Sherry (Xiwei); Lu, Qinghua; Zhu, Liming","Automated Soap Opera Testing Directed by LLMs and Scenario Knowledge: Feasibility, Challenges, and Road Ahead","Proc. ACM Softw. Eng.","","","10.1145/3715752","https://doi.org/10.1145/3715752","Exploratory testing (ET) harnesses tester's knowledge, creativity, and experience to create varying tests that uncover unexpected bugs from the end-user's perspective. Although ET has proven effective in system-level testing of interactive systems, the need for manual execution has hindered large-scale adoption. In this work, we explore the feasibility, challenges and road ahead of automated scenario-based ET (a.k.a soap opera testing). We conduct a formative study, identifying key insights for effective manual soap opera testing and challenges in automating the process. We then develop a multi-agent system leveraging LLMs and a Scenario Knowledge Graph (SKG) to automate soap opera testing. The system consists of three multi-modal agents, Planner, Player, and Detector that collaborate to execute tests and identify potential bugs. Experimental results demonstrate the potential of automated soap opera testing, but there remains a significant gap compared to manual execution, especially under-explored scenario boundaries and incorrectly identified bugs. Based on the observation, we envision road ahead for the future of automated soap opera testing, focusing on three key aspects: the synergy of neural and symbolic approaches, human-AI co-learning, and the integration of soap opera testing with broader software engineering practices. These insights aim to guide and inspire the future research.","2025-06","2025-11-25 22:29:48","2025-11-25 22:29:48","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; Knowledge Graph; Soap Opera Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F8IX3SRL","conferencePaper","2024","Li, Yifan; Shi, Ensheng; Zheng, Dewu; Duan, Kefeng; Chen, Jiachi; Wang, Yanlin","RepoMinCoder: Improving Repository-Level Code Generation Based on Information Loss Screening","Proceedings of the 15th Asia-Pacific Symposium on Internetware","979-8-4007-0705-6","","10.1145/3671016.3674819","https://doi.org/10.1145/3671016.3674819","Repository-level code generation task involves generating code at a specified location based on unfinished code with repository context. Existing research mainly rely on retrieval-augmented generation methods to complete code. Existing work mainly investigates on improving retrieval results based on the unfinished code, but rarely pays attention to the information loss in the prompt encoding process. In this paper, we propose RepoMinCoder, a novel repository-level code generation framework that adds another round of screening and ranking based on information loss, building upon the canonical retrieval-augmented generation method. Extensive experimental results demonstrate that RepoMinCoder consistently outperforms state-of-the-art methods on public benchmark RepoEval, achieving 3.3% EM and 2.1% ES improvement over previous methods. Moreover, we conduct additional experiments to study the effect of various factors in the existing code generation pipeline, including the number of retrieval candidates, the slicing strategy of the retrieval database, and different prompting strategies.","2024","2025-11-25 22:29:48","2025-11-25 22:29:48","","229–238","","","","","","","Internetware '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Macau, China","","","","Large Language Model; Code Generation; Screening and Ranking","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CG965EB9","conferencePaper","2024","Ishizue, Ryosuke; Sakamoto, Kazunori; Washizaki, Hironori; Fukazawa, Yoshiaki","Improved Program Repair Methods using Refactoring with GPT Models","Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1","979-8-4007-0423-9","","10.1145/3626252.3630875","https://doi.org/10.1145/3626252.3630875","Teachers often utilize automatic program repair methods to provide feedback on submitted student code using model answer code. A state-of-the-art tool is Refactory, which achieves a high repair success rate and small patch size (less code repair) by refactoring code to expand the variety of correct code samples that can be referenced. However, Refactory has two major limitations. First, it cannot fix code with syntax errors. Second, it has difficulty fixing code when there are few correct submissions. Herein we propose a new method that combines Refactory and OpenAI's GPT models to address these issues and conduct a performance measurement experiment. The experiment uses a dataset consisting of 5 programming assignment problems and almost 1,800 real-life incorrect Python program submissions from 361 students for an introductory programming course at a large public university. The proposed method improves the repair success rate by 1-21% when the set of correct code samples is sufficient and the patch size is smaller than Refactory alone in 16-45% of the cases. When there was no set of correct code samples at all (only the model answer code was used as a reference for repair), method improves the repair success rate by 1-43% and the patch size is smaller than Refactory alone in 42-68% of the cases.","2024","2025-11-25 22:29:49","2025-11-25 22:29:49","","569–575","","","","","","","SIGCSE 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Portland, OR, USA","","","","generative ai; program repair; programming assignment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R9H2J57D","conferencePaper","2024","Wu, Ruolan; Yu, Chun; Pan, Xiaole; Liu, Yujia; Zhang, Ningning; Fu, Yue; Wang, Yuhan; Zheng, Zhi; Chen, Li; Jiang, Qiaolei; Xu, Xuhai; Shi, Yuanchun","MindShift: Leveraging Large Language Models for Mental-States-Based Problematic Smartphone Use Intervention","Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems","979-8-4007-0330-0","","10.1145/3613904.3642790","https://doi.org/10.1145/3613904.3642790","Problematic smartphone use negatively affects physical and mental health. Despite the wide range of prior research, existing persuasive techniques are not flexible enough to provide dynamic persuasion content based on users’ physical contexts and mental states. We first conducted a Wizard-of-Oz study (N=12) and an interview study (N=10) to summarize the mental states behind problematic smartphone use: boredom, stress, and inertia. This informs our design of four persuasion strategies: understanding, comforting, evoking, and scaffolding habits. We leveraged large language models (LLMs) to enable the automatic and dynamic generation of effective persuasion content. We developed MindShift, a novel LLM-powered problematic smartphone use intervention technique. MindShift takes users’ in-the-moment app usage behaviors, physical contexts, mental states, goals &amp; habits as input, and generates personalized and dynamic persuasive content with appropriate persuasion strategies. We conducted a 5-week field experiment (N=25) to compare MindShift with its simplified version (remove mental states) and baseline techniques (fixed reminder). The results show that MindShift improves intervention acceptance rates by 4.7-22.5% and reduces smartphone usage duration by 7.4-9.8%. Moreover, users have a significant drop in smartphone addiction scale scores and a rise in self-efficacy scale scores. Our study sheds light on the potential of leveraging LLMs for context-aware persuasion in other behavior change domains.","2024","2025-11-25 22:29:49","2025-11-25 22:29:49","","","","","","","","","CHI '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Honolulu, HI, USA","","","","large language model; mental model; persuasion; Problematic smartphone use","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FE8UHDYE","journalArticle","2025","Huang, Dong; Zhang, Jie M.; Bu, Qingwen; Xie, Xiaofei; Chen, Junjie; Cui, Heming","Bias Testing and Mitigation in LLM-based Code Generation","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3724117","https://doi.org/10.1145/3724117","As the adoption of LLMs becomes more widespread in software coding ecosystems, a pressing issue has emerged: does the generated code contain social bias and unfairness, such as those related to age, gender, and race? This issue concerns the integrity, fairness, and ethical foundation of software applications that depend on the code generated by these models but are underexplored in the literature. This paper presents a novel bias testing framework that is specifically designed for code generation tasks. Based on this framework, we conduct an extensive empirical study on the biases in code generated by five widely studied LLMs (i.e., PALM-2-CodeChat-bison, Claude-instant-1, GPT-3.5-turbo, GPT-4-turbo, and GPT-4). Our findings reveal that biases are prevalent. For example, 13.47% to 49.10% of the codes generated by these LLMs have biased behaviors towards gender. Moreover, we study five bias mitigation prompt strategies that are commonly used in current code generation scenarios, i.e., zero-shot, one-shot, few-shot, and two Chain-of-Thought (CoT) prompts, with and without provided feedback-driven refinement. Our evaluation results illustrate that using direct prompt engineering strategies has limited effectiveness in mitigating bias, but our test execution feedback can help to reduce the ratio of code biases to a large extent (e.g., from 59.88% to 4.79% for GPT-4)1.","2025-03","2025-11-25 22:29:49","2025-11-25 22:29:49","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","code generation; Fairness testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UHTWVKWK","journalArticle","2025","Zhang, Ziyao; Wang, Chong; Wang, Yanlin; Shi, Ensheng; Ma, Yuchi; Zhong, Wanjun; Chen, Jiachi; Mao, Mingzhi; Zheng, Zibin","LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation","Proc. ACM Softw. Eng.","","","10.1145/3728894","https://doi.org/10.1145/3728894","Code generation aims to automatically generate code from input requirements, significantly enhancing development efficiency. Recent large language models (LLMs) based approaches have shown promising results and revolutionized code generation task. Despite the promising performance, LLMs often generate contents with hallucinations, especially for the code generation scenario requiring the handling of complex contextual dependencies in practical development process. Although previous study has analyzed hallucinations in LLM-powered code generation, the study is limited to standalone function generation. In this paper, we conduct an empirical study to study the phenomena, mechanism, and mitigation of LLM hallucinations within more practical and complex development contexts in repository-level generation scenario. First, we manually examine the code generation results from six mainstream LLMs to establish a hallucination taxonomy of LLM-generated code. Next, we elaborate on the phenomenon of hallucinations, analyze their distribution across different models. We then analyze causes of hallucinations and identify four potential factors contributing to hallucinations. Finally, we propose an RAG-based mitigation method, which demonstrates consistent effectiveness in all studied LLMs.","2025-06","2025-11-25 22:29:49","2025-11-25 22:29:49","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; Hallucination; Repository-Level Code Generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6XB5MPVI","journalArticle","2025","Wang, Ruiqi; Guo, Jiyu; Gao, Cuiyun; Fan, Guodong; Chong, Chun Yong; Xia, Xin","Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering","Proc. ACM Softw. Eng.","","","10.1145/3728963","https://doi.org/10.1145/3728963","Recently, large language models (LLMs) have been deployed to tackle various software engineering (SE) tasks like code generation, significantly advancing the automation of SE tasks. However, assessing the quality of these LLM-generated code and text remains challenging. The commonly used Pass@k metric necessitates extensive unit tests and configured environments, demands a high labor cost, and is not suitable for evaluating LLM-generated text. Conventional metrics like BLEU, which measure only lexical rather than semantic similarity, have also come under scrutiny. In response, a new trend has emerged to employ LLMs for automated evaluation, known as LLM-as-a-judge. These LLM-as-a-judge methods are claimed to better mimic human assessment than conventional metrics without relying on high-quality reference answers. Nevertheless, their exact human alignment in SE tasks remains unexplored. In this paper, we empirically explore LLM-as-a-judge methods for evaluating SE tasks, focusing on their alignment with human judgments. We select seven LLM-as-a-judge methods that utilize general-purpose LLMs, alongside two LLMs specifically fine-tuned for evaluation. After generating and manually scoring LLM responses on three recent SE datasets of code translation, code generation, and code summarization, we then prompt these methods to evaluate each response. Finally, we compare the scores generated by these methods with human evaluation. The results indicate that output-based methods reach the highest Pearson correlation of 81.32 and 68.51 with human scores in code translation and generation, achieving near-human evaluation, noticeably outperforming ChrF++, one of the best conventional metrics, at 34.23 and 64.92. Such output-based methods prompt LLMs to output judgments directly, and exhibit more balanced score distributions that resemble human score patterns. Finally, we provide insights and implications, concluding that current state-of-the-art LLM-as-a-judge methods can potentially replace human evaluations in certain SE tasks.","2025-06","2025-11-25 22:29:49","2025-11-25 22:29:49","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","large language models; human preference; model evaluation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T5XFBWNB","conferencePaper","2025","Colavito, Giuseppe","Foundation Models for Automatic Issue Labeling","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings","979-8-3315-3683-1","","10.1109/ICSE-Companion66252.2025.00038","https://doi.org/10.1109/ICSE-Companion66252.2025.00038","Foundation models are transforming software engineering practices through their ability to understand and generate code, process natural language, and automate various development tasks. Despite their potential, effectively applying these models to specialized software engineering tasks remains challenging due to the need for domain-specific understanding and accurate labeling of data. This research project investigates how foundation models can be leveraged to automate labeling tasks in software engineering, with a specific focus on issue classification as a representative case study. Issue tracking systems, while essential for collaborative software development, often suffer from misclassification problems that require significant manual effort to correct. We explore how foundation models can be adapted to automatically label issues accurately, reducing the need for manual intervention while maintaining high-quality classification. The project examines several key aspects: the capabilities of different foundation models in understanding software engineering artifacts, methods for adapting these models to specific labeling tasks through techniques like prompt engineering and few-shot learning, and approaches for integrating automated labeling into real-world scenarios. This research contributes to the broader understanding of how foundation models can be effectively applied to reduce manual labeling efforts across various software engineering contexts, using issue classification as a concrete demonstration of their potential.","2025","2025-11-25 22:29:49","2025-11-25 22:29:49","","127–131","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","generative AI; large language models; fewshot learning; issue tracking; software maintenance and evolution","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NWSSD7I5","conferencePaper","2025","Mao, Ziyu; Wang, Jingyi; Sun, Jun; Qin, Shengchao; Xiong, Jiawen","LLM-Aided Automatic Modeling for Security Protocol Verification","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00197","https://doi.org/10.1109/ICSE55347.2025.00197","Symbolic protocol analysis serves as a pivotal technique for protocol design, security analysis, and the safeguarding of information assets. Several modern tools such as Tamarin and ProVerif have been proven successful in modeling and verifying real-world protocols, including complex protocols like TLS 1.3 and 5G AKA. However, developing formal models for protocol verification is a non-trivial task, which hinders the wide adoption of these powerful tools in practical protocol analysis.In this work, we aim to bridge the gap by developing an automatic method for generating symbolic protocol models using Large Language Models (LLMs) from protocol descriptions in natural language document. Although LLMs are powerful in various code generation tasks, it is shown to be ineffective in generating symbolic models (according to our empirical study). Therefore, rather than applying LLMs naively, we carefully decompose the symbolic protocol modeling task into several stages so that a series of formal models are incrementally developed towards generating the final correct symbolic model. Specifically, we apply LLMs for semantic parsing, enable lightweight manual interaction for disambiguation, and develop algorithms to transform the intermediate models for final symbolic model generation. To ensure the correctness of the generated symbolic model, each stage is designed based on a formal execution model and the model transformations are proven sound. To the best of our knowledge, this is the first work aiming to generate symbolic models for protocol verification from natural language documents. We also introduce a benchmark for symbolic protocol model generation, with 18 real-world security protocol's text description and their corresponding symbolic models. We then demonstrate the potential of our tool, which successfully generated correct models of moderate scale in 10 out of 18 cases. Our tool is released at [1].","2025","2025-11-25 22:29:49","2025-11-25 22:29:49","","642–654","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","LLMs; symbolic analysis; automatic modeling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4BNP8BDS","journalArticle","2025","Joel, Sathvik; Wu, Jie; Fard, Fatemeh","A Survey on LLM-based Code Generation for Low-Resource and Domain-Specific Programming Languages","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3770084","https://doi.org/10.1145/3770084","Large Language Models (LLMs) have shown remarkable capabilities in code generation for popular programming languages. However, their performance in Low-Resource Programming Languages (LRPLs) and Domain-Specific Languages (DSLs) remains a critical challenge. This gap affects millions of developers - with Rust alone having 3.5 million users - who are currently unable to fully leverage LLM capabilities. LRPLs and DSLs face unique challenges, including severe data scarcity and, for DSLs, highly specialized syntax and semantics that are poorly represented in general-purpose datasets. Addressing these challenges is crucial as LRPLs and DSLs significantly enhance development efficiency in specialized domains and applications, including financial and scientific works. While several surveys on LLMs for software engineering and code exist, none comprehensively address the challenges and opportunities specific to LRPLs and DSLs. Our survey fills this gap by providing a systematic review of the current state, methodologies, and challenges in leveraging LLMs for code generation in LRPL and DSL. We filtered 111 papers from over 27,000 published studies from 2020 – 2024 to understand the capabilities and limitations of LLMs in these specialized domains. We also expanded our literature search to include 5 recent papers from 2024 – 2025. We report LLMs used, benchmarks, and metrics to evaluate code generation in LRPLs and DSLs, as well as strategies used to enhance LLM performance, and the collected datasets and curation methods in this context.We identified four main evaluation techniques used in the literature, along with several metrics to assess code generation in LRPL and DSL. We categorized the methods used for LLM improvement into six main groups and summarized the novel methods and architectures proposed by the researchers. We also classified different approaches used for data collection and preparation. While different techniques, metrics, and datasets are used, there is a lack of a standard approach and a benchmark dataset to evaluate code generation in several LRPLs and DSLs. We discuss several distinctions of the studied approaches with the ones used in high-resource programming languages (HRPLs), as well as several challenges unique to these languages, especially DSLs. The challenges stem from the scarcity of data, the unique requirements, and specialized domains, which often need expertise guidelines or domain-specific tools. Accordingly, we provide insights into different research opportunities for the studied aspects. This survey serves as a comprehensive resource for researchers and practitioners working at the intersection of LLMs, software engineering, and specialized programming languages, providing a foundation for future advancements in LRPL and DSL code generation. A GitHub repository was created to organize the papers of this survey at .","2025-10","2025-11-25 22:29:49","2025-11-25 22:29:49","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large language models; code generation; domain-specific languages (DSLs); low-resource programming languages (LRPLs); systematic literature review","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ITL58X3S","conferencePaper","2024","Jiang, Zongze; Wen, Ming; Cao, Jialun; Shi, Xuanhua; Jin, Hai","Towards Understanding the Effectiveness of Large Language Models on Directed Test Input Generation","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695513","https://doi.org/10.1145/3691620.3695513","Automatic testing has garnered significant attention and success over the past few decades. Techniques such as unit testing and coverage-guided fuzzing have revealed numerous critical software bugs and vulnerabilities. However, a long-standing, formidable challenge for existing techniques is how to achieve higher testing coverage. Constraint-based techniques, such as symbolic execution and concolic testing, have been well-explored and integrated into the existing approaches. With the popularity of Large Language Models (LLMs), recent research efforts to design tailored prompts to generate inputs that can reach more uncovered target branches. However, the effectiveness of using LLMs for generating such directed inputs and the comparison with the proven constraint-based solutions has not been systematically explored.To bridge this gap, we conduct the first systematic study on the mainstream LLMs and constraint-based tools for directed input generation with a comparative perspective. We find that LLMs such as ChatGPT are comparable to or even better than the constraint-based tools, succeeding in 43.40%-58.57% samples in our dataset. Meanwhile, there are also limitations for LLMs in specific scenarios such as sequential calculation, where constraint-based tools are in a position of strength. Based on these findings, we propose a simple yet effective method to combine these two types of tools and implement a prototype based on ChatGPT and constraint-based tools. Our evaluation shows that our approach can outperform the baselines by 1.4x to 2.3x relatively. We believe our study can provide novel insights into directed input generation using LLMs, and our findings are essential for future testing research.","2024","2025-11-25 22:29:49","2025-11-25 22:48:08","","1408–1420","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","Large language models; LLM; Software; Software engineering; Chatbots; Codes; Prototypes; symbolic execution; Symbolic Execution; directed input generation; Systematics; Sensitivity; Computer languages; Transforms; Directed Input Generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RYSICM6Q","conferencePaper","2024","Du, Xueying; Liu, Mingwei; Wang, Kaixin; Wang, Hanlin; Liu, Junwei; Chen, Yixuan; Feng, Jiayi; Sha, Chaofeng; Peng, Xin; Lou, Yiling","Evaluating Large Language Models in Class-Level Code Generation","Proceedings of the IEEE/ACM 46th International Conference on Software Engineering","979-8-4007-0217-4","","10.1145/3597503.3639219","https://doi.org/10.1145/3597503.3639219","Recently, many large language models (LLMs) have been proposed, showing advanced proficiency in code generation. Meanwhile, many efforts have been dedicated to evaluating LLMs on code generation benchmarks such as HumanEval. Although being very helpful for comparing different LLMs, existing evaluation focuses on a simple code generation scenario (i.e., function-level or statement-level code generation), which mainly asks LLMs to generate one single code unit (e.g., a function or a statement) for the given natural language description. Such evaluation focuses on generating independent and often small-scale code units, thus leaving it unclear how LLMs perform in real-world software development scenarios.To fill this knowledge gap, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e., class-level code generation. Compared with existing code generation benchmarks, it better reflects real-world software development scenarios due to it comprising broader contextual dependencies and multiple, interdependent units of code. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on the new benchmark ClassEval, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we find that all LLMs perform much worse on class-level code generation compared to the method-level. While GPT models still dominate other LLMs on class-level code generation, the performance rankings of other models on method-level code generation no longer holds for class-level code generation. Besides, most models (except GPT models) perform better when generating the class method by method; and they have the limited ability of generating dependent code. Based on our findings, we call for software engineering (SE) researchers' expertise to build more LLM benchmarks based on practical and complicated software development scenarios.","2024","2025-11-25 22:29:49","2025-11-25 22:29:49","","","","","","","","","ICSE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","large language model; benchmark; class-level code generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AU5DP7SL","conferencePaper","2025","Bhutamapuram, Umamaheswara Sharma; Chonari, Farhan; K Anilkumar, Gokul; Konchada, Sai Kiran","LLMs for Defect Prediction in Evolving Datasets: Emerging Results and Future Directions","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","10.1145/3696630.3728491","https://doi.org/10.1145/3696630.3728491","Software evolves rapidly, making it challenging for defect prediction models to remain effective without frequent retraining. While the Large Language Models (LLMs) have been widely used in SE tasks, their application to evolving datasets for quality assurance, particularly defect prediction, remains under-explored. This study investigates the use of dynamic fine-tuning techniques to enable LLMs to predict defective modules in the face of codebase evolution. We begin our study by curating datasets such as the publicly available QuixBugs and multiple GitHub software projects. We followed a dynamic-fine-tuning approach to adopt the LLMs for the evolving datasets. To mitigate catastrophic forgetting, the LLMs are then evaluated using continual learning strategies such as Elastic Weight Consolidation and memory replay. Preliminary results indicate that LLMs such as LLaMA-LoRA, PolyCoder, and StarCoder when dynamically fine-tuned, achieved comparative performance on medium-sized models such as CodeBERT, GraphCodeBERT, and CodeT5 across evolving datasets. Through this preliminary study, we provide early evidence and reflections on the promise of LLMs for software quality assurance and outline research directions that can shape the future of defect prediction using large-scale models.","2025","2025-11-25 22:29:49","2025-11-25 22:29:49","","520–524","","","","","","","FSE Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","large language models; software defect prediction; software quality assurance","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2K3RRUP3","conferencePaper","2024","Wu, Shengnan; Hu, Yongxiang; Wang, Yingchuan; Gu, Jiazhen; Meng, Jin; Fan, Liujie; Luan, Zhongshi; Wang, Xin; Zhou, Yangfan","Combating Missed Recalls in E-commerce Search: A CoT-Prompting Testing Approach","Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering","979-8-4007-0658-5","","10.1145/3663529.3663842","https://doi.org/10.1145/3663529.3663842","Search components in e-commerce apps, often complex AI-based systems, are prone to bugs that can lead to missed recalls—situations where items that should be listed in search results aren't. This can frustrate shop owners and harm the app's profitability. However, testing for missed recalls is challenging due to difficulties in generating user-aligned test cases and the absence of oracles. In this paper, we introduce mrDetector, the first automatic testing approach specifically for missed recalls. To tackle the test case generation challenge, we use findings from how users construct queries during searching to create a CoT prompt to generate user-aligned queries by LLM. In addition, we learn from users who create multiple queries for one shop and compare search results, and provide a test oracle through a metamorphic relation. Extensive experiments using open access data demonstrate that mrDetector outperforms all baselines with the lowest false positive ratio. Experiments with real industrial data show that mrDetector discovers over one hundred missed recalls with only 17 false positives.","2024","2025-11-25 22:29:49","2025-11-25 22:29:49","","220–231","","","","","","","FSE 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","LLM; Metamorphic Testing; Search Components","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WMVRHAWW","conferencePaper","2025","Wang, Kevin Shukang; Lawrence, Ramon","Quantitative Evaluation of Using Large Language Models and Retrieval-Augmented Generation in Computer Science Education","Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1","979-8-4007-0531-1","","10.1145/3641554.3701917","https://doi.org/10.1145/3641554.3701917","Generative artificial intelligence (GenAI) is transforming Computer Science education, and every instructor is reflecting on how AI will impact their courses. Instructors must determine how students may use AI for course activities and what AI systems they will support and encourage students to use. This task is challenging with the proliferation of large language models (LLMs) and related AI systems. The contribution of this work is an experimental evaluation of the performance of multiple open-source and commercial LLMs utilizing retrieval-augmented generation in answering questions for computer science courses and a cost-benefit analysis for instructors when determining what systems to use. A key factor is the time an instructor has to maintain their supported AI systems and the most effective activities for improving their performance. The paper offers recommendations for deploying, using, and enhancing AI in educational settings.","2025","2025-11-25 22:29:49","2025-11-25 22:29:49","","1183–1189","","","","","","","SIGCSETS 2025","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Pittsburgh, PA, USA","","","","large language model; artificial intelligence; retrieval-augmented generation; human-in-the-loop; question answering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LPMZ5NLJ","bookSection","2025","Zhang, Hang; Shen, Yanxin; Wang, Lun; Shi, Chuanqi; Du, Shaoshuai; Tao, Yiyi; Shen, Yixian","Comparative Analysis of Large Language Models for Context-Aware Code Completion using SAFIM Framework","Proceedings of the 2025 International Conference on Artificial Intelligence and Computational Intelligence","979-8-4007-1363-7","","","https://doi.org/10.1145/3730436.3730529","Large language models (LLMs) have transformed code completion and made it a more intelligent, context-aware tool in contemporary integrated development systems. These developments have greatly improved developers' capacity for error-free, effective code writing. Using the Syntax-Aware Fill-in- the- Middle (SAFIM) dataset, this work assesses different chat-based LLMs, including Gemini 1.5 Flash, Gemini 1.5 Pro, GPT-4o, GPT-4o-mini, and GPT-4 Turbo. This benchmark is intended especially to evaluate models' syntactic sensitivity in code creation. Accuracy and efficiency were assessed using performance benchmarks including cosine similarity with ground-truth complements and latency. The results expose significant variations in the code completion skills of the models, therefore providing insightful analysis of their distinct strengths and shortcomings. This paper offers a baseline for next developments in LLM-based code completion by means of a comparative study stressing the trade-offs between correctness and speed.","2025","2025-11-25 22:29:49","2025-11-25 22:29:49","","572–576","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XCDM7G37","conferencePaper","2024","Nguyen, Sydney; Babe, Hannah McLean; Zi, Yangtian; Guha, Arjun; Anderson, Carolyn Jane; Feldman, Molly Q","How Beginning Programmers and Code LLMs (Mis)read Each Other","Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems","979-8-4007-0330-0","","10.1145/3613904.3642706","https://doi.org/10.1145/3613904.3642706","Generative AI models, specifically large language models (LLMs), have made strides towards the long-standing goal of text-to-code generation. This progress has invited numerous studies of user interaction. However, less is known about the struggles and strategies of non-experts, for whom each step of the text-to-code problem presents challenges: describing their intent in natural language, evaluating the correctness of generated code, and editing prompts when the generated code is incorrect. This paper presents a large-scale controlled study of how 120 beginning coders across three academic institutions approach writing and editing prompts. A novel experimental design allows us to target specific steps in the text-to-code process and reveals that beginners struggle with writing and editing prompts, even for problems at their skill level and when correctness is automatically determined. Our mixed-methods evaluation provides insight into student processes and perceptions with key implications for non-expert Code LLM use within and outside of education.","2024","2025-11-25 22:29:49","2025-11-25 22:29:49","","","","","","","","","CHI '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Honolulu, HI, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QBV6DTIF","journalArticle","2024","Chen, Simin; Li, Zexin; Yang, Wei; Liu, Cong","DeciX: Explain Deep Learning Based Code Generation Applications","Proc. ACM Softw. Eng.","","","10.1145/3660814","https://doi.org/10.1145/3660814","Deep learning-based code generation (DL-CG) applications have shown great potential for assisting developers in programming with human-competitive accuracy. However, lacking transparency in such applications due to the uninterpretable nature of deep learning models makes the automatically generated programs untrustworthy. In this paper, we develop DeciX, a first explanation method dedicated to DL-CG applications. DeciX is motivated by observing two unique properties of DL-CG applications: output-to-output dependencies and irrelevant value and semantic space. These properties violate the fundamental assumptions made in existing explainable DL techniques and thus cause applying existing techniques to DL-CG applications rather pessimistic and even incorrect. DeciX addresses these two limitations by constructing a causal inference dependency graph, containing a novel method leveraging causal inference that can accurately quantify the contribution of each dependency edge in the graph to the end prediction result. Proved by extensive experiments assessing popular, widely-used DL-CG applications and several baseline methods, DeciX is able to achieve significantly better performance compared to state-of-the-art in terms of several critical performance metrics, including correctness, succinctness, stability, and overhead. Furthermore, DeciX can be applied to practical scenarios since it does not require any knowledge of the DL-CG model under explanation. We have also conducted case studies that demonstrate the applicability of DeciX in practice.","2024-07","2025-11-25 22:29:49","2025-11-25 22:29:49","","","","FSE","1","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Model; Program Synthesis; Explainable AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T34STEPZ","conferencePaper","2024","Zhao, Hanning; Silverajan, Bilhanan","Evaluating Cyber Security Dashboards for Smart Cities and Buildings: Enhancing User Modeling with LLMs","Proceedings of the 19th International Conference on Availability, Reliability and Security","979-8-4007-1718-5","","10.1145/3664476.3670943","https://doi.org/10.1145/3664476.3670943","Designing effective cybersecurity visualization has become a crucial component of cyber defense strategies in many domains and industrial environments. Human behaviour, modeling and input are major aspects of designing visualization systems. Yet, the task of evaluating these developed visualization systems is both time-consuming and challenging, and it is often prone to cases where user evaluation is limited owing to a lack of different stakeholders and end users during the design process. Recognizing the potential of advanced Generative Artificial Intelligence and Large Language Models (LLMs), our study aims to explore their capabilities in evaluating web-based security visualization tools and dashboards, particularly in the context of smart cities and buildings. We study and compare the feasibility of using various LLMs available today, for conducting usability testing, serving as an additional resource given the limited availability of human participants. In particular, we focus on three different LLMs: Bing Chat, ChatGPT-4 and ChatGPT-4o. While each had its strengths and drawbacks, our findings revealed that the results obtained had a strong correlation with human test subjects. LLMs can be a valuable aid during evaluation, by offering in-depth insights and evaluations, tailored to the specific requirements of smart buildings, cities and automation cybersecurity. Moreover, our research and findings also reveal that LLMs can similarly be used for the evaluation of a wide range of other visual systems for industrial environments.","2024","2025-11-25 22:29:49","2025-11-25 22:29:49","","","","","","","","","ARES '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","LLM; Security Visualization; Smart City; Usability Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"67QBFEPH","conferencePaper","2024","Xu, Junjielong; Yang, Ruichun; Huo, Yintong; Zhang, Chengyu; He, Pinjia","DivLog: Log Parsing with Prompt Enhanced In-Context Learning","Proceedings of the IEEE/ACM 46th International Conference on Software Engineering","979-8-4007-0217-4","","10.1145/3597503.3639155","https://doi.org/10.1145/3597503.3639155","Log parsing, which involves log template extraction from semi-structured logs to produce structured logs, is the first and the most critical step in automated log analysis. However, current log parsers suffer from limited effectiveness for two reasons. First, traditional data-driven log parsers solely rely on heuristics or handcrafted features designed by domain experts, which may not consistently perform well on logs from diverse systems. Second, existing supervised log parsers require model tuning, which is often limited to fixed training samples and causes sub-optimal performance across the entire log source. To address this limitation, we propose DivLog, an effective log parsing framework based on the in-context learning (ICL) ability of large language models (LLMs). Specifically, before log parsing, DivLog samples a small amount of offline logs as candidates by maximizing their diversity. Then, during log parsing, DivLog selects five appropriate labeled candidates as examples for each target log and constructs them into a prompt. By mining the semantics of examples in the prompt, DivLog generates a target log template in a training-free manner. In addition, we design a straightforward yet effective prompt format to extract the output and enhance the quality of the generated log templates. We conducted experiments on 16 widely-used public datasets. The results show that DivLog achieves (1) 98.1% Parsing Accuracy, (2) 92.1% Precision Template Accuracy, and (3) 92.9% Recall Template Accuracy on average, exhibiting state-of-the-art performance.","2024","2025-11-25 22:29:49","2025-11-25 22:29:49","","","","","","","","","ICSE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","large language model; in-context learning; log parsing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4JTASXMS","conferencePaper","2024","Zhang, Xuchao; Ghosh, Supriyo; Bansal, Chetan; Wang, Rujia; Ma, Minghua; Kang, Yu; Rajmohan, Saravan","Automated Root Causing of Cloud Incidents using In-Context Learning with GPT-4","Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering","979-8-4007-0658-5","","10.1145/3663529.3663846","https://doi.org/10.1145/3663529.3663846","Root Cause Analysis (RCA) plays a pivotal role in the incident diagnosis process for cloud services, requiring on-call engineers to identify the primary issues and implement corrective actions to prevent future recurrences. Improving the incident RCA process is vital for minimizing service downtime, customer impact and manual toil. Recent advances in artificial intelligence have introduced state-of-the-art Large Language Models (LLMs) like GPT-4, which have proven effective in tackling various AIOps problems, ranging from code authoring to incident management. Nonetheless, the GPT-4 model’s immense size presents challenges when trying to fine-tune it on user data because of the significant GPU resource demand and the necessity for continuous model fine-tuning with the emergence of new data. To address the high cost of fine-tuning LLM, we propose an in-context learning approach for automated root causing, which eliminates the need for fine-tuning. We conduct extensive study over 100,000 production incidents from Microsoft, comparing several large language models using multiple metrics. The results reveal that our in-context learning approach outperforms the previous fine-tuned large language models such as GPT-3 by an average of 24.8% across all metrics, with an impressive 49.7% improvement over the zero-shot model. Moreover, human evaluation involving actual incident owners demonstrates its superiority over the fine-tuned model, achieving a 43.5% improvement in correctness and an 8.7% enhancement in readability. The impressive results demonstrate the viability of utilizing a vanilla GPT model for the RCA task, thereby avoiding the high computational and maintenance costs associated with a fine-tuned model.","2024","2025-11-25 22:29:49","2025-11-25 22:29:49","","266–277","","","","","","","FSE 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","Large Language Model; In-context Learning; Incident Diagnosis; Root Cause Analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MUBB8RT3","conferencePaper","2025","Kravchuk-Kirilyuk, Anastasiya; Graciolli, Fernanda; Amin, Nada","The Modular Imperative: Rethinking LLMs for Maintainable Software","Proceedings of the 1st ACM SIGPLAN International Workshop on Language Models and Programming Languages","979-8-4007-2148-9","","10.1145/3759425.3763392","https://doi.org/10.1145/3759425.3763392","Large language models (LLMs) are becoming increasingly integrated into software development, with a majority of developers now adopting AI tools for code generation. Although the current models can often produce syntactically and functionally correct code, they often generate unnecessarily complex solutions, and struggle with large, evolving code bases that have rich internal structure. Most evaluations of LLM-generated code to date have focused primarily on test-based accuracy, unfairly overlooking other essential aspects of software quality. In this paper, we emphasize the importance of modularity — the practice of structuring code into well-defined, reusable components — as a critical lens for improving the maintainability of AI-generated code. We argue that modularity should be a foundational principle in LLM-assisted code generation, empowering models to produce more maintainable, production-ready software.","2025","2025-11-25 22:29:49","2025-11-25 22:29:49","","106–111","","","","","","","LMPL '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Singapore, Singapore","","","","LLM; software; maintainability; modularity","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DFLGD5XK","conferencePaper","2024","Groce, Alex; Dong, Liming; Lu, Qinghua; Zhu, Liming","A Pilot Study in Surveying Data Challenges of Automatic Software Engineering Tasks","Proceedings of the 4th International Workshop on Software Engineering and AI for Data Quality in Cyber-Physical Systems/Internet of Things","979-8-4007-0672-1","","10.1145/3663530.3665020","https://doi.org/10.1145/3663530.3665020","The surge in automatic SE research aims to boost development efficiency and quality while reducing costs. However, challenges such as limited real-world project data and inadequate data conditions constrain the effectiveness of these methods. To systematically understand these challenges, our pilot study reviews prevalent data challenges across various SE tasks. Despite these challenges, thanks to the advances of large language model offers promising performance on SE tasks. Overall, this pilot survey focused on provide a quick retrospective review on SE data challenges and introduce practical LLM solutions from the SE community to mitigate these challenges.","2024","2025-11-25 22:29:49","2025-11-25 22:29:49","","6–11","","","","","","","SEA4DQ 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","LLM; Automatic Software Engineering; Data Challenge; Pilot Survey","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V22P5K7M","conferencePaper","2024","Wang, Tianjia; Ramanujan, Ramaraja; Lu, Yi; Mao, Chenyu; Chen, Yan; Brown, Chris","DevCoach: Supporting Students in Learning the Software Development Life Cycle at Scale with Generative Agents","Proceedings of the Eleventh ACM Conference on Learning @ Scale","979-8-4007-0633-2","","10.1145/3657604.3664663","https://doi.org/10.1145/3657604.3664663","Supporting novice computer science students in learning the software development life cycle (SDLC) at scale is vital for ensuring the quality of future software systems. However, this presents unique challenges, including the need for effective interactive collaboration and access to diverse skill sets of members in the software development team. To address these problems, we present ”DevCoach”, an online system designed to support students learning the SDLC at scale by interacting with generative agents powered by large language models simulating members with different roles in a software development team. Our preliminary user study results reveal that DevCoach improves the experiences and outcomes for students, with regard to learning concepts in SDLC's ”Plan and Design” and ”Develop” phases. We aim to use our findings to enhance DevCoach to support the entire SDLC workflow by incorporating additional simulated roles and enabling students to choose their project topics. Future studies will be conducted in an online Software Engineering class at our institution, aiming to explore and inspire the development of intelligent systems that provide comprehensive SDLC learning experiences to students at scale.","2024","2025-11-25 22:29:50","2025-11-25 22:29:50","","351–355","","","","","","","L@S '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Atlanta, GA, USA","","","","generative ai; software engineering; computer science education; software development life cycle","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KR27U8N3","journalArticle","2024","Lee, Jaeseong; Chen, Simin; Mordahl, Austin; Liu, Cong; Yang, Wei; Wei, Shiyi","Automated Testing Linguistic Capabilities of NLP Models","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3672455","https://doi.org/10.1145/3672455","Natural language processing (NLP) has gained widespread adoption in the development of real-world applications. However, the black-box nature of neural networks in NLP applications poses a challenge when evaluating their performance, let alone ensuring it. Recent research has proposed testing techniques to enhance the trustworthiness of NLP-based applications. However, most existing works use a single, aggregated metric (i.e., accuracy) which is difficult for users to assess NLP model performance on fine-grained aspects, such as LCs. To address this limitation, we present ALiCT, an automated testing technique for validating NLP applications based on their LCs. ALiCT takes user-specified LCs as inputs and produces diverse test suite with test oracles for each of given LC. We evaluate ALiCT on two widely adopted NLP tasks, sentiment analysis and hate speech detection, in terms of diversity, effectiveness, and consistency. Using Self-BLEU and syntactic diversity metrics, our findings reveal that ALiCT generates test cases that are 190% and 2213% more diverse in semantics and syntax, respectively, compared to those produced by state-of-the-art techniques. In addition, ALiCT is capable of producing a larger number of NLP model failures in 22 out of 25 LCs over the two NLP applications.","2024-09","2025-11-25 22:29:50","2025-11-25 22:29:50","","","","7","33","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Software testing; sentiment analysis; hate speech detection; LC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2EP3S6UM","conferencePaper","2024","Hora, Andre","SpotFlow: Tracking Method Calls and States at Runtime","Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings","979-8-4007-0502-1","","10.1145/3639478.3640029","https://doi.org/10.1145/3639478.3640029","Understanding the runtime behavioral aspects of a software system is fundamental for several software engineering tasks, such as testing and code comprehension. For this purpose, typically, one needs to instrument the system and collect data from its execution. Despite the importance of runtime analysis, few tools have been created and made public to support developers extracting information from software execution. In this paper, we propose SpotFlow, a tool to ease the runtime analysis of Python programs. With Spot-Flow, practitioners and researchers can easily extract information about executed methods, run lines, argument values, return values, variable states, and thrown exceptions. Finally, we present tool prototypes built on top of SpotFlow to support software testing and code comprehension and we detail how SpotFlow runtime data can support novel empirical studies and datasets. SpotFlow is publicly available at https://github.com/andrehora/spotflow. Video: https://youtu.be/jhOv3nKz_u4.","2024","2025-11-25 22:29:50","2025-11-25 22:29:50","","35–39","","","","","","","ICSE-Companion '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","software testing; python; debugging; dynamic analysis; code comprehension; runtime monitoring","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NVRWV3ME","conferencePaper","2024","Jin, Kailun; Wang, Chung-Yu; Pham, Hung Viet; Hemmati, Hadi","Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation","Proceedings of the 21st International Conference on Mining Software Repositories","979-8-4007-0587-8","","10.1145/3643991.3645074","https://doi.org/10.1145/3643991.3645074","Large language models (LLMs) have demonstrated notable proficiency in code generation, with numerous prior studies showing their promising capabilities in various development scenarios. However, these studies mainly provide evaluations in research settings, which leaves a significant gap in understanding how effectively LLMs can support developers in real-world. To address this, we conducted an empirical analysis of conversations in DevGPT, a dataset collected from developers' conversations with ChatGPT (captured with the Share Link feature on platforms such as GitHub). Our empirical findings indicate that the current practice of using LLM-generated code is typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready code. These findings indicate that there is much future work needed to improve LLMs in code generation before they can be integral parts of modern software development.","2024","2025-11-25 22:29:50","2025-11-25 22:29:50","","167–171","","","","","","","MSR '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PMKTAD2M","journalArticle","2025","Zhang, Yuntong; Costea, Andreea; Shariffdeen, Ridwan; McCall, Davin; Roychoudhury, Abhik","EffFix: Efficient and Effective Repair of Pointer Manipulating Programs","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3705310","https://doi.org/10.1145/3705310","This work introduces EffFix, a tool that applies a novel static analysis-driven automated program repair (APR) technique for fixing memory errors. APR tools typically rely on a given test-suite to guide the repair process. Apart from the need to provide test oracles, this reliance is also one of the main contributors to the over-fitting problem. Static analysis based APR techniques bypass these issues only to introduce new ones, such as soundness, scalability, and generalizability. This work demonstrates how we can overcome these challenges and achieve sound memory bug repair at scale by leveraging static analysis (specifically incorrectness separation logic (ISL)) to guide repair. This is the first repair approach to use ISL. Our key insight is that the abstract domain used by static analysis to detect the bugs also contains key information to derive correct patches. Our proposed approach learns what a desirable patch is by inspecting how close a patch is to fixing the bug based on the feedback from ISL based static analysis (specifically the Pulse analyzer), and turning this information into a distribution of probabilities over context free grammars. This approach to repair is generic in that its learning strategy allows for finding patches without relying on the commonly used patch templates. Furthermore, to achieve efficient program repair, instead of focusing on heuristics for reducing the search space of patches, we make repair scalable by creating classes of equivalent patches according to the effect they have on the symbolic heap. We then conduct candidate patch validation only once per patch equivalence class. This allows EffFix to efficiently discover quality repairs even in the presence of a large pool of patch candidates. Experimental evaluation of fixing real world memory errors in medium to large scale subjects like OpenSSL, Linux Kernel, swoole, shows the efficiency and effectiveness of EffFix— in terms of automatically producing repairs from large search spaces. In particular, EffFix has a fix ratio of 66% for memory leak bugs and 83% for Null Pointer Dereferences for the considered dataset.","2025-02","2025-11-25 22:29:50","2025-11-25 22:29:50","","","","3","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Automated Program Repair; Incorrectness Separation Logic; Probabilistic Context Free Grammars","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YJ65DL26","journalArticle","2025","He, Junda; Treude, Christoph; Lo, David","LLM-Based Multi-Agent Systems for Software Engineering: Literature Review, Vision, and the Road Ahead","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3712003","https://doi.org/10.1145/3712003","Integrating Large Language Models (LLMs) into autonomous agents marks a significant shift in the research landscape by offering cognitive abilities that are competitive with human planning and reasoning. This article explores the transformative potential of integrating Large Language Models into Multi-Agent (LMA) systems for addressing complex challenges in software engineering (SE). By leveraging the collaborative and specialized abilities of multiple agents, LMA systems enable autonomous problem-solving, improve robustness, and provide scalable solutions for managing the complexity of real-world software projects. In this article, we conduct a systematic review of recent primary studies to map the current landscape of LMA applications across various stages of the software development lifecycle (SDLC). To illustrate current capabilities and limitations, we perform two case studies to demonstrate the effectiveness of state-of-the-art LMA frameworks. Additionally, we identify critical research gaps and propose a comprehensive research agenda focused on enhancing individual agent capabilities and optimizing agent synergy. Our work outlines a forward-looking vision for developing fully autonomous, scalable, and trustworthy LMA systems, laying the foundation for the evolution of Software Engineering 2.0.","2025-05","2025-11-25 22:29:50","2025-11-25 22:29:50","","","","5","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; Software Engineering; Multi-Agent Systems; Autonomous Agents","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GBHRMVYT","journalArticle","2025","Daneshvar, Seyed Shayan; Nong, Yu; Yang, Xu; Wang, Shaowei; Cai, Haipeng","VulScribeR: Exploring RAG-based Vulnerability Augmentation with LLMs","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3760775","https://doi.org/10.1145/3760775","Detecting vulnerabilities is vital for software security, yet deep learning-based vulnerability detectors (DLVD) face a data shortage, which limits their effectiveness. Data augmentation can potentially alleviate the data shortage, but augmenting vulnerable code is challenging and requires a generative solution that maintains vulnerability. Previous works have only focused on generating samples that contain single statements or specific types of vulnerabilities. Recently, large language models (LLMs) have been used to solve various code generation and comprehension tasks with inspiring results, especially when fused with retrieval augmented generation (RAG). Therefore, we propose VulScribeR, a novel LLM-based solution that leverages carefully curated prompt templates to augment vulnerable datasets. More specifically, we explore three strategies to augment both single and multi-statement vulnerabilities, with LLMs, namely Mutation, Injection, and Extension. Our extensive evaluation across four vulnerability datasets and DLVD models, using three LLMs, show that our approach beats two SOTA methods Vulgen and VGX, and Random Oversampling (ROS) by 27.48%, 27.93%, and 15.41% in f1-score with 5K generated vulnerable samples on average, and 53.84%, 54.10%, 69.90%, and 40.93% with 15K generated vulnerable samples. Our approach demonstrates its feasibility for large-scale data augmentation by generating 1K samples at as cheap as US$ 1.88.","2025-08","2025-11-25 22:29:50","2025-11-25 22:29:50","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Deep Learning; Program Generation; Vulnerability Augmentation; Vulnerability Generation; Vulnerability Injection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CY5FG6QM","conferencePaper","2024","Kong, Xiangxing; Li, Yangyang; Fan, Manyi; Shi, Jiayi; Wei, Lingxiang; Qu, Shaojie","Automated Knowledge Mining and Knowledge Graph Reasoning for Aircraft Engine Maintenance","Proceedings of the 2024 6th International Conference on Pattern Recognition and Intelligent Systems","979-8-4007-1825-0","","10.1145/3689218.3689221","https://doi.org/10.1145/3689218.3689221","The maintenance process for aircraft engines is fraught with significant challenges due to their inherent complexity. Large Language Models excel in general Natural Language Processing tasks, yet they lack domain-specific knowledge, thereby compromising their performance in specialized areas. The varied descriptions of engine faults also render traditional text matching algorithms unsuitable for this maintenance domain. In this paper, we construct a knowledge graph integrated with fault diagnosis reasoning ability with knowledge mined from aircraft engine maintenance data. Firstly, we propose the Knowledge Mining and Knowledge Graph Reasoning framework for aircraft engine maintenance data knowledge mining and aircraft engine fault diagnosis. Secondly, we utilize prompt with in-context learning to mitigate the issue of the model lacking expertise in the field of aircraft engine maintenance. Finally, we adopt a sentence similarity calculation method based on BERT, which enables more effective processing of semantic information. We apply our method to Aircraft Engine Fault dataset which is collected from maintenance records of civil aircraft engine since 2007 to 2015, and experimental results demonstrate the effectiveness of our knowledge mining method and aircraft engine fault reasoning algorithm.","2024","2025-11-25 22:29:50","2025-11-25 22:29:50","","35–40","","","","","","","PRIS '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Hong Kong, Hong Kong","","","","large language model; aircraft engine maintenance; knowledge graph reasoning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3S9ZAX8D","conferencePaper","2024","Ren, Xiaoxue; Ye, Xinyuan; Zhao, Dehai; Xing, Zhenchang; Yang, Xiaohu","From Misuse to Mastery: Enhancing Code Generation with Knowledge-Driven AI Chaining","Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering","979-8-3503-2996-4","","10.1109/ASE56229.2023.00143","https://doi.org/10.1109/ASE56229.2023.00143","Large Language Models (LLMs) have shown promising results in automatic code generation by improving coding efficiency to a certain extent. However, generating high-quality and reliable code remains a formidable task because of LLMs' lack of good programming practice, especially in exception handling. In this paper, we first conduct an empirical study and summarize three crucial challenges of LLMs in exception handling, i.e., incomplete exception handling, incorrect exception handling and abuse of try-catch. We then try prompts with different granularities to address such challenges, finding fine-grained knowledge-driven prompts works best. Based on our empirical study, we propose a novel Knowledge-driven Prompt Chaining-based code generation approach, name Kpc, which decomposes code generation into an AI chain with iterative check-rewrite steps and chains fine-grained knowledge-driven prompts to assist LLMs in considering exception-handling specifications. We evaluate our KPC-based approach with 3,079 code generation tasks extracted from the Java official API documentation. Extensive experimental results demonstrate that the KPC-based approach has considerable potential to ameliorate the quality of code generated by LLMs. It achieves this through proficiently managing exceptions and obtaining remarkable enhancements of 109.86% and 578.57% with static evaluation methods, as well as a reduction of 18 runtime bugs in the sampled dataset with dynamic validation.","2024","2025-11-25 22:29:50","2025-11-25 22:47:25","","976–987","","","","","","","ASE '23","","","","IEEE Press","Echternach, Luxembourg","","","","","","","","","","","","large language model; Java; Codes; Large Language Model; code generation; Code Generation; Programming; API misuse; knowledge-driven prompt; Documentation; Encoding; Runtime; Knowledge based systems; API Misuse; Knowledge-driven Prompt","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SYIDMU33","conferencePaper","2024","Chen, Tianyi; Jiang, Yanjie; Fan, Fu; Liu, Bo; Liu, Hui","A Position-Aware Approach to Decomposing God Classes","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3694992","https://doi.org/10.1145/3691620.3694992","God classes are widely recognized as code smells, significantly impairing the maintainability and readability of source code. However, resolving the identified God classes remains a formidable challenge, and we still lack automated and accurate tools to resolve God classes automatically. To this end, in this paper, we propose a novel approach (called ClassSplitter) to decompose God classes. The key observation behind the proposed approach is that software entities (i.e., methods and fields) that are physically adjacent often have strong semantic correlations and thus have a great chance of being classified into the same class during God class deposition. We validate this hypothesis by analyzing 54 God class decomposition refactorings actually conducted in the wild. According to the observation, we measure the similarity between software entities by exploiting not only traditional code metrics but also their relative physical positions. Based on the similarity, we customize a clustering algorithm to classify the methods within a given God class, and each of the resulting clusters is taken as a new class. Finally, ClassSplitter allocates the fields of the God class to the new classes according to the field-access-based coupling between fields and classes. We evaluate ClassSplitter using 133 real-world God classes from open-source applications. Our evaluation results suggest that ClassSplitter could substantially improve the state of the art in God class decomposition, improving the average MoJoFM by 47%. Manual evaluation also confirmed that in most cases (77%) the solutions suggested by ClassSplitter were preferred by developers to alternatives suggested by the state-of-the-art baseline approach.","2024","2025-11-25 22:29:50","2025-11-25 22:29:50","","129–140","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","large language model; code smells; god class; software refactoring","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y2EH6JPJ","journalArticle","2025","Ni, Yunbo; Li, Shaohua","Interleaving Large Language Models for Compiler Testing","Proc. ACM Program. Lang.","","","10.1145/3763079","https://doi.org/10.1145/3763079","Testing compilers with AI models, especially large language models (LLMs), has shown great promise. However, current approaches struggle with two key problems: The generated programs for testing compilers are often too simple, and extensive testing with the LLMs is computationally expensive. In this paper, we propose a novel compiler testing framework that decouples the testing process into two distinct phases: an offline phase and an online phase. In the offline phase, we use LLMs to generate a collection of small but feature-rich code pieces. In the online phase, we reuse these code pieces by strategically combining them to build high-quality and valid test programs, which are then used to test compilers. We implement this idea in a tool, LegoFuzz, for testing C compilers. The results are striking: we found 66 bugs in GCC and LLVM, the most widely used C compilers. Almost half of the bugs are miscompilation bugs, which are serious and hard-to-find bugs that none of the existing LLM-based tools could find. We believe this efficient design opens up new possibilities for using AI models in software testing beyond just C compilers.","2025-10","2025-11-25 22:29:50","2025-11-25 22:29:50","","","","OOPSLA2","9","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Testing; Reliability; Compilers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AF6ST5E8","conferencePaper","2025","Guo, Yongjian; Ma, Wanlun; Xiao, Xi; Wen, Sheng; Di, Peng; Zhu, Xiaogang","Patch the Leak: Strengthening CodeLLMs Against Privacy Extraction Threats","Proceedings of the 34th ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-1474-0","","10.1145/3713081.3732931","https://doi.org/10.1145/3713081.3732931","CodeLLMs tend to memorize their training data and can reconstruct personal information (PI) when given specific prompts. Despite the application of privacy anonymization methods to remove PI in foundational LLMs, the previous experiments using state-of-the-art PI extraction attacks like CODEBREAKER and CodexLeaks on multiple open-source and commercial CodeLLMs demonstrate that such information cannot be fully eliminated. Furthermore, we found that commercial models exhibit significantly lower leakage rates (approximately 20% lower) compared to open-source models, and we hypothesize this is related to the stronger model alignment. Addressing the lack of effective defenses against PI extraction, we treat PI leakage as a form of misalignment and propose PI-ALIGN, a novel framework inspired by adversarial learning. PI-ALIGN pairs CodeLLMs with the CODEBREAKER attack framework as an adversarial dual model and leverages the optimized GRPO (Group Relative Policy Optimization) process to realign the model during fine-tuning. This approach is expected to enhance the model's robustness against PI extraction attacks by adversarially training it against CODEBREAKER. We also outline our experimental evaluation framework to systematically validate PI-ALIGN's effectiveness, aiming to provide insights into countering PI extraction attacks on CodeLLMs.","2025","2025-11-25 22:29:50","2025-11-25 22:29:50","","195–199","","","","","","","ISSTA Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","CodeLLMs; extraction attacks; personal information; privacy protection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VKHJ3BWZ","journalArticle","2024","Qiang, Zhangcheng; Wang, Weiqing; Taylor, Kerry","Agent-OM: Leveraging LLM Agents for Ontology Matching","Proc. VLDB Endow.","","2150-8097","10.14778/3712221.3712222","https://doi.org/10.14778/3712221.3712222","Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks.","2024-11","2025-11-25 22:29:50","2025-11-25 22:29:50","","516–529","","3","18","","","","","","","","","","","","","","","","","Publisher: VLDB Endowment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"65TP9PJW","conferencePaper","2024","Rubio-Medrano, Carlos E.; Kotak, Akash; Wang, Wenlu; Sohr, Karsten","Pairing Human and Artificial Intelligence: Enforcing Access Control Policies with LLMs and Formal Specifications","Proceedings of the 29th ACM Symposium on Access Control Models and Technologies","979-8-4007-0491-8","","10.1145/3649158.3657032","https://doi.org/10.1145/3649158.3657032","Large Language Models (LLMs), such as ChatGPT and Google Bard, have performed interestingly well when assisting developers on computer programming tasks, a.k.a., coding, thus potentially resulting in convenient and faster software constructions. This new approach significantly enhances efficiency but also presents challenges in unsupervised code construction with limited security guarantees. LLMs excel in producing code with accurate grammar, yet they are not specifically trained to guarantee the security of the code. In this paper, we provide an initial exploration into using formal software specifications as a starting point for software construction, allowing developers to translate descriptions of security-related behavior into natural language instructions for LLMs, a.k.a., prompts. In addition, we leveraged automated verification tools to evaluate the code produced against the aforementioned specifications , following a modular, step-by-step software construction process. For our study, we leveraged Role-based Access Control (RBAC), a mature security model, and the Java Modeling Language (JML), a behavioral specification language for Java. We test our approach on different publicly-available LLMs, namely, OpenAI ChatGPT 4.0, Google Bard, and Microsoft CoPilot. We provide a description of two applications-a security-sensitive Banking application employing RBAC and an RBAC API module itself-, the corresponding JML specifications, as well as a description of the prompts, the generated code, the verification results, as well as a series of interesting insights for practitioners interested in further exploring the use of LLMs for securely constructing applications.","2024","2025-11-25 22:29:50","2025-11-25 22:29:50","","105–116","","","","","","","SACMAT 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: San Antonio, TX, USA","","","","prompt engineering; large language models; chatgpt; formal specifications; software construction. java modeling language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2HZ8TGPW","conferencePaper","2025","Gao, Hao; Wang, Jingyue; Fang, Wenyang; Xu, Jingwei; Huang, Yunpeng; Chen, Taolue; Ma, Xiaoxing","LASER: Script Execution by Autonomous Agents for On-demand Traffic Simulation","Proceedings of the 16th International Conference on Internetware","979-8-4007-1926-4","","10.1145/3755881.3755905","https://doi.org/10.1145/3755881.3755905","Autonomous Driving Systems (ADS) are advancing rapidly due to progress in deep learning, yet critical challenges remain, particularly in the realm of safety verification. As safety-critical systems, ADS must undergo rigorous testing across diverse scenarios. Real-world data, while valuable, are inherently inflexible for interaction and scenario customization. In contrast, simulator-generated synthetic scenarios provide a platform that enables interaction, control, editability, and adaptability to specific needs. However, current simulation approaches are limited—either relying on costly, manually crafted, overly templated scenarios or generating unconditioned trivial behaviors based on learned distributions. In this work, we introduce LASER, an innovative framework that leverages large language models (LLMs) to conduct traffic simulations based on natural language inputs. The framework operates in two phases. First, it generates scripts from user-provided descriptions. Second, it executes these scripts by guiding autonomous agents within the CARLA simulator to perform tasks in real-time. This method effectively decomposes tasks, allocates controls, and integrates interactive elements to create dynamic and scalable simulations that align with user requirements. By using LASER, we overcome the rigid constraints of traditional simulation methods, enabling the creation of complex, diverse, flexible and on-demand driving scenarios. The approach significantly enhances the process of generating ADS training and testing data, addressing the scalability and diversity issues associated with previous simulation models. The code and all demos are available anonymously at https://njudeepengine.github.io/LASER/.","2025","2025-11-25 22:29:50","2025-11-25 22:29:50","","84–95","","","","","","","Internetware '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Test Generation; Autonomous Agents; Autonomous Driving Systems; Traffic Simulation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SVNKEMAW","journalArticle","2023","Tran, Nghia D.; May, James J.; Ho, Nguyen; Ngo, Linh B.","Exploring ChatGPT's Ability to Solve Programming Problems with Complex Context","J. Comput. Sci. Coll.","","1937-4771","","","This paper presents a preliminary study on ChatGPT's ability to generate a working solution from a complex programming problem's textual description. Utilizing an online competitive programming platform's problem statements and its respective difficulty measures, we were able to examine ChatGPT's capabilities using the platform's solution status as a performance indicator. The experimental results show a strong relationship between the problem's perceived difficulty level, as provided by the platform, and the final solution status. Various techniques were used to measure the readability level of the problems' text, and we also found statistical relationship among several of them regarding the final status. The results also hint at a potential limitation of ChatGPT to understand complex programming problem context.","2023-10","2025-11-25 22:29:50","2025-11-25 22:29:50","","195–209","","3","39","","","","","","","","","","","","","","","","","Place: Evansville, IN, USA Publisher: Consortium for Computing Sciences in Colleges","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8M4UV62H","conferencePaper","2025","Tamascelli, Michael; Bunch, Olivia; Fowler, Blake; Taeb, Maryam; Cohen, Achraf","Academic Advising Chatbot Powered with AI Agent","Proceedings of the 2025 ACM Southeast Conference","979-8-4007-1277-7","","10.1145/3696673.3723065","https://doi.org/10.1145/3696673.3723065","Academic advising plays a crucial role in fostering student success. However, challenges such as limited advisor availability can hinder effective support. Generative AI, particularly AI-powered chatbots, offers the potential to enhance student advising in higher education by providing personalized guidance. These technologies help college students find the information and resources needed to create degree plans aligned with their academic goals. This research introduces ARGObot, an intelligent advising system that facilitates student navigation of university policies through automated interpretation of the student handbook as its primary knowledge base. ARGObot enhances accessibility to critical academic policies and procedures, supporting incoming students' success through personalized guidance. Our system integrates a multifunctional agent enhanced by a Large Language Model (LLM). The architecture employs multiple external tools to enhance its capabilities: a Retrieval-Augmented Generation (RAG) system accesses verified university sources; email integration facilitates Human-in-the-Loop (HITL) interaction; and a web search function expands the system's knowledge base beyond predefined constraints. This approach enables the system to provide contextually relevant and verified responses to various student queries. This architecture evolved from our initial implementation based on Gemini 1 Pro, which revealed significant limitations due to its lack of agent-based functionality, resulting in hallucination issues and irrelevant responses. Subsequent evaluation demonstrated that our enhanced version, integrating GPT-4 with the text-embedding-ada-002 model, achieved superior performance across all metrics. This paper also presents a comparative analysis of both implementations, highlighting the architectural improvements and their impact on system performance.","2025","2025-11-25 22:29:50","2025-11-25 22:29:50","","195–202","","","","","","","ACMSE 2025","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Southeast Missouri State University, Cape Girardeau, MO, USA","","","","large language models; higher education; retrieval-augmented generation; chatbot; academic advising; AI agent","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5SCYNW2U","conferencePaper","2025","Zhou, Zhongyi; Jin, Jing; Phadnis, Vrushank; Yuan, Xiuxiu; Jiang, Jun; Qian, Xun; Wright, Kristen; Sherwood, Mark; Mayes, Jason; Zhou, Jingtao; Huang, Yiyi; Xu, Zheng; Zhang, Yinda; Lee, Johnny; Olwal, Alex; Kim, David; Iyengar, Ram; Li, Na; Du, Ruofei","InstructPipe: Generating Visual Blocks Pipelines with Human Instructions and LLMs","Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems","979-8-4007-1394-1","","10.1145/3706598.3713905","https://doi.org/10.1145/3706598.3713905","Visual programming has the potential of providing novice programmers with a low-code experience to build customized processing pipelines. Existing systems typically require users to build pipelines from scratch, implying that novice users are expected to set up and link appropriate nodes from a blank workspace. In this paper, we introduce InstructPipe, an AI assistant for prototyping machine learning (ML) pipelines with text instructions. We contribute two large language model (LLM) modules and a code interpreter as part of our framework. The LLM modules generate pseudocode for a target pipeline, and the interpreter renders the pipeline in the node-graph editor for further human-AI collaboration. Both technical and user evaluation (N=16) shows that InstructPipe empowers users to streamline their ML pipeline workflow, reduce their learning curve, and leverage open-ended commands to spark innovative ideas.","2025","2025-11-25 22:29:50","2025-11-25 22:29:50","","","","","","","","","CHI '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Large Language Models; Deep Learning; Deep Neural Networks; Graph Compiler; Low-code Development; Node-graph Editor; Visual Analytics; Visual Programming; Visual Prototyping","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"39U9BCNP","conferencePaper","2024","Chen, Jiachi; Chen, Chong; Hu, Jiang; Grundy, John; Wang, Yanlin; Chen, Ting; Zheng, Zibin","Identifying Smart Contract Security Issues in Code Snippets from Stack Overflow","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680353","https://doi.org/10.1145/3650212.3680353","Smart contract developers frequently seek solutions to developmental challenges on Q&amp;A platforms such as Stack Overflow (SO). Although community responses often provide viable solutions, the embedded code snippets can also contain hidden vulnerabilities. Integrating such code directly into smart contracts may make them susceptible to malicious attacks. We conducted an online survey and received 74 responses from smart contract developers. The results of this survey indicate that the majority (86.4%) of participants do not sufficiently consider security when reusing SO code snippets. Despite the existence of various tools designed to detect vulnerabilities in smart contracts, these tools are typically developed for analyzing fully-completed smart contracts and thus are ineffective for analyzing typical code snippets as found on SO. We introduce SOChecker, the first tool designed to identify potential vulnerabilities in incomplete SO smart contract code snippets. SOChecker first leverages a fine-tuned Llama2 model for code completion, followed by the application of symbolic execution methods for vulnerability detection. Our experimental results, derived from a dataset comprising 897 code snippets collected from smart contract-related SO posts, demonstrate that SOChecker achieves an F1 score of 68.2%, greatly surpassing GPT-3.5 and GPT-4 (20.9% and 33.2% F1 Scores respectively). Our findings underscore the need to improve the security of code snippets from Q&amp;A websites.","2024","2025-11-25 22:29:50","2025-11-25 22:29:50","","1198–1210","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","large language models; smart contracts; program analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5MVW8DJF","conferencePaper","2025","Zhao, Yicong; Chen, Shisong; Zhang, Jiacheng; Li, Zhixu","ReCode: Improving LLM-based Code Repair with Fine-Grained Retrieval-Augmented Generation","Proceedings of the 34th ACM International Conference on Information and Knowledge Management","979-8-4007-2040-6","","10.1145/3746252.3761035","https://doi.org/10.1145/3746252.3761035","Recent advances in large language models (LLMs) have demonstrated impressive capabilities in code-related tasks such as code generation and automated program repair. Despite their promising performance, most existing approaches for code repair suffer from high training costs or computationally expensive inference. Retrieval-augmented generation (RAG), with its efficient in-context learning paradigm, offers a more scalable alternative. However, conventional retrieval strategies, which are often based on holistic code-text embeddings, fail to capture the structural intricacies of code, resulting in suboptimal retrieval quality. To address the above limitations, we propose ReCode, a fine-grained retrieval-augmented in-context learning framework designed for accurate and efficient code repair. Specifically, ReCode introduces two key innovations: (1) an algorithm-aware retrieval strategy that narrows the search space using preliminary algorithm type predictions; and (2) a modular dual-encoder architecture that separately processes code and textual inputs, enabling fine-grained semantic matching between input and retrieved contexts. Furthermore, we propose RACodeBench, a new benchmark constructed from real-world user-submitted buggy code, which addresses the limitations of synthetic benchmarks and supports realistic evaluation. Experimental results on RACodeBench and competitive programming datasets demonstrate that ReCode achieves higher repair accuracy with significantly reduced inference cost, highlighting its practical value for real-world code repair scenarios.","2025","2025-11-25 22:29:50","2025-11-25 22:29:50","","4368–4378","","","","","","","CIKM '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Seoul, Republic of Korea","","","","in-context learning; benchmark; code repair; retrieval augmented","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XG8HNHCG","conferencePaper","2025","Ruan, Haifeng; Zhang, Yuntong; Roychoudhury, Abhik","SpecRover: Code Intent Extraction via LLMs","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00080","https://doi.org/10.1109/ICSE55347.2025.00080","Autonomous program improvement typically involves automatically producing bug fixes and feature additions. Such program improvement can be accomplished by a combination of large language model (LLM) and program analysis capabilities, in the form of an LLM agent. Since program repair or program improvement typically requires a specification of intended behavior - specification inference can be useful for producing high quality program patches. In this work, we examine efficient and low-cost workflows for iterative specification inference within an LLM agent. Given a GitHub issue to be resolved in a software project, our goal is to conduct iterative code search accompanied by specification inference - thereby inferring intent from both the project structure and behavior. The intent thus captured is examined by a reviewer agent with the goal of vetting the patches as well as providing a measure of confidence in the vetted patches. Our approach SpecRover is built on the open-source LLM agent AutoCodeRover. In an evaluation on the full SWE-Bench consisting of 2294 GitHub issues, it shows more than 50% improvement in efficacy over AutoCodeRover. Compared to the open-source agents available, our work shows modest cost ($0.65 per issue) in resolving an average GitHub issue in SWE-Bench lite. The production of explanation by SpecRover allows for a better ""signal"" to be given to the developer, on when the suggested patches can be accepted with confidence. SpecRover also seeks to demonstrate the continued importance of specification inference in automated program repair, even as program repair technologies enter the LLM era.","2025","2025-11-25 22:29:50","2025-11-25 22:29:50","","963–974","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"63LWBXQ8","journalArticle","2024","Guglielmi, Emanuela; Rosa, Giovanni; Scalabrino, Simone; Bavota, Gabriele; Oliveto, Rocco","Help Them Understand: Testing and Improving Voice User Interfaces","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3654438","https://doi.org/10.1145/3654438","Voice-based virtual assistants are becoming increasingly popular. Such systems provide frameworks to developers for building custom apps. End-users can interact with such apps through a Voice User Interface (VUI), which allows the user to use natural language commands to perform actions. Testing such apps is not trivial: The same command can be expressed in different semantically equivalent ways. In this article, we introduce VUI-UPSET, an approach that adapts chatbot-testing approaches to VUI-testing. We conducted an empirical study to understand how VUI-UPSET compares to two state-of-the-art approaches (i.e., a chatbot testing technique and ChatGPT) in terms of (i) correctness of the generated paraphrases, and (ii) capability of revealing bugs. To this aim, we analyzed 14,898 generated paraphrases for 40 Alexa Skills. Our results show that VUI-UPSET generates more bug-revealing paraphrases than the two baselines with, however, ChatGPT being the approach generating the highest percentage of correct paraphrases. We also tried to use the generated paraphrases to improve the skills. We tried to include in the voice interaction models of the skills (i) only the bug-revealing paraphrases, (ii) all the valid paraphrases. We observed that including only bug-revealing paraphrases is sometimes not sufficient to make all the tests pass.","2024-06","2025-11-25 22:29:50","2025-11-25 22:29:50","","","","6","33","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","software testing; NLP; Voice user interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GL279FCP","conferencePaper","2024","Wang, Guoqing; Sun, Zeyu; Chen, Yizhou; Zhao, Yifan; Liang, Qingyuan; Hao, Dan","Commit Artifact Preserving Build Prediction","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680356","https://doi.org/10.1145/3650212.3680356","In Continuous Integration (CI), accurate build prediction is crucial for minimizing development costs and enhancing efficiency. However, existing build prediction methods, typically based on predefined rules or machine learning classifiers employing feature engineering, have been constrained by their limited ability to fully capture the intricate details of commit artifacts, such as code change and commit messages. These artifacts are critical for understanding the commit under a build but have been inadequately utilized in existing approaches. To address this problem, we propose GitSense, a Transformer-based model specifically designed to incorporate the rich and complex information contained within commit artifacts for the first. GitSense employs an advanced textual encoder with built-in sliding window text samplers for textual features and a statistical feature encoder for extracted statistical features. This innovative approach allows for a comprehensive analysis of lengthy and intricate commit artifacts, surpassing the capabilities of traditional methods. We conduct comprehensive experiments to compare GitSense with five state-of-the-art build prediction models, Longformer, and ChatGPT. The experimental results show that GitSense outperforms these models in predicting failed builds, evidenced by 32.7%-872.1.0% better on F1-score, 23.9%-437.5% better on Precision, and 40.2%-1396.0% better on Recall.","2024","2025-11-25 22:29:51","2025-11-25 22:29:51","","1236–1248","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Deep Learning; Build Prediction; Continuous Integration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PH7HTVRU","conferencePaper","2025","Ugarte, Miriam; Valle, Pablo; Parejo, José Antonio; Segura, Sergio; Arrieta, Aitor","ASTRAL: A Tool for the Automated Safety Testing of Large Language Models","Proceedings of the 34th ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-1474-0","","10.1145/3713081.3731733","https://doi.org/10.1145/3713081.3731733","In this paper, we present ASTRAL, a tool that automates the generation and execution of test inputs (i.e., prompts) to evaluate the safety of Large Language Models (LLMs). ASTRAL consists of three microservice modules. The first is a test generator, which employs a novel black-box coverage criterion to create balanced and diverse unsafe test inputs across a wide range of safety categories and linguistic characteristics (e.g., different writing styles and persuasion techniques). Additionally, the test generator incorporates an LLM-based approach that leverages Retrieval-Augmented Generation (RAG), few-shot prompting strategies, and web browsing to produce up-to-date test inputs. The second module is the test executor, which runs the generated test inputs on the LLM under test. Finally, the test evaluator acts an oracle to assess the execution outputs to identify unsafe responses, enabling a fully automated LLM testing process.","2025","2025-11-25 22:29:51","2025-11-25 22:29:51","","31–35","","","","","","","ISSTA Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","LLMs; safety; automated testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PZDW43FW","conferencePaper","2025","Böttcher, Axel; Thurner, Veronika","Identifying Competence Gaps and Student Struggle by Monitoring Testing Performance","Proceedings of the 6th European Conference on Software Engineering Education","979-8-4007-1282-1","","10.1145/3723010.3723033","https://doi.org/10.1145/3723010.3723033","Creating sufficient motivation for unit testing is often a challenge in software engineering education. As well, it is often difficult for students to decide whether they have written sufficient tests. Classical coverage measures require thorough explanation and thus introduce great complexity, which risks to overwhelm programming novices. Instead, we started using mutation testing early on in programming courses, as mutations are an easy concept to understand. In addition, integrating mutation testing into projects is nowadays straightforward. Furthermore, hunting mutations can create a sense of gaming challenge, and thus, fun for novice programmers.In this paper, we describe experiences with mutation testing in the first and second semester of an introductory course on software development in a Bachelor’s degree program on Computer Science. In order to critically evaluate our approach, we analyze in detail the meta data on testing activities generated in the students’ repositories. Specifically, we analyze their work performance and identify typical performance patterns in terms of programming activities over time, as well as in terms of improvement of code quality during this process. Furthermore, we correlate these performance data to the exam performance.Finally, we classify leftover mutants in terms of skill levels according to Bloom’s revised taxonomy of learning objectives. This provides an insight into students’ abilities, and identifies where we need to adapt our teaching and exercises in the next iteration of teaching these courses, to better support our students along their learning path.","2025","2025-11-25 22:29:51","2025-11-25 22:29:51","","221–230","","","","","","","ECSEE '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","software testing; mutation testing; CS education; programming education; struggle detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P8C4PEWQ","journalArticle","2024","Chen, Simin; Feng, XiaoNing; Han, Xiaohong; Liu, Cong; Yang, Wei","PPM: Automated Generation of Diverse Programming Problems for Benchmarking Code Generation Models","Proc. ACM Softw. Eng.","","","10.1145/3643780","https://doi.org/10.1145/3643780","In recent times, a plethora of Large Code Generation Models (LCGMs) have been proposed, showcasing significant potential in assisting developers with complex programming tasks. Within the surge of LCGM proposals, a critical aspect of code generation research involves effectively benchmarking the programming capabilities of models. Benchmarking LCGMs necessitates the creation of a set of diverse programming problems, and each problem comprises the prompt (including the task description), canonical solution, and test inputs. The existing methods for constructing such a problem set can be categorized into two main types: manual methods and perturbation-based methods. However, %both these methods exhibit major limitations. %Firstly, manually-based methods require substantial human effort and are not easily scalable. Moreover, programming problem sets created manually struggle to maintain long-term data integrity due to the greedy training data collection mechanism in LCGMs. On the other hand, perturbation-based approaches primarily produce semantically homogeneous problems, resulting in generated programming problems with identical Canonical Solutions to the seed problem. These methods also tend to introduce typos to the prompt, easily detectable by IDEs, rendering them unrealistic. manual methods demand high effort and lack scalability, while also risking data integrity due to LCGMs' potentially contaminated data collection, and perturbation-based approaches mainly generate semantically homogeneous problems with the same canonical solutions and introduce typos that can be easily auto-corrected by IDE, making them ineffective and unrealistic. Addressing the aforementioned limitations presents several challenges: (1) How to automatically generate semantically diverse Canonical Solutions to enable comprehensive benchmarking on the models, (2) how to ensure long-term data integrity to prevent data contamination, and (3) how to generate natural and realistic programming problems. To tackle the first challenge, we draw key insights from viewing a program as a series of mappings from the input to the output domain. These mappings can be transformed, split, reordered, or merged to construct new programs. Based on this insight, we propose programming problem merging, where two existing programming problems are combined to create new ones. In addressing the second challenge, we incorporate randomness to our programming problem-generation process. Our tool can probabilistically guarantee no data repetition across two random trials. To tackle the third challenge, we propose the concept of a Lambda Programming Problem, comprising a concise one-sentence task description in natural language accompanied by a corresponding program implementation. Our tool ensures the program prompt is grammatically correct. Additionally, the tool leverages return value type analysis to verify the correctness of newly created Canonical Solutions. In our empirical evaluation, we utilize our tool on two widely-used datasets and compare it against nine baseline methods using eight code generation models. The results demonstrate the effectiveness of our tool in generating more challenging, diverse, and natural programming problems, comparing to the baselines.","2024-07","2025-11-25 22:29:51","2025-11-25 22:29:51","","","","FSE","1","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","large language model; datasets; program synthesis; neural networks","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KZ64CX76","conferencePaper","2024","Okuda, Katsumi; Amarasinghe, Saman","AskIt: Unified Programming Interface for Programming with Large Language Models","Proceedings of the 2024 IEEE/ACM International Symposium on Code Generation and Optimization","979-8-3503-9509-9","","10.1109/CGO57630.2024.10444830","https://doi.org/10.1109/CGO57630.2024.10444830","Large Language Models (LLMs) exhibit a unique phenomenon known as emergent abilities, demonstrating adeptness across numerous tasks, from text summarization to code generation. While these abilities open up novel avenues in software design and crafting, their incorporation presents substantial challenges. Developers face decisions regarding the use of LLMs for directly performing tasks within applications as well as for generating and executing code to accomplish these tasks. Moreover, effective prompt design becomes a critical concern, given the necessity of extracting data from natural language outputs. To address these complexities, this paper introduces AskIt, a domain-specific language (DSL) specifically designed for LLMs. AskIt simplifies LLM integration by providing a unified interface that not only allows for direct task execution using LLMs but also supports the entire cycle of code generation and execution. This dual capability is achieved through (1) type-guided output control, (2) template-based function definitions, and (3) prompt generation for both usage modes. Our evaluations underscore AskIt's effectiveness. Across 50 tasks, AskIt generated concise prompts, achieving a 16.14 % reduction in prompt length compared to benchmarks. Additionally, by enabling a seamless transition between using LLMs directly in applications and for generating code, AskIt achieved significant efficiency improvements, as observed in our GSM8K benchmark experiments. The implementations of AskIt in TypeScript and Python are available at https://github.com/katsumiok/ts-askit and https://github.com/katsumiok/pyaskit, respectively.","2024","2025-11-25 22:29:51","2025-11-25 22:29:51","","41–54","","","","","","","CGO '24","","","","IEEE Press","Edinburgh, United Kingdom","","","","","","","","","","","","large language model; artificial intelligence; software engineering; code generation; domain specific language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PC3D6ZRM","journalArticle","2025","Khati, Dipin; Liu, Yijin; Palacio, David N.; Zhang, Yixuan; Poshyvanyk, Denys","Mapping the Trust Terrain: LLMs in Software Engineering - Insights and Perspectives","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3771282","https://doi.org/10.1145/3771282","The application of Large Language Models (LLMs) in Software Engineering (SE) continues to grow rapidly across both industry and academia. As these models become integral to critical SE processes, ensuring their reliability and trustworthiness becomes essential. Achieving this requires a balanced approach to trust: excessive trust can introduce security vulnerabilities, while insufficient trust may hinder innovation. However, the conceptual landscape of trust in LLMs for SE(LLM4SE) remains unclear. Key concepts such as trust, distrust, and trustworthiness lack precise definitions, factors that shape trust formation remain underexplored, and metrics for trust in LLMs remain undeveloped. To clarify the current research landscape and identify future directions, we conducted a comprehensive review of (88) articles: a systematic review of (18) studies on LLMs in SE, supplemented by an analysis of (70) articles from the broader trust literature. Furthermore, we surveyed (25) domain experts to gather practitioners’ perspectives on trust and identify gaps between their experiences and the existing literature. Our findings provide a structured overview of trust-related concepts in LLM4SE, outlining key areas for future research. This study contributes to building more trustworthy LLM-assisted software engineering processes, ultimately supporting safer and more effective adoption of LLMs in SE.","2025-10","2025-11-25 22:29:51","2025-11-25 22:29:51","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","LLMs; Trust; Trustworthiness; Distrust","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q84WMXGV","conferencePaper","2025","Ni, Yunbo","LegoFuzz: Interleaving Large Language Models for Compiler Testing","Companion Proceedings of the 2025 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for Humanity","979-8-4007-2141-0","","10.1145/3758316.3763251","https://doi.org/10.1145/3758316.3763251","Using large language models (LLMs) to test compilers is promising but faces two major challenges: the generated programs are often too simple, and large-scale testing is costly. We present a new framework that splits the process into an offline phase—where LLMs generate small, diverse code pieces—and an online phase that assembles them into complex test programs. Our tool, LegoFuzz, applies this method to test C compilers and has discovered 66 bugs in GCC and LLVM, including many serious miscompilation bugs that prior tools missed. This efficient design shows strong potential for broader AI-assisted software testing.","2025","2025-11-25 22:29:51","2025-11-25 22:29:51","","37–39","","","","","","","SPLASH Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Singapore, Singapore","","","","Testing; Reliability; Compilers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HC5ZVCMQ","conferencePaper","2024","Sánchez Cuadrado, Jesús; Pérez-Soler, Sara; Guerra, Esther; De Lara, Juan","Automating the Development of Task-oriented LLM-based Chatbots","Proceedings of the 6th ACM Conference on Conversational User Interfaces","979-8-4007-0511-3","","10.1145/3640794.3665538","https://doi.org/10.1145/3640794.3665538","Task-oriented chatbots are increasingly used to access all sorts of services – like booking a flight, or setting a medical appointment – through natural language conversation. There are many technologies for implementing task-oriented chatbots, including Dialogflow, Watson, and Rasa. They rely on an explicit definition of the user intents, conversation flows, and chatbot outputs, which is costly to specify, and sometimes results in suboptimal user experiences and artificial conversations with limited diversity of chatbot responses. Recently, the advances in generative artificial intelligence fostered by Large Language Models (LLMs) have enabled a new range of open-domain chatbots, like ChatGPT, able to converse fluently on any topic. However, they are general-purpose, and therefore not directly usable to solve specialised tasks reliably. In this paper, we study the power of LLMs to build task-oriented chatbots, resulting in lighter specifications – no intent definition required – and more natural conversations than in intent-based approaches. To this end, we propose a lightweight domain-specific language based on YAML to specify chatbots using modules of different types (e.g., menus, question-answering, data gathering). These specifications are compiled into structured LLM prompts that use the ReAct framework to inform our runtime how to interpret the user input and coordinate the tasks that the chatbot must perform. The paper presents the design and realisation of our framework, and an assessment that encodes a set of existing intent-based chatbots using our approach, showing its benefits in terms of specification size, conversation flexibility and output diversity.","2024","2025-11-25 22:29:51","2025-11-25 22:29:51","","","","","","","","","CUI '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Luxembourg, Luxembourg","","","","Large Language Models; Domain-Specific Languages; Task-oriented Chatbots","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZBE2QCPR","conferencePaper","2024","Fu, Ying; Wang, Teng; Li, Shanshan; Ding, Jinyan; Zhou, Shulin; Jia, Zhouyang; Li, Wang; Jiang, Yu; Liao, Xiangke","MissConf: LLM-Enhanced Reproduction of Configuration-Triggered Bugs","Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings","979-8-4007-0502-1","","10.1145/3639478.3647635","https://doi.org/10.1145/3639478.3647635","Bug reproduction stands as a pivotal phase in software development, but the absence of configuration information emerges as the main obstacle to effective bug reproduction. Since configuration options generally control critical branches of the software, many bugs can only be triggered under specific configuration settings. We refer to these bugs as configuration-triggered bugs or CTBugs for short. The reproduction of CTBugs consumes considerable time and manual efforts due to the challenges in deducing the missing configuration options within the vast search space of configurations. This complexity contributes to a form of technical debt in software development.To address these challenges, we first conducted an empirical study on 120 CTBugs from 4 widely used systems to understand the root causes and factors influencing the reproduction of CTBugs. Based on our study, we designed and implemented MissConf, the first LLM-enhanced automated tool for CTBug reproduction. Miss-Conf first leverages the LLM to infer whether crucial configuration options are missing in the bug report. Once a suspect CTBug is found, MissConf employs configuration taint analysis and dynamic monitoring methods to filter suspicious configuration options set. Furthermore, it adopts a heuristic strategy for identifying crucial configuration options and their corresponding values. We evaluated MissConf on 5 real-world software systems. The experimental results demonstrate that MissConf successfully infers the 84% (41/49) of the CTBugs and reproduces the 65% (32/49) CTBugs. In the reproduction phase, MissConf eliminates up to 76% of irrelevant configurations, offering significant time savings for developers.","2024","2025-11-25 22:29:51","2025-11-25 22:29:51","","484–495","","","","","","","ICSE-Companion '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","software maintenance; bug reproduction; software configuration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5XECX242","journalArticle","2025","Risse, Niklas; Liu, Jing; Böhme, Marcel","Top Score on the Wrong Exam: On Benchmarking in Machine Learning for Vulnerability Detection","Proc. ACM Softw. Eng.","","","10.1145/3728887","https://doi.org/10.1145/3728887","According to our survey of machine learning for vulnerability detection (ML4VD), 9 in every 10 papers published in the past five years define ML4VD as a function-level binary classification problem: Given a function, does it contain a security flaw? From our experience as security researchers, faced with deciding whether a given function makes the program vulnerable to attacks, we would often first want to understand the context in which this function is called. In this paper, we study how often this decision can really be made without further context and study both vulnerable and non-vulnerable functions in the most popular ML4VD datasets. We call a function vulnerable if it was involved in a patch of an actual security flaw and confirmed to cause the program’s vulnerability. It is non-vulnerable otherwise. We find that in almost all cases this decision cannot be made without further context. Vulnerable functions are often vulnerable only because a corresponding vulnerability-inducing calling context exists while non-vulnerable functions would often be vulnerable if a corresponding context existed. But why do ML4VD techniques achieve high scores even though there is demonstrably not enough information in these samples? Spurious correlations: We find that high scores can be achieved even when only word counts are available. This shows that these datasets can be exploited to achieve high scores without actually detecting any security vulnerabilities. We conclude that the prevailing problem statement of ML4VD is ill-defined and call into question the internal validity of this growing body of work. Constructively, we call for more effective benchmarking methodologies to evaluate the true capabilities of ML4VD, propose alternative problem statements, and examine broader implications for the evaluation of machine learning and programming analysis research.","2025-06","2025-11-25 22:29:51","2025-11-25 22:29:51","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","LLM; machine learning; benchmark; vulnerability detection; software security; context; data quality; function; ML4VD; spurious correlations","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y8BAI939","conferencePaper","2025","Lopez-Duran, Noelia; Romero Organvídez, David; Cruz, Fermín; Benavides, David","Configuration Bugs Classification using LLMs and Encoders","Proceedings of the 29th ACM International Systems and Software Product Line Conference - Volume A","979-8-4007-2024-6","","10.1145/3744915.3748477","https://doi.org/10.1145/3744915.3748477","Configuration-related bugs in software development are reported to be one of the main sources of problems in different domains and are especially relevant in software product lines. Classifying configuration bugs is difficult, since most of the bug tracking systems use descriptions written in natural language that have to be interpreted to categorize them. Some projects use labels to classify bugs, but others do not. In this paper, we address the challenge of automatically classifying bug reports as either configuration-related or non-configuration-related by leveraging the capabilities of large language models (LLMs). We conducted experiments involving zero-shot classification using general-purpose LLMs, as well as fine-tuning a smaller encoder-based model. These approaches were evaluated on two different datasets: first, a dataset consisting of 1,331 bug reports from a web-based software product line, manually labelled by us; and second, a refined version of an existing dataset from the literature, improved through additional data curation. The results are promising, with an F1 score of 0.81 using a combination of both datasets. Furthermore, we applied the fine-tuned encoder-based model to estimate the proportion of configuration-related bugs within a large, previously unseen dataset extracted from various highly configurable Eclipse projects, which contains over 300k bug reports. Our analysis yielded an estimated proportion of 36% for configuration-related bugs. To the best of our knowledge, this is the first work using LLMs to classify configuration bugs. Our results open a new perspective on configuration bugs classification and pave the way to investigate further in this direction.","2025","2025-11-25 22:29:51","2025-11-25 22:29:51","","190–200","","","","","","","SPLC-A '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","LLMs; Bug report classification; Configurable systems; Software Configuration; Software Variability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SZJGW4PG","conferencePaper","2025","He, Shuai; Yan, Hao; Li, Wenke; Hong, Sheng; Guo, Xiaowei; Liu, Xiaofan; Fu, Cai","From Large Language Models to Adversarial Malware: How far are we","Proceedings of the 34th ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-1474-0","","10.1145/3713081.3732928","https://doi.org/10.1145/3713081.3732928","Large Language Models (LLMs) have achieved notable progress in fields including natural language processing, cyber threat detection, and automated penetration testing, increasingly being applied in practical settings. However, the rapid advancement of these models has also led to their potential misuse, posing new challenges to cyberspace security. Security incidents have already been reported in areas such as phishing attacks and disinformation campaigns. Nevertheless, the progress and potential impact of LLMs in generating adversarial malware remain underexplored. This study systematically investigates the evasion capability of adversarial malware generated by LLMs. By integrating chain of thought into a Markov process and designing prompt based state transition functions and reward mechanisms, this research evaluates the effectiveness and efficiency against mainstream static detection methods on a dataset comprising over 2,000 real-world malware samples. Experimental results demonstrate an average evasion rate of 89.92% across 12 commercial antivirus engines on VirusTotal. The findings reveal that individuals with minimal technical expertise and basic natural language skills can generate malware that evades static detection, which underscores potential vulnerabilities in current cyberspace defense and detection systems regarding adversarial malware countermeasures.","2025","2025-11-25 22:29:51","2025-11-25 22:29:51","","178–182","","","","","","","ISSTA Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","large language models; adversarial malware; static detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MR2N4AGT","conferencePaper","2024","Zhang, Mengxiao; Tian, Yongqiang; Xu, Zhenyang; Dong, Yiwen; Tan, Shin Hwei; Sun, Chengnian","LPR: Large Language Models-Aided Program Reduction","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3652126","https://doi.org/10.1145/3650212.3652126","Program reduction is a widely used technique to facilitate debugging compilers by automatically minimizing programs that trigger compiler bugs. Existing program reduction techniques are either generic to a wide range of languages (such as Perses and Vulcan) or specifically optimized for one certain language by exploiting language-specific knowledge (e.g., C-Reduce). However, synergistically combining both generality across languages and optimality to a specific language in program reduction is yet to be explored. This paper proposes LPR, the first LLMs-aided technique leveraging LLMs to perform language-specific program reduction for multiple languages. The key insight is to utilize both the language generality of program reducers such as Perses and the languagespecific semantics learned by LLMs. Concretely, language-generic program reducers can efficiently reduce programs into a small size that is suitable for LLMs to process; LLMs can effectively transform programs via the learned semantics to create new reduction opportunities for the language-generic program reducers to further reduce the programs. Our thorough evaluation on 50 benchmarks across three programming languages (i.e., C, Rust and JavaScript) has demonstrated LPR’s practicality and superiority over Vulcan, the state-of-the-art language-generic program reducer. For effectiveness, LPR surpasses Vulcan by producing 24.93%, 4.47%, and 11.71% smaller programs on benchmarks in C, Rust and JavaScript, separately. Moreover, LPR and Vulcan have the potential to complement each other. For the C language for which C-Reduce is optimized, by applying Vulcan to the output produced by LPR, we can attain program sizes that are on par with those achieved by C-Reduce. For efficiency perceived by users, LPR is more efficient when reducing large and complex programs, taking 10.77%, 34.88%, 36.96% less time than Vulcan to finish all the benchmarks in C, Rust and JavaScript, separately.","2024","2025-11-25 22:29:51","2025-11-25 22:29:51","","261–273","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large Language Models; Program Reduction; Program Semantics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6J963X9J","journalArticle","2025","Boz, Artun; Zorgdrager, Wouter; Kotti, Zoe; Harte, Jesse; Louridas, Panos; Karakoidas, Vassilios; Jannach, Dietmar; Fragkoulis, Marios","Improving Sequential Recommendations with LLMs","ACM Trans. Recomm. Syst.","","","10.1145/3711667","https://doi.org/10.1145/3711667","The sequential recommendation problem has attracted considerable research attention in the past few years, leading to the rise of numerous recommendation models. In this work, we explore how Large Language Models (LLMs), which are nowadays introducing disruptive effects in many AI-based applications, can be used to build or improve sequential recommendation approaches. Specifically, we design three orthogonal approaches and hybrids of those to leverage the power of LLMs in different ways. In addition, we investigate the potential of each approach by focusing on its technical aspects and determining an array of alternative choices for each one. We conduct extensive experiments on three datasets and explore a large variety of configurations, including different language models and baseline recommendation models, to obtain a comprehensive picture of the performance of each approach.Among other observations, we highlight that initializing state-of-the-art sequential recommendation models such as BERT4Rec or SASRec with embeddings obtained from an LLM can lead to substantial performance gains in terms of accuracy. Furthermore, we find that fine-tuning an LLM for recommendation tasks enables it to learn not only the tasks but also the concepts of a domain to some extent. We also show that fine-tuning OpenAI GPT leads to considerably better performance than fine-tuning Google PaLM 2. Overall, our extensive experiments indicate a huge potential value of leveraging LLMs in future recommendation approaches. We publicly share the code and data of our experiments to ensure reproducibility. 1","2025-11","2025-11-25 22:29:51","2025-11-25 22:29:51","","","","2","4","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","large language models; evaluation; Recommender systems; sequential recommendation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2499QJ6Z","conferencePaper","2024","Guo, Qi; Li, Xiaohong; Xie, Xiaofei; Liu, Shangqing; Tang, Ze; Feng, Ruitao; Wang, Junjie; Ge, Jidong; Bu, Lei","FT2Ra: A Fine-Tuning-Inspired Approach to Retrieval-Augmented Code Completion","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3652130","https://doi.org/10.1145/3650212.3652130","The rise of code pre-trained models has significantly enhanced various coding tasks, such as code completion, and tools like GitHub Copilot. However, the substantial size of these models, especially large models, poses a significant challenge when it comes to fine-tuning them for specific downstream tasks. As an alternative approach, retrieval-based methods have emerged as a promising solution, augmenting model predictions without the need for fine-tuning. Despite their potential, a significant challenge is that the designs of these methods often rely on heuristics, leaving critical questions about what information should be stored or retrieved and how to interpolate such information for augmenting predictions. To tackle this challenge, we first perform a theoretical analysis of the fine-tuning process, highlighting the importance of delta logits as a catalyst for improving model predictions. Building on this insight, we develop a novel retrieval-based method, FT2Ra, which aims to mimic genuine fine-tuning. While FT2Ra adopts a retrieval-based mechanism, it uniquely adopts a paradigm with a learning rate and multi-epoch retrievals, which is similar to fine-tuning. We conducted a comprehensive evaluation of FT2Ra in both token-level and line-level code completions. Our findings demonstrate the remarkable effectiveness of FT2Ra when compared to state-of-the-art methods and its potential to genuine fine-tuning. In token-level completion, which represents a relatively easier task, FT2Ra achieves a 4.29% improvement in accuracy compared to the best baseline method on UniXcoder. In the more challenging line-level completion task, we observe a substantial more than twice increase in Exact Match (EM) performance, indicating the significant advantages of our theoretical analysis. Notably, even when operating without actual fine-tuning, FT2Ra exhibits competitive performance compared to the models with real fine-tuning.","2024","2025-11-25 22:29:51","2025-11-25 22:29:51","","313–324","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","ode Completion; Retrieval-Augmented Language Models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WSPHN93Y","journalArticle","2025","Sheng, Ze; Chen, Zhicheng; Gu, Shuning; Huang, Heqing; Gu, Guofei; Huang, Jeff","LLMs in Software Security: A Survey of Vulnerability Detection Techniques and Insights","ACM Comput. Surv.","","0360-0300","10.1145/3769082","https://doi.org/10.1145/3769082","Large Language Models (LLMs) are emerging as transformative tools for software vulnerability detection. Traditional methods, including static and dynamic analysis, face limitations in efficiency, false-positive rates, and scalability with modern software complexity. Through code structure analysis, pattern identification, and repair suggestion generation, LLMs demonstrate a novel approach to vulnerability mitigation.This survey examines LLMs in vulnerability detection, analyzing problem formulation, model selection, application methodologies, datasets, and evaluation metrics. We investigate current research challenges, emphasizing cross-language detection, multimodal integration, and repository-level analysis. Based on our findings, we propose solutions addressing dataset scalability, model interpretability, and low-resource scenarios.Our contributions include: (1) a systematic analysis of LLM applications in vulnerability detection; (2) a unified framework examining patterns and variations across studies; and (3) identification of key challenges and research directions. This work advances the understanding of LLM-based vulnerability detection. The latest findings are maintained at","2025-11","2025-11-25 22:29:51","2025-11-25 22:29:51","","","","5","58","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large language models; vulnerability detection; cybersecurity","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RWVPY7DK","conferencePaper","2024","Huotala, Aleksi; Kuutila, Miikka; Ralph, Paul; Mäntylä, Mika","The Promise and Challenges of Using LLMs to Accelerate the Screening Process of Systematic Reviews","Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering","979-8-4007-1701-7","","10.1145/3661167.3661172","https://doi.org/10.1145/3661167.3661172","Context: Systematic review (SR) is a popular research method in software engineering (SE). However, conducting an SR takes an average of 67 weeks. Thus, automating any step of the SR process could reduce the effort associated with SRs. Objective: Our objective is to investigate the extent to which Large Language Models (LLMs) can accelerate title-abstract screening by (1) simplifying abstracts for human screeners, and (2) automating title-abstract screening entirely. Method: We performed an experiment where human screeners performed title-abstract screening for 20 papers with both original and simplified abstracts from a prior SR. The experiment with human screeners was reproduced by instructing GPT-3.5 and GPT-4 LLMs to perform the same screening tasks. We also studied whether different prompting techniques (Zero-shot (ZS), One-shot (OS), Few-shot (FS), and Few-shot with Chain-of-Thought (FS-CoT) prompting) improve the screening performance of LLMs. Lastly, we studied if redesigning the prompt used in the LLM reproduction of title-abstract screening leads to improved screening performance. Results: Text simplification did not increase the screeners’ screening performance, but reduced the time used in screening. Screeners’ scientific literacy skills and researcher status predict screening performance. Some LLM and prompt combinations perform as well as human screeners in the screening tasks. Our results indicate that a more recent LLM (GPT-4) is better than its predecessor LLM (GPT-3.5). Additionally, Few-shot and One-shot prompting outperforms Zero-shot prompting. Conclusion: Using LLMs for text simplification in the screening process does not significantly improve human performance. Using LLMs to automate title-abstract screening seems promising, but current LLMs are not significantly more accurate than human screeners. To recommend the use of LLMs in the screening process of SRs, more research is needed. We recommend future SR studies to publish replication packages with screening data to enable more conclusive experimenting with LLM screening.","2024","2025-11-25 22:29:51","2025-11-25 22:29:51","","262–271","","","","","","","EASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Salerno, Italy","","","","ChatGPT; GPT-4; LLMs; GPT-3.5; Screening Process of Systematic Reviews; Text Simplification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UFQH76G6","journalArticle","2025","Asgari, Ali; de Koning, Milan; Derakhshanfar, Pouria; Panichella, Annibale","Metamorphic Testing of Deep Code Models: A Systematic Literature Review","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3766552","https://doi.org/10.1145/3766552","Large language models and deep learning models designed for code intelligence have revolutionized the software engineering field due to their ability to perform various code-related tasks. These models can process source code and software artifacts with high accuracy in tasks such as code completion, defect detection, and code summarization; therefore, they can potentially become an integral part of modern software engineering practices. Despite these capabilities, robustness remains a critical quality attribute for deep-code models as they may produce different results under varied and adversarial conditions (e.g., variable renaming). Metamorphic testing has become a widely used approach to evaluate models’ robustness by applying semantic-preserving transformations to input programs and analyzing the stability of model outputs. While prior research has explored testing deep learning models, this systematic literature review focuses specifically on metamorphic testing for deep code models. By studying 45 primary papers, we analyze the transformations, techniques, and evaluation methods used to assess robustness. Our review summarizes the current landscape, identifying frequently evaluated models, programming tasks, datasets, target languages, and evaluation metrics, and highlights key challenges and future directions for advancing the field.","2025-09","2025-11-25 22:29:51","2025-11-25 22:29:51","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Software testing; Natural language processing; Systematic literature review; Deep code models; Metamorphic testing; Robustness evaluation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NQA26NYS","conferencePaper","2025","Wen, Zhihua; Liu, Zhizhao; Tian, Zhiliang; Pan, Shilong; Huang, Zhen; Li, Dongsheng; Huang, Minlie","Scenario-independent Uncertainty Estimation for LLM-based Question Answering via Factor Analysis","Proceedings of the ACM on Web Conference 2025","979-8-4007-1274-6","","10.1145/3696410.3714880","https://doi.org/10.1145/3696410.3714880","Large language models (LLMs) demonstrate significant potential in various applications; however, they are susceptible to generating hallucinations, which can lead to the spread of online misinformation. Existing studies address hallucination detection by (1) employing reference-based methods that consult external resources for verification or (2) utilizing reference-free methods that mainly estimate answer uncertainty based on LLM's internal states. However, reference-based methods incur significant costs and can be infeasible for obtaining reliable external references. Besides, existing uncertainty estimation (UE) methods often overlook the impact of scenario backgrounds inherited from the query's lexical resources, leading to noise in UE. In almost all real-world applications, users care about the uncertainty concerning semantics or facts instead of the query's scenario information. Therefore, we argue that mitigating scenario-related noise and focusing on semantic information can yield a more desirable UE. In this paper, we introduce a plug-and-play scenario-independent framework to enhance unsupervised UE in LLMs by removing scenario-related noise and focusing on semantic information. This framework is compatible with most existing UE methods, as it leverages only the existing UE methods' outputs. Specifically, we design a scenario-specific sampling to paraphrase queries, maintaining their common semantics while diversifying the scenario distribution. Subsequently, to estimate the contribution of the common semantics, we design a factor analysis (FA) model to disentangle the UE score obtained from the given UE method into a combination of multiple latent factors, which represent the contribution of the common semantics and scenario-related noise. By solving the FA model, we decompose the impact of the most significant factor to approximate the uncertainty caused by the common semantics, thus achieving scenario-independent UE. Extensive experiments and analysis across multiple models and datasets demonstrate the effectiveness of our approach.","2025","2025-11-25 22:29:51","2025-11-25 22:29:51","","2378–2390","","","","","","","WWW '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sydney NSW, Australia","","","","large language models; hallucination; uncertainty estimation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I36783BR","conferencePaper","2024","Kerslake, Chris; Denny, Paul; Smith, IV, David H.; Prather, James; Leinonen, Juho; Luxton-Reilly, Andrew; MacNeil, Stephen","Integrating Natural Language Prompting Tasks in Introductory Programming Courses","Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 1","979-8-4007-0598-4","","10.1145/3649165.3690125","https://doi.org/10.1145/3649165.3690125","Introductory programming courses often emphasize mastering syntax and basic constructs before progressing to more complex and interesting programs. This bottom-up approach can be frustrating for novices, shifting the focus away from problem solving and potentially making computing less appealing to a broad range of students. The rise of generative AI for code production could partially address these issues by fostering new skills via interaction with AI models, including constructing high-level prompts and evaluating code that is automatically generated. In this experience report, we explore the inclusion of two prompt-focused activities in an introductory course, implemented across four labs in a six-week module. The first requires students to solve computational problems by writing natural language prompts, emphasizing problem-solving over syntax. The second involves students crafting prompts to generate code equivalent to provided fragments, to foster an understanding of the relationship between prompts and code. Most of the students in the course had reported finding programming difficult to learn, often citing frustrations with syntax and debugging. We found that self-reported difficulty with learning programming had a strong inverse relationship with performance on traditional programming assessments such as tests and projects, as expected. However, performance on the natural language tasks was less strongly related to self-reported difficulty, suggesting they may target different skills. Learning how to communicate with AI coding models is becoming an important skill, and natural language prompting tasks may appeal to a broad range of students.","2024","2025-11-25 22:29:52","2025-11-25 22:29:52","","88–94","","","","","","","SIGCSE Virtual 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Virtual Event, NC, USA","","","","prompt engineering; llm; cs1; eipe; explain in plain english; introductory programming; natural language prompting","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TNAYMVXF","bookSection","2025","Deligiannis, Pantazis; Lal, Akash; Mehrotra, Nikita; Poddar, Rishi; Rastogi, Aseem","RustAssistant: Using LLMs to Fix Compilation Errors in Rust Code","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00022","The Rust programming language, with its safety guarantees, has established itself as a viable choice for low-level systems programming language over the traditional, unsafe alternatives like C/C++. These guarantees come from a strong ownership-based type system, as well as primitive support for features like closures, pattern matching, etc., that make the code more concise and amenable to reasoning. These unique Rust features also pose a steep learning curve for programmers.This paper presents a tool called RustAssistant that leverages the emergent capabilities of Large Language Models (LLMs) to automatically suggest fixes for Rust compilation errors. RustAssistant uses a careful combination of prompting techniques as well as iteration between an LLM and the Rust compiler to deliver high accuracy of fixes. RUSTASSISTANT is able to achieve an impressive peak accuracy of roughly 74% on real-world compilation errors in popular open-source Rust repositories. We also contribute a dataset of Rust compilation errors to enable further research.","2025","2025-11-25 22:29:52","2025-11-25 22:29:52","","3097–3109","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"83J7EHLJ","conferencePaper","2024","Stoica, Bogdan Alexandru; Sethi, Utsav; Su, Yiming; Zhou, Cyrus; Lu, Shan; Mace, Jonathan; Musuvathi, Madanlal; Nath, Suman","If At First You Don’t Succeed, Try, Try, Again...? Insights and LLM-informed Tooling for Detecting Retry Bugs in Software Systems","Proceedings of the ACM SIGOPS 30th Symposium on Operating Systems Principles","979-8-4007-1251-7","","10.1145/3694715.3695971","https://doi.org/10.1145/3694715.3695971","Retry—the re-execution of a task on failure—is a common mechanism to enable resilient software systems. Yet, despite its commonality and long history, retry remains difficult to implement and test.Guided by our study of real-world retry issues, we propose a novel suite of static and dynamic techniques to detect retry problems in software. We find that the ad-hoc nature of retry implementation poses challenges for traditional program analysis but can be well suited for large language models; and that carefully repurposing existing unit tests can, along with fault injection, expose various types of retry problems.","2024","2025-11-25 22:29:52","2025-11-25 22:29:52","","63–78","","","","","","","SOSP '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Austin, TX, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BFXEA2XL","conferencePaper","2023","Nguyen, Cuong; Bui, Huy; Nguyen, Vu; Nguyen, Tien","An Approach to Generating API Test Scripts Using GPT","Proceedings of the 12th International Symposium on Information and Communication Technology","979-8-4007-0891-6","","10.1145/3628797.3628947","https://doi.org/10.1145/3628797.3628947","As more software systems publish and use web services or APIs today, automated API testing is an important activity to help effectively ensure the quality of software services before they are released for their usage. Generating test scripts and data is a crucial step to perform API test automation successfully. In this paper, we propose an approach leveraging GPT, a large language model, and API’s Swagger specification to automatically generate test scripts and test data for API testing. Our approach also applies GPT’s self-refining with the feedback by executing tests on Katalon. We evaluate our proposed approach using a data set of seven APIs consisting of 157 endpoints and 179 operations. The result shows that while our approach generates fewer test scripts and data inputs, it can cover more successful status codes of 2xx than a state-of-the-art tool. This result suggests that leveraging the ability of GPT as a large language model to interpret API’s Swagger specification has the potential for improving the efficacy of generating test scripts and data for API testing.","2023","2025-11-25 22:29:52","2025-11-25 22:29:52","","501–509","","","","","","","SOICT '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Ho Chi Minh, Vietnam","","","","GPT; Prompt Engineering; Large Language Models (LLMs); API Test Data; API Test Script; Automated API Testing; Katalon; Restful APIs","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F3ILTT4K","conferencePaper","2021","Weisz, Justin D.; Muller, Michael; Houde, Stephanie; Richards, John; Ross, Steven I.; Martinez, Fernando; Agarwal, Mayank; Talamadupula, Kartik","Perfection Not Required? Human-AI Partnerships in Code Translation","Proceedings of the 26th International Conference on Intelligent User Interfaces","978-1-4503-8017-1","","10.1145/3397481.3450656","https://doi.org/10.1145/3397481.3450656","Generative models have become adept at producing artifacts such as images, videos, and prose at human-like levels of proficiency. New generative techniques, such as unsupervised neural machine translation (NMT), have recently been applied to the task of generating source code, translating it from one programming language to another. The artifacts produced in this way may contain imperfections, such as compilation or logical errors. We examine the extent to which software engineers would tolerate such imperfections and explore ways to aid the detection and correction of those errors. Using a design scenario approach, we interviewed 11 software engineers to understand their reactions to the use of an NMT model in the context of application modernization, focusing on the task of translating source code from one language to another. Our three-stage scenario sparked discussions about the utility and desirability of working with an imperfect AI system, how acceptance of that system’s outputs would be established, and future opportunities for generative AI in application modernization. Our study highlights how UI features such as confidence highlighting and alternate translations help software engineers work with and better understand generative NMT models.","2021","2025-11-25 22:29:52","2025-11-25 22:29:52","","402–412","","","","","","","IUI '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: College Station, TX, USA","","","","generative AI; code translation; application modernization; imperfect AI; neural machine translation; NMT","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BNH72LLD","conferencePaper","2025","Papadimitriou, Michail; Xekalaki, Maria; Stratikopoulos, Athanasios; Papadakis, Orion; Fumero, Juan; Kotselidis, Christos","TornadoViz: Visualizing Heterogeneous Execution Patterns in Modern Managed Runtime Systems","Proceedings of the 22nd ACM SIGPLAN International Conference on Managed Programming Languages and Runtimes","979-8-4007-2149-6","","10.1145/3759426.3760978","https://doi.org/10.1145/3759426.3760978","With the increasing prevalence of machine learning and large language model (LLM) inference, heterogeneous computing has become essential. Modern JVMs are embracing this transition through projects such as TornadoVM and Babylon, which enable hardware acceleration on diverse hardware resources, including GPUs and FPGAs. However, while performance results are promising, developers currently face a significant tooling gap: traditional profilers excel at CPU-bound execution but become a “black box” when execution transitions to accelerators, providing no visibility into device memory management, execution patterns or cross-device data movement. This gap leaves developers without a unified view of how their Java applications behave across the heterogeneous computing stack. In this paper, we present TornadoViz, a visual analytics tool that leverages TornadoVM’s specialized bytecode system to provide interactive analysis of heterogeneous execution and object lifecycles in managed runtime systems. Unlike existing tools, TornadoViz bridges the managed-native divide by interpreting the bytecode stream that orchestrates heterogeneous execution, hence connecting high-level application logic with low-level hardware utilization patterns. Our tool enables developers to visualize task dependencies, track memory operations across devices, analyze bytecode distribution patterns, and identify performance bottlenecks through interactive dashboards.","2025","2025-11-25 22:29:52","2025-11-25 22:29:52","","31–37","","","","","","","MPLR '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Singapore, Singapore","","","","Visualization; bytecodes; GPU acceleration; Heterogeneous computing; Managed runtimes; TornadoVM","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6H69LQTV","conferencePaper","2025","Krishna, Rahul; Pan, Rangeet; Sinha, Saurabh; Tamilselvam, Srikanth; Pavuluri, Raju; Vukovic, Maja","Codellm-Devkit: A Framework for Contextualizing Code LLMs with Program Analysis Insights","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","10.1145/3696630.3728555","https://doi.org/10.1145/3696630.3728555","Large Language Models for Code (or code LLMs) are increasingly gaining popularity and capabilities, offering a wide array of functionalities such as code completion, code generation, code summarization, test generation, code translation, and more. To leverage code LLMs to their full potential, developers must provide code-specific contextual information to the models. These are typically derived and distilled using program analysis tools. However, there exists a significant gap—these static analysis tools are often language-specific and come with a steep learning curve, making their effective use challenging. These tools are tailored to specific program languages, requiring developers to learn and manage multiple tools to cover various aspects of the their code base. Moreover, the complexity of configuring and integrating these tools into the existing development environments add an additional layer of difficulty. This challenge limits the potential benefits that could be gained from more widespread and effective use of static analysis in conjunction with LLMs.To address this challenge, we present codellm-devkit (hereafter, cldk), an open-source library that significantly simplifies the process of performing program analysis at various levels of granularity for different programming languages to support code LLM use cases. As a Python library, cldk offers developers an intuitive and user-friendly interface, making it incredibly easy to provide rich program analysis context to code LLMs. With this library, developers can effortlessly integrate detailed, code-specific insights that enhance the operational efficiency and effectiveness of LLMs in coding tasks. CLDK is available as an open-source library at https://github.com/codellm-devkit.","2025","2025-11-25 22:29:52","2025-11-25 22:29:52","","308–318","","","","","","","FSE Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E2RKWM9W","conferencePaper","2025","Singhee, Amith","Keynote Talk: AI for Enterprise Software","Proceedings of the 18th Innovations in Software Engineering Conference","979-8-4007-1424-5","","10.1145/3717383.3719299","https://doi.org/10.1145/3717383.3719299","The discipline of Software Engineering is being disrupted with the advent of large language models (LLMs) and agentic AI. There is a plethora of research, examples and testimonials out there on this. We will pull the curtain a bit more and look at the world of enterprise software – large application development and maintenance scenarios that are the primary interest for major enterprises and organizations across sectors and across the world. In this setting there are unique challenges that emerge related to data scarcity, complex application architecture, legacy programming languages, application modernization and working within the requirements imposed by the business. We will see examples of how LLMs and generative AI are being leveraged to address these challenges, but how it is important to combine these with algorithmic advances and program analysis to enable production use of such AI-based solutions.","2025","2025-11-25 22:29:52","2025-11-25 22:29:52","","","","","","","","","ISEC '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F2KBMK8T","conferencePaper","2024","Yang, Jackie (Junrui); Shi, Yingtian; Zhang, Yuhan; Li, Karina; Rosli, Daniel Wan; Jain, Anisha; Zhang, Shuning; Li, Tianshi; Landay, James A.; Lam, Monica S.","ReactGenie: A Development Framework for Complex Multimodal Interactions Using Large Language Models","Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems","979-8-4007-0330-0","","10.1145/3613904.3642517","https://doi.org/10.1145/3613904.3642517","By combining voice and touch interactions, multimodal interfaces can surpass the efficiency of either modality alone. Traditional multimodal frameworks require laborious developer work to support rich multimodal commands where the user’s multimodal command involves possibly exponential combinations of actions/function invocations. This paper presents ReactGenie, a programming framework that better separates multimodal input from the computational model to enable developers to create efficient and capable multimodal interfaces with ease. ReactGenie translates multimodal user commands into NLPL (Natural Language Programming Language), a programming language we created, using a neural semantic parser based on large-language models. The ReactGenie runtime interprets the parsed NLPL and composes primitives in the computational model to implement complex user commands. As a result, ReactGenie allows easy implementation and unprecedented richness in commands for end-users of multimodal apps. Our evaluation showed that 12 developers can learn and build a non-trivial ReactGenie application in under 2.5 hours on average. In addition, compared with a traditional GUI, end-users can complete tasks faster and with less task load using ReactGenie apps.","2024","2025-11-25 22:29:52","2025-11-25 22:29:52","","","","","","","","","CHI '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Honolulu, HI, USA","","","","natural language processing; development frameworks; large-language model; multimodal interactions; programming framework","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TJFIU3A7","conferencePaper","2024","Hao, Zixu; Jiang, Huiqiang; Jiang, Shiqi; Ren, Ju; Cao, Ting","Hybrid SLM and LLM for Edge-Cloud Collaborative Inference","Proceedings of the Workshop on Edge and Mobile Foundation Models","979-8-4007-0663-9","","10.1145/3662006.3662067","https://doi.org/10.1145/3662006.3662067","Edge-Cloud collaboration for deep learning inference has been actively studied, to enhance the inference performance by leveraging both Edge and Cloud resources. However, traditional Edge-Cloud collaboration based on model partitioning or confidence score are not suitable in the LLM (large language models) era, because of its autoregressive generation and the generality across diverse tasks. This paper proposes a dynamic token-level Edge-Cloud collaboration for LLMs. A SLM (small language model) such as TinyLlama resides on the Edge devices, through token-level interaction with the Cloud-side LLMs during inference, approaching LLM quality with a controllable cost similar to SLM. Evaluation results show that our method can only use 25.8% LLM cost to achieve LLM-comparable quality on GSM8K task.","2024","2025-11-25 22:29:52","2025-11-25 22:29:52","","36–41","","","","","","","EdgeFM '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Minato-ku, Tokyo, Japan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"76ZFWYKF","bookSection","2025","Chen, Lvcheng; Wu, Ying; Wen, Chenyi; Wang, Shizhang; Zhang, Li; Yu, Bei; Sun, Qi; Zhuo, Cheng","An Agile Framework for Efficient LLM Accelerator Development and Model Inference","Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design","979-8-4007-1077-3","","","https://doi.org/10.1145/3676536.3676753","Large Language Models (LLMs) have revolutionized many domains with exceptional performance while their large sizes hinder their broad applicability, especially in the edge computation scenarios. Designing large-scale LLM-specific accelerators is also challenging, suffering from the complicated, cumbersome, and time-consuming design, simulation, and optimization process. This paper meticulously proposes an agile framework for accelerator development, supporting efficient LLM inference. Firstly, we investigate the architecture of LLMs, uncover performance bottlenecks, and design an optimized binarized accelerator and a configurable RISC-V-based SoC to boost the inference of binary LLMs. Further, a novel fidelity-driven method is proposed to learn the multi-fidelity representation, solving the modeling and accuracy issues due to the lack of accurate later-stage data in the EDA flow, by capturing complex relationships among simulation metrics in and across different fidelities. Tailored strategies across model preparation, backend kernel implementations, agile accelerator and SoC design, and inference simulation are incorporated into our framework to refine the development workflow. Our method significantly accelerates the hardware design, simulation, and optimization processes. Experimental results illustrate the impressive speed and effectiveness of our framework in designing edge LLM accelerators and optimizing LLM inference.","2025","2025-11-25 22:29:52","2025-11-25 22:29:52","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KJD3FJEK","conferencePaper","2024","Korpimies, Kai; Laaksonen, Antti; Luukkainen, Matti","Unrestricted Use of LLMs in a Software Project Course: Student Perceptions on Learning and Impact on Course Performance","Proceedings of the 24th Koli Calling International Conference on Computing Education Research","979-8-4007-1038-4","","10.1145/3699538.3699541","https://doi.org/10.1145/3699538.3699541","Large language models (LLMs) provide round-the-clock personalized programming assistance, unlike course instructors or traditional online information sources such as Stack Overflow. While LLMs can aid in code generation, concerns about over-reliance and the impact on learning persist. This study discusses students’ experiences with LLMs in a software project course where students were allowed to use LLMs freely except for unit test generation. We conducted surveys during course instances in autumn 2023 and spring 2024. The surveys assessed the extent of LLM usage, methods of application, and perceived impact on learning. Results indicate diverse usage patterns, with many students finding LLMs beneficial for efficiency and problem-solving, though over-reliance and poor-quality outputs were noted concerns. The usage patterns can be linked to course performance and time spent on the project.","2024","2025-11-25 22:29:52","2025-11-25 22:29:52","","","","","","","","","Koli Calling '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Large language models; Computer Science Education; Code generation; User Study; Software project","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6YK47S9G","conferencePaper","2024","Baresi, Luciano; Camilli, Matteo; Dolci, Tommaso; Quattrocchi, Giovanni","A Conceptual Framework for Quality Assurance of LLM-based Socio-critical Systems","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695306","https://doi.org/10.1145/3691620.3695306","Recent breakthroughs in Artificial Intelligence (AI) obfuscate the boundaries between digital, physical, and social spaces, a trend expected to continue in the foreseeable future. Traditionally, software engineering has prioritized technical aspects, focusing on functional correctness and reliability while often neglecting broader societal implications. With the rise of software agents enabled by Large Language Models (LLMs) and capable of emulating human intelligence and perception, there is a growing recognition of the need for addressing socio-critical issues. Unlike technical challenges, these issues cannot be resolved through traditional, deterministic approaches due to their subjective nature and dependence on evolving factors such as culture and demographics. This paper dives into this problem and advocates the need for revising existing engineering principles and methodologies. We propose a conceptual framework for quality assurance where AI is not only the driver of socio-critical systems but also a fundamental tool in their engineering process. Such framework encapsulates pre-production and runtime workflows where LLM-based agents, so-called artificial doppelgängers, continuously assess and refine socio-critical systems ensuring their alignment with established societal standards.","2024","2025-11-25 22:29:52","2025-11-25 22:29:52","","2314–2318","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","large language models; quality assurance; AI-enabled agents","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DM9FKZUM","journalArticle","2024","Huang, Kai; Xu, Zhengzi; Yang, Su; Sun, Hongyu; Li, Xuejun; Yan, Zheng; Zhang, Yuqing","Evolving Paradigms in Automated Program Repair: Taxonomy, Challenges, and Opportunities","ACM Comput. Surv.","","0360-0300","10.1145/3696450","https://doi.org/10.1145/3696450","With the rapid development and large-scale popularity of program software, modern society increasingly relies on software systems. However, the problems exposed by software have also come to the fore. The software bug has become an important factor troubling developers. In this context, Automated Program Repair (APR) techniques have emerged, aiming to automatically fix software bug problems and reduce manual debugging work. In particular, benefiting from the advances in deep learning, numerous learning-based APR techniques have emerged in recent years, which also bring new opportunities for APR research. To give researchers a quick overview of APR techniques’ complete development and future opportunities, we review the evolution of APR techniques and discuss in depth the latest advances in APR research. In this article, the development of APR techniques is introduced in terms of four different patch generation schemes: search-based, constraint-based, template-based, and learning-based. Moreover, we propose a uniform set of criteria to review and compare each APR tool and then discuss the current state of APR development. Finally, we analyze current challenges and future directions, especially highlighting the critical opportunities that large language models bring to APR research.","2024-10","2025-11-25 22:29:52","2025-11-25 22:29:52","","","","2","57","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Automated program repair","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VQ6KFQPB","conferencePaper","2024","Liu, Chenyan; Cai, Yufan; Lin, Yun; Huang, Yuhuan; Pei, Yunrui; Jiang, Bo; Yang, Ping; Dong, Jin Song; Mei, Hong","CoEdPilot: Recommending Code Edits with Learned Prior Edit Relevance, Project-wise Awareness, and Interactive Nature","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3652142","https://doi.org/10.1145/3650212.3652142","Recent years have seen the development of LLM-based code generation. Compared to generating code in a software project, incremental code edits are empirically observed to be more frequent. The emerging code editing approaches usually formulate the problem as generating an edit based on known relevant prior edits and context. However, practical code edits can be more complicated. First, an editing session can include multiple (ir)relevant edits to the code under edit. Second, the inference of the subsequent edits is non-trivial as the scope of its ripple effect can be the whole project. In this work, we propose CoEdPilot, an LLM-driven solution to recommend code edits by discriminating the relevant edits, exploring their interactive natures, and estimating its ripple effect in the project. Specifically, CoEdPilot orchestrates multiple neural transformers to identify what and how to edit in the project regarding both edit location and edit content. When a user accomplishes an edit with an optional editing description, an Subsequent Edit Analysis first reports the most relevant files in the project with what types of edits (e.g., keep, insert, and replace) can happen for each line of their code. Next, an Edit-content Generator generates concrete edit options for the lines of code, regarding its relevant prior changes reported by an Edit-dependency Analyzer. Last, both the Subsequent Edit Analysis and the Edit-content Generator capture relevant prior edits as feedback to readjust their recommendations. We train our models by collecting over 180K commits from 471 open-source projects in 5 programming languages. Our extensive experiments show that (1) CoEdPilot can well predict the edits (i.e., predicting edit location with accuracy of 70.8%-85.3%, and the edit content with exact match rate of 41.8% and BLEU4 score of 60.7); (2) CoEdPilot can well boost existing edit generators such as GRACE and CCT5 on exact match rate by 8.57% points and BLEU4 score by 18.08. Last, our user study on 18 participants with 3 editing tasks (1) shows that CoEdPilot can be effective in assisting users to edit code in comparison with Copilot, and (2) sheds light on the future improvement of the tool design. The video demonstration of our tool is available at https://sites.google.com/view/coedpilot/home.","2024","2025-11-25 22:29:52","2025-11-25 22:29:52","","466–478","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","code edit generation; edit location; interaction; language model","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XB2N9ERZ","journalArticle","2025","Brachman, Michelle; El-Ashry, Amina; Dugan, Casey; Geyer, Werner","Current and Future Use of Large Language Models for Knowledge Work","Proc. ACM Hum.-Comput. Interact.","","","10.1145/3757403","https://doi.org/10.1145/3757403","Large Language Models (LLMs) have introduced a paradigm shift in interaction with AI technology, enabling knowledge workers to complete tasks by specifying their desired outcome in natural language. LLMs have the potential to increase productivity and reduce tedious tasks in an unprecedented way. A systematic study of LLM adoption for work can provide insight into how LLMs can best support these workers. To explore knowledge workers' current and desired usage of LLMs, we ran a survey (n=216). Workers described tasks they already used LLMs for, like generating code or improving text, but imagined a future with LLMs integrated into their workflows and data. We ran a second survey (n=107) a year later that validated our initial findings and provides insight into up-to-date LLM use by knowledge workers. We discuss implications for adoption and design of generative AI technologies for knowledge work.","2025-10","2025-11-25 22:29:52","2025-11-25 22:29:52","","","","7","9","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","large language models; survey; adoption; knowledge workers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T7UWQH6R","conferencePaper","2024","Ma, Yunlong; Tian, Wentong; Gao, Xiang; Sun, Hailong; Li, Li","API Misuse Detection via Probabilistic Graphical Model","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3652112","https://doi.org/10.1145/3650212.3652112","API misuses can cause a range of issues in software development, including program crashes, bugs, and vulnerabilities. Different approaches have been developed to automatically detect API misuses by checking the program against usage rules extracted from extensive codebase or API documents. However, these mined rules may not be precise or complete, leading to high false positive/negative rates. In this paper, we propose a novel solution to this problem by representing the mined API usage rules as a probabilistic graphical model, where each rule's probability value represents its trustworthiness of being correct. Our approach automatically constructs probabilistic usage rules by mining codebase and documents, and aggregating knowledge from different sources. Here, the usage rules obtained from the codebase initialize the probabilistic model, while the knowledge from the documents serves as a supplement for adjusting and complementing the probabilities accordingly. We evaluate our approach on the MuBench benchmark. Experimental results show that our approach achieves 42.0% precision and 54.5% recall, significantly outperforming state-of-the-art approaches.","2024","2025-11-25 22:29:52","2025-11-25 22:29:52","","88–99","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Mining Software Repository; API misuse detection; Document Mining; Probabilistic Graphical Model","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X2HY666E","conferencePaper","2024","Yang, Mingke; Chen, Yuqi; Liu, Yi; Shi, Ling","DistillSeq: A Framework for Safety Alignment Testing in Large Language Models using Knowledge Distillation","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680304","https://doi.org/10.1145/3650212.3680304","Large Language Models (LLMs) have showcased their remarkable capabilities in diverse domains, encompassing natural language understanding, translation, and even code generation. The potential for LLMs to generate harmful content is a significant concern. This risk necessitates rigorous testing and comprehensive evaluation of LLMs to ensure safe and responsible use. However, extensive testing of LLMs requires substantial computational resources, making it an expensive endeavor. Therefore, exploring cost-saving strategies during the testing phase is crucial to balance the need for thorough evaluation with the constraints of resource availability. To address this, our approach begins by transferring the moderation knowledge from an LLM to a small model. Subsequently, we deploy two distinct strategies for generating malicious queries: one based on a syntax tree approach, and the other leveraging an LLM-based method. Finally, our approach incorporates a sequential filter-test process designed to identify test cases that are prone to eliciting toxic responses. By doing so, we significantly curtail unnecessary or unproductive interactions with LLMs, thereby streamlining the testing process. Our research evaluated the efficacy of DistillSeq across four LLMs: GPT-3.5, GPT-4.0, Vicuna-13B, and Llama-13B. In the absence of DistillSeq, the observed attack success rates on these LLMs stood at 31.5% for GPT-3.5, 21.4% for GPT-4.0, 28.3% for Vicuna-13B, and 30.9% for Llama-13B. However, upon the application of DistillSeq, these success rates notably increased to 58.5%, 50.7%, 52.5%, and 54.4%, respectively. This translated to an average escalation in attack success rate by a factor of 93.0% when compared to scenarios without the use of DistillSeq. Such findings highlight the significant enhancement DistillSeq offers in terms of reducing the time and resource investment required for effectively testing LLMs.","2024","2025-11-25 22:29:52","2025-11-25 22:29:52","","578–589","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large Language Models; Automated Testing; Knowledge Distillation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ELN3Z78I","conferencePaper","2024","Morales, Sergio; Clarisó, Robert; Cabot, Jordi","Automating Bias Testing of LLMs","Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering","979-8-3503-2996-4","","10.1109/ASE56229.2023.00018","https://doi.org/10.1109/ASE56229.2023.00018","Large Language Models (LLMs) are being quickly integrated in a myriad of software applications. This may introduce a number of biases, such as gender, age or ethnicity, in the behavior of such applications. To face this challenge, we explore the automatic generation of tests suites to assess the potential biases of an LLM. Each test is defined as a prompt used as input to the LLM and a test oracle that analyses the LLM output to detect the presence of biases.","2024","2025-11-25 22:29:52","2025-11-25 22:29:52","","1705–1707","","","","","","","ASE '23","","","","IEEE Press","Echternach, Luxembourg","","","","","","","","","","","","large language models; testing; bias; ethics; fairness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YRKUI98D","conferencePaper","2024","Bin Murtaza, Sardar; Mccoy, Aidan; Ren, Zhiyuan; Murphy, Aidan; Banzhaf, Wolfgang","LLM Fault Localisation within Evolutionary Computation Based Automated Program Repair","Proceedings of the Genetic and Evolutionary Computation Conference Companion","979-8-4007-0495-6","","10.1145/3638530.3664174","https://doi.org/10.1145/3638530.3664174","Repairing bugs can be a daunting task for even a human experienced in debugging, so naturally, attempting to automatically repair programs with a computer system is quite challenging. The existing methods of automated program repair leave a lot of room for improvement. Fault localization, which aims to find lines of code that are potentially buggy, minimises the search space of an automated program repair system. Recent work has shown improvement in these fault localization methods, with the use of Large Language Models. Here, we propose a system where a LLM-based fault localization tool, which we call SemiAutoFL, is used within a fully automatic program repair program, ARJA-e. We show that utilising LLM-based fault localization with ARJA-e can significantly improve its performance on real world bugs. ARJA-e with SemiAutoFL can repair 10 bugs that ARJA-e was previously unable to so do. This finding adds to our understanding of how to improve fault localization and automated program repair, highlighting the potential for more efficient and accurate fault localisation methods being applied to automated program repair.","2024","2025-11-25 22:29:52","2025-11-25 22:29:52","","1824–1829","","","","","","","GECCO '24 Companion","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Melbourne, VIC, Australia","","","","large language models; fault localisation; genetic improvement","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F6TLFTV9","conferencePaper","2025","Stennett, Tyler; Kim, Myeongsoo; Sinha, Saurabh; Orso, Alessandro","AutoRestTest: A Tool for Automated REST API Testing Using LLMs and MARL","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings","979-8-3315-3683-1","","10.1109/ICSE-Companion66252.2025.00015","https://doi.org/10.1109/ICSE-Companion66252.2025.00015","As REST APIs have become widespread in modern web services, comprehensive testing of these APIs is increasingly crucial. Because of the vast search space of operations, parameters, and parameter values, along with their dependencies and constraints, current testing tools often achieve low code coverage, resulting in suboptimal fault detection. To address this limitation, we present AutoRestTest, a novel tool that integrates the Semantic Property Dependency Graph (SPDG) with Multi-Agent Reinforcement Learning (MARL) and large language models (LLMs) for effective REST API testing. AutoRestTest determines operation-dependent parameters using the SPDG and employs five specialized agents (operation, parameter, value, dependency, and header) to identify dependencies of operations and generate operation sequences, parameter combinations, and values. Through an intuitive command-line interface, users can easily configure and monitor tests with successful operation count, unique server errors detected, and time elapsed. Upon completion, AutoRestTest generates a detailed report highlighting errors detected and operations exercised. In this paper, we introduce our tool and present preliminary findings, with a demonstration video available at https://www.youtube.com/watch?v=VVus2W8rap8.","2025","2025-11-25 22:29:52","2025-11-25 22:29:52","","21–24","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","automated rest API testing; multi agent reinforcement learning for testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6BLPM3EX","conferencePaper","2024","Chen, Yujia; Gao, Cuiyun; Yang, Zezhou; Zhang, Hongyu; Liao, Qing","Bridge and Hint: Extending Pre-trained Language Models for Long-Range Code","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3652127","https://doi.org/10.1145/3650212.3652127","In the field of code intelligence, effectively modeling long-range code poses a significant challenge. Existing pre-trained language models (PLMs) such as UniXcoder have achieved remarkable success, but they still face difficulties with long code inputs. This is mainly due to their limited capacity to maintain contextual continuity and memorize the key information over long-range code. To alleviate the difficulties, we propose EXPO, a framework for EXtending Pre-trained language models for lOng-range code. EXPO incorporates two innovative memory mechanisms we propose in this paper: Bridge Memory and Hint Memory. Bridge Memory uses a tagging mechanism to connect disparate snippets of long-range code, helping the model maintain contextual coherence. Hint Memory focuses on crucial code elements throughout the global context, such as package imports, by integrating a 𝑘NN attention layer to adaptively select the relevant code elements. This dual-memory approach bridges the gap between understanding local code snippets and maintaining global code coherence, thereby enhancing the model’s overall comprehension of long code sequences. We validate the effectiveness of EXPO on five popular pre-trained language models such as UniXcoder and two code intelligence tasks including API recommendation and vulnerability detection. Experimental results demonstrate that EXPO significantly improves the pre-training language models.","2024","2025-11-25 22:29:52","2025-11-25 22:29:52","","274–286","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","vulnerability detection; API recommendation; code representation; long-range code; Pre-trained language model","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C5AAC9BF","journalArticle","2025","Li, Rui; Liu, Huai; Poon, Pak-Lok; Towey, Dave; Sun, Chang-Ai; Zheng, Zheng; Zhou, Zhi Quan; Chen, Tsong Yueh","Metamorphic Relation Generation: State of the Art and Research Directions","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3708521","https://doi.org/10.1145/3708521","Metamorphic testing has become one mainstream technique to address the notorious oracle problem in software testing, thanks to its great successes in revealing real-life bugs in a wide variety of software systems. Metamorphic relations, the core component of metamorphic testing, have continuously attracted research interests from both academia and industry. In the last decade, a rapidly increasing number of studies have been conducted to systematically generate metamorphic relations from various sources and for different application domains. In this article, based on the systematic review on the state of the art for metamorphic relations’ generation, we summarize and highlight visions for further advancing the theory and techniques for identifying and constructing metamorphic relations and discuss promising research directions in related areas.","2025-05","2025-11-25 22:29:52","2025-11-25 22:29:52","","","","5","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Metamorphic testing; Metamorphic relation; Metamorphic relation generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EKYC6CCT","journalArticle","2024","Wu, Yaoxuan; Humayun, Ahmad; Gulzar, Muhammad Ali; Kim, Miryung","Natural Symbolic Execution-Based Testing for Big Data Analytics","Proc. ACM Softw. Eng.","","","10.1145/3660825","https://doi.org/10.1145/3660825","Symbolic execution is an automated test input generation technique that models individual program paths as logical constraints. However, the realism of concrete test inputs generated by SMT solvers often comes into question. Existing symbolic execution tools only seek arbitrary solutions for given path constraints. These constraints do not incorporate the naturalness of inputs that observe statistical distributions, range constraints, or preferred string constants. This results in unnatural-looking inputs that fail to emulate real-world data. In this paper, we extend symbolic execution with consideration for incorporating naturalness. Our key insight is that users typically understand the semantics of program inputs, such as the distribution of height or possible values of zipcode, which can be leveraged to advance the ability of symbolic execution to produce natural test inputs. We instantiate this idea in NaturalSym, a symbolic execution-based test generation tool for data-intensive scalable computing (DISC) applications. NaturalSym generates natural-looking data that mimics real-world distributions by utilizing user-provided input semantics to drastically enhance the naturalness of inputs, while preserving strong bug-finding potential. On DISC applications and commercial big data test benchmarks, NaturalSym achieves a higher degree of realism —as evidenced by a perplexity score 35.1 points lower on median, and detects 1.29× injected faults compared to the state-of-the-art symbolic executor for DISC, BigTest. This is because BigTest draws inputs purely based on the satisfiability of path constraints constructed from branch predicates, while NaturalSym is able to draw natural concrete values based on user-specified semantics and prioritize using these values in input generation. Our empirical results demonstrate that NaturalSym finds injected faults 47.8× more than NaturalFuzz (a coverage-guided fuzzer) and 19.1× more than ChatGPT. Meanwhile, TestMiner (a mining-based approach) fails to detect any injected faults. NaturalSym is the first symbolic executor that combines the notion of input naturalness in symbolic path constraints during SMT-based input generation. We make our code available at https://github.com/UCLA-SEAL/NaturalSym.","2024-07","2025-11-25 22:29:52","2025-11-25 22:29:52","","","","FSE","1","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Symbolic Execution; DISC Applications; Naturalness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X93EQPK4","conferencePaper","2023","Ackerman, Joshua; Cybenko, George","Large Language Models for Fuzzing Parsers (Registered Report)","Proceedings of the 2nd International Fuzzing Workshop","979-8-4007-0247-1","","10.1145/3605157.3605173","https://doi.org/10.1145/3605157.3605173","Ambiguity in format specifications is a significant source of software vulnerabilities. In this paper, we propose a natural language processing (NLP) driven approach that implicitly leverages the ambiguity of format specifications to generate instances of a format for fuzzing. We employ a large language model (LLM) to recursively examine a natural language format specification to generate instances from the specification for use as strong seed examples to a mutation fuzzer. Preliminary experiments show that our method outperforms a basic mutation fuzzer, and is capable of synthesizing examples from novel handwritten formats.","2023","2025-11-25 22:29:53","2025-11-25 22:29:53","","31–38","","","","","","","FUZZING 2023","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Seattle, WA, USA","","","","Large Language Models; Fuzzing; Deep Learning; Parsers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U4GRFWP3","conferencePaper","2025","Coppa, Emilio; Sokolowski, Daniel; Salvaneschi, Guido","Hybrid Fuzzing of Infrastructure as Code Programs (Short Paper)","Proceedings of the 34th ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-1474-0","","10.1145/3713081.3731721","https://doi.org/10.1145/3713081.3731721","Infrastructure as Code (IaC) has become a cornerstone of modern cloud and system deployment, enabling automated and repeatable infrastructure provisioning. However, ensuring the correctness of IaC programs remains challenging due to their complexity and dynamic nature. In particular, IaC programs can exhibit different behaviors depending on the state of the resources they manage. Since these resources are deployed on external providers, accounting for their possible states is difficult, making the testing phase particularly challenging. This paper presents HIT, a novel unit-testing framework for IaC programs that effectively tests IaC code using relevant resource states. HIT combines fuzzing and concolic execution, two effective yet previously unexplored techniques for IaC code. Our experiments confirm that HIT achieves better code coverage than state-of-the-art approaches.","2025","2025-11-25 22:29:53","2025-11-25 22:29:53","","92–97","","","","","","","ISSTA Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","symbolic execution; fuzzing; DevOps; infrastructure as code","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QBUTATSR","journalArticle","2024","Yang, Chenyuan; Deng, Yinlin; Lu, Runyu; Yao, Jiayi; Liu, Jiawei; Jabbarvand, Reyhaneh; Zhang, Lingming","WhiteFox: White-Box Compiler Fuzzing Empowered by Large Language Models","Proc. ACM Program. Lang.","","","10.1145/3689736","https://doi.org/10.1145/3689736","Compiler correctness is crucial, as miscompilation can falsify program behaviors, leading to serious consequences over the software supply chain. In the literature, fuzzing has been extensively studied to uncover compiler defects. However, compiler fuzzing remains challenging: Existing arts focus on black- and grey-box fuzzing, which generates test programs without sufficient understanding of internal compiler behaviors. As such, they often fail to construct test programs to exercise intricate optimizations. Meanwhile, traditional white-box techniques, such as symbolic execution, are computationally inapplicable to the giant codebase of compiler systems. Recent advances demonstrate that Large Language Models (LLMs) excel in code generation/understanding tasks and even have achieved state-of-the-art performance in black-box fuzzing. Nonetheless, guiding LLMs with compiler source-code information remains a missing piece of research in compiler testing. To this end, we propose WhiteFox, the first white-box compiler fuzzer using LLMs with source-code information to test compiler optimization, with a spotlight on detecting deep logic bugs in the emerging deep learning (DL) compilers. WhiteFox adopts a multi-agent framework: (i) an LLM-based analysis agent examines the low-level optimization source code and produces requirements on the high-level test programs that can trigger the optimization; (ii) an LLM-based generation agent produces test programs based on the summarized requirements. Additionally, optimization-triggering tests are also used as feedback to further enhance the test generation prompt on the fly. Our evaluation on the three most popular DL compilers (i.e., PyTorch Inductor, TensorFlow-XLA, and TensorFlow Lite) shows that WhiteFox can generate high-quality test programs to exercise deep optimizations requiring intricate conditions, practicing up to 8 times more optimizations than state-of-the-art fuzzers. To date, WhiteFox has found in total 101 bugs for the compilers under test, with 92 confirmed as previously unknown and 70 already fixed. Notably, WhiteFox has been recently acknowledged by the PyTorch team, and is in the process of being incorporated into its development workflow. Finally, beyond DL compilers, WhiteFox can also be adapted for compilers in different domains, such as LLVM, where WhiteFox has already found multiple bugs.","2024-10","2025-11-25 22:29:53","2025-11-25 22:29:53","","","","OOPSLA2","8","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; Fuzzing; Code Analysis; White-box Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7V4V6SIY","conferencePaper","2024","Chen, Yang; Jabbarvand, Reyhaneh","Neurosymbolic Repair of Test Flakiness","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680369","https://doi.org/10.1145/3650212.3680369","Test flakiness, a non-deterministic behavior of builds irrelevant to code changes, is a major and continuing impediment to deliver- ing reliable software. The very few techniques for the automated repair of test flakiness are specifically crafted to repair either Order- Dependent (OD) or Implementation-Dependent (ID) flakiness. They are also all symbolic approaches, i.e., they leverage program analy- sis to detect and repair known test flakiness patterns and root causes, failing to generalize. To bridge the gap, we propose FlakyDoctor, a neuro-symbolic technique that combines the power of LLMs— generalizability—and program analysis—soundness—to fix different types of test flakiness. Our extensive evaluation using 873 confirmed flaky tests (332 OD and 541 ID) from 243 real-world projects demonstrates the ability of FlakyDoctor in repairing flakiness, achieving 57% (OD) and 59% (ID) success rate. Comparing to three alternative flakiness repair approaches, FlakyDoctor can repair 8% more ID tests than DexFix, 12% more OD flaky tests than ODRepair, and 17% more OD flaky tests than iFixFlakies. Regardless of underlying LLM, the non-LLM components of FlakyDoctor contribute to 12–31 % of the overall performance, i.e., while part of the FlakyDoctor power is from using LLMs, they are not good enough to repair flaky tests in real-world projects alone. What makes the proposed technique superior to related research on test flakiness mitigation specifically and program repair, in general, is repairing 79 previously unfixed flaky tests in real-world projects. We opened pull requests for all cases with corresponding patches; 19 of them were accepted and merged at the time of submission.","2024","2025-11-25 22:29:53","2025-11-25 22:29:53","","1402–1414","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large Language Models; Program Repair; Test Flakiness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9QJFE3X6","conferencePaper","2025","Wickramasekara, Akila; Densmore, Alanna; Breitinger, Frank; Studiawan, Hudan; Scanlon, Mark","AutoDFBench: A Framework for AI Generated Digital Forensic Code and Tool Testing and Evaluation","Proceedings of the Digital Forensics Doctoral Symposium","979-8-4007-1076-6","","10.1145/3712716.3712718","https://doi.org/10.1145/3712716.3712718","Generative AI (GenAI) and Large Language Models (LLMs) show great potential in various domains, including digital forensics. A notable use case of these technologies is automatic code generation, which can reasonably be expected to include digital forensic applications in the not-too-distant future. As with any digital forensic tool, these systems must undergo extensive testing and validation. However, manually evaluating outputs, including generated DF code, remains a challenge. AutoDFBench is an automated framework designed to address this by validating AI-generated code and tools against NIST’s Computer Forensics Tool Testing Program (CFTT) procedures and subsequently calculating an AutoDFBench benchmarking score. The framework operates in four phases: data preparation, API handling, code execution, and result recording with score calculation. It benchmarks generative AI systems, such as LLMs and automated code generation agents, for DF applications. This benchmark can support iterative development or serve as a comparison metric between GenAI DF systems. As a proof of concept, NIST’s forensic string search tests were used, involving more than 24,200 tests with five top-performing code generation LLMs. These tests validated the output of 121 cases, considering two levels of user expertise, two programming languages, and ten iterations per case with varying prompts. The results also highlight the significant limitations of the DF-specific solutions generated by generic LLMs.","2025","2025-11-25 22:29:53","2025-11-25 22:29:53","","","","","","","","","DFDS '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Automation; Large Language Models; Challenges; Digital Forensics; Investigative Process","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X6MTX3YE","journalArticle","2025","Zan, Daoguang; Yu, Ailun; Liu, Wei; Chen, Dong; Shen, Bo; Yao, Yafen; Li, Wei; Chen, Xiaolin; Gong, Yongshun; Guan, Bei; Yang, Zhiguang; Wang, Yongji; Cui, Lizhen; Wang, Qianxiang","CodeS: Natural Language to Code Repository via Multi-Layer Sketch","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3768577","https://doi.org/10.1145/3768577","The impressive performance of large language models (LLMs) on code-related tasks has shown the potential of fully automated software development. In light of this, we introduce a new software engineering task, namely Natural Language to code Repository (NL2Repo). This task aims to generate an entire code repository from its natural language requirements. To address this task, we propose a simple yet effective framework CodeS, which decomposes NL2Repo into multiple sub-tasks by a multi-layer sketch. Specifically, CodeS includes three modules: RepoSketcher, FileSketcher, and SketchFiller. RepoSketcher first generates a repository’s directory structure for given requirements; FileSketcher then generates a file sketch for each file in the generated structure; SketchFiller finally fills in the details for each function in the generated file sketch. To rigorously assess CodeS on the NL2Repo task, we carry out evaluations through both automated benchmarking and manual feedback analysis. For benchmark-based evaluation, we craft a repository-oriented benchmark, SketchEval, and design an evaluation metric, SketchBLEU. For feedback-based evaluation, we develop a VSCode plugin for CodeS and engage 30 participants in conducting empirical studies. Extensive experiments prove the effectiveness and practicality of CodeS on the NL2Repo task.","2025-09","2025-11-25 22:29:53","2025-11-25 22:29:53","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Code Large Language Model; Instruction Tuning; Multi-Layer Sketch; Natural Language to Code Repository","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SG8ZD3UZ","journalArticle","2025","Chen, Chunyu; Miao, Zhengjie; Zhang, Yong; Wang, Jiannan","ParSEval: Plan-Aware Test Database Generation for SQL Equivalence Evaluation","Proc. VLDB Endow.","","2150-8097","10.14778/3749646.3749727","https://doi.org/10.14778/3749646.3749727","Deciding query equivalence has played an essential role in many real-world applications, including evaluating the accuracy of text-to-SQL models, where one needs to compare model-generated queries against ground truth queries. Although query equivalence is undecidable in general, researchers have developed two significant approaches to check query equivalence: formal verification-based and test-case-based. Verification-based solutions ensure correctness but may lack support for advanced SQL features and cross-database adaptability. Test cases are versatile but suffer from ad-hoc constraints and potential incorrectness (false positives).In this paper, we propose ParSEval, a Plan-aware SQL Equivalence evaluation framework to generate test database instances for given queries. We observed that existing test data generation methods fail to fully explore the query structure. To address this limitation, ParSEval formally models specific behaviors of each query operator and considers all possible execution paths of the logical query plan by adapting the notion of branch coverage. We validated the effectiveness and efficiency of ParSEval on four datasets with AI-generated and human-crafted queries. The experimental results show that ParSEval supports up to 40% more query pairs than state-of-the-art verification-based approaches. Compared to existing test-case-based approaches, ParSEval reveals more non-equivalent pairs while being 21× faster.","2025-07","2025-11-25 22:29:53","2025-11-25 22:29:53","","4750–4762","","11","18","","","","","","","","","","","","","","","","","Publisher: VLDB Endowment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"63P9MAZL","conferencePaper","2024","Yuan, Yuanyuan; Wang, Shuai; Su, Zhendong","See the Forest, not Trees: Unveiling and Escaping the Pitfalls of Error-Triggering Inputs in Neural Network Testing","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680385","https://doi.org/10.1145/3650212.3680385","Recent efforts in deep neural network (DNN) testing commonly use error-triggering inputs (ETIs) to quantify DNN errors and to fine-tune the tested DNN for repairing. This study reveals the pitfalls of ETIs in DNN testing. Specifically, merely seeking for more ETIs “traps” the testing campaign into local plateaus, where similar ETIs are continuously generated using a few fixed input transformations. Similarly, fine-tuning the DNN with ETIs, while capable of fixing the exposed DNN mis-predictions, undermines the DNN’s resilience towards certain input transformations. However, these ETI-induced pitfalls have been overlooked in previous research, due to the insufficient input transformations (usually &lt; 10), and we show that the severity of such deceptive phenomena is enlarged when testing DNNs with more and diverse real-life input transformations. This paper presents a comprehensive study on the pitfalls of ETIs in DNN testing. We first augment conventional DNN testing pipelines with a large set of input transformations; the correctness and validity of these new transformations are verified with large-scale human studies. Based on this, we show that launching an endless pursuit for ETIs cannot alleviate the “trapped testing” issue, and the undermined resilience pervasively occurs in many input transformations. Accordingly, we propose a novel and holistic viewpoint over DNN errors: instead of counting which input triggers a DNN mis-prediction, we record which input transformation can generate ETIs. The targeted input property of this transformation, termed erroneous property (EP), counts one DNN error and guides DNN testing (i.e., our new paradigm aims to find more EPs rather than ETIs). Evaluation shows that this EP-oriented testing paradigm significantly expands the explored DNN error space. Moreover, fine-tuning DNNs with EPs effectively improves their resilience towards different input transformations.","2024","2025-11-25 22:29:53","2025-11-25 22:29:53","","1605–1617","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Deep learning testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZMAA4B4C","conferencePaper","2024","Silva, André; Saavedra, Nuno; Monperrus, Martin","GitBug-Java: A Reproducible Benchmark of Recent Java Bugs","Proceedings of the 21st International Conference on Mining Software Repositories","979-8-4007-0587-8","","10.1145/3643991.3644884","https://doi.org/10.1145/3643991.3644884","Bug-fix benchmarks are essential for evaluating methodologies in automatic program repair (APR) and fault localization (FL). However, existing benchmarks, exemplified by Defects4J, need to evolve to incorporate recent bug-fixes aligned with contemporary development practices. Moreover, reproducibility, a key scientific principle, has been lacking in bug-fix benchmarks. To address these gaps, we present GitBug-Java, a reproducible benchmark of recent Java bugs. GitBug-Java features 199 bugs extracted from the 2023 commit history of 55 notable open-source repositories. The methodology for building GitBug-Java ensures the preservation of bug-fixes in fully-reproducible environments. We publish GitBug-Java at https://github.com/gitbugactions/gitbug-java.","2024","2025-11-25 22:29:53","2025-11-25 22:29:53","","118–122","","","","","","","MSR '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","software testing; program analysis; bug benchmark; bug database; Java benchmark; reproducibility; software bugs","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5TZL9AP6","journalArticle","2025","Zhang, Xiaoyu; Jiang, Weipeng; Shen, Chao; Li, Qi; Wang, Qian; Lin, Chenhao; Guan, Xiaohong","Deep Learning Library Testing: Definition, Methods and Challenges","ACM Comput. Surv.","","0360-0300","10.1145/3716497","https://doi.org/10.1145/3716497","Recently, software systems powered by deep learning (DL) techniques have significantly facilitated people’s lives in many aspects. As the backbone of these DL systems, various DL libraries undertake the underlying optimization and computation. However, like traditional software, DL libraries are not immune to bugs. These bugs may be propagated to programs and software developed based on DL libraries, thereby posing serious threats to users’ personal property and safety. Studying the characteristics of DL libraries, their associated bugs, and the corresponding testing methods is crucial for enhancing the security of DL systems and advancing the widespread application of DL technology. This paper provides an overview of the testing research on various DL libraries, discusses the strengths and weaknesses of existing methods, and provides guidance and reference for the application of DL library testing methods. This paper first introduces the workflow of DL underlying libraries and the characteristics of three kinds of DL libraries involved, namely DL framework, DL compiler, and DL hardware library. Subsequently, this paper constructs a literature collection pipeline and comprehensively summarizes existing testing methods on these DL libraries to analyze their effectiveness and limitations. It also reports findings and the challenges of existing DL library testing in real-world applications for future research.","2025-03","2025-11-25 22:29:53","2025-11-25 22:29:53","","","","7","57","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","software testing; Deep learning testing; deep learning; deep learning library testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B4WEGFCC","conferencePaper","2024","Plein, Laura","Learning the Effects of Software Changes","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3685550","https://doi.org/10.1145/3650212.3685550","Software development requires several stages of code iterations, each one requiring debugging, testing, localizing and fixing bugs. While several tools have been developed to automate one of those tasks individually, integrating and combining those results still requires a huge manual effort from software developers. Additionally, many approaches, e.g., in Automated Program Repair, are based on specifically curated fix templates that fail to generalize to most complex software projects. To address those challenges, I propose a new research agenda to learn the effects of software changes in order to localize and fix bugs without being limited to a specific project or a specific programming language. Additionally, my approach can be used to predict the effects of software changes. My preliminary results indicate the feasibility of successfully training a model on Python software changes and their effects, that is capable of producing 80% of correct patches and predicting the effect of a change with an accuracy of 81%. My results highlight the potential of learning the effects of software changes for better software development.","2024","2025-11-25 22:29:53","2025-11-25 22:29:53","","1886–1890","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Automated Program Repair; Fault Localization; Code Changes; Effect Prediction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VYZP5ZX7","conferencePaper","2024","He, Ye; Chen, Zimin; Goues, Claire Le","PreciseBugCollector: Extensible, Executable and Precise Bug-Fix Collection","Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering","979-8-3503-2996-4","","10.1109/ASE56229.2023.00163","https://doi.org/10.1109/ASE56229.2023.00163","Bug datasets are vital for enabling deep learning techniques to address software maintenance tasks related to bugs. However, existing bug datasets suffer from precise and scale limitations: they are either small-scale but precise with manual validation or large-scale but imprecise with simple commit message processing. In this paper, we introduce Precise-BugCollector, a precise, multi-language bug collection approach that overcomes these two limitations. PreciseBugCollector is based on two novel components: a) A bug tracker to map the codebase repositories with external bug repositories to trace bug type information, and b) A bug injector to generate project-specific bugs by injecting noise into the correct codebases and then executing them against their test suites to obtain test failure messages.We implement PreciseBugCollector against three sources: 1) A bug tracker that links to the national vulnerability data set (NVD) to collect general-wise vulnerabilities, 2) A bug tracker that links to OSS-Fuzz to collect general-wise bugs, and 3) A bug injector based on 16 injection rules to generate project-wise bugs. To date, PreciseBugCollector comprises 1 057 818 bugs extracted from 2 968 open-source projects. Of these, 12 602 bugs are sourced from bug repositories (NVD and OSS-Fuzz), while the remaining 1 045 216 project-specific bugs are generated by the bug injector. Considering the challenge objectives, we argue that a bug injection approach is highly valuable for the industrial setting, since project-specific bugs align with domain knowledge, share the same codebase, and adhere to the coding style employed in industrial projects.","2024","2025-11-25 22:29:53","2025-11-25 22:29:53","","1899–1910","","","","","","","ASE '23","","","","IEEE Press","Echternach, Luxembourg","","","","","","","","","","","","program repair; bug datasets; software testing and debugging","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"28SCHQL3","journalArticle","2024","Yan, Chuan; Meng, Mark Huasong; Xie, Fuman; Bai, Guangdong","Investigating Documented Privacy Changes in Android OS","Proc. ACM Softw. Eng.","","","10.1145/3660826","https://doi.org/10.1145/3660826","Android has empowered third-party apps to access data and services on mobile devices since its genesis.This involves a wide spectrum of user privacy-sensitive data, such as the device ID and location. In recent years, Android has taken proactive measures to adapt its access control policies for such data, in response to the increasingly strict privacy protection regulations around the world. When each new Android version is released, its privacy changes induced by the version evolution are transparently disclosed, and we refer to them as documented privacy changes (DPCs). Implementing DPCs in Android OS is a non-trivial task, due to not only the dispersed nature of those access control points within the OS, but also the challenges posed by backward compatibility. As a result, whether the actual access control enforcement in the OS implementations aligns with the disclosed DPCs becomes a critical concern. In this work, we conduct the first systematic study on the consistency between the operational behaviors of the OS at runtime and the officially disclosed DPCs. We propose DopCheck, an automatic DPC-driven testing framework equipped with a large language model (LLM) pipeline. It features a serial of analysis to extract the ontology from the privacy change documents written in natural language, and then harnesses the few-shot capability of LLMs to construct test cases for the detection of DPC-compliance issues in OS implementations. We apply DopCheck with the latest versions (10 to 13) of Android Open Source Project (AOSP). Our evaluation involving 79 privacy-sensitive APIs demonstrates that DopCheck can effectively recognize DPCs from Android documentation and generate rigorous test cases. Our study reveals that the status quo of the DPC-compliance issues is concerning, evidenced by 19 bugs identified by DopCheck. Notably, 12 of them are discovered in Android 13 and 6 in Android 10 for the first time, posing more than 35% Android users to the risk of privacy leakage. Our findings should raise an alert to Android users and app developers on the DPC compliance issues when using or developing an app, and would also underscore the necessity for Google to comprehensively validate the actual implementation against its privacy documentation prior to the OS release.","2024-07","2025-11-25 22:29:53","2025-11-25 22:29:53","","","","FSE","1","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","testing; Android; documentation; privacy","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L9X3SNVV","conferencePaper","2024","Qiu, Jie","Search-Based Translations for Tensor Operations","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3685558","https://doi.org/10.1145/3650212.3685558","Tensor operations are essential for various tasks, including image processing, deep learning, and scientific computing. Both hardware and software communities are working to make tensor processing more efficient. This includes creating new hardware and developing domain-specific languages (DSLs). A crucial link between these communities is the compiler. In this work, we propose developing efficient compilers that translate programs written in general-purpose languages into tensor operations, enabling them to benefit from these optimizations. Unlike traditional pattern-matching approaches, our method uses program synthesis and verification to find semantically equivalent translations.","2024","2025-11-25 22:29:53","2025-11-25 22:29:53","","1917–1919","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Program Synthesis; Code Transpilation; Tensor DSLs; Verification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SZ6L6KQC","conferencePaper","2024","Arawjo, Ian; Swoopes, Chelse; Vaithilingam, Priyan; Wattenberg, Martin; Glassman, Elena L.","ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing","Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems","979-8-4007-0330-0","","10.1145/3613904.3642016","https://doi.org/10.1145/3613904.3642016","Evaluating outputs of large language models (LLMs) is challenging, requiring making—and making sense of—many responses. Yet tools that go beyond basic prompting tend to require knowledge of programming APIs, focus on narrow domains, or are closed-source. We present ChainForge, an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs. ChainForge provides a graphical interface for comparison of responses across models and prompt variations. Our system was designed to support three tasks: model selection, prompt template design, and hypothesis testing (e.g., auditing). We released ChainForge early in its development and iterated on its design with academics and online users. Through in-lab and interview studies, we find that a range of people could use ChainForge to investigate hypotheses that matter to them, including in real-world settings. We identify three modes of prompt engineering and LLM hypothesis testing: opportunistic exploration, limited evaluation, and iterative refinement.","2024","2025-11-25 22:29:53","2025-11-25 22:29:53","","","","","","","","","CHI '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Honolulu, HI, USA","","","","prompt engineering; language models; auditing; toolkits; visual programming environments","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PSRMJEVH","conferencePaper","2024","Colavito, Giuseppe; Lanubile, Filippo; Novielli, Nicole; Quaranta, Luigi","Leveraging GPT-like LLMs to Automate Issue Labeling","Proceedings of the 21st International Conference on Mining Software Repositories","979-8-4007-0587-8","","10.1145/3643991.3644903","https://doi.org/10.1145/3643991.3644903","Issue labeling is a crucial task for the effective management of software projects. To date, several approaches have been put forth for the automatic assignment of labels to issue reports. In particular, supervised approaches based on the fine-tuning of BERT-like language models have been proposed, achieving state-of-the-art performance. More recently, decoder-only models such as GPT have become prominent in SE research due to their surprising capabilities to achieve state-of-the-art performance even for tasks they have not been trained for. To the best of our knowledge, GPT-like models have not been applied yet to the problem of issue classification, despite the promising results achieved for many other software engineering tasks. In this paper, we investigate to what extent we can leverage GPT-like LLMs to automate the issue labeling task. Our results demonstrate the ability of GPT-like models to correctly classify issue reports in the absence of labeled data that would be required to fine-tune BERT-like LLMs.","2024","2025-11-25 22:29:53","2025-11-25 22:29:53","","469–480","","","","","","","MSR '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","LLM; GPT; software maintenance and evolution; issue labeling; labeling unstructured data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GWBDGXQJ","bookSection","2025","Xu, Ke; Sun, Jialin; Hu, Yuchen; Fang, Xinwei; Shan, Weiwei; Wang, Xi; Jiang, Zhe","MEIC: Re-thinking RTL Debug Automation using LLMs","Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design","979-8-4007-1077-3","","","https://doi.org/10.1145/3676536.3676801","The deployment of Large Language Models (LLMs) for code debugging (e.g., C and Python) is widespread, benefiting from their ability to understand and interpret intricate concepts. However, in the semiconductor industry, utilising LLMs to debug Register Transfer Level (RTL) code is still insufficient, largely due to the underrepre-sentation of RTL-specific data in training sets. This work introduces a novel framework, Make Each Iteration Count (MEIC), which contrasts with traditional one-shot LLM-based debugging methods that heavily rely on prompt engineering, model tuning, and model training. MEIC utilises LLMs in an iterative process to overcome the limitation of LLMs in RTL code debugging, which is suitable for identifying and correcting both syntax and function errors, while effectively managing the uncertainties inherent in LLM operations. To evaluate our framework, we provide an open-source dataset comprising 178 common RTL programming errors. The experimental results demonstrate that the proposed debugging framework achieves fix rate of 93% for syntax errors and 78% for function errors, with up to 48x speedup in debugging processes when compared with experienced engineers. The Repo. of dataset and code: https://github.com/SEU-ACAL/reproduce-MEIC-ICCAD.","2025","2025-11-25 22:29:53","2025-11-25 22:29:53","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q2JGKUMB","conferencePaper","2024","Xie, Maoyi; Hu, Ming; Kong, Ziqiao; Zhang, Cen; Feng, Yebo; Wang, Haijun; Xue, Yue; Zhang, Hao; Liu, Ye; Liu, Yang","DeFort: Automatic Detection and Analysis of Price Manipulation Attacks in DeFi Applications","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3652137","https://doi.org/10.1145/3650212.3652137","Although Decentralized Finance (DeFi) applications facilitate tamper-proof transactions among multiple anonymous users, since attackers can access the smart contract bytecode directly, vulnerabilities in the transaction mechanism, contract code, or third-party components can be easily exploited to manipulate token prices, leading to financial losses. Since price manipulation often relies on specific states and complex trading sequences, existing detection tools have limitations in addressing this problem. In addition, to swiftly identify the root cause of an attack and implement targeted defense and remediation measures, auditors typically prioritize understanding the methodology behind the attack, emphasizing 'how' it occurred rather than simply confirming its existence. To address these problems, this paper presents a novel automatic price manipulation detection and analysis framework, named DeFort, which contains a price manipulation behavior model to guide on-chain detection, multiple price monitoring strategies to detect pools with abnormal token prices, and various profit calculation mechanisms to confirm attacks. Based on behavioral models, DeFort can automatically locate transactions and functions that cause abnormal price fluctuations and identify attackers and victims. Experimental results demonstrate that DeFort can outperform state-of-the-art price manipulation detection methods. Furthermore, after monitoring 441 real-world projects for two months, DeFort successfully detected five price manipulation attacks.","2024","2025-11-25 22:29:53","2025-11-25 22:29:53","","402–414","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","blockchain; decentralized finance (DeFi); price manipulation attack; smart contract","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QUU9NHAT","conferencePaper","2025","Rathore, Santosh Singh; Tiwari, Saurabh; Farooq, Sheikh Umar","Seventh Workshop on Emerging Software Engineering Education(WESEE 2025)","Proceedings of the 18th Innovations in Software Engineering Conference","979-8-4007-1424-5","","10.1145/3717383.3721236","https://doi.org/10.1145/3717383.3721236","The seventh Workshop on Emerging Software Engineering Education (WESEE) aims to discuss and examine the development of learning environments that are influencing the pedagogical strategies for the education of software engineering courses in institutions, specifically through the adoption of Generative AI (GenAI) tools and techniques. Additionally, the workshop aims to examine how industries are utilizing GenAI tools and technologies for teaching software development methods and how the developers are utilizing the material for self-learning and skill acquisition. The report is an overview of the upcoming seventh edition of WESEE, which will be held on 20th February 2025 at NIT Kurukshetra. The workshop will be held alongside the 18th Innovations in Software Engineering Conference (ISEC 2025).","2025","2025-11-25 22:29:53","2025-11-25 22:29:53","","","","","","","","","ISEC '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RGG92Q2Z","conferencePaper","2024","Kulsum, Ummay; Zhu, Haotian; Xu, Bowen; d'Amorim, Marcelo","A Case Study of LLM for Automated Vulnerability Repair: Assessing Impact of Reasoning and Patch Validation Feedback","Proceedings of the 1st ACM International Conference on AI-Powered Software","979-8-4007-0685-1","","10.1145/3664646.3664770","https://doi.org/10.1145/3664646.3664770","Recent work in automated program repair (APR) proposes the use of reasoning and patch validation feedback to reduce the semantic gap between the LLMs and the code under analysis. The idea has been shown to perform well for general APR, but its effectiveness in other particular contexts remains underexplored. In this work, we assess the impact of reasoning and patch validation feedback to LLMs in the context of vulnerability repair, an important and challenging task in security. To support the evaluation, we present VRpilot, an LLM-based vulnerability repair technique based on reasoning and patch validation feedback. VRpilot (1) uses a chain-of-thought prompt to reason about a vulnerability prior to generating patch candidates and (2) iteratively refines prompts according to the output of external tools (e.g., compiler, code sanitizers, test suite, etc.) on previously generated patches. To evaluate performance, we compare VRpilot against the state-of-the-art vulnerability repair techniques for C and Java using public datasets from the literature. Our results show that VRpilot generates, on average, 14% and 7.6% more correct patches than the baseline techniques on C and Java, respectively. We show, through an ablation study, that reasoning and patch validation feedback are critical. We report several lessons from this study and potential directions for advancing LLM-empowered vulnerability repair.","2024","2025-11-25 22:29:53","2025-11-25 22:29:53","","103–111","","","","","","","AIware 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","Large Language Models; Automated Vulnerability Repair","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W8FRBG6R","conferencePaper","2024","Zhou, Zhuotong; Yang, Yongzhuo; Wu, Susheng; Huang, Yiheng; Chen, Bihuan; Peng, Xin","Magneto: A Step-Wise Approach to Exploit Vulnerabilities in Dependent Libraries via LLM-Empowered Directed Fuzzing","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695531","https://doi.org/10.1145/3691620.3695531","The wide adoption of open source third-party libraries can propagate vulnerabilities that originally exist in third-party libraries through dependency chains to downstream projects. To mitigate this security risk, vulnerability exploitation analysis has been proposed to further reduce false positives of vulnerability reachability analysis. However, existing approaches work less effectively when the vulnerable function of the vulnerable library is indirectly invoked by a client project through a call chain of multiple steps.To address this problem, we propose a step-wise approach, named Magneto, to exploit vulnerabilities in dependent libraries of a client project through LLM-empowered directed fuzzing. Its core idea is to decompose the directed fuzzing for the whole call chain (from the client project to the vulnerable function) into a series of step-wise directed fuzzing for each step of the call chain. To empower directed fuzzing, it leverages LLM to facilitate the initial seed generation. Our evaluation has demonstrated the effectiveness of Magneto over the state-of-the-art; i.e., Magneto achieves an improvement of at least 75.6% in successfully exploiting the vulnerability.","2024","2025-11-25 22:29:53","2025-11-25 22:47:41","","1633–1644","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","Software engineering; Fuzzing; Security; Libraries; directed fuzzing; exploit generation; library vulnerabilities; Privacy; Computer languages; Reachability analysis; Magnetic analysis; Magnetic cores","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EZY22J3N","conferencePaper","2023","Balse, Rishabh; Kumar, Viraj; Prasad, Prajish; Warriem, Jayakrishnan Madathil","Evaluating the Quality of LLM-Generated Explanations for Logical Errors in CS1 Student Programs","Proceedings of the 16th Annual ACM India Compute Conference","979-8-4007-0840-4","","10.1145/3627217.3627233","https://doi.org/10.1145/3627217.3627233","When students in CS1 (Introductory Programming) write erroneous code, course staff can use automated tools to provide various types of helpful feedback. In this paper, we focus on syntactically correct student code containing logical errors. Tools that explain logical errors typically require course staff to invest greater effort than tools that detect such errors. To reduce this effort, prior work has investigated the use of Large Language Models (LLMs) such as GPT-3 to generate explanations. Unfortunately, these explanations can be incomplete or incorrect, and therefore unhelpful if presented to students directly. Nevertheless, LLM-generated explanations may be of adequate quality for Teaching Assistants (TAs) to efficiently craft helpful explanations on their basis. We evaluate the quality of explanations generated by an LLM (GPT-3.5-turbo) in two ways, for 30&nbsp;buggy student solutions across 6&nbsp;code-writing problems. First, in a study with 5&nbsp;undergraduate TAs, we compare TA perception of LLM-generated and peer-generated explanation quality. TAs were unaware which explanations were LLM-generated, but they found them to be comparable in quality to peer-generated explanations. Second, we performed a detailed manual analysis of LLM-generated explanations for all 30&nbsp;buggy solutions. We found at least one incorrect statement in 15/30 explanations (50%). However, in 28/30 cases (93%), the LLM-generated explanation correctly identified at least one logical error. Our results suggest that for large CS1 courses, TAs with adequate training to detect erroneous statements may be able to extract value from such explanations.","2023","2025-11-25 22:29:53","2025-11-25 22:29:53","","49–54","","","","","","","COMPUTE '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Hyderabad, India","","","","Explanation; GPT-3.5-Turbo; Large language models (LLMs); Logical Errors; Python Programming","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TZE9K9LX","conferencePaper","2024","Qayyum, Khushboo; Hassan, Muhammad; Ahmadi-Pour, Sallar; Jha, Chandan Kumar; Drechsler, Rolf","Late Breaking Results: LLM-assisted Automated Incremental Proof Generation for Hardware Verification","Proceedings of the 61st ACM/IEEE Design Automation Conference","979-8-4007-0601-1","","10.1145/3649329.3663498","https://doi.org/10.1145/3649329.3663498","In this paper, we propose a methodology for hardware verification assisted by Large Language Models (LLMs) in the incremental proof generation process. First, an LLM identifies the basic module of the Design Under Verification (DUV), followed by expanding the proof scope as more modules are added. LLMs assist in defining and verifying invariants for each module using the Z3 solver, and in formulating integration properties at module interfaces. Our case studies on a Ripple Carry Adder (RCA) and a Dadda Tree Multiplier (DTM) demonstrate that LLMs enhance the efficiency and accuracy of hardware verification.","2024","2025-11-25 22:29:54","2025-11-25 22:29:54","","","","","","","","","DAC '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: San Francisco, CA, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A3Z7YUGR","conferencePaper","2024","Wu, Shuohan; Li, Zihao; Zhou, Hao; Luo, Xiapu; Li, Jianfeng; Wang, Haoyu","Following the “Thread”: Toward Finding Manipulatable Bottlenecks in Blockchain Clients","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680372","https://doi.org/10.1145/3650212.3680372","Blockchain clients are the fundamental element of the blockchain network, each keeping a copy of the blockchain’s ledger. They play a crucial role in ensuring the network’s decentralization, integrity, and stability. As complex software systems, blockchain clients are not exempt from bottlenecks. Some bottlenecks create new attack surfaces, where attackers deliberately overload these weak points to congest client’s execution, thereby causing denial of service (DoS). We call them manipulatable bottlenecks. Existing research primarily focuses on a few such bottlenecks, and heavily relies on manual analysis. To the best of our knowledge, there has not been any study proposing a systematic approach to identify manipulatable bottlenecks in blockchain clients. To bridge the gap, this paper delves into the primary causes of bottlenecks in software, and develops a novel tool named ThreadNeck to monitor the symptoms that signal these issues during client runtime. ThreadNeck models the clients as a number of threads, delineating their inter-relationship to accurately characterize the client’s behavior. Building on this, we can identify the suspicious bottlenecks and determine if they could be exploited by external attackers. After applying ThreadNeck to four mainstream clients developed in different programming languages, we totally discover 13 manipulatable bottlenecks, six of which are previously unknown.","2024","2025-11-25 22:29:54","2025-11-25 22:29:54","","1440–1452","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Blockchain; Bottleneck; DoS Attack","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D3IVXSSV","conferencePaper","2025","Zhao, Gengwu","Automated Detection and Refactoring of Mock Clones in Java Projects","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings","979-8-3315-3683-1","","10.1109/ICSE-Companion66252.2025.00035","https://doi.org/10.1109/ICSE-Companion66252.2025.00035","Mocking is an essential technique in unit testing, allowing developers to isolate dependencies for more focused and reliable tests. However, repetitive patterns of mock creation, termed mock clones, introduce significant redundancy, increasing maintenance costs, reducing readability, and complicating updates in test code. To address these challenges, our research investigates the detection and refactoring of mock clones, aiming to improve the maintainability and efficiency of test code in large software projects.We began with an empirical study of mocking framework usage across open-source Java projects, revealing widespread adoption of frameworks like Mockito, along with patterns and practices that commonly lead to mock clones. Following this, we analyzed the prevalence and impact of mock clones across six representative projects, identifying significant redundancy and demonstrating the limitations of traditional clone detection tools. Manual refactoring of identified mock clones reduced thousands of redundant lines, with developers responding positively to these improvements.Building on these foundational studies, our ongoing work seeks to develop automated tools for detecting and refactoring mock clones, enabling systematic and scalable improvements to test code. This research offers both empirical insights and practical solutions that contribute toward automated test code maintenance, advancing sustainable software testing practices.","2025","2025-11-25 22:29:54","2025-11-25 22:29:54","","112–116","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","unit testing; refactoring; code clone detection; mock clones","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HZ4UFJKA","journalArticle","2025","Zeng, Yishun; Wu, Yue; Zhang, Chao","TypeNFuzz: Dynamic Type-aware Object Dependence Graph Guided Fuzzing for JavaScript Library Bug Discovery","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3765760","https://doi.org/10.1145/3765760","Node.js owes much of its popularity to an enormous library ecosystem. While this abundance speeds development, widely varying code quality complicates efforts to assure robustness. Effective library testing is hard for two reasons: (1) dynamic typing obscures the construction of valid, complex inputs, and (2) crucial internal logic is hidden behind shallow exports, making deep paths difficult to reach. Existing tools handle neither challenge well. To address these challenges and elevate library quality, we propose TypeNFuzz, a novel testing approach for Node.js libraries. TypeNFuzz integrates: 1. Type-driven Input Synthesis: mines TypeScript declaration files to build semantically correct objects, defeating dynamic-typing ambiguities. 2. Deep Reachability Exploration: fuzzing guided by a dynamic and type-aware object dependence graph, systematically triggering execution deep within the internal code paths to uncover deep logic. The evaluation demonstrates the effectiveness of TypeNFuzz: it achieves 1.70 - 6.74 times higher code coverage than state-of-the-art tools, directly attributed to its ability to handle complex types and penetrate deep logic. Critically, it uncovered 77 previously unknown defects in popular built-in and third-party libraries, significantly contributing to improved robustness and stability.","2025-09","2025-11-25 22:29:54","2025-11-25 22:29:54","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Testing; Fuzzing; Defect Detection; JavaScript; js; Node","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"799N9RKN","conferencePaper","2024","Li, Kaixuan; Zhang, Jian; Chen, Sen; Liu, Han; Liu, Yang; Chen, Yixiang","PatchFinder: A Two-Phase Approach to Security Patch Tracing for Disclosed Vulnerabilities in Open-Source Software","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680305","https://doi.org/10.1145/3650212.3680305","Open-source software (OSS) vulnerabilities are increasingly prevalent, emphasizing the importance of security patches. However, in widely used security platforms like NVD, a substantial number of CVE records still lack trace links to patches. Although rank-based approaches have been proposed for security patch tracing, they heavily rely on handcrafted features in a single-step framework, which limits their effectiveness. In this paper, we propose PatchFinder, a two-phase framework with end-to-end correlation learning for better-tracing security patches. In the initial retrieval phase, we employ a hybrid patch retriever to account for both lexical and semantic matching based on the code changes and the description of a CVE, to narrow down the search space by extracting those commits as candidates that are similar to the CVE descriptions. Afterwards, in the re-ranking phase, we design an end-to-end architecture under the supervised fine-tuning paradigm for learning the semantic correlations between CVE descriptions and commits. In this way, we can automatically rank the candidates based on their correlation scores while maintaining low computation overhead. We evaluated our system against 4,789 CVEs from 532 OSS projects. The results are highly promising: PatchFinder achieves a Recall@10 of 80.63% and a Mean Reciprocal Rank (MRR) of 0.7951. Moreover, the Manual Effort@10 required is curtailed to 2.77, marking a 1.94 times improvement over current leading methods. When applying PatchFinder in practice, we initially identified 533 patch commits and submitted them to the official, 482 of which have been confirmed by CVE Numbering Authorities.","2024","2025-11-25 22:29:54","2025-11-25 22:29:54","","590–602","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Large language models; Patch ranking; Security patches","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F29TLVFT","conferencePaper","2024","Xu, Yihui; Li, Yanhui; Wang, Jun; Zhang, Xiaofang","Evaluating Terminology Translation in Machine Translation Systems via Metamorphic Testing","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695069","https://doi.org/10.1145/3691620.3695069","Machine translation has become an integral part of daily life, with terminology translation playing a crucial role in ensuring the accuracy of translation results. However, existing translation systems, such as Google Translate, have been shown to occasionally produce errors in terminology translation. Current metrics for assessing terminology translation rely on reference translations and bilingual dictionaries, limiting their effectiveness in large-scale automated MT system testing.To address this challenge, we propose a novel method: Metamorphic Testing for Terminology Translation (TermMT), which achieves effective and efficient testing for terminology translation in MT systems without relying on reference translations or bilingual terminology dictionaries. Our approach involves constructing metamorphic relations based on the characteristics of terms: (a) adding an appropriate reference of the term in the given context would not change the translation of the term; (b) if we modify part of a multi-word term, the translation of the revised word combination would change. To evaluate the effectiveness of TermMT, we tested the terminology translation capabilities of three machine translation systems, Google Translate, Bing Microsoft Translator, and mBART, using the English portion of the bilingual UM-corpus dataset. The results show that TermMT detected a total of 3,765 translation errors on Google Translate, 2,351 on Bing Microsoft Translator, and 6,011 on mBART, with precisions of 82.33%, 83.00%, and 86.33%, respectively.","2024","2025-11-25 22:29:54","2025-11-25 22:29:54","","758–769","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","software testing; metamorphic testing; machine translation; terminology translation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"STTH97DV","conferencePaper","2025","Panichella, Annibale","Metamorphic-Based Many-Objective Distillation of LLMs for Code-Related Tasks","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00230","https://doi.org/10.1109/ICSE55347.2025.00230","Knowledge distillation compresses large language models (LLMs) into more compact and efficient versions that achieve similar accuracy on code-related tasks. However, as we demonstrate in this study, compressed models are four times less robust than the original LLMs when evaluated with metamorphic code. They exhibit a 440% higher probability of misclassifying code clones due to minor changes in the code fragment under analysis, such as replacing parameter names with synonyms. To address this issue, we propose Morph, a novel method that combines metamorphic testing with many-objective optimization for a robust distillation of LLMs for code. Morph efficiently explores the models' configuration space and generates Pareto-optimal models that effectively balance accuracy, efficiency, and robustness to metamorphic code. Metamorphic testing measures robustness as the number of code fragments for which a model incorrectly makes different predictions between the original and their equivalent metamorphic variants (prediction flips). We evaluate Morph on two tasks—code clone and vulnerability detection—targeting CodeBERT and GraphCodeBERT for distillation. Our comparison includes Morph, the state-of-the-art distillation method Avatar, and the fine-tuned non-distilled LLMs. Compared to Avatar, Morph produces compressed models that are (i) 47% more robust, (ii) 25% more efficient (fewer floating-point operations), while maintaining (iii) equal or higher accuracy (up to +6%), and (iv) similar model size.","2025","2025-11-25 22:29:54","2025-11-25 22:29:54","","1001–1013","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","large language models; AI for SE; metamorphic testing; green-AI; knowledge distillation; many-objective optimization; search-based software engineering; sustainability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FBJCUTTJ","journalArticle","2025","O'leary, Daniel E.","Using Large Language Models for Armchair Auditors","Digit. Gov.: Res. Pract.","","","10.1145/3676280","https://doi.org/10.1145/3676280","Armchair auditors are citizens who use open data to investigate and monitor government activities, typically using analytics and other approaches. Armchair auditors provide a valuable role in holding governments and organizations accountable. This paper investigates the potential use of large language models (LLM) to support armchair auditor analyses of different governmental entities. Unfortunately, the literature, prior to the development of LLM suggested several challenges for armchair auditors. However, the analysis in this paper suggests that LLM can provide substantial data and analytic process support for armchair auditors mitigating issues such as providing guidelines for analyses, guiding users to appropriate communities, suggesting potential data availability opportunities, doing analysis, and other issues. As part of an approach to unifying armchair auditor searches, this paper also suggests a prompt library designed to support, standardize and promote best practice analyzes among armchair auditors. In addition to these issues, this paper also analyzes emerging ethical issues associated with armchair auditors and their use of open data and LLMs. Finally, this paper extends the activity theory model to account for LLMs.","2025-06","2025-11-25 22:29:54","2025-11-25 22:29:54","","","","2","6","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","ChatGPT; large language models; Activity theory; armchair auditor; BARD; open data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KW9RRSQI","book","2024","","L@S '24: Proceedings of the Eleventh ACM Conference on Learning @ Scale","","979-8-4007-0633-2","","","","It is our great pleasure to present the Proceedings of the Eleventh Annual ACM Conference on Learning at Scale, L@S 2024, held July 18-20, 2024 at Georgia Tech in Atlanta, Georgia, USA.The Learning at Scale conference was created by the Association for Computing Machinery (ACM), inspired by the emergence of Massive Open Online Courses (MOOCs) and the accompanying shift in thinking about education. During the last few years, new opportunities for scaling up learning have emerged, like hybrid learning environments combining online and face-to-face, and informal learning enabled by all sorts of platforms (e.g., gamified language learning, citizen science communities, and collaborative programming communities). In the recent two years, the unprecedented development of generative AI has brought profound opportunities to scale the teaching and learning experiences, with the goal of enhancing learning for the increasingly diverse group of learners in both formal and informal contexts. L@S has evolved along with these emergent massive learning scenarios and opportunities and is today one of the most prominent venues for discussion of the highest quality of research on how learning and teaching can be transformed at scale, in diverse learning environments.The theme of L@S 2024 is Scaling Learning in the Age of AI. Rapid advances in AI have created new opportunities but also challenges for the Learning@Scale community. The advances in generative AI show potential to enhance pedagogical practices and the efficacy of learning at scale. This has led to an unprecedented level of interest in employing generative AI for scaling tutoring and feedback. The prevalence of such tools calls for new practices and understanding on how AI-based methods should be designed and developed to enhance the experiences and outcomes of teachers and learners.Learning@Scale 2024 solicits empirical and theoretical papers on, but not limited to, the following topics (in no particular order): 1) Instruction at scale: studies that examine how teachers and educators scale their instructions, what aspects of instruction could be scaled effectively, and which of these instructional strategies are the most effective for learning. 2) Interventions at scale: studies that examine the effects of interventions on student learning and performance when implemented at scale. We welcome studies that use both qualitative and quantitative methods. 3) The use of generative AI to scale learning: studies that investigate stakeholders' experiences with generative AI, students' and teachers' interactions with generative AI, and the potentials and limitations of using generative AI in education. 4) Systems and tools to support learning at scale: research that designs and develops systems and tools to support learning at scale. For example, this involves scaling learning through web-based systems, MOOCs, visualization, intelligent tutoring systems, gamification, immersive techniques (AR/VR/MR), mobile technologies, tangible interfaces, and various other technologies. 5) The evaluation of existing learning at scale systems and online learning environments using but not limited to the above-mentioned technologies. 6) Methods and algorithms that model learner behavior: research that contributes methods, algorithms, and pipelines that process large student data to enhance learning at scale. 7) Scaling learning in informal contexts: studies that explore how people take advantage of online environments to pursue their interests informally. 8) Review and synthesis of existing literature related to learning at scale. 9) Empirical studies and interventions that address equity, trust, algorithmic transparency and explainability, fairness and bias when using AI in education. 10) Research that addresses accessibility in learning at scale contexts. 11) Design and deployment of learning at scale systems for learners from underrepresented groups.","2024","2025-11-25 22:29:54","2025-11-25 22:29:54","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MAR3PT9X","conferencePaper","2024","Sun, Zhensu; Du, Xiaoning; Luo, Xiapu; Song, Fu; Lo, David; Li, Li","FDI: Attack Neural Code Generation Systems through User Feedback Channel","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680300","https://doi.org/10.1145/3650212.3680300","Neural code generation systems have recently attracted increasing attention to improve developer productivity and speed up software development. Typically, these systems maintain a pre-trained neural model and make it available to general users as a service (e.g., through remote APIs) and incorporate a feedback mechanism to extensively collect and utilize the users' reaction to the generated code, i.e., user feedback. However, the security implications of such feedback have not yet been explored. With a systematic study of current feedback mechanisms, we find that feedback makes these systems vulnerable to feedback data injection (FDI) attacks. We discuss the methodology of FDI attacks and present a pre-attack profiling strategy to infer the attack constraints of a targeted system in the black-box setting. We demonstrate two proof-of-concept examples utilizing the FDI attack surface to implement prompt injection attacks and backdoor attacks on practical neural code generation systems. The attacker may stealthily manipulate a neural code generation system to generate code with vulnerabilities, attack payload, and malicious and spam messages. Our findings reveal the security implications of feedback mechanisms in neural code generation systems, paving the way for increasing their security.","2024","2025-11-25 22:29:54","2025-11-25 22:29:54","","528–540","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Code Generation; Data Poisoning; User Feedback","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JDWB5WPX","conferencePaper","2024","Denzler, Benjamin; Vahid, Frank; Pang, Ashley; Salloum, Mariam","Style Anomalies Can Suggest Cheating in CS1 Programs","Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1","979-8-4007-0600-4","","10.1145/3649217.3653626","https://doi.org/10.1145/3649217.3653626","Student cheating on at-home programming assignments is a well- known problem. A key contributor is externally-obtained solutions from websites, contractors, and recently generative AI. In our experience, such externally-obtained solutions often use coding styles that depart from a class' style, which we call ""style anomalies,"" such as using untaught or advanced constructs like pointers or ternary operators, or having different indenting or brace usage from the class style. We developed a tool to auto-count style anomalies. For six labs across four terms in 2021-2022, and 50 sampled students per lab, we found 18% of submissions on average had unusually-high style anomaly counts. Importantly, 8% of submissions on average had a high style anomaly count but were not flagged by a similarity checker, meaning 8% of submissions are suspicious but might have been missed if using similarity checking alone. We repeated a similar analysis for Spring 2023 when generative AI (ChatGPT) was gaining popularity, and the numbers rose to 26% and 18%, respectively. Detailed investigations by instructors led to a majority (but not all) high style anomaly submissions being deemed cheating. Even for high-similarity submissions, counting style anomalies can help instructors focus investigations on the most-likely cheating cases, and can strengthen cases sent to student conduct offices. With the rise of externally-obtained solutions from websites, contractors, and generative AI, counting style anomalies may become an increasingly important complement to similarity checking; in fact, it is now the primary cheat-detection tool in our CS1 at a large state university, with similarity secondary.","2024","2025-11-25 22:29:54","2025-11-25 22:29:54","","381–387","","","","","","","ITiCSE 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Milan, Italy","","","","cs1; cheating; plagiarism; program autograders; program style","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZX2W3J6I","conferencePaper","2024","Zhou, Mingyi; Gao, Xiang; Liu, Pei; Grundy, John; Chen, Chunyang; Chen, Xiao; Li, Li","Model-less Is the Best Model: Generating Pure Code Implementations to Replace On-Device DL Models","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3652119","https://doi.org/10.1145/3650212.3652119","Recent studies show that on-device deployed deep learning (DL) models, such as those of Tensor Flow Lite (TFLite), can be easily extracted from real-world applications and devices by attackers to generate many kinds of adversarial and other attacks. Although securing deployed on-device DL models has gained increasing attention, no existing methods can fully prevent these attacks. Traditional software protection techniques have been widely explored. If on-device models can be implemented using pure code, such as C++, it will open the possibility of reusing existing robust software protection techniques. However, due to the complexity of DL models, there is no automatic method that can translate DL models to pure code. To fill this gap, we propose a novel method, CustomDLCoder, to automatically extract on-device DL model information and synthesize a customized executable program for a wide range of DL models. CustomDLCoder first parses the DL model, extracts its backend computing codes, configures the extracted codes, and then generates a customized program to implement and deploy the DL model without explicit model representation. The synthesized program hides model information for DL deployment environments since it does not need to retain explicit model representation, preventing many attacks on the DL model. In addition, it improves ML performance because the customized code removes model parsing and preprocessing steps and only retains the data computing process. Our experimental results show that CustomDLCoder improves model security by disabling on-device model sniffing. Compared with the original on-device platform (i.e., TFLite), our method can accelerate model inference by 21.0% and 24.3% on x86-64 and ARM64 platforms, respectively. Most importantly, it can significantly reduce memory consumption by 68.8% and 36.0% on x86-64 and ARM64 platforms, respectively.","2024","2025-11-25 22:29:54","2025-11-25 22:29:54","","174–185","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","AI safety; SE for AI; software optimization for AI deployment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6CUDCBTR","conferencePaper","2024","Batten, Christopher; Pinckney, Nathaniel; Liu, Mingjie; Ren, Haoxing; Khailany, Brucek","PyHDL-Eval: An LLM Evaluation Framework for Hardware Design Using Python-Embedded DSLs","Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD","979-8-4007-0699-8","","10.1145/3670474.3685948","https://doi.org/10.1145/3670474.3685948","Embedding hardware design frameworks within Python is a promising technique to improve the productivity of hardware engineers. At the same time, there is significant interest in using large-language models (LLMs) to improve key chip design tasks. This paper describes PyHDL-Eval, a new framework for evaluating LLMs on specification-to-RTL tasks in the context of Python-embedded domain-specific languages (DSLs). The framework includes 168 problems, Verilog reference solutions, Verilog test benches, Python test scripts, and workflow orchestration scripts. We use the framework to conduct a detailed case study comparing five LLMs (CodeGemma 7B, Llama3 8B/70B, GPT4, and GPT4 Turbo) targeting Verilog and five Python-embedded DSLs (PyMTL3, PyRTL, MyHDL, Migen, and Amaranth). Our results demonstrate the promise of in-context learning when applied to smaller models (e.g., pass rate for CodeGemma 7B improves from 14.9% to 32.7% on Verilog) and Python-embedded DSLs (e.g., pass rate for LLama3 70B improves from 0.6% to 33.0% on PyMTL3). We find LLMs perform better when targeting Verilog as compared to Python-embedded DSLs (e.g., pass rate for GPT4 Turbo is 72.2% on Verilog and 29.8-62.0% on the Python-embedded DSLs) despite using a popular general-purpose host language. PyHDL-Eval will serve as a useful framework for future research at the intersection of Python-embedded DSLs and LLMs.","2024","2025-11-25 22:29:54","2025-11-25 22:29:54","","","","","","","","","MLCAD '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Salt Lake City, UT, USA","","","","large language models; hardware description languages; Python-embedded domain-specific languages","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R2TJLEEM","conferencePaper","2024","Fakhoury, Sarah; Naik, Aaditya; Sakkas, Georgios; Chakraborty, Saikat; Musuvathi, Madan; Lahiri, Shuvendu","Exploring the Effectiveness of LLM based Test-driven Interactive Code Generation: User Study and Empirical Evaluation","Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings","979-8-4007-0502-1","","10.1145/3639478.3643525","https://doi.org/10.1145/3639478.3643525","We introduce a novel workflow, TiCoder, designed to enhance the trust and accuracy of LLM-based code generation through interactive and guided intent formalization. TiCoder partially formalizes ambiguous intent in natural language prompts by generating a set of tests to distinguish common divergent behaviours in generated code suggestions. We evaluate the code generation accuracy improvements provided by TiCoder at scale across four competitive LLMs, and evaluate the cost-benefit trade off of evaluating tests surfaced by TiCoder through a user study with 15 participants.","2024","2025-11-25 22:29:54","2025-11-25 22:29:54","","390–391","","","","","","","ICSE-Companion '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I4AWUA5A","conferencePaper","2023","Happe, Andreas; Cito, Jürgen","Understanding Hackers’ Work: An Empirical Study of Offensive Security Practitioners","Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","979-8-4007-0327-0","","10.1145/3611643.3613900","https://doi.org/10.1145/3611643.3613900","Offensive security-tests are commonly employed to pro-actively discover potential vulnerabilities. They are performed by specialists, also known as penetration-testers or white-hat hackers. The chronic lack of available white-hat hackers prevents sufficient security test coverage of software. Research into automation tries to alleviate this problem by improving the efficiency of security testing. To achieve this, researchers and tool builders need a solid understanding of how hackers work, their assumptions, and pain points. In this paper, we present a first data-driven exploratory qualitative study of twelve security professionals, their work and problems occurring therein. We perform a thematic analysis to gain insights into the execution of security assignments, hackers' thought processes and encountered challenges. This analysis allows us to conclude with recommendations for researchers and tool builders, to increase the efficiency of their automation and identify novel areas for research.","2023","2025-11-25 22:29:54","2025-11-25 22:29:54","","1669–1680","","","","","","","ESEC/FSE 2023","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: San Francisco, CA, USA","","","","software testing; ethical hacking; offensive security testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RVLHIJEZ","conferencePaper","2022","Weisz, Justin D.; Muller, Michael; Ross, Steven I.; Martinez, Fernando; Houde, Stephanie; Agarwal, Mayank; Talamadupula, Kartik; Richards, John T.","Better Together? An Evaluation of AI-Supported Code Translation","Proceedings of the 27th International Conference on Intelligent User Interfaces","978-1-4503-9144-3","","10.1145/3490099.3511157","https://doi.org/10.1145/3490099.3511157","Generative machine learning models have recently been applied to source code, for use cases including translating code between programming languages, creating documentation from code, and auto-completing methods. Yet, state-of-the-art models often produce code that is erroneous or incomplete. In a controlled study with 32 software engineers, we examined whether such imperfect outputs are helpful in the context of Java-to-Python code translation. When aided by the outputs of a code translation model, participants produced code with fewer errors than when working alone. We also examined how the quality and quantity of AI translations affected the work process and quality of outcomes, and observed that providing multiple translations had a larger impact on the translation process than varying the quality of provided translations. Our results tell a complex, nuanced story about the benefits of generative code models and the challenges software engineers face when working with their outputs. Our work motivates the need for intelligent user interfaces that help software engineers effectively work with generative code models in order to understand and evaluate their outputs and achieve superior outcomes to working alone.","2022","2025-11-25 22:29:54","2025-11-25 22:29:54","","369–391","","","","","","","IUI '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Helsinki, Finland","","","","generative AI; imperfect AI; Code translation; human-AI co-creation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VWMV3GSR","conferencePaper","2024","Lang, Zhe; Xu, Zhengzi; Chen, Xiaohui; Lv, Shichao; Song, Zhanwei; Shi, Zhiqiang; Sun, Limin","DeLink: Source File Information Recovery in Binaries","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680338","https://doi.org/10.1145/3650212.3680338","Program comprehension can help analysts understand the primary behavior of a binary and enhance the efficiency of reverse engineering analysis. The existing works focus on instruction translation and function name prediction. However, they are limited in understanding the entire program. The recovered source file information can offer insights into the primary behavior of a binary, serving as high-level program summaries. Nevertheless, the files recovered by the function clustering-based approach contain binary functions with discontinuous distributions, resulting in low accuracy. Additionally, there is no existing research related to predicting the names of these recovered files. To this end, we propose a framework for source file information recovery in binaries, DeLink. This framework first leverages a file structure recovery approach based on boundary location to recognize files within a binary. Then, it utilizes an encoder-decoder model to predict the names of these files. The experimental results show that our file structure recovery approach achieves an average improvement of 14% across six evaluation metrics and requires only an average time of 16.74 seconds, outperforming the state-of-the-art work in both recovery quality and efficiency. Additionally, our file name prediction model achieves 70.09% precision and 63.91% recall. Moreover, we demonstrate the effective application of DeLink in malware homology analysis.","2024","2025-11-25 22:29:54","2025-11-25 22:29:54","","1009–1021","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Program Comprehension; Source File Information Recovery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AW83FQAD","conferencePaper","2024","Wang, Hao; Gao, Zeyu; Zhang, Chao; Sha, Zihan; Sun, Mingyang; Zhou, Yuchen; Zhu, Wenyu; Sun, Wenju; Qiu, Han; Xiao, Xi","CLAP: Learning Transferable Binary Code Representations with Natural Language Supervision","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3652145","https://doi.org/10.1145/3650212.3652145","Binary code representation learning has shown significant performance in binary analysis tasks. But existing solutions often have poor transferability, particularly in few-shot and zero-shot scenarios where few or no training samples are available for the tasks. To address this problem, we present CLAP (Contrastive Language-Assembly Pre-training), which employs natural language supervision to learn better representations of binary code (i.e., assembly code) and get better transferability. At the core, our approach boosts superior transfer learning capabilities by effectively aligning binary code with their semantics explanations (in natural language), resulting a model able to generate better embeddings for binary code. To enable this alignment training, we then propose an efficient dataset engine that could automatically generate a large and diverse dataset comprising of binary code and corresponding natural language explanations. We have generated 195 million pairs of binary code and explanations and trained a prototype of CLAP. The evaluations of CLAP across various downstream tasks in binary analysis all demonstrate exceptional performance. Notably, without any task-specific training, CLAP is often competitive with a fully supervised baseline, showing excellent transferability.","2024","2025-11-25 22:29:54","2025-11-25 22:29:54","","503–515","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Binary Analysis; Deep Learning; Representation Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IZRMWIZ7","conferencePaper","2025","Sölch, Maximilian; Dietrich, Felix T. J.; Krusche, Stephan","Direct Automated Feedback Delivery for Student Submissions based on LLMs","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","10.1145/3696630.3727247","https://doi.org/10.1145/3696630.3727247","Timely and individualized feedback is essential for students' learning progress and motivation, yet providing such feedback has become increasingly challenging due to growing student numbers. This has resulted in a time-consuming, repetitive, and often manual task for educators, contributing to a high workload.This paper presents DAFeeD, an LLM-based approach for automated feedback on student submissions across various exercise domains. The defined feedback process enables interactive learning by allowing students to submit solutions multiple times and automatically receive iterative LLM feedback on their submission attempts before deadlines. By incorporating task details, grading criteria, student solutions, and custom instructions into the prompt, DAFeeD provides clear, personalized, and pedagogically meaningful feedback to support continuous improvement.To evaluate the feedback process, we implemented DAFeeD in an open-source reference implementation integrated into the learning platform Artemis. A controlled study with students working on a programming task in a supervised environment showed that students found the feedback relevant and beneficial. They reported feeling more comfortable and willing to request automated feedback due to its convenience and immediacy. Additionally, deploying DAFeeD in a software engineering course with 450 students demonstrated improvements in student performance and encouraged iterative refinement through multiple submissions.These findings highlight DAFeeD's potential to enhance feedback processes in computing education, improving both learning efficiency and student outcomes.","2025","2025-11-25 22:29:54","2025-11-25 22:29:54","","901–911","","","","","","","FSE Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","software engineering; formative feedback; education; grading","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"837Y77RM","journalArticle","2025","Kim, Myeongsoo; Sinha, Saurabh; Orso, Alessandro","LlamaRestTest: Effective REST API Testing with Small Language Models","Proc. ACM Softw. Eng.","","","10.1145/3715737","https://doi.org/10.1145/3715737","Modern web services rely heavily on REST APIs, typically documented using the OpenAPI specification. The widespread adoption of this standard has resulted in the development of many black-box testing tools that generate tests based on OpenAPI specifications. Although Large Language Models (LLMs) have shown promising test-generation abilities, their application to REST API testing remains mostly unexplored. We present LlamaRestTest, a novel approach that employs two custom LLMs-created by fine-tuning and quantizing the Llama3-8B model using mined datasets of REST API example values and inter-parameter dependencies-to generate realistic test inputs and uncover inter-parameter dependencies during the testing process by analyzing server responses. We evaluated LlamaRestTest on 12 real-world services (including popular services such as Spotify), comparing it against RESTGPT, a GPT-powered specification-enhancement tool, as well as several state-of-the-art REST API testing tools, including RESTler, MoRest, EvoMaster, and ARAT-RL. Our results demonstrate that fine-tuning enables smaller models to outperform much larger models in detecting actionable parameter-dependency rules and generating valid inputs for REST API testing. We also evaluated different tool configurations, ranging from the base Llama3-8B model to fine-tuned versions, and explored multiple quantization techniques, including 2-bit, 4-bit, and 8-bit integer formats. Our study shows that small language models can perform as well as, or better than, large language models in REST API testing, balancing effectiveness and efficiency. Furthermore, LlamaRestTest outperforms state-of-the-art REST API testing tools in code coverage achieved and internal server errors identified, even when those tools use RESTGPT-enhanced specifications. Finally, through an ablation study, we show that each component of LlamaRestTest contributes to its overall performance.","2025-06","2025-11-25 22:29:54","2025-11-25 22:29:54","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Automated REST API Testing; Language Models for Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VQ2CWS2J","journalArticle","2025","Alonso, Juan C.; Ernst, Michael D.; Segura, Sergio; Ruiz-Cortés, Antonio","Test Oracle Generation for REST APIs","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3726524","https://doi.org/10.1145/3726524","The number and complexity of test case generation tools for REST APIs have significantly increased in recent years. These tools excel in automating input generation but are limited by their test oracles, which can only detect crashes, regressions, and violations of API specifications or design best practices. This article introduces AGORA+, an approach for generating test oracles for REST APIs through the detection of invariants—output properties that should always hold. AGORA+ learns the expected behavior of an API by analyzing API requests and their corresponding responses. We enhanced the Daikon tool for dynamic detection of likely invariants, adding new invariant types and creating a front-end called Beet. Beet translates any OpenAPI specification and a set of API requests and responses into Daikon inputs. AGORA+ can detect 106 different types of invariants in REST APIs. We also developed PostmanAssertify, which converts the invariants identified by AGORA+ into executable JavaScript assertions. AGORA+ achieved a precision of 80% on 25 operations from 20 industrial APIs. It also identified 48% of errors systematically seeded in the outputs of the APIs under test. AGORA+ uncovered 32 bugs in popular APIs, including Amadeus, Deutschebahn, GitHub, Marvel, NYTimesBooks, and YouTube, leading to fixes and documentation updates.","2025-03","2025-11-25 22:29:54","2025-11-25 22:29:54","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","REST APIs; automated testing; invariant detection; test oracle","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XMCZTDNX","conferencePaper","2025","Copia, Juan Manuel; Molina, Facundo; Gorla, Alessandra; Aguirre, Nazareno; Ponzio, Pablo","Search-based Inference of Class Invariants","Proceedings of the Genetic and Evolutionary Computation Conference Companion","979-8-4007-1464-1","","10.1145/3712255.3726698","https://doi.org/10.1145/3712255.3726698","Many techniques in formal verification and software testing rely on repOk routines to verify the consistency and validity of software components with complex data representations. A repOk function encodes the state properties necessary for an instance to be a valid object of the class under analysis, enabling early error detection and simplifying debugging. However, writing a correct and complete repOk can be challenging. This paper introduces Express, the first search-based algorithm designed to automatically generate a correct repOk for a given class. Express leverages simulated annealing, using the source code and test suite of the class under analysis to iteratively construct a repOk. We demonstrate how Express works on the LinkedList class of the Java standard library, and show that it produces a correct and complete repOK.","2025","2025-11-25 22:29:54","2025-11-25 22:29:54","","803–806","","","","","","","GECCO '25 Companion","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: NH Malaga Hotel, Malaga, Spain","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8SQ4X42J","conferencePaper","2024","Xie, Linna; Li, Chongmin; Pei, Yu; Zhang, Tian; Pan, Minxue","BRAFAR: Bidirectional Refactoring, Alignment, Fault Localization, and Repair for Programming Assignments","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680326","https://doi.org/10.1145/3650212.3680326","The problem of automated feedback generation for introductory programming assignments (IPAs) has attracted significant attention with the increasing demand for programming education. While existing approaches, like Refactory, that employ the ”block-by-block” repair strategy have produced promising results, they suffer from two limitations. First, Refactory randomly applies refactoring and mutation operations to correct and buggy programs, respectively, to align their control-flow structures (CFSs), which, however, has a relatively low success rate and often complicates the original repairing tasks. Second, Refactory generates repairs for each basic block of the buggy program when its semantics differs from the counterpart in the correct program, which, however, ignores the different roles that basic blocks play in the programs and often produces unnecessary repairs. To overcome these limitations, we propose the Brafar approach to feedback generation for IPAs. The core innovation of Brafar lies in its novel bidirectional refactoring algorithm and coarse-to-fine fault localization. The former aligns the CFSs of buggy and correct programs by applying semantics-preserving refactoring operations to both programs in a guided manner, while the latter identifies basic blocks that truly need repairs based on the semantics of their enclosing statements and themselves. In our experimental evaluation on 1783 real-life incorrect student submissions from a publicly available dataset, Brafar significantly outperformed Refactory and Clara, generating correct repairs for more incorrect programs with smaller patch sizes in a shorter time.","2024","2025-11-25 22:29:54","2025-11-25 22:29:54","","856–868","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Program Repair; Programming Education; Software Refactoring","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GWHN39EF","conferencePaper","2024","Kang, Eunsuk; Shaw, Mary","tl;dr: Chill, y’all: AI Will Not Devour SE","Proceedings of the 2024 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software","979-8-4007-1215-9","","10.1145/3689492.3689816","https://doi.org/10.1145/3689492.3689816","Social media provide a steady diet of dire warnings that artificial intelligence (AI) will make software engineering (SE) irrelevant or obsolete. To the contrary, the engineering discipline of software is rich and robust; it encompasses the full scope of software design, development, deployment, and practical use; and it has regularly assimilated radical new offerings from AI. Current AI innovations such as machine learning, large language models (LLMs) and generative AI will offer new opportunities to extend the models and methods of SE. They may automate some routine development processes, and they will bring new kinds of components and architectures. If we're fortunate they may force SE to rethink what we mean by correctness and reliability. They will not, however, render SE irrelevant.","2024","2025-11-25 22:29:54","2025-11-25 22:29:54","","303–315","","","","","","","Onward! '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Pasadena, CA, USA","","","","AI-assisted development; software correctness; software engineering principles","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G72MW2FL","journalArticle","2025","Sartaj, Hassan; Ali, Shaukat; Gjøby, Julie Marie","REST API Testing in DevOps: A Study on an Evolving Healthcare IoT Application","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3765744","https://doi.org/10.1145/3765744","Healthcare Internet of Things (IoT) applications often integrate various third-party healthcare applications and medical devices through REST APIs, resulting in complex and interdependent networks of REST APIs. Oslo City’s healthcare department collaborates with various industry partners to develop these applications, enriched with diverse REST APIs that evolve during the DevOps process to accommodate evolving needs such as new features, services, and devices. Oslo City’s primary goal is to utilize automated solutions for continuous testing of REST APIs at each evolution stage to ensure dependability. Although the literature offers various automated REST API testing tools, their effectiveness in regression testing of the evolving REST APIs of healthcare IoT applications within a DevOps context remains undetermined. This paper evaluates state-of-the-art and well-established REST API testing tools—specifically, RESTest, EvoMaster, Schemathesis, RESTler, and RestTestGen—for the regression testing of a real-world healthcare IoT application, considering failures, faults, coverage, regressions, and cost. We conducted experiments using all accessible REST APIs (17 APIs with 120 endpoints), and 14 releases evolved during DevOps. Overall, all tools generated tests leading to several failures, 18 potential faults, up to 84% coverage, and 23 regressions. Over 70% of tests generated by all tools fail to detect failures, resulting in significant overhead.","2025-09","2025-11-25 22:29:54","2025-11-25 22:29:54","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Black-box Testing; Fuzzing; REST APIs; CI/CD; DevOps; Continuous Regression Testing; Healthcare IoT; Web Services","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C38XFHR6","journalArticle","2025","Wang, Xu; Zhang, Mingming; Meng, Xiangxin; Zhang, Jian; Liu, Yang; Hu, Chunming","Element-Based Automated DNN Repair with Fine-Tuned Masked Language Model","Proc. ACM Softw. Eng.","","","10.1145/3715716","https://doi.org/10.1145/3715716","Deep Neural Networks (DNNs) are prevalent across a wide range of applications. Despite their success, the complexity and opaque nature of DNNs pose significant challenges in debugging and repairing DNN models, limiting their reliability and broader adoption. In this paper, we propose MLM4DNN, an element-based automated DNN repair method. Unlike previous techniques that focus on post-training adjustments or rely heavily on predefined bug patterns, MLM4DNN repairs DNNs by leveraging a fine-tuned Masked Language Model (MLM) to predict correct fixes for nine predefined key elements in DNNs. We construct a large-scale dataset by masking nine key elements from the correct DNN source code and then force the MLM to restore the correct elements to learn the deep semantics that ensure the normal functionalities of DNNs. Afterwards, a light-weight static analysis tool is designed to filter out low-quality patches to enhance the repair efficiency. We introduce a patch validation method specifically for DNN repair tasks, which consists of three evaluation metrics from different aspects to model the effectiveness of generated patches. We construct a benchmark, BenchmarkAPR4DNN, including 51 buggy DNN models and an evaluation tool that outputs the three metrics. We evaluate MLM4DNN against six baselines on BenchmarkAPR4DNN, and results show that MLM4DNN outperforms all state-of-the-art baselines, including two dynamic-based and four zero-shot learning-based methods. After applying the fine-tuned MLM design to several prevalent Large Language Models (LLMs), we consistently observe improved performance in DNN repair tasks compared to the original LLMs, which demonstrates the effectiveness of the method proposed in this paper.","2025-06","2025-11-25 22:29:55","2025-11-25 22:29:55","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Program Repair; Deep Neural Network; Fine-Tune; Masked Language Model","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MZ5X65QF","journalArticle","2025","Zhang, Quanjun; Sun, Weifeng; Fang, Chunrong; Yu, Bowen; Li, Hongyan; Yan, Meng; Zhou, Jianyi; Chen, Zhenyu","Exploring Automated Assertion Generation via Large Language Models","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3699598","https://doi.org/10.1145/3699598","Unit testing aims to validate the correctness of software system units and has become an essential practice in software development and maintenance. However, it is incredibly time-consuming and labor-intensive for testing experts to write unit test cases manually, including test inputs (i.e., prefixes) and test oracles (i.e., assertions). Very recently, some techniques have been proposed to apply Large Language Models (LLMs) to generate unit assertions and have proven the potential in reducing manual testing efforts. However, there has been no systematic comparison of the effectiveness of these LLMs, and their pros and cons remain unexplored.To bridge this gap, we perform the first extensive study on applying various LLMs to automated assertion generation. The experimental results on two independent datasets show that studied LLMs outperform six state-of-the-art techniques with a prediction accuracy of 51.82%–58.71% and 38.72%–48.19%. The improvements achieve 29.60% and 12.47% on average. Besides, as a representative LLM, CodeT5 consistently outperforms all studied LLMs and all baselines on both datasets, with an average improvement of 13.85% and 26.64%, respectively. We also explore the performance of generated assertions in detecting real-world bugs, and find LLMs are able to detect 32 bugs from Defects4J on average, with an improvement of 52.38% against the most recent approach EditAS. Inspired by the findings, we construct a simplistic retrieval-and-repair-enhanced LLM-based approach by transforming the assertion generation problem into a program repair task for retrieved similar assertions. Surprisingly, such a simplistic approach can further improve the prediction accuracy of LLMs by 9.40% on average, leading to new records on both datasets. Besides, we provide additional discussions from different aspects (e.g., the impact of assertion types and test lengths) to illustrate the capacity and limitations of LLM-based approaches. Finally, we further pinpoint various practical guidelines (e.g., the improvement of multiple candidate assertions) for advanced LLM-based assertion generation in the near future. Overall, our work underscores the promising future of adopting off-the-shelf LLMs to generate accurate and meaningful assertions in real-world test cases and reduce the manual efforts of unit testing experts in practical scenarios.","2025-02","2025-11-25 22:29:55","2025-11-25 22:29:55","","","","3","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","LLM; Unit Testing; AI4SE; Assertion Generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"URVDLPG7","conferencePaper","2024","Agarwal, Nimisha","Finding and Investigating Buggy Codes to Make CS1 Learning Efficient","Proceedings of the 17th Innovations in Software Engineering Conference","979-8-4007-1767-3","","10.1145/3641399.3641442","https://doi.org/10.1145/3641399.3641442","In an introductory programming course, students make several logical errors and struggle to fix those errors. This PhD research focuses on creating tools and methods that can aid students in the learning process and help them to find such errors more efficiently.","2024","2025-11-25 22:29:55","2025-11-25 22:29:55","","","","","","","","","ISEC '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Bangalore, India","","","","test case generation; CS1; automated feedback; error localization; Refute questions; targeted test cases","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EB9S2RRU","conferencePaper","2024","Huang, Hanxian; Zhao, Jishen","Multi-modal Learning for WebAssembly Reverse Engineering","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3652141","https://doi.org/10.1145/3650212.3652141","The increasing adoption of WebAssembly (Wasm) for performance-critical and security-sensitive tasks drives the demand for WebAssembly program comprehension and reverse engineering. Recent studies have introduced machine learning (ML)-based WebAssembly reverse engineering tools. Yet, the generalization of task-specific ML solutions remains challenging, because their effectiveness hinges on the availability of an ample supply of high-quality task-specific labeled data. Moreover, previous works trained models only with features extracted from WebAssembly, overlooking the high-level semantics present in the corresponding source code and its documentation. Acknowledging the abundance of available source code with documentation, which can be compiled into WebAssembly, we propose to learn representations of them concurrently and harness their mutual relationships for effective WebAssembly reverse engineering. In this paper, we present WasmRev, the first multi-modal pre-trained language model for WebAssembly reverse engineering. WasmRev is pre-trained using self-supervised learning on a large-scale multi-modal corpus encompassing source code, code documentation and the compiled WebAssembly, without requiring labeled data. WasmRev incorporates three tailored multi-modal pre-training tasks to capture various characteristics of WebAssembly and cross-modal relationships. WasmRev is only trained once to produce general-purpose representations that can broadly support WebAssembly reverse engineering tasks through few-shot fine-tuning with much less labeled data, improving data efficiency. We fine-tune WasmRev onto three important reverse engineering tasks: type recovery, function purpose identification and WebAssembly summarization. Our results show that WasmRev pre-trained on the corpus of multi-modal samples establishes a robust foundation for these tasks, achieving high task accuracy and outperforming the state-of-the-art ML methods for WebAssembly reverse engineering.","2024","2025-11-25 22:29:55","2025-11-25 22:29:55","","453–465","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","WebAssembly; code summarization; function purpose identification; multi-modal learning; program language modeling; representation learning; reverse engineering; type recovery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MR79F34J","conferencePaper","2025","Pickering, Madison; Williams, Helena; Gan, Alison; He, Weijia; Park, Hyojae; Piedrahita Velez, Francisco; Littman, Michael L.; Ur, Blase","How Humans Communicate Programming Tasks in Natural Language and Implications For End-User Programming with LLMs","Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems","979-8-4007-1394-1","","10.1145/3706598.3713271","https://doi.org/10.1145/3706598.3713271","Large language models (LLMs) like GPT-4 can convert natural-language descriptions of a task into computer code, making them a promising interface for end-user programming. We undertake a systematic analysis of how people with and without programming experience describe information-processing tasks (IPTs) in natural language, focusing on the characteristics of successful communication. Across two online between-subjects studies, we paired crowdworkers either with one another or with an LLM, asking senders (always humans) to communicate IPTs in natural language to their receiver (either a human or LLM). Both senders and receivers tried to answer test cases, the latter based on their sender’s description. While participants with programming experience tended to communicate IPTs more successfully than non-programmers, this advantage was not overwhelming. Furthermore, a user interface that solicited example test cases from senders often, but not always, improved IPT communication. Allowing receivers to request clarification, though, was less successful at improving communication.","2025","2025-11-25 22:29:55","2025-11-25 22:29:55","","","","","","","","","CHI '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Large Language Models; LLMs; End-User Programming","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JRSBUBZH","conferencePaper","2025","Freund, Stephen N.; Simon, Brooke; Berger, Emery D.; Jun, Eunice","Flowco: Mixed-Initiative Authoring of Reliable End-to-End Data Analyses via Dataflow Graphs and LLMs","Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology","979-8-4007-2037-6","","10.1145/3746059.3747636","https://doi.org/10.1145/3746059.3747636","Conducting data analysis typically involves authoring code to transform, visualize, analyze, and interpret data. Large Language Models (LLMs) are now capable of generating such code for simple, routine analyses, and they have the potential to democratize data science by enabling those with limited programming expertise to conduct data analyses, including in scientific research, business, and policymaking. However, analysts in many real-world settings must often exercise fine-grained control over specific analysis steps, verify intermediate results explicitly, and iteratively refine their analytical approaches. Such tasks present barriers to building robust and reproducible analyses using LLMs alone or even in conjunction with existing authoring tools (e.g., computational notebooks). This paper introduces Flowco, a new mixed-initiative system to address these challenges. Flowco leverages a visual dataflow programming model and integrates LLMs into every phase of the authoring process. A preliminary user evaluation suggests that Flowco supports analysts, particularly those with less programming experience, in quickly authoring, debugging, and refining data analyses.","2025","2025-11-25 22:29:55","2025-11-25 22:29:55","","","","","","","","","UIST '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","LLMs; data analysis; dataflow graphs; interactive systems; mixed-initiative systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LB3NXRLV","journalArticle","2025","Yang, Borui; Al Mamun, Md Afif; Zhang, Jie M.; Uddin, Gias","Hallucination Detection in Large Language Models with Metamorphic Relations","Proc. ACM Softw. Eng.","","","10.1145/3715735","https://doi.org/10.1145/3715735","Large Language Models (LLMs) are prone to hallucinations, e.g., factually incorrect information, in their responses. These hallucinations present challenges for LLM-based applications that demand high factual accuracy. Existing hallucination detection methods primarily depend on external resources, which can suffer from issues such as low availability, incomplete coverage, privacy concerns, high latency, low reliability, and poor scalability. There are also methods depending on output probabilities, which are often inaccessible for closed-source LLMs like GPT models. This paper presents MetaQA, a self-contained hallucination detection approach that leverages metamorphic relation and prompt mutation. Unlike existing methods, MetaQA operates without any external resources and is compatible with both open-source and closed-source LLMs. MetaQA is based on the hypothesis that if an LLM’s response is a hallucination, the designed metamorphic relations will be violated. We compare MetaQA with the state-of-the-art zero-resource hallucination detection method, SelfCheckGPT, across multiple datasets, and on two open-source and two closed-source LLMs. Our results reveal that MetaQA outperforms SelfCheckGPT in terms of precision, recall, and f1 score. For the four LLMs we study, MetaQA outperforms SelfCheckGPT with a superiority margin ranging from 0.041 - 0.113 (for precision), 0.143 - 0.430 (for recall), and 0.154 - 0.368 (for F1-score). For instance, with Mistral-7B, MetaQA achieves an average F1-score of 0.435, compared to SelfCheckGPT’s F1-score of 0.205, representing an improvement rate of 112.2%. MetaQA also demonstrates superiority across all different categories of questions.","2025-06","2025-11-25 22:29:55","2025-11-25 22:29:55","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; Hallucination Detection; Metamorphic Relations","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"276D5MTW","bookSection","2025","Huang, Kai; Zhang, Jian; Meng, Xiangxin; Liu, Yang","Template-Guided Program Repair in the Era of Large Language Models","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00030","Recent advancements in automated program repair (APR) have been significantly driven by the application of Large Language Models (LLMs). In particular, the integration of LLMs with traditional template-based repair methods has demonstrated effective outcomes. Despite this, the synergy between the strengths of traditional methods and LLMs remains underexploited. This oversight originates from the indiscriminate use of templates and their insufficient coverage. Also, using small-scale LLMs within the zero-shot learning context proves to be suboptimal.To alleviate the limitations, we propose NTR (Neural Template Repair), a two-stage repair framework including template selection and patch generation, both of which are under the fine-tuning paradigm. In the template selection phase, we formulate it as a multiclass classification problem and fine-tune million-level LLMs for better selecting possible templates. During the patch generation phase, we leverage the chosen templates as probable directions (e.g., 'Mutate Conditional Expression') to guide the fine-tuning process of LLMs at the billion-level scale for precise patch creation. Moreover, we incorporate a unique template to signify the absence of a suitable template and employ a probability-based prioritization of templates, thereby optimizing patch generation. This framework not only effectively addresses template mismatch issues, but also enables the billion-level LLMs to explore the patch space more efficiently, despite the GPU memory constraints.We evaluate NTR with different foundational models on Defects4J V1.2 and HumanEval-Java, the framework consistently demonstrates significant effectiveness. When utilizing StarCoder as the foundational model for patch generation, NTR fixes 128 and 129 bugs in Defects4J and HumanEval, outperforming the best baseline APR tool by 14 and 59 bugs. With the larger CodeLlama model, the fixed bugs rise to 139 and 136, respectively, exceeding the baseline by 25 and 66 bugs. Notably, the performance stems not only from the foundational models but also benefits greatly from our NTR framework. Specifically, NTR's implementation with StarCoder and CodeLlama leads to 22 and 23 additional fixes, which is beyond what the models achieve on their own. This emphasizes the success of our new perspective on utilizing templates to unlock the bug-fixing potential of LLMs.","2025","2025-11-25 22:29:55","2025-11-25 22:29:55","","1895–1907","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VVX7STLT","conferencePaper","2023","Savelka, Jaromir; Agarwal, Arav; An, Marshall; Bogart, Chris; Sakr, Majd","Thrilled by Your Progress! Large Language Models (GPT-4) No Longer Struggle to Pass Assessments in Higher Education Programming Courses","Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 1","978-1-4503-9976-0","","10.1145/3568813.3600142","https://doi.org/10.1145/3568813.3600142","This paper studies recent developments in large language models’ (LLM) abilities to pass assessments in introductory and intermediate Python programming courses at the postsecondary level. The emergence of ChatGPT resulted in heated debates of its potential uses (e.g., exercise generation, code explanation) as well as misuses in programming classes (e.g., cheating). Recent studies show that while the technology performs surprisingly well on diverse sets of assessment instruments employed in typical programming classes the performance is usually not sufficient to pass the courses. The release of GPT-4 largely emphasized notable improvements in the capabilities related to handling assessments originally designed for human test-takers. This study is the necessary analysis in the context of this ongoing transition towards mature generative AI systems. Specifically, we report the performance of GPT-4, comparing it to the previous generations of GPT models, on three Python courses with assessments ranging from simple multiple-choice questions (no code involved) to complex programming projects with code bases distributed into multiple files (599 exercises overall). Additionally, we analyze the assessments that were not handled well by GPT-4 to understand the current limitations of the model, as well as its capabilities to leverage feedback provided by an auto-grader. We found that the GPT models evolved from completely failing the typical programming class’ assessments (the original GPT-3) to confidently passing the courses with no human involvement (GPT-4). While we identified certain limitations in GPT-4’s handling of MCQs and coding exercises, the rate of improvement across the recent generations of GPT models strongly suggests their potential to handle almost any type of assessment widely used in higher education programming courses. These findings could be leveraged by educators and institutions to adapt the design of programming assessments as well as to fuel the necessary discussions into how programming classes should be updated to reflect the recent technological developments. This study provides evidence that programming instructors need to prepare for a world in which there is an easy-to-use widely accessible technology that can be utilized by learners to collect passing scores, with no effort whatsoever, on what today counts as viable programming knowledge and skills assessments.","2023","2025-11-25 22:29:55","2025-11-25 22:29:55","","78–92","","","","","","","ICER '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Chicago, IL, USA","","","","ChatGPT; GPT; Codex; GitHub Copilot; AlphaCode; AI code generation; coding exercises; generative pre-trained transformers; introductory and intermediate programming; MCQ; Multiple-choice question answering; programming knowledge assessment; Python course","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8QCW3GVU","conferencePaper","2025","Wang, Zhaodong; Lin, Samuel; Yan, Guanqing; Ghorbani, Soudeh; Yu, Minlan; Zhou, Jiawei; Hu, Nathan; Baruah, Lopa; Peters, Sam; Kamath, Srikanth; Yang, Jerry; Zhang, Ying","Intent-Driven Network Management with Multi-Agent LLMs: The Confucius Framework","Proceedings of the ACM SIGCOMM 2025 Conference","979-8-4007-1524-2","","10.1145/3718958.3750537","https://doi.org/10.1145/3718958.3750537","Advancements in Large Language Models (LLMs) are significantly transforming network management practices. In this paper, we present our experience developing Confucius, a multi-agent framework for network management at Meta. We model network management workflows as directed acyclic graphs (DAGs) to aid planning. Our framework integrates LLMs with existing management tools to achieve seamless operational integration, employs retrieval-augmented generation (RAG) to improve long-term memory, and establishes a set of primitives to systematically support human/model interaction. To ensure the accuracy of critical network operations, Confucius closely integrates with existing network validation methods and incorporates its own validation framework to prevent regressions. Remarkably, Confucius is a production-ready LLM development framework that has been operational for two years, with over 60 applications onboarded. To our knowledge, this is the first report on employing multi-agent LLMs for hyper-scale networks.","2025","2025-11-25 22:29:55","2025-11-25 22:29:55","","347–362","","","","","","","SIGCOMM '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: São Francisco Convent, Coimbra, Portugal","","","","large language models (LLMs); network planning; RAG","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VRLWIU65","conferencePaper","2024","Fagadau, Ionut Daniel; Mariani, Leonardo; Micucci, Daniela; Riganelli, Oliviero","Analyzing Prompt Influence on Automated Method Generation: An Empirical Study with Copilot","Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension","979-8-4007-0586-1","","10.1145/3643916.3644409","https://doi.org/10.1145/3643916.3644409","Generative AI is changing the way developers interact with software systems, providing services that can produce and deliver new content, crafted to satisfy the actual needs of developers. For instance, developers can ask for new code directly from within their IDEs by writing natural language prompts, and integrated services based on generative AI, such as Copilot, immediately respond to prompts by providing ready-to-use code snippets. Formulating the prompt appropriately, and incorporating the useful information while avoiding any information overload, can be an important factor in obtaining the right piece of code. The task of designing good prompts is known as prompt engineering.In this paper, we systematically investigate the influence of eight prompt features on the style and the content of prompts, on the level of correctness, complexity, size, and similarity to the developers' code of the generated code. We specifically consider the task of using Copilot with 124,800 prompts obtained by systematically combining the eight considered prompt features to generate the implementation of 200 Java methods. Results show how some prompt features, such as the presence of examples and the summary of the purpose of the method, can significantly influence the quality of the result.","2024","2025-11-25 22:29:55","2025-11-25 22:29:55","","24–34","","","","","","","ICPC '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","prompt engineering; code generation; copilot","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BTDJ7KNS","conferencePaper","2023","Downing, Mara","Quantitative Robustness Analysis of Neural Networks","Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0221-1","","10.1145/3597926.3605231","https://doi.org/10.1145/3597926.3605231","Neural networks are an increasingly common tool for solving problems that require complex analysis and pattern matching, such as identifying stop signs or processing medical imagery. Accordingly, verification of neural networks for safety and correctness is of great importance, as mispredictions can have catastrophic results in safety critical domains. One metric for verification is robustness, which answers whether or not a misclassified input exists in a given input neighborhood. I am focusing my research at quantitative robustness—finding not only if there exist misclassified inputs within a given neighborhood but also how many exist as a proportion of the neighborhood size. My overall goal is to expand the research on quantitative neural network robustness verification and create a variety of quantitative verification tools geared towards expanding our understanding of neural network robustness.","2023","2025-11-25 22:29:55","2025-11-25 22:29:55","","1527–1531","","","","","","","ISSTA 2023","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Seattle, WA, USA","","","","Neural Network Verification; Quantitative Verification; Robustness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZWY548B3","conferencePaper","2023","Wu, Yi; Jiang, Nan; Pham, Hung Viet; Lutellier, Thibaud; Davis, Jordan; Tan, Lin; Babkin, Petr; Shah, Sameena","How Effective Are Neural Networks for Fixing Security Vulnerabilities","Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0221-1","","10.1145/3597926.3598135","https://doi.org/10.1145/3597926.3598135","Security vulnerability repair is a difficult task that is in dire need of automation. Two groups of techniques have shown promise: (1) large code language models (LLMs) that have been pre-trained on source code for tasks such as code completion, and (2) automated program repair (APR) techniques that use deep learning (DL) models to automatically fix software bugs. This paper is the first to study and compare Java vulnerability repair capabilities of LLMs and DL-based APR models. The contributions include that we (1) apply and evaluate five LLMs (Codex, CodeGen, CodeT5, PLBART and InCoder), four fine-tuned LLMs, and four DL-based APR techniques on two real-world Java vulnerability benchmarks (Vul4J and VJBench), (2) design code transformations to address the training and test data overlapping threat to Codex, (3) create a new Java vulnerability repair benchmark VJBench, and its transformed version VJBench-trans, to better evaluate LLMs and APR techniques, and (4) evaluate LLMs and APR techniques on the transformed vulnerabilities in VJBench-trans. Our findings include that (1) existing LLMs and APR models fix very few Java vulnerabilities. Codex fixes 10.2 (20.4%), the most number of vulnerabilities. Many of the generated patches are uncompilable patches. (2) Fine-tuning with general APR data improves LLMs’ vulnerability-fixing capabilities. (3) Our new VJBench reveals that LLMs and APR models fail to fix many Common Weakness Enumeration (CWE) types, such as CWE-325 Missing cryptographic step and CWE-444 HTTP request smuggling. (4) Codex still fixes 8.7 transformed vulnerabilities, outperforming all the other LLMs and APR models on transformed vulnerabilities. The results call for innovations to enhance automated Java vulnerability repair such as creating larger vulnerability repair training data, tuning LLMs with such data, and applying code simplification transformation to facilitate vulnerability repair.","2023","2025-11-25 22:29:55","2025-11-25 22:29:55","","1282–1294","","","","","","","ISSTA 2023","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Seattle, WA, USA","","","","Automated Program Repair; AI and Software Engineering; Language Model; Vulnerability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ELI2KRXW","conferencePaper","2024","Liu, Yunqi; Song, Wei","FunRedisp: Reordering Function Dispatch in Smart Contract to Reduce Invocation Gas Fees","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3652146","https://doi.org/10.1145/3650212.3652146","Smart contracts mostly written in Solidity are Turing-complete programs executed on the blockchain platforms such as Ethereum. To prevent resource abuse, a gas fee is required when users deploy or invoke smart contracts. Although saving gas consumption has received much attention, no work investigates the effect of function dispatch on the invocation gas consumption. In this paper, after demystifying how the function dispatch affects the invocation gas consumption, we present FunRedisp, a bytecode refactoring method and an open-source tool, to reduce the overall invocation gas consumption of smart contracts. At the source code level, FunRedisp initially identifies hot functions in a smart contract that have a big chance to be invoked, and then move them to the front of the function dispatch at the bytecode level. We implement FunRedisp and evaluate it on 50 real-world smart contracts randomly selected from Ethereum. The experimental results demonstrate that FunRedisp can save approximately 125.17 units of gas per transaction with the compilation overhead increased by only 0.37 seconds.","2024","2025-11-25 22:29:55","2025-11-25 22:29:55","","516–527","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Ethereum; Solidity; refactoring; smart contract; function dispatch; gas optimization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"THMGHF47","bookSection","2025","Fu, Xing","Test Script Repair of Deep Learning Library Testing","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","","https://doi.org/10.1145/3696630.3728616","Deep learning (DL) libraries such as TensorFlow and PyTorch are widely utilized to develop machine learning models. However, when testing DL libraries, evolving library versions and flaws of testing approaches may result in invalid models within test scripts, which influence testing efficiency severely. Repairing these models can be difficult to achieve manually because of the complexity of DL models. In this paper, we propose a novel approach that utilizes a specialized prompt-based strategy with Large Language Model (LLM) to repair invalid DL models. We manually provide structured error information and model configurations to LLM, allowing it to generate code to fix invalid models. Our work shows that most invalid models can be repaired successfully with our strategy. Moreover, our approach can help to detect flaws in the DL library testing approaches and issues caused by version updates, which enhances the robustness and transferability of DL models.","2025","2025-11-25 22:29:55","2025-11-25 22:29:55","","1016–1018","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EV9M2CCE","journalArticle","2025","Wang, Chong; Zhang, Jian; Feng, Yebo; Li, Tianlin; Sun, Weisong; Liu, Yang; Peng, Xin","Teaching Code LLMs to Use Autocompletion Tools in Repository-Level Code Generation","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3714462","https://doi.org/10.1145/3714462","Recent code large language models (LLMs) have shown promising performance in generating standalone functions. However, they face limitations in repository-level code generation due to their lack of awareness of repository-level dependencies (e.g., user-defined attributes), resulting in dependency errors such as undefined-variable and no-member errors. In this work, we introduce ToolGen, an approach that integrates autocompletion tools into the code LLM generation process to address these dependencies. ToolGen comprises two main phases: Trigger Insertion and Model Fine-tuning (Offline), and Tool-integrated Code Generation (Online). During the offline phase, ToolGen augments functions within a given code corpus with a special mark token, indicating positions to trigger autocompletion tools. These augmented functions, along with their corresponding descriptions, are then used to fine-tune a selected code LLM. In the online phase, ToolGen iteratively generates functions by predicting tokens step-by-step using the fine-tuned LLM. Whenever a mark token is encountered, ToolGen invokes the autocompletion tool to suggest code completions and selects the most appropriate one through constrained greedy search.We conduct comprehensive experiments to evaluate ToolGen’s effectiveness in repository-level code generation across three distinct code LLMs: CodeGPT, CodeT5, and CodeLlama. To facilitate this evaluation, we create a benchmark comprising 671 real-world code repositories and introduce two new dependency-based metrics: Dependency Coverage and Static Validity Rate. The results demonstrate that ToolGen significantly improves Dependency Coverage by 31.4% to 39.1% and Static Validity Rate by 44.9% to 57.7% across the three LLMs, while maintaining competitive or improved performance in widely recognized similarity metrics such as BLEU-4, CodeBLEU, Edit Similarity, and Exact Match. On the CoderEval dataset, ToolGen achieves improvements of 40.0% and 25.0% in test pass rate (Pass@1) for CodeT5 and CodeLlama, respectively, while maintaining the same pass rate for CodeGPT. ToolGen also demonstrates high efficiency in repository-level code generation, with latency ranging from 0.63 to 2.34 seconds for generating each function. Furthermore, our generalizability evaluation confirms ToolGen’s consistent performance when applied to diverse code LLMs, encompassing various model architectures and scales.","2025-08","2025-11-25 22:29:55","2025-11-25 22:29:55","","","","7","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","code LLMs; repository-level code generation; tool integration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KNL73M3B","conferencePaper","2025","Valero Lara, Pedro; Young, Aaron; Vetter, Jeffrey S.; Jin, Zheming; Pophale, Swaroop; Alaul Haque Monil, Mohammad; Teranishi, Keita; Godoy, William F.","ChatHPC: Building the Foundations for a Productive and Trustworthy AI-Assisted HPC Ecosystem","Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis","979-8-4007-1466-5","","10.1145/3712285.3759787","https://doi.org/10.1145/3712285.3759787","ChatHPC democratizes large language models for the high-performance computing (HPC) community by providing the infrastructure, ecosystem, and knowledge needed to apply modern generative AI technologies to rapidly create specific capabilities for critical HPC components while using relatively modest computational resources. Our divide-and-conquer approach focuses on creating a collection of reliable, highly specialized, and optimized AI assistants for HPC based on the cost-effective and fast Code Llama fine-tuning processes and expert supervision. We target major components of the HPC software stack, including programming models, runtimes, I/O, tooling, and math libraries. Thanks to AI, ChatHPC provides a more productive HPC ecosystem by boosting important tasks related to portability, parallelization, optimization, scalability, and instrumentation, among others. With relatively small datasets (on the order of KB), the AI assistants, which are created in a few minutes by using one node with two NVIDIA H100 GPUs and the ChatHPC library, can create new capabilities with Meta’s 7-billion parameter Code Llama base model to produce high-quality software with a level of trustworthiness of up to 90% higher than the 1.8-trillion parameter OpenAI ChatGPT-4o model for critical programming tasks in the HPC software stack.","2025","2025-11-25 22:29:55","2025-11-25 22:29:55","","458–474","","","","","","","SC '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Large Language Models; Productivity; Trustworthiness; High Performance Computing.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MF6IJ2IG","conferencePaper","2024","Saha, Antu; Song, Yang; Mahmud, Junayed; Zhou, Ying; Moran, Kevin; Chaparro, Oscar","Toward the Automated Localization of Buggy Mobile App UIs from Bug Descriptions","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680357","https://doi.org/10.1145/3650212.3680357","Bug report management is a costly software maintenance process comprised of several challenging tasks. Given the UI-driven nature of mobile apps, bugs typically manifest through the UI, hence the identification of buggy UI screens and UI components (Buggy UI Localization) is important to localizing the buggy behavior and eventually fixing it. However, this task is challenging as developers must reason about bug descriptions (which are often low-quality), and the visual or code-based representations of UI screens. This paper is the first to investigate the feasibility of automating the task of Buggy UI Localization through a comprehensive study that evaluates the capabilities of one textual and two multi-modal deep learning (DL) techniques and one textual unsupervised technique. We evaluate such techniques at two levels of granularity, Buggy UI Screen and UI Component localization. Our results illustrate the individual strengths of models that make use of different representations, wherein models that incorporate visual information perform better for UI screen localization, and models that operate on textual screen information perform better for UI component localization – highlighting the need for a localization approach that blends the benefits of both types of techniques. Furthermore, we study whether Buggy UI Localization can improve traditional buggy code localization, and find that incorporating localized buggy UIs leads to improvements of 9%-12% in Hits@10.","2024","2025-11-25 22:29:55","2025-11-25 22:29:55","","1249–1261","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Bug Reports; Information Retrieval; Mobile Applications; UI Data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BAH4LFUG","journalArticle","2025","Shi, Jieke; Yang, Zhou; He, Junda; Xu, Bowen; Kim, Dongsun; Han, Donggyun; Lo, David","Finding Safety Violations of AI-Enabled Control Systems through the Lens of Synthesized Proxy Programs","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3715105","https://doi.org/10.1145/3715105","Given the increasing adoption of modern AI-enabled control systems, ensuring their safety and reliability has become a critical task in software testing. One prevalent approach to testing control systems is falsification, which aims to find an input signal that causes the control system to violate a formal safety specification using optimization algorithms. However, applying falsification to AI-enabled control systems poses two significant challenges: (1) it requires the system to execute numerous candidate test inputs, which can be time-consuming, particularly for systems with AI models that have many parameters, and (2) multiple safety requirements are typically defined as a conjunctive specification, which is difficult for existing falsification approaches to comprehensively cover.This article introduces Synthify, a falsification framework tailored for AI-enabled control systems, i.e., control systems equipped with AI controllers. Our approach performs falsification in a two-phase process. At the start, Synthify synthesizes a program that implements one or a few linear controllers to serve as a proxy for the AI controller. This proxy program mimics the AI controller’s functionality but is computationally more efficient. Then, Synthify employs the (epsilon) -greedy strategy to sample a promising sub-specification from the conjunctive safety specification. It then uses a Simulated Annealing-based falsification algorithm to find violations of the sampled sub-specification for the control system. To evaluate Synthify, we compare it to PSY-TaLiRo, a state-of-the-art and industrial-strength falsification tool, on eight publicly available control systems. On average, Synthify achieves a 83.5% higher success rate in falsification compared to PSY-TaLiRo with the same budget of falsification trials. Additionally, our method is 12.8 (times) faster in finding a single safety violation than the baseline. The safety violations found by Synthify are also more diverse than those found by PSY-TaLiRo, covering 137.7% more sub-specifications.","2025-08","2025-11-25 22:29:55","2025-11-25 22:29:55","","","","7","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Program Synthesis; Search-based Testing; AI-enabled Control Systems; Falsification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B6XLNB9E","conferencePaper","2025","Guimaraes, Everton; Nascimento, Nathalia","AI in the Software Development Lifecycle: Insights and Open Research Questions","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","10.1145/3696630.3730538","https://doi.org/10.1145/3696630.3730538","The rapid advancements in Artificial Intelligence (AI) and Large Language Models (LLMs) are reshaping software engineering and automating tasks such as code generation, debugging, testing, and maintenance. AI-powered tools (i.e. ChatGPT, DeepSeek), have demonstrated significant potential in enhancing developer productivity and accelerating software development processes. Integrating AI and LLMs into software engineering presents notable challenges despite these advancements. Concerns regarding the reliability of AI-generated code, security vulnerabilities, and the propagation of biases in training data pose substantial risks. Additionally, ethical considerations, including intellectual property rights, transparency, and the need for human oversight, highlight the complexities of AI adoption in critical software systems. The rapid evolution of these technologies requires continuous adaptation of software engineering methodologies to mitigate risks while maximizing benefits. This paper analyzes AI's role in software engineering, identifying key applications, challenges, and future research directions. We examine AI's impact across various phases of the software development lifecycle, This paper contributes to the ongoing discussion on AI-driven software engineering and outlines a research agenda for navigating this rapidly evolving field.","2025","2025-11-25 22:29:55","2025-11-25 22:29:55","","1353–1357","","","","","","","FSE Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","artificial intelligence; AI-assisted software development; large language models (LLMs) software engineering; LeetCode","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8ANFNV3K","conferencePaper","2024","Wang, Yiran; López, José Antonio Hernández; Nilsson, Ulf; Varró, Dániel","Using Run-Time Information to Enhance Static Analysis of Machine Learning Code in Notebooks","Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering","979-8-4007-0658-5","","10.1145/3663529.3663785","https://doi.org/10.1145/3663529.3663785","A prevalent method for developing machine learning (ML) prototypes involves the use of notebooks. Notebooks are sequences of cells containing both code and natural language documentation. When executed during development, these code cells provide valuable run-time information. Nevertheless, current static analyzers for notebooks do not leverage this run-time information to detect ML bugs. Consequently, our primary proposition in this paper is that harvesting this run-time information in notebooks can significantly improve the effectiveness of static analysis in detecting ML bugs. To substantiate our claim, we focus on bugs related to tensor shapes and conduct experiments using two static analyzers: 1) PYTHIA, a traditional rule-based static analyzer, and 2) GPT-4, a large language model that can also be used as a static analyzer. The results demonstrate that using run-time information in static analyzers enhances their bug detection performance and it also helped reveal a hidden bug in a public dataset.","2024","2025-11-25 22:29:55","2025-11-25 22:29:55","","497–501","","","","","","","FSE 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","static analysis; large language models; machine learning bugs; notebook; run-time information","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"48LVSK9N","journalArticle","2025","Liu, Jiakun; Zhang, Peixin; Hu, Han; Liu, Yonghui; Minn, Wei; Thung, Ferdian; Maoz, Shahar; Toch, Eran; Gao, Debin; Lo, David","Activity Transition Graph Generation: How Far Are We?","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3776553","https://doi.org/10.1145/3776553","Android applications (i.e., apps) are indispensable nowadays and are getting bigger and bigger with an increasing number of functionalities. To understand how to access functionalities in an app, prior studies proposed tools to model the transitions between functionalities with the activity transition graph (ATG). ATG is an important data structure and has been used for various Android app analyses, including app design, understanding, and testing. However, there is no benchmarking work on ATG generation. It is still unclear whether the transitions identified by tools are correct and how many transitions are missed.To fill this gap, we manually identified all transitions in 98 applications to build a benchmark. Using the benchmark, we evaluate seven popular ATG generation tools that were used in prior studies. We observe that these tools not only report incorrect transitions but also missed transitions, and different tools do not report the same set of transitions. Compared with the transitions reported by a single tool, the union set of the transitions reported by different tools contains fewer missed transitions but more incorrect transitions. We summarize five potential reasons that explain why GUI testing tools fail to identify transitions in ATG, revealing the limitations in the current design of exploration strategies. For instance, we observe that learning-based tools may overlook subtle distinctions between states, potentially misclassifying different states as identical. This will lead the tool to always focus on the old state while the new state is not fully explored. Based on our findings, we propose a series of suggestions for researchers using and building ATG generation tools. For example, when aiming to identify more transitions, one can combine the results of different tools by running each tool for 10 minutes, which will produce better results than running a single tool for 60 minutes.","2025-11","2025-11-25 22:29:55","2025-11-25 22:29:55","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Android; Program Analysis; Activity Transition Graph","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9Y6PLY3H","conferencePaper","2024","Ding, Yangruibo","Semantic-aware Source Code Modeling","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695605","https://doi.org/10.1145/3691620.3695605","Source code modeling represents a promising avenue for automating software development, such as code generation, bug repair, and program analysis. This research direction aims to train deep neural nets to learn the statistical predictability inherent in human-written programs to enhance developer productivity, code quality, and the overall software development life cycle.Although existing code modeling approaches, particularly those underpinned by Transformer-based language models, have demonstrated effectiveness across various software engineering tasks, most of them have directly adopted learning schemes from natural language processing (e.g., data collection and processing, training objectives) to source code, primarily focusing on learning code text and syntax. However, such a direct transplant limits the models' capability to capture deep program semantics, such as code functionality, dependencies, and program states during execution.In this research proposal, we highlight the critical role of program semantics in source code modeling. We propose a range of innovative methodologies to bridge the gap between the text-based language models for large-scale code training and the requirement of deep semantic understanding to assist with software engineering tasks effectively. Furthermore, we showcase the efficacy of the proposed semantic-aware code modeling through a handful of published papers and preliminary results, with motivations to delve deeper into this avenue during doctoral research.","2024","2025-11-25 22:29:56","2025-11-25 22:29:56","","2494–2497","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GTD62BS4","conferencePaper","2024","Oelgoetz, Megan; Walker, Tony","Improving an NSF ACCESS Program AI Chatbot: Response Data Logistic Regression","Practice and Experience in Advanced Research Computing 2024: Human Powered Computing","979-8-4007-0419-2","","10.1145/3626203.3670628","https://doi.org/10.1145/3626203.3670628","The NSF ACCESS program has implemented a vendor-supplied AI chatbot using the OpenAI GPT-4 large language model. ACCESS provides high performance computing (HPC) resources to researchers by allocating time at computing centers at diverse institutions of higher education across the United States. Effectively implementing a large language model on a limited knowledge base for the diversity of the resources and technical nature of HPC in general has raised questions in optimal knowledge base construction. The following analysis takes a limited test case to investigate the driving factors contributing to the accuracy of the chatbot’s response to predetermined prompts, specifically regarding the clusters on which specific software applications are currently available. It additionally tests the necessity of providing documentation of synonymous terms in the form of a synonym dictionary in the knowledge base. While ongoing, this initial research utilizing logistic regression suggests the knowledge base is yet insufficient for the prompts given and that the synonym dictionary has no statistically significant effect on the response.","2024","2025-11-25 22:29:56","2025-11-25 22:29:56","","","","","","","","","PEARC '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Providence, RI, USA","","","","Natural Language Processing; Artificial Intelligence; HPC Facilitation; Logistic Regression","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MV689L2K","conferencePaper","2024","Nguyen, Thanh-Dat; Do-Viet, Tung; Nguyen-Duy, Hung; Luu, Tuan-Hai; Le, Hung; Le, Bach; Thongtanunam, Patanamon","VRDSynth: Synthesizing Programs for Multilingual Visually Rich Document Information Extraction","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3680314","https://doi.org/10.1145/3650212.3680314","Businesses often need to query visually rich documents (VRDs), e.g., purchase receipts, medical records, and insurance forms, among many other forms from multiple vendors, to make informed decisions. As such, several techniques have been proposed to automatically extract independent entities of interest from VRDs such as extracting price tags from purchase receipts, etc. However, for extracting semantically linked entities, such as finding corresponding price tags for each item, these techniques either have limited capability in handling new layouts, e.g., template-based approaches, or require extensive amounts of pre-training data and do not perform well, e.g., deep-learning approaches. In this work, we introduce a program synthesis method, namely VRDSynth, to automatically generate programs to extract entity relations from multilingual VRDs. Two key novelties, which empower VRDSynth to tackle flexible layouts while requiring no pre-training data for extracting entity relations, include: (1) a new domain-specific language (DSL) to effectively capture the spatial and textual relations between document entities, and (2) a novel synthesis algorithm that makes use of frequent spatial relations between entities to construct initial programs, equivalent reduction to prune the search space, and a combination of positive, negative, and mutually exclusive programs to improve the coverage of programs. We evaluate our method on two popular VRD understanding benchmarks, namely FUNSD and XFUND, on the semantic entity linking task, consisting of 1,600 forms in 8 different languages. Experiments show that VRDSynth, despite having no prior pre-training data, outperforms the state-of-the-art pre-trained deep-learning approach, namely LayoutXLM, in 5 out of 8 languages. Noticeably, VRDSynth achieved an improvement of 42% over LayoutXLM in terms of F1 score on FUNSD while being complementary to LayoutXLM in 7/8 languages. Regarding efficiency, VRDSynth significantly improves the memory footprint required for storage and inference over LayoutXLM (1M and 380MB versus that of 1.48GB and 3GB required by LayoutXLM), while maintaining similar time efficiency despite the speed differences between the languages used for implementation (Python vs C++).","2024","2025-11-25 22:29:56","2025-11-25 22:29:56","","704–716","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Program Synthesis; Automatic Programming; Information Extraction; Programming By Example; Visually-Rich Document","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A2L5ALJW","journalArticle","2025","Liang, Jenny T.; Lin, Melissa; Rao, Nikitha; Myers, Brad A.","Prompts Are Programs Too! Understanding How Developers Build Software Containing Prompts","Proc. ACM Softw. Eng.","","","10.1145/3729342","https://doi.org/10.1145/3729342","Generative pre-trained models power intelligent software features used by millions of users controlled by developer-written natural language prompts. Despite the impact of prompt-powered software, little is known about its development process and its relationship to programming. In this work, we argue that some prompts are programs and that the development of prompts is a distinct phenomenon in programming known as “prompt programming”. We develop an understanding of prompt programming using Straussian grounded theory through interviews with 20 developers engaged in prompt development across a variety of contexts, models, domains, and prompt structures. We contribute 15 observations to form a preliminary understanding of current prompt programming practices. For example, rather than building mental models of code, prompt programmers develop mental models of the foundation model (FM)’s behavior on the prompt by interacting with the FM. While prior research shows that experts have well-formed mental models, we find that prompt programmers who have developed dozens of prompts still struggle to develop reliable mental models. Our observations show that prompt programming differs from traditional software development, motivating the creation of prompt programming tools and providing implications for software engineering stakeholders.","2025-06","2025-11-25 22:29:56","2025-11-25 22:29:56","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","prompt engineering; Prompt programming; Straussian grounded theory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WL3WYFEM","journalArticle","2025","Xie, Shuyi; Yao, Wenlin; Dai, Yong; Wang, Shaobo; Xu, Zishan; Lin, Fan; Zhou, Donglin; Jin, Lifeng; Feng, Xinhua; Wei, Pengzhi; Lin, Yujie; Hu, Zhichao; Yu, Dong; Zhang, Zhengyou; Nie, Jing; Liu, Yuhong","TencentLLMEval: A Hierarchical Evaluation of Real-World Capabilities for Human-Aligned LLMs","ACM Trans. Intell. Syst. Technol.","","2157-6904","10.1145/3732784","https://doi.org/10.1145/3732784","Large language models (LLMs) have shown impressive capabilities across various natural language tasks. However, evaluating their alignment with human preferences remains a challenge. To this end, we propose a comprehensive human evaluation framework to assess LLMs’ proficiency in following instructions on diverse real-world tasks. We construct a hierarchical task tree encompassing 7 major areas covering over 200 categories and over 800 tasks, which covers diverse capabilities such as question answering, reasoning, multiturn dialogue, and text generation, to evaluate LLMs in a comprehensive and in-depth manner. We also design detailed evaluation standards and processes to facilitate consistent, unbiased judgments from human evaluators. A test set of over 3,000 instances is released, spanning different difficulty levels and knowledge domains. Our work provides a standardized methodology to evaluate human alignment in LLMs for both English and Chinese. We also analyze the feasibility of automating parts of evaluation with a strong LLM (GPT-4). Our framework supports a thorough assessment of LLMs as they are integrated into real-world applications. We have made publicly available the task tree, TencentLLMEval dataset, and evaluation methodology which have been demonstrated as effective in assessing the performance of Tencent Hunyuan LLMs 1. By doing so, we aim to facilitate the benchmarking of advances in the development of safe and human-aligned LLMs.","2025-04","2025-11-25 22:29:56","2025-11-25 22:29:56","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","LLMs; Evaluation; human-aligned","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"36549S4G","conferencePaper","2024","Yang, Junjie; Jiang, Jiajun; Sun, Zeyu; Chen, Junjie","A Large-Scale Empirical Study on Improving the Fairness of Image Classification Models","Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0612-7","","10.1145/3650212.3652122","https://doi.org/10.1145/3650212.3652122","Fairness has been a critical issue that affects the adoption of deep learning models in real practice. To improve model fairness, many existing methods have been proposed and evaluated to be effective in their own contexts. However, there is still no systematic evaluation among them for a comprehensive comparison under the same context, which makes it hard to understand the performance distinction among them, hindering the research progress and practical adoption of them. To fill this gap, this paper endeavours to conduct the first large-scale empirical study to comprehensively compare the performance of existing state-of-the-art fairness improving techniques. Specifically, we target the widely-used application scenario of image classification, and utilized three different datasets and five commonly-used performance metrics to assess in total 13 methods from diverse categories. Our findings reveal substantial variations in the performance of each method across different datasets and sensitive attributes, indicating over-fitting on specific datasets by many existing methods. Furthermore, different fairness evaluation metrics, due to their distinct focuses, yield significantly different assessment results. Overall, we observe that pre-processing methods and in-processing methods outperform post-processing methods, with pre-processing methods exhibiting the best performance. Our empirical study offers comprehensive recommendations for enhancing fairness in deep learning models. We approach the problem from multiple dimensions, aiming to provide a uniform evaluation platform and inspire researchers to explore more effective fairness solutions via a set of implications.","2024","2025-11-25 22:29:56","2025-11-25 22:29:56","","210–222","","","","","","","ISSTA 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vienna, Austria","","","","Deep learning; Empirical study; Model fairness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YXKCVZZ4","conferencePaper","2025","Ramanathan, Sriram; Lim, Lisa-Angelique; Mottaghi, Nazanin Rezazadeh; Buckingham Shum, Simon","When the Prompt becomes the Codebook: Grounded Prompt Engineering (GROPROE) and its application to Belonging Analytics","Proceedings of the 15th International Learning Analytics and Knowledge Conference","979-8-4007-0701-8","","10.1145/3706468.3706564","https://doi.org/10.1145/3706468.3706564","With the emergence of generative AI, the field of Learning Analytics (LA) has increasingly embraced the use of Large Language Models (LLMs) to automate qualitative analysis. Deductive analysis requires theoretical or other conceptual grounding to inform coding. However, few studies detail the process of translating the literature into a codebook, and then into an effective LLM prompt. In this paper, we introduce Grounded Prompt Engineering (GROPROE) as a systematic process to develop a literature-grounded prompt for deductive analysis. We demonstrate our GROPROE process on a dataset of 860 written reflections, coding for students’ affective engagement and sense of belonging. To evaluate the quality of the coding we demonstrate substantial human/LLM Inter-Annotator Reliability (IAR). To evaluate the consistency of LLM coding, a subset of the data was analysed 60 times using the LLM Quotient showing how this stabilized for most codes. We discuss the dynamics of human-AI interaction when following GROPROE, foregrounding how the prompt took over as the iteratively revised codebook, and how the LLM provoked codebook revision. The contributions to the LA field are threefold: (i) GROPROE as a systematic prompt-design process for deductive coding grounded in literature, (ii) a detailed worked example showing its application to Belonging Analytics, and (iii) implications for human-AI interaction in automated deductive analysis.","2025","2025-11-25 22:29:56","2025-11-25 22:29:56","","713–725","","","","","","","LAK '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VMU5DPLR","conferencePaper","2024","Qiao, Yining; Rojas, José Miguel","What’s in a Display Name? An Empirical Study on the Use of Display Names in Open-Source JUnit Tests","Proceedings of the Third ACM/IEEE International Workshop on NL-Based Software Engineering","979-8-4007-0576-2","","10.1145/3643787.3648037","https://doi.org/10.1145/3643787.3648037","Readability is an important aspect of any production or test code artefact. During development, testing and maintenance, the readability of a unit test can be a key contributor to its usefulness for a range of tasks, including refactoring, debugging, and test augmentation. Several strategies have been proposed both in academia and in industry to build readability into unit tests, e.g., test code summarisation and test naming conventions. Display names constitute an industry-led effort to incorporate natural language descriptions into unit tests. In this paper, we investigate the use of display names in a large dataset of open-source Java projects. Our study reveals that despite being a stable feature of the JUnit framework for at least five years, display names are not widely used in open-source projects yet. We analyse existing display names in terms of length, language and grammatical structure, explore the use of a large language model to generate display names similar to those open-source developers write, and develop a taxonomy of display name smells aimed at fostering a more cohesive and coherent use of the feature by developers.","2024","2025-11-25 22:29:56","2025-11-25 22:48:15","","17–24","","","","","","","NLBSE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","Large language models; Java; LLM; Maintenance; Codes; Taxonomy; Readability; Natural languages; Production; Display names; JUnit; Test smells","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XCKQQQRT","journalArticle","2025","Liu, Liwei; Wang, Tao; Chen, Wei; Wei, Jun; Wang, Wei; Wu, Guoquan","LEGO: Synthesizing IoT Device Components Based on Static Analysis and Large Language Models","Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.","","","10.1145/3729482","https://doi.org/10.1145/3729482","IoT device components—digital representations of IoT devices within a platform and typically developed using Software Development Kits (SDKs)—are essential for ensuring seamless connectivity between IoT platforms and physical devices. However, developing these components demands extensive domain knowledge, as developers must understand the necessary elements of an IoT device and effectively utilize SDKs. Unfortunately, limited research has focused on automating this process, resulting in labor-intensive, time-consuming development.To tackle these challenges, we introduce LEGO, a method for synthesizing IoT device components based on the observation that APIs provided by device SDKs would eventually call network protocol methods to access physical devices. LEGO analyzes the SDK source code to identify candidate APIs that communicate with physical devices. Using static analysis, it generates a dataflow-enhanced call graph, extracts call paths containing network protocol methods, and heuristically identifies APIs that invoke these methods. To efficiently classify each API type and infer relevant device properties, LEGO employs a large language model-based program comprehension technique with an information-augmented prompt. LEGO then synthesizes device components using a platform-specific template, built from a common IoT device component model. It assembles IoT device components by populating the template with inferred properties and identified APIs, enabling developers to efficiently develop device components with minimal SDK knowledge. Comprehensive experiments on a set of open-source device SDKs and ten real-world IoT devices demonstrate the efficiency and effectiveness of LEGO in creating IoT device components.","2025-06","2025-11-25 22:29:56","2025-11-25 22:29:56","","","","2","9","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","LLM; Static Analysis; API; End-user Programming; IoT Device Component; SDK","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TF6R3RYL","conferencePaper","2025","Vandeputte, Frederik","Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems","Proceedings of the 2025 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software","979-8-4007-2151-9","","10.1145/3759429.3762620","https://doi.org/10.1145/3759429.3762620","Generative AI (GenAI) has emerged as a transformative technology, demonstrating remarkable capabilities across diverse application domains. However, GenAI faces several major challenges in developing reliable and efficient GenAI-empowered systems due to its unpredictability and inefficiency. This paper advocates for a paradigm shift: future GenAI-native systems should integrate GenAI's cognitive capabilities with traditional software engineering principles to create robust, adaptive, and efficient systems. We introduce foundational GenAI-native design principles centered around five key pillars—reliability, excellence, evolvability, self-reliance, and assurance—and propose architectural patterns such as GenAI-native cells, organic substrates, and programmable routers to guide the creation of resilient and self-evolving systems. Additionally, we outline the key ingredients of a GenAI-native software stack and discuss the impact of these systems from technical, user adoption, economic, and legal perspectives, underscoring the need for further validation and experimentation. Our work aims to inspire future research and encourage relevant communities to implement and refine this conceptual framework.","2025","2025-11-25 22:29:56","2025-11-25 22:29:56","","44–62","","","","","","","Onward! '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Singapore, Singapore","","","","GenAI; reliability; architectural patterns; best practices; design principles; excellence; software systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JEIYHVGP","conferencePaper","2024","Zhang, Yakun; Zhang, Wenjie; Ran, Dezhi; Zhu, Qihao; Dou, Chengfeng; Hao, Dan; Xie, Tao; Zhang, Lu","Learning-based Widget Matching for Migrating GUI Test Cases","Proceedings of the IEEE/ACM 46th International Conference on Software Engineering","979-8-4007-0217-4","","10.1145/3597503.3623322","https://doi.org/10.1145/3597503.3623322","GUI test case migration is to migrate GUI test cases from a source app to a target app. The key of test case migration is widget matching. Recently, researchers have proposed various approaches by formulating widget matching as a matching task. However, since these matching approaches depend on static word embeddings without using contextual information to represent widgets and manually formulated matching functions, there are main limitations of these matching approaches when handling complex matching relations in apps. To address the limitations, we propose the first learning-based widget matching approach named TEMdroid (TEst Migration) for test case migration. Unlike the existing approaches, TEMdroid uses BERT to capture contextual information and learns a matching model to match widgets. Additionally, to balance the significant imbalance between positive and negative samples in apps, we design a two-stage training strategy where we first train a hard-negative sample miner to mine hard-negative samples, and further train a matching model using positive samples and mined hard-negative samples. Our evaluation on 34 apps shows that TEM-droid is effective in event matching (i.e., widget matching and target event synthesis) and test case migration. For event matching, TEM-droid's Top1 accuracy is 76%, improving over 17% compared to baselines. For test case migration, TEMdroid's F1 score is 89%, also 7% improvement compared to the baseline approach.","2024","2025-11-25 22:29:56","2025-11-25 22:29:56","","","","","","","","","ICSE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","test migration; GUI testing; deep learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T7S7RG4Z","conferencePaper","2025","Piciorea, Stefania; Nguyen, ThanhVu","Bringing Invariant Analysis to modern IDEs: The DIG+ Extension for VS Code","Proceedings of the 34th ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-1474-0","","10.1145/3713081.3731737","https://doi.org/10.1145/3713081.3731737","Program invariants, which are properties that hold at specific program locations, are important in formal program verification and analysis. Traditional invariant generation methods using dynamic and static analyses are abundant and powerful, supporting a wide range of applications. However, these tools often remain underutilized due to their complex command-line interfaces and the technical expertise required for usage.To bridge the gap between research and practical application, we have developed DIG+, which integrates the DIG invariant generator and the CIVL symbolic execution tool using the Language Server Protocol design used in modern IDEs, such as VS Code. This integration allows users to generate and check invariants for C programs directly within their favorite IDEs, enhancing accessibility and usability. We hope DIG+ will inspire researchers to develop similar IDE integration for their research tools, making them more attractive to end users.","2025","2025-11-25 22:29:56","2025-11-25 22:29:56","","51–55","","","","","","","ISSTA Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","IDE; invariant generation and checking; LSP; vs code extension","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PM7LBUI6","conferencePaper","2024","Saavedra, Nuno; Silva, André; Monperrus, Martin","GitBug-Actions: Building Reproducible Bug-Fix Benchmarks with GitHub Actions","Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings","979-8-4007-0502-1","","10.1145/3639478.3640023","https://doi.org/10.1145/3639478.3640023","Bug-fix benchmarks are fundamental in advancing various sub-fields of software engineering such as automatic program repair (APR) and fault localization (FL). A good benchmark must include recent examples that accurately reflect technologies and development practices of today. To be executable in the long term, a benchmark must feature test suites that do not degrade overtime due to, for example, dependencies that are no longer available. Existing benchmarks fail in meeting both criteria. For instance, Defects4J, one of the foremost Java benchmarks, last received an update in 2020. Moreover, full-reproducibility has been neglected by the majority of existing benchmarks. In this paper, we present GitBug-Actions: a novel tool for building bug-fix benchmarks with modern and fully-reproducible bug-fixes. GitBug-Actions relies on the most popular CI platform, GitHub Actions, to detect bug-fixes and smartly locally execute the CI pipeline in a controlled and reproducible environment. To the best of our knowledge, we are the first to rely on GitHub Actions to collect bug-fixes. To demonstrate our toolchain, we deploy GitBug-Actions to build a proof-of-concept Go bug-fix benchmark containing executable, fully-reproducible bug-fixes from different repositories. A video demonstrating GitBug-Actions is available at: https://youtu.be/aBWwa1sJYBs.","2024","2025-11-25 22:29:56","2025-11-25 22:29:56","","1–5","","","","","","","ICSE-Companion '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","software testing; program analysis; bug benchmark; bug database; reproducibility; software bugs; github actions","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"87A73CPF","conferencePaper","2024","Gramacki, Piotr; Martins, Bruno; Szymański, Piotr","Evaluation of Code LLMs on Geospatial Code Generation","Proceedings of the 7th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery","979-8-4007-1176-3","","10.1145/3687123.3698286","https://doi.org/10.1145/3687123.3698286","Software development support tools have been studied for a long time, with recent approaches using Large Language Models (LLMs) for code generation. These models can generate Python code for data science and machine learning applications. LLMs are helpful for software engineers because they increase productivity in daily work. An LLM can also serve as a ""mentor"" for inexperienced software developers, and be a viable learning support. High-quality code generation with LLMs can also be beneficial in geospatial data science. However, this domain poses different challenges, and code generation LLMs are typically not evaluated on geospatial tasks. Here, we show how we constructed an evaluation benchmark for code generation models, based on a selection of geospatial tasks. We categorised geospatial tasks based on their complexity and required tools. Then, we created a dataset with tasks that test model capabilities in spatial reasoning, spatial data processing, and geospatial tools usage. The dataset consists of specific coding problems that were manually created for high quality. For every problem, we proposed a set of test scenarios that make it possible to automatically check the generated code for correctness. In addition, we tested a selection of existing code generation LLMs for code generation in the geospatial domain. We share our dataset and reproducible evaluation code on a public GitHub repository1, arguing that this can serve as an evaluation benchmark for new LLMs in the future. Our dataset will hopefully contribute to the development new models capable of solving geospatial coding tasks with high accuracy. These models will enable the creation of coding assistants tailored for geospatial applications.","2024","2025-11-25 22:29:56","2025-11-25 22:29:56","","54–62","","","","","","","GeoAI '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Atlanta, GA, USA","","","","large language models; code generation; geospatial data science","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YTE5IHFF","conferencePaper","2025","Liu, Xinpeng; Wang, Qinying; Liu, Peiyu; Wang, Wenhai; Ji, Shouling","MQueez: Specification-Driven Fuzzing for MQTT Broker (Registered Report)","Proceedings of the 34th ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-1474-0","","10.1145/3713081.3731724","https://doi.org/10.1145/3713081.3731724","Recently, the MQTT protocol, favored for its lightweight nature, has emerged as a preferred choice for IoT communications. However, MQTT brokers—the critical components responsible for message routing— are vulnerable to memory corruption, posing significant security risks. Although several fuzzers have been proposed to uncover memory corruption in brokers, their effectiveness is constrained by two fundamental limitations. First, existing fuzzers struggle to satisfy MQTT's complex constraints when generating valid test cases. Second, the protocol's extensive field variations across different packets complicate the mutation process, as existing black-box fuzzers cannot prioritize high-risk fields, leading to blind mutations.To address these challenges, we propose the Interaction Constraints Model (ICM), designed to finely represent MQTT protocol constraints. Then, we generate test cases following constraints by traversing ICM, ensuring compliant interactions that cover complex scenarios and minimize abnormal connection interruptions. Furthermore, we design a heuristic strategy for mutation energy allocation. By parsing responses in real-time, we adjust the energy allocation dynamically to concentrate on the fields more prone to bugs. Finally, we implement prototype MQueez, a new framework for MQTT protocol modeling, and efficient MQTT broker fuzzing. We evaluated MQueez on six widely used MQTT brokers and compared it with state-of-the-art fuzzers. The result shows that MQueez achieved a 30.88% improvement in compliance interaction within test cases, successfully identified five new vulnerabilities, and reproduced more than 150% known bugs that other fuzzers could not.","2025","2025-11-25 22:29:56","2025-11-25 22:29:56","","133–142","","","","","","","ISSTA Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","black-box fuzzing; MQTT modeling; protocol fuzzing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JSU2XZTZ","journalArticle","2025","Qiu, Ketai; Puccinelli, Niccolò; Ciniselli, Matteo; Di Grazia, Luca","From Today’s Code to Tomorrow’s Symphony: The AI Transformation of Developer’s Routine by 2030","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3709353","https://doi.org/10.1145/3709353","In the rapidly evolving landscape of software engineering, the integration of AI into the Software Development Lifecycle (SDLC) heralds a transformative era for developers. Recently, we have assisted to a pivotal shift toward AI-assisted programming, exemplified by tools like GitHub Copilot and OpenAI’s ChatGPT, which have become a crucial element for coding, debugging, and software design. In this article, we provide a comparative analysis between the current state of AI-assisted programming in 2024 and our projections for 2030, by exploring how AI advancements are set to enhance the implementation phase, fundamentally altering developers’ roles from manual coders to orchestrators of AI-driven development ecosystems. We envision HyperAssistant, an augmented AI tool that offers comprehensive support to 2030 developers, addressing current limitations in mental health support, fault detection, code optimization, team interaction, and skill development. We emphasize AI as a complementary force, augmenting developers’ capabilities rather than replacing them, leading to the creation of sophisticated, reliable, and secure software solutions. Our vision seeks to anticipate the evolution of programming practices, challenges, and future directions, shaping a new paradigm where developers and AI collaborate more closely, promising a significant leap in SE efficiency, security, and creativity.","2025-05","2025-11-25 22:29:56","2025-11-25 22:29:56","","","","5","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Software Engineering; AI for Code; Human Factors in Software Engineering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"26GZ6VMZ","journalArticle","2025","Xia, Yifan; Xie, Zichen; Liu, Peiyu; Lu, Kangjie; Liu, Yan; Wang, Wenhai; Ji, Shouling","Beyond Static Pattern Matching? Rethinking Automatic Cryptographic API Misuse Detection in the Era of LLMs","Proc. ACM Softw. Eng.","","","10.1145/3728875","https://doi.org/10.1145/3728875","While the automated detection of cryptographic API misuses has progressed significantly, its precision diminishes for intricate targets due to the reliance on manually defined patterns. Large Language Models (LLMs) offer a promising context-aware understanding to address this shortcoming, yet the stochastic nature and the hallucination issue pose challenges to their applications in precise security analysis. This paper presents the first systematic study to explore LLMs’ application in cryptographic API misuse detection. Our findings are noteworthy: The instability of directly applying LLMs results in over half of the initial reports being false positives. Despite this, the reliability of LLM-based detection could be significantly enhanced by aligning detection scopes with realistic scenarios and employing a novel code &amp; analysis validation technique, achieving a nearly 90% detection recall. This improvement substantially surpasses traditional methods and leads to the discovery of previously unknown vulnerabilities in established benchmarks. Nevertheless, we identify recurring failure patterns that illustrate current LLMs’ blind spots, including cryptographic knowledge deficiencies and code semantics misinterpretations. Leveraging these findings, we deploy an LLM-based detection system and uncover 63 new vulnerabilities (47 confirmed, 7 already fixed) in open-source Java and Python repositories, including prominent projects like Apache.","2025-06","2025-11-25 22:29:56","2025-11-25 22:29:56","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","API Misuse Detection; Large Language Models for Software Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FH5CD2WI","conferencePaper","2024","Szatmári, Attila; Sarhan, Qusay Idrees; Soha, Peter Attila; Balogh, Gergo; Beszedes, Arpad","On the Integration of Spectrum-Based Fault Localization Tools into IDEs","Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments","979-8-4007-0580-9","","10.1145/3643796.3648448","https://doi.org/10.1145/3643796.3648448","Spectrum-Based Fault Localization (SBFL) is a technique to be used during debugging, the premise of which is that, based on the test case outcomes and code coverage, faulty code elements can be automatically detected. SBFL is popular among researchers because it is lightweight and easy to implement, and there is a lot of potential in it when it comes to research that aims to improve its effectiveness. Despite this, the technique cannot be found in contemporary development and debugging tools, only a handful of research prototypes are available. Reasons for this can be multiple, including the algortihms' sub-optimal effectiveness and other technical weaknesses. But, also the lack of clear functional and non-functional requirements for such a tool, either standalone or integrated into IDEs. In this paper, we attempt to provide such a list in form of recommendations, based on surveying the most popular SBFL tools and on our own researchers' and tool builders' experience.","2024","2025-11-25 22:29:56","2025-11-25 22:29:56","","24–29","","","","","","","IDE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","IDE; debugging; SBFL; spectrum-based fault localization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YCEA4W4E","journalArticle","2025","Tambon, Florian; Nikanjam, Amin; Zid, Cyrine; Khomh, Foutse; Antoniol, Giuliano","TaskEval: Assessing Difficulty of Code Generation Tasks for Large Language Models","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3773285","https://doi.org/10.1145/3773285","Large Language Models (LLMs) excel in code-related tasks like code generation, but benchmark evaluations often overlook task characteristics, such as difficulty. Moreover, benchmarks are usually built using tasks described with a single prompt, despite the formulation of prompts having a profound impact on the outcome. This paper introduces a generalist approach, TaskEval, a framework using diverse prompts and Item Response Theory (IRT) to efficiently assess LLMs’ capabilities and benchmark task characteristics, improving the understanding of their performance.Using two code generation benchmarks, HumanEval+ and ClassEval, as well as 8 code generation LLMs, we show that TaskEval is capable of characterising the properties of tasks. Using topic analysis, we identify and analyse the tasks of 17 and 21 topics within the benchmarks. We also cross-analyse tasks’ characteristics with programming constructs (e.g., variable assignment, conditions, etc.) used by LLMs, emphasising some patterns with tasks’ difficulty. Finally, we conduct a comparison between the difficulty assessment of tasks by human annotators and LLMs. Orthogonal to current benchmarking evaluation efforts, TaskEval can assist researchers and practitioners in fostering better assessments of LLMs. The tasks’ characteristics can be used to identify shortcomings within existing benchmarks or improve the evaluation of LLMs.","2025-10","2025-11-25 22:29:56","2025-11-25 22:29:56","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; Code Benchmark Assessments; Item Response Theory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NQ7SLKVD","journalArticle","2024","Huang, Qing; Luo, Zhiwen; Xing, Zhenchang; Zeng, Jinshan; Chen, Jieshan; Xu, Xiwei; Chen, Yong","Revealing the Unseen: AI Chain on LLMs for Predicting Implicit Dataflows to Generate Dataflow Graphs in Dynamically Typed Code","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3672458","https://doi.org/10.1145/3672458","Dataflow graphs (DFGs) capture definitions (defs) and uses across program blocks, which is a fundamental program representation for program analysis, testing and maintenance. However, dynamically typed programming languages like Python present implicit dataflow issues that make it challenging to determine def-use flow information at compile time. Static analysis methods like Soot and WALA are inadequate for handling these issues, and manually enumerating comprehensive heuristic rules is impractical. Large pre-trained language models (LLMs) offer a potential solution, as they have powerful language understanding and pattern matching abilities, allowing them to predict implicit dataflow by analyzing code context and relationships between variables, functions, and statements in code. We propose leveraging LLMs’ in-context learning ability to learn implicit rules and patterns from code representation and contextual information to solve implicit dataflow problems. To further enhance the accuracy of LLMs, we design a five-step chain of thought (CoT) and break it down into an Artificial Intelligence (AI) chain, with each step corresponding to a separate AI unit to generate accurate DFGs for Python code. Our approach’s performance is thoroughly assessed, demonstrating the effectiveness of each AI unit in the AI Chain. Compared to static analysis, our method achieves 82% higher def coverage and 58% higher use coverage in DFG generation on implicit dataflow. We also prove the indispensability of each unit in the AI Chain. Overall, our approach offers a promising direction for building software engineering tools by utilizing foundation models, eliminating significant engineering and maintenance effort, but focusing on identifying problems for AI to solve.","2024-09","2025-11-25 22:29:56","2025-11-25 22:29:56","","","","7","33","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; AI chain; Dataflow graph","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8RB3G3NF","conferencePaper","2025","Tan, Sixu; Li, Zeyu; Liu, Zhutian; Patel, Harsh; Tan, Zhaowei","Automated Model-Based Fuzzing for 5G O-RAN","Proceedings of the 31st Annual International Conference on Mobile Computing and Networking","979-8-4007-1129-9","","10.1145/3680207.3723469","https://doi.org/10.1145/3680207.3723469","The evolution of 5G and future-G technologies has led to the development of an open, distributed, and multi-vendor architecture known as Open Radio Access Network (O-RAN). While O-RAN offers greater flexibility and stimulates innovation, it also introduces potential issues such as bugs, inconsistencies, and vulnerabilities. In this paper, we present a novel model-based fuzzing system, ARCANE, to address these challenges. Unlike existing fuzzing techniques that struggle with the complexity and dynamism of O-RAN, ARCANE distinguishes itself by employing a smart, model-based approach. It first analyzes O-RAN specifications using a Large Language Model (LLM) and incorporates a unique method that integrates passive model learning to refine the model. With this refined model, ARCANE designs an O-RAN-aware, model-based fuzzing scheme that maintains high efficiency by adhering to O-RAN's specific semantics and syntax. We implement ARCANE and evaluate it on the OAI5G. It finds 9 root bugs from three categories, all having serious security implications. ARCANE has shown superior efficiency and coverage compared to state-of-the-arts.","2025","2025-11-25 22:29:56","2025-11-25 22:29:56","","201–215","","","","","","","ACM MOBICOM '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Kerry Hotel, Hong Kong, Hong Kong, China","","","","large language models (LLM); 5G security; automata learning; model-based fuzzing; open radio access network (O-RAN); protocol testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YIXJKKDN","conferencePaper","2024","Duque-Torres, Alejandra","Selecting and Constraining Metamorphic Relations","Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings","979-8-4007-0502-1","","10.1145/3639478.3639781","https://doi.org/10.1145/3639478.3639781","Software testing is a critical aspect of ensuring the reliability and quality of software systems. However, it often poses challenges, particularly in determining the expected output of a System Under Test (SUT) for a given set of inputs, a problem commonly referred to as the test oracle problem. Metamorphic Testing (MT) offers a promising solution to the test oracle problem by examining the relations between input-output pairs in consecutive executions of the SUT. These relations, referred to as Metamorphic Relations (MRs), define the expected changes in the output when specific changes are made to the input. Our research is focused on developing methods and tools to assist testers in the selection of MRs, the definition of constraints, and providing explanations for MR outcomes. The research is divided in three parts. The first part focuses on MR collection and description, entailing the creation of a comprehensive repository of MRs from various sources. A standardised MR representation is devised to promote machine-readability and wide-ranging applicability. The second part introduces MetraTrimmer, a test-data-driven approach for systematically selecting and constraining MRs. This approach acknowledges that MRs may not be universally applicable to all test data space. The final part, evaluation and validation, encompasses empirical studies aimed at assessing the effectiveness of the developed methods and validating their suitability for real-world regression testing scenarios. Through this research, we aim to advance the automation of MR generation, enhance the understanding of MR violations, and facilitate their effective application in regression testing.","2024","2025-11-25 22:29:56","2025-11-25 22:29:56","","212–216","","","","","","","ICSE-Companion '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","metamorphic testing; test oracle; metamorphic relations; pattern mining; test data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AKDWIXPA","conferencePaper","2025","Ganiyu, Abiodun; Gajjar, Pranshav; Shah, Vijay K","AI5GTest: AI-Driven Specification-Aware Automated Testing and Validation of 5G O-RAN Components","18th ACM Conference on Security and Privacy in Wireless and Mobile Networks","979-8-4007-1530-3","","10.1145/3734477.3734703","https://doi.org/10.1145/3734477.3734703","The advent of Open Radio Access Networks (O-RAN) has transformed the telecommunications industry by promoting interoperability, vendor diversity, and rapid innovation. However, its disaggregated architecture introduces complex testing challenges, particularly in validating multi-vendor components against O-RAN ALLIANCE and 3GPP specifications. Existing frameworks, such as those provided by Open Testing and Integration Centres (OTICs), rely heavily on manual processes, are fragmented and prone to human error, leading to inconsistency and scalability issues. To address these limitations, we present AI5GTest – an AI-powered, specification-aware testing framework designed to automate the validation of O-RAN components. AI5GTest leverages a cooperative Large Language Models (LLM) framework consisting of Gen-LLM, Val-LLM, and Debug-LLM. Gen-LLM automatically generates expected procedural flows for test cases based on 3GPP and O-RAN specifications, while Val-LLM cross-references signaling messages against these flows to validate compliance and detect deviations. If anomalies arise, Debug-LLM performs root cause analysis, providing insight to the failure cause. To enhance transparency and trustworthiness, AI5GTest incorporates a human-in-the-loop mechanism, where the Gen-LLM presents top-k relevant official specifications to the tester for approval before proceeding with validation. Evaluated using a range of test cases obtained from O-RAN TIFG and WG5-IOT test specifications, AI5GTest demonstrates a significant reduction in overall test execution time compared to traditional manual methods, while maintaining high validation accuracy.","2025","2025-11-25 22:29:56","2025-11-25 22:29:56","","53–64","","","","","","","WiSec 2025","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Arlington, VA, USA","","","","llm; automated testing; o-ran; 3gpp; ai5gtest","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CPTB5TSX","journalArticle","2025","Yu, Shengcheng; Fang, Chunrong; Liu, Jia; Chen, Zhenyu","Test Script Intention Generation for Mobile Application via GUI Image and Code Understanding","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3722105","https://doi.org/10.1145/3722105","Testing is the most direct and effective technique to ensure software quality. Test scripts always play a more important role in mobile app testing than test cases for source code, due to the GUI-intensive and event-driven characteristics of mobile applications (app). Test scripts focus on user interactions and the corresponding response events, which is significant for testing the target app functionalities. Therefore, it is critical to understand the test scripts for better script maintenance and modification. There exist some mature code understanding (i.e., code comment generation, code summarization) technologies that can be directly applied to functionality source code with business logic. However, such technologies will have difficulties when being applied to test scripts, because test scripts are loosely linked to apps under test (AUT) by widget selectors, and do not contain business logic themselves.In order to solve the test script understanding gap, this paper presents a novel approach, namely TestIntention, to infer the intention of GUI test scripts. Test intention refers to the user expectations of app behaviors for specific operations. TestIntention formalizes test scripts with an operation sequence model. For each operation within the sequence, TestIntention extracts the target widget selector and links the selector to the GUI layout information or the corresponding response events. For widgets identified by XPath, TestIntention utilizes the image understanding technologies to explore the detailed information of the widget images, the intention of which is understood with a deep learning model. For widgets identified by ID, TestIntention first maps the selectors to the response methods with business logic, and then adopts code understanding technologies to describe code in natural language form. Results of all operations are combined to generate test intention for test scripts. An empirical experiment including different metrics proves the outstanding performance of TestIntention, outperforming baselines by much. Also, it is shown that TestIntention can save about 80% developers’ time to understand test scripts.","2025-03","2025-11-25 22:29:57","2025-11-25 22:29:57","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Mobile App Testing; Code Understanding; GUI Understanding","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"85TYDJBS","conferencePaper","2024","Hu, Qianchao; Wang, Feng; Liu, Binglin; Liu, Haitian","Research on Deep Neural Network Testing Techniques","Proceedings of the 2023 4th International Conference on Machine Learning and Computer Application","979-8-4007-0944-9","","10.1145/3650215.3650237","https://doi.org/10.1145/3650215.3650237","Profiting by the rapid development of computer science and technology, deep neural networks have been widely used in security-related fields such as face recognition, automatic driving, medical diagnosis and decision-making reasoning, and there is an urgent need for testers to conduct comprehensive and in-depth testing of these software to ensure their quality and security. However, intelligent software based on neural networks is fundamentally different from traditional software. In recent years, more and more researchers have shifted their attention from traditional software testing to intelligent software testing, and a series of evaluation criteria, test frameworks, and test case generation methods, etc. have been proposed for deep neural network models. This paper summarises and concludes the existing research from the perspectives of testing techniques based on test adequacy theory, testing techniques based on traditional testing theory and testing techniques based on adversarial samples. Finally, it summarises and looks forward to deep neural network testing and points out the problems in deep neural network testing, in order to provide some thoughts for researchers in related fields.","2024","2025-11-25 22:29:57","2025-11-25 22:29:57","","113–119","","","","","","","ICMLCA '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Hangzhou, China","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WRTSTDIS","conferencePaper","2024","Rasnayaka, Sanka; Wang, Guanlin; Shariffdeen, Ridwan; Iyer, Ganesh Neelakanta","An Empirical Study on Usage and Perceptions of LLMs in a Software Engineering Project","Proceedings of the 1st International Workshop on Large Language Models for Code","979-8-4007-0579-3","","10.1145/3643795.3648379","https://doi.org/10.1145/3643795.3648379","Large Language Models (LLMs) represent a leap in artificial intelligence, excelling in tasks using human language(s). Although the main focus of general-purpose LLMs is not code generation, they have shown promising results in the domain. However, the usefulness of LLMs in an academic software engineering project has not been fully explored yet. In this study, we explore the usefulness of LLMs for 214 students working in teams consisting of up to six members. Notably, in the academic course through which this study is conducted, students were encouraged to integrate LLMs into their development tool-chain, in contrast to most other academic courses that explicitly prohibit the use of LLMs.In this paper, we analyze the AI-generated code, prompts used for code generation, and the human intervention levels to integrate the code into the code base. We also conduct a perception study to gain insights into the perceived usefulness, influencing factors, and future outlook of LLM from a computer science student's perspective. Our findings suggest that LLMs can play a crucial role in the early stages of software development, especially in generating foundational code structures, and helping with syntax and error debugging. These insights provide us with a framework on how to effectively utilize LLMs as a tool to enhance the productivity of software engineering students, and highlight the necessity of shifting the educational focus toward preparing students for successful human-AI collaboration.","2024","2025-11-25 22:29:57","2025-11-25 22:29:57","","111–118","","","","","","","LLM4Code '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","software engineering; LLM for code generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QQCSFFIJ","conferencePaper","2024","Piya, Sanyogita; Sullivan, Allison","LLM4TDD: Best Practices for Test Driven Development Using Large Language Models","Proceedings of the 1st International Workshop on Large Language Models for Code","979-8-4007-0579-3","","10.1145/3643795.3648382","https://doi.org/10.1145/3643795.3648382","In today's society, we are becoming increasingly dependent on software systems. However, we also constantly witness the negative impacts of buggy software. Program synthesis aims to improve software correctness by automatically generating the program given an outline of the expected behavior. For decades, program synthesis has been an active research field, with recent approaches looking to incorporate Large Language Model. This paper explores the concept of LLM4TDD, where we guide Large Language Models to generate code iteratively using a test-driven development methodology. We conduct an empirical evaluation using ChatGPT and coding problems from LeetCode to investigate the impact of different test, prompt and problem attributes on the efficacy of LLM4TDD.","2024","2025-11-25 22:29:57","2025-11-25 22:29:57","","14–21","","","","","","","LLM4Code '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WG5TX8KB","journalArticle","2025","Zhou, Bo; Shi, Jiaqi; Wang, Ying; Li, Li; Li, Tsz On; Yu, Hai; Zhu, Zhiliang","Porting Software Libraries to OpenHarmony: Transitioning from TypeScript or JavaScript to ArkTS","Proc. ACM Softw. Eng.","","","10.1145/3728941","https://doi.org/10.1145/3728941","OpenHarmony emerges as a potent force in the mobile app domain, poised to stand alongside established industry giants. ArkTS is its main language, enhancing TypeScript (TS) and JavaScript (JS) with strict typing for improved performance. Developers are encouraged to port popular TS/JS libraries to OpenHarmony, supported by detailed guidelines. However, this requires a deep understanding of ArkTS syntax, following porting specifications, and making manual changes. An automated solution is crucial to streamline this process and foster a robust software ecosystem. As a new programming language, ArkTS currently lacks essential analysis tools for automated analysis and porting of software libraries. However, the rise of Large Language Models (LLMs) shows promise for effectively addressing automated porting tasks. There are two challenges in using LLMs to automate the porting of TS/JS libraries to OpenHarmony: (1) LLMs have limited exposure to ArkTS code, making it difficult for them to grasp the syntactical differences between ArkTS and JS/TS, as well as the various adaptation scenarios. (2) Project-level code adaptation often involves correcting numerous syntax mismatches, which complicates matters for LLMs as they must handle the interactions between different mismatches and interdependent code. In response, we introduce ArkAdapter, a project-level automatic code adaptation approach. ArkAdapter addresses Challenge 1 by establishing an adaptation knowledge repository for ArkTS syntax comprehension. It expands a collection of real code adaptation examples based on expert experience across various scenarios, improving the adaptation capabilities of LLMs through few-shot learning. ArkAdapter overcomes Challenge 2 based on an adaptation priority strategy by considering both the dependency structure and the granularity of syntax-mismatching code. This strategy helps prevent interference among various syntax mismatches and their interdependent code. Evaluation shows ArkAdapter achieves high precision (86.84","2025-06","2025-11-25 22:29:57","2025-11-25 22:29:57","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","LLM; ArkTS; OpenHarmony; Software Adaptation; Software Libraries","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"767IBZNA","conferencePaper","2025","Jeung, Matthew; Sathya, Anup; Qian, Wanli; Arellano, Steven; Jimenez, Luke; Nakagaki, Ken","Shape n’ Swarm: Hands-on, Shape-aware Generative Authoring with Swarm UI and LLMs","Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology","979-8-4007-2037-6","","10.1145/3746059.3747781","https://doi.org/10.1145/3746059.3747781","This paper introduces a novel authoring method for swarm user interfaces that combines hands-on shape manipulation and speech to convey intent for generative motion and interaction. We refer to this authoring method as shape-aware generative authoring, which is generalizable to actuated tangible user interfaces. The proof-of-concept Shape n’ Swarm tool allows users to create diverse animations and interactions with tabletop robots by hand-arranging the robots and providing spoken instructions. The system employs multiple script-generating LLM agents that work together to handle user inputs for three major generative tasks: (1) thematically interpreting the shapes created by users; (2) creating animations for the manipulated shape; and (3) flexibly building interaction by mapping I/O. In a user study (n = 11), participants could easily create diverse physical animations and interactions without coding. To lead this novel research space, we also share limitations, research challenges, and design recommendations.","2025","2025-11-25 22:29:57","2025-11-25 22:29:57","","","","","","","","","UIST '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Large Language Models; Actuated Tangible User Interfaces; Swarm User Interfaces; Tangible Interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QELLL565","journalArticle","2025","Qiu, Ruidi; Zhang, Grace Li; Drechsler, Rolf; Ho, Tsungyi; Schlichtmann, Ulf; Li, Bing","ConfiBench: Automatic Testbench Generation with Confidence-Based Scenario Mask and Testbench Ensemble using LLMs for HDL Design","ACM Trans. Des. Autom. Electron. Syst.","","1084-4309","10.1145/3773087","https://doi.org/10.1145/3773087","Functional simulation is an essential step in digital hardware design. Recently, there has been a growing interest in leveraging Large Language Models (LLMs) for hardware testbench generation tasks. However, the inherent instability associated with LLMs often leads to functional errors in the generated testbenches. Previous methods do not incorporate automatic functional correction mechanisms without human intervention and still suffer from low success rates, especially for sequential tasks. To address this issue, we propose ConfiBench, an automatic testbench generation framework with functional self-validation, self-correction, scenario masking and an ensemble of multiple testbenches. Utilizing only the RTL specification in natural language, the proposed approach can validate the correctness of the generated testbenches, perform functional self-correction on the generated testbenches and construct an ensemble of them with effective masks. The comparative analysis demonstrates that our method achieves a pass rate of 72.22% across all evaluated tasks, compared with the previous LLM-based testbench generation framework’s 52.18% and a direct LLM-based generation method’s 33.33%. Specifically in sequential circuits, our work’s performance achieves a testbench pass rate that is 21.06 percentage points higher than previous work AutoBench in sequential tasks and almost 5 times the pass rate of the direct method. More importantly, ConfiBench significantly improves the correctness of generated testbenches under golden RTLs, particularly for sequential circuits, which had been a major weakness in the previous work, CorrectBench. The codes and experimental results are open-sourced at the link: https://github.com/AutoBench/ConfiBench.","2025-10","2025-11-25 22:29:57","2025-11-25 22:29:57","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GCWX5GWK","conferencePaper","2025","Silvis-Cividjian, Natalia; Kenyon, Joshua; Gallup, Maximilian; Groot, Elias; Wezenbeek, Hugo van; Lira-Cossio, Eduardo; Althuisius, Niels","Integrating Small-scale Autonomous Vehicles in CS Education: An Experience Report","Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 1","979-8-4007-1567-9","","10.1145/3724363.3729078","https://doi.org/10.1145/3724363.3729078","Teaching software systems engineering is neither effective, nor inspiring if students cannot practice the conveyed theory. We report on a hands-on approach that closes the gap by means of off-the-shelf and in-house realistic miniature models of autonomous vehicles. It is innovative that the infrastructure extends beyond the widely-used solutions, to digitally controlled cars and trains, with open hardware and extendable sensory/actuating capabilities (cameras, opto-sensors, proximity sensors, LEDs, displays) operating in realistic mini-environments, including line-marked roads, railways, traffic-lights and -signs, tunnels and railroad switches. For many years already, we have been using these vehicles to structurally teach physical computing (300 undergraduates yearly) and systems testing (12 graduates yearly), and for incidental individual research projects. Recently, we started two initiatives that stimulate a more dynamic know-how transfer across generations. The first is a better scalable undergraduate capstone project, where groups of students work on a 'hot' research topic in autonomous driving. The second motivates student teams to go the extra mile in an international intelligent car race competition. Evaluations showed that although unusual for a non-engineering curriculum, 'playing' with autonomous vehicles is an excellent strategy to discover the interaction of software with hardware and environment, to better consolidate existing knowledge on programming and testing, and to explore new fields, such as AI-based computer vision, navigation and safety, adding in all cases more fun and motivation. We share the design of the scaffolding and teaching initiatives, together with lessons learned which will hopefully inspire other educators in shaping engaging and future-proof CS curricula.","2025","2025-11-25 22:29:57","2025-11-25 22:29:57","","235–241","","","","","","","ITiCSE 2025","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Nijmegen, Netherlands","","","","software testing; autonomous systems engineering; cs capstone project; instructional scaffolding; intelligent cars competitions; mobile robotics; physical computing; physical manipulatives; small-scale vehicles; unattended train operation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W6YD6SHC","conferencePaper","2024","Majdoub, Yacine; Ben Charrada, Eya","Debugging with Open-Source Large Language Models: An Evaluation","Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement","979-8-4007-1047-6","","10.1145/3674805.3690758","https://doi.org/10.1145/3674805.3690758","Large language models have shown good potential in supporting software development tasks. This is why more and more developers turn to LLMs (e.g. ChatGPT) to support them in fixing their buggy code. While this can save time and effort, many companies prohibit it due to strict code sharing policies. To address this, companies can run open-source LLMs locally. But until now there is not much research evaluating the performance of open-source large language models in debugging. This work is a preliminary evaluation of the capabilities of open-source LLMs in fixing buggy code. The evaluation covers five open-source large language models and uses the benchmark DebugBench which includes more than 4000 buggy code instances written in Python, Java and C++. Open-source LLMs achieved scores ranging from 43.9% to 66.6% with DeepSeek-Coder achieving the best score for all three programming languages.","2024","2025-11-25 22:29:57","2025-11-25 22:29:57","","510–516","","","","","","","ESEM '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Barcelona, Spain","","","","Large Language Models; Debugging; Open-Source LLMs","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VRDSWW2V","conferencePaper","2025","Braconaro, Elisa; Losiouk, Eleonora","A Dataset for Evaluating LLMs Vulnerability Repair Performance in Android Applications: Data/Toolset paper","Proceedings of the Fifteenth ACM Conference on Data and Application Security and Privacy","979-8-4007-1476-4","","10.1145/3714393.3726486","https://doi.org/10.1145/3714393.3726486","Automated Program Repair (APR) is a well-established research area that enhances software reliability and security by automatically fixing bugs, reducing manual effort, and accelerating debugging. Despite progress in publishing benchmarks to evaluate APR tools, datasets specifically targeting Android are lacking.To address this gap, we introduce a dataset of 272 real-world violations of Google's Android Security Best Practices, identified by statically analyzing 113 real-world Android apps. In addition to the faulty code, we manually crafted repairs based on Google's guidelines, covering 176 Java-based and 96 XML-based violations from Android Java classes and Manifest files, respectively. Additionally, we leveraged our novel dataset to evaluate Large Language Models (LLMs) as they are the latest promising APR tools. In particular, we evaluated GPT-4o, Gemini 1.5 Flash and Gemini in Android Studio and we found that GPT-4o outperforms Google's models, demonstrating higher accuracy and robustness across a range of violations types. Hence, with this dataset, we aim to provide valuable insights for advancing APR research and improving tools for Android security.","2025","2025-11-25 22:29:57","2025-11-25 22:29:57","","353–358","","","","","","","CODASPY '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Pittsburgh, PA, USA","","","","large language models; automated program repair; android vulnerabilities","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5VUTFKZB","conferencePaper","2025","Rahman, Shanto; Chanumolu, Bala Naren; Rafi, Suzzana; Shi, August; Lam, Wing","Ranking Relevant Tests for Order-Dependent Flaky Tests","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00178","https://doi.org/10.1109/ICSE55347.2025.00178","One major challenge of regression testing are flaky tests, i.e., tests that may pass in one run but fail in another run for the same version of code. One prominent category of flaky tests is order-dependent (OD) flaky tests, which can pass or fail depending on the order in which the tests are run. To help developers debug and fix OD tests, prior work attempts to automatically find OD-relevant tests, which are tests that determine whether an OD test passes or fails, depending on whether the OD-relevant tests run before or after the OD test. Prior work found OD-relevant tests by running different tests before the OD test, without considering each test's likelihood of being OD-relevant tests.We propose RankF to rank tests in order of likelihood of being OD-relevant tests, finding the first OD-relevant test for a given OD test more quickly. We propose two ranking approaches, each requiring different information. Our first approach, RankFL, relies on training a large-language model to analyze test code. Our second approach, RankFO, relies on analyzing prior test-order execution information. We evaluate our approaches on 155 OD tests across 24 open-source projects. We compare RankF against baselines from prior work, where we find that RankF finds the first OD-relevant test for an OD test faster than the best baseline; depending on the type of OD-relevant test, RankF takes 9.4 to 14.1 seconds on median, compared to the baseline's 34.2 to 118.5 seconds on median.","2025","2025-11-25 22:29:57","2025-11-25 22:29:57","","1999–2011","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SFFDKA56","conferencePaper","2025","Babatunde, Ibukun David; Nnanna, Obiabuchi Martin; Klein, Mark","Moderating Large Scale Online Deliberative Processes with Large Language Models (LLMs): Enhancing Collective Decision-Making.","Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing","979-8-4007-0629-5","","10.1145/3672608.3707925","https://doi.org/10.1145/3672608.3707925","This study investigates the use of LLMs, specifically ChatGPT-4o, to enhance the moderation of online deliberative processes. Traditionally, decision-making has been controlled by small groups, often excluding the vital insights that crowd intelligence can provide. As global challenges grow more complex, broader and more inclusive participation is essential. While online platforms allow for such large-scale participation, they also face significant issues, including content fragmentation, low signal-to-noise ratios, and inefficient argumentation. Human moderators can address these challenges, but scaling them is prohibitively costly. This research introduces a more scalable solution by leveraging LLMs to automate critical moderation tasks, including unbundling multiple ideas, categorizing them into solutions, metrics, and barriers, and implementing efficient argument mining and classification techniques. Additionally, it evaluates the effectiveness of different prompting styles in optimizing moderation. The findings demonstrate that LLMs can successfully moderate key aspects of large-scale online deliberations, such as unbundling and categorization, improving the structure of discussions and representing a significant step forward in collective decision-making.","2025","2025-11-25 22:29:57","2025-11-25 22:29:57","","996–1003","","","","","","","SAC '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Catania International Airport, Catania, Italy","","","","large language models; natural language processing; answer classification; argument mining; collective decision-making; online deliberation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WCBJ828J","conferencePaper","2024","Barnett, Scott; Kurniawan, Stefanus; Thudumu, Srikanth; Brannelly, Zach; Abdelrazek, Mohamed","Seven Failure Points When Engineering a Retrieval Augmented Generation System","Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI","979-8-4007-0591-5","","10.1145/3644815.3644945","https://doi.org/10.1145/3644815.3644945","Software engineers are increasingly adding semantic search capabilities to applications using a strategy known as Retrieval Augmented Generation (RAG). A RAG system involves finding documents that semantically match a query and then passing the documents to a large language model (LLM) such as ChatGPT to extract the right answer using an LLM. RAG systems aim to: a) reduce the problem of hallucinated responses from LLMs, b) link sources/references to generated responses, and c) remove the need for annotating documents with meta-data. However, RAG systems suffer from limitations inherent to information retrieval systems and from reliance on LLMs. In this paper, we present an experience report on the failure points of RAG systems from three case studies from separate domains: research, education, and biomedical. We share the lessons learned and present 7 failure points to consider when designing a RAG system. The two key takeaways arising from our work are: 1) validation of a RAG system is only feasible during operation, and 2) the robustness of a RAG system evolves rather than designed in at the start. We conclude with a list of potential research directions on RAG systems for the software engineering community.","2024","2025-11-25 22:29:57","2025-11-25 22:47:51","","194–199","","","","","","","CAIN '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","Software; Chatbots; Education; Retrieval Augmented Generation; case study; retrieval augmented generation; RAG; Robustness; SE4AI; Task analysis; Semantic search; Case Study; Information retrieval","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"54LVMTJ8","journalArticle","2025","Yang, Huiwen; Zhou, Yu; Chen, Taolue","SimADFuzz: Simulation-Feedback Fuzz Testing for Autonomous Driving Systems","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3744242","https://doi.org/10.1145/3744242","Autonomous driving systems (ADS) have achieved remarkable progress in recent years. However, ensuring their safety and reliability remains a critical challenge due to the complexity and uncertainty of driving scenarios. In this paper, we focus on simulation testing for ADS, where generating diverse and effective testing scenarios is a central task. Existing fuzz testing methods face limitations, such as overlooking the temporal and spatial dynamics of scenarios and failing to leverage simulation feedback (e.g., speed, acceleration and heading) to guide scenario selection and mutation. To address these issues, we propose SimADFuzz, a novel framework designed to generate high-quality scenarios that reveal violations in ADS behavior. Specifically, SimADFuzz employs violation prediction models, which evaluate the likelihood of ADS violations, to optimize scenario selection. Moreover, SimADFuzz proposes distance-guided mutation strategies to enhance interactions among vehicles in offspring scenarios, thereby triggering more edge-case behaviors of vehicles. Comprehensive experiments demonstrate that SimADFuzz outperforms state-of-the-art fuzzers by identifying 73 more unique violations, including 5 reproducible cases of vehicle-vehicle, vehicle-pedestrian and vehicle-roadside collisions. These results demonstrate SimADFuzz's effectiveness in enhancing the robustness and safety of autonomous driving systems.","2025-06","2025-11-25 22:29:57","2025-11-25 22:29:57","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Fuzz Testing; Autonomous Driving Systems; Simulation-based Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N6NSQ5L8","journalArticle","2025","Taleby Ahvanooey, Milad; Mazurczyk, Wojciech; Lee, Dongwon","Socioeconomic Threats of Deepfakes and the Role of Cyber-Wellness Education in Defense","Commun. ACM","","0001-0782","10.1145/3715317","https://doi.org/10.1145/3715317","In recent years, society has witnessed accelerated advancement in generative artificial intelligence (GenAI) technologies, which may be viewed as a double-edged sword. On one hand, GenAI tools can be used to create synthetic content legitimately. For example, advertising agencies may, with permission, generate celebrities’ images or videos using GenAI tools without putting them in front of cameras and thus reducing the overall cost of media construction. On the other hand, scammers may utilize GenAI tools to craft or edit artificial content (for example, texts, images, videos, or audio), so-called deepfakes, to mislead or deceive netizens with robocalls or voice cloning phishing, potentially causing detrimental consequences for society. This article briefly debates emerging socioeconomic threats of deepfakes in today’s society and how cyber-wellness (or digital media literacy) education can help netizens mitigate their risks.To the mitigate the risks of deepfakes, enhanced cyber-wellness programs are needed that empower both producers and consumers of generative AI–based content.","2025-08","2025-11-25 22:29:57","2025-11-25 22:29:57","","70–79","","9","68","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I6H9LZHP","journalArticle","2024","Cassano, Federico; Gouwar, John; Lucchetti, Francesca; Schlesinger, Claire; Freeman, Anders; Anderson, Carolyn Jane; Feldman, Molly Q; Greenberg, Michael; Jangda, Abhinav; Guha, Arjun","Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs","Proc. ACM Program. Lang.","","","10.1145/3689735","https://doi.org/10.1145/3689735","Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as building blocks for research in programming languages and software engineering. However, the quality of code produced by a Code LLM varies significantly by programming language. Code LLMs produce impressive results on high-resource programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages that have limited training data available (e.g., OCaml, Racket, and several others). This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach, called MultiPL-T, generates high-quality datasets for low-resource languages, which can then be used to fine-tune any pretrained Code LLM. MultiPL-T translates training data from high-resource languages into training data for low-resource languages in the following way. 1) We use a Code LLM to synthesize unit tests for commented code from a high-resource source language, filtering out faulty tests and code with low test coverage. 2) We use a Code LLM to translate the code from the high-resource source language to a target low-resource language. This gives us a corpus of candidate training data in the target language, but many of these translations are wrong. 3) We use a lightweight compiler to compile the test cases generated in (1) from the source language to the target language, which allows us to filter our obviously wrong translations. The result is a training corpus in the target low-resource language where all items have been validated with test cases. We apply this approach to generate tens of thousands of new, validated training items for five low-resource languages: Julia, Lua, OCaml, R, and Racket, using Python as the source high-resource language. Furthermore, we use an open Code LLM (StarCoderBase) with open training data (The Stack), which allows us to decontaminate benchmarks, train models without violating licenses, and run experiments that could not otherwise be done. Using datasets generated with MultiPL-T, we present fine-tuned versions of StarCoderBase and Code Llama for Julia, Lua, OCaml, R, and Racket that outperform other fine-tunes of these base models on the natural language to code task. We also present Racket fine-tunes for two very recent models, DeepSeek Coder and StarCoder2, to show that MultiPL-T continues to outperform other fine-tuning approaches for low-resource languages. The MultiPL-T approach is easy to apply to new languages, and is significantly more efficient and effective than alternatives such as training longer.","2024-10","2025-11-25 22:29:57","2025-11-25 22:29:57","","","","OOPSLA2","8","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models trained on Code","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7XUC9I3A","journalArticle","2025","Li, Ziwei; Wu, Jiajing; Wu, Zhiying; Tan, Dongcheng; Zou, Weipeng; Jiang, Zigui; Zhen, Yi; Zhang, Zheng; Zheng, Zibin","CCIHunter: Enhancing Smart Contract Code-Comment Inconsistencies Detection via Two-Stage Pre-training","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3764867","https://doi.org/10.1145/3764867","Smart contracts are self-executing computer programs on blockchains. With the development of blockchain technology, the number of smart contracts has grown rapidly, as has the concern for their security. Regrettably, inconsistencies between the logic implemented in the code and the intentions described in the comments, known as Code-Comment Inconsistencies (CCI), are frequently present in some smart contracts. These inconsistencies can mislead readers in understanding the contract code and, in severe cases, may lead to vulnerabilities and economic losses. Existing learning-based methods are not tailored for smart contract languages, overlook the the issue of insufficient context information caused by comment references and nested intentions, and rely on large-scale labeled data; whereas rule-based methods struggle to accommodate the flexibility with which developers express intentions, often resulting in false positives. To tackle the challenges posed by insufficient context information and the scarcity of labeled data, we introduce CCIHunter, a tool designed to detect CCIs in smart contracts. CCIHunter addresses the issue of insufficient context information during data modeling and incorporates a two-stage pretraining process that does not depend on labeled data to enhance its detection capabilities. Specifically, CCIHunter enhances comments based on templates and models code as a heterogeneous graph based on function calls. It utilizes CodeBERT and UniMp to generate embeddings for comments and code, respectively, and then calculates the similarity between these two embeddings. Consistency is judged by combining code embeddings, comment embeddings, and similarity scores. Notably, CCIHunter undergoes a two-stage pre-training that includes contrastive learning and mutation analysis, aiming to improve its ability to bridge the gap between code and comments and to focus on code elements at different granularities. Experimental results demonstrate that CCIHunter achieves a precision of 0.95, a recall of 0.90, and an F1 score of 0.93, outperforming existing tools.","2025-08","2025-11-25 22:29:57","2025-11-25 22:29:57","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Smart Contract; Code-comment Consistency","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7PTHA8BI","conferencePaper","2025","Khati, Dipin","Trustworthiness of Large Language Models for Code","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings","979-8-3315-3683-1","","10.1109/ICSE-Companion66252.2025.00063","https://doi.org/10.1109/ICSE-Companion66252.2025.00063","In recent years, Large Language Models for code (LLMc) have transformed the landscape of software engineering (SE), demonstrating significant efficacy in tasks such as code completion, summarization, review, tracing, translation, test case generation, clone detection, and bug fixing. Notably, GitHub Copilot and Google's CodeBot exemplify how LLMc contributes to substantial time and effort savings in software development. However, the widespread application of these models has raised critical concerns regarding their trustworthiness. The lack of well-defined trust metrics beyond mere accuracy poses significant risks, including potential security vulnerabilities and compromised data integrity. This dissertation proposes solving this pressing need by developing a comprehensive framework to evaluate LLMc's trustworthiness. We aim to establish contextualized definitions of trust, distrust, and trustworthiness specific to LLMc, identify key influencing factors, and create a standardized evaluation framework encompassing both model-based attributes and human-centric considerations. We will validate the framework's effectiveness through rigorous empirical studies and user evaluations and provide insights for targeted improvements in LLMc development. This dissertation seeks to enhance the reliability and transparency of LLMc, fostering their responsible integration into software engineering practices and paving the way for more trustworthy AI-assisted code generation.","2025","2025-11-25 22:29:57","2025-11-25 22:48:09","","208–210","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","Large language models; LLM; Software engineering; Software reliability; Software development management; Codes; Software Engineering; software engineering; Security; trust; LLM4SE; Reviews; Translation; Reliability engineering; Pressing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2VIHHLEW","journalArticle","2025","Callaghan, Dylan; Fischer, Bernd","FLITSR: Improved Spectrum-Based Localization of Multiple Faults by Iterative Test Suite Reduction","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3745027","https://doi.org/10.1145/3745027","Spectrum-based fault localization (SBFL) works well for single-fault programs but its accuracy decays for increasing fault numbers. We present FLITSR (Fault Localization by Iterative Test Suite Reduction), a novel SBFL approach that improves the localization of a given SBFL base metric specifically in the presence of multiple faults. FLITSR iteratively selects reduced versions of the test suite that better localize the individual faults in the system. This allows it to identify and re-rank faults ranked too low by the base metric because they were masked by other program elements. Through this process, FLITSR returns a set of highly suspicious program elements (called a basis), where the execution of each failing test involves at least one basis element, considered as the cause of the failure.We implemented the FLITSR algorithm in an open-source toolset and extensively evaluated it over three true multi-fault datasets, varying the fault type, coverage granularity and programming language. Our evaluation shows that FLITSR consistently outperforms existing localization metrics and methods, including those designed to handle multiple faults such as ARTEMIS and parallel debugging. For the Defects4J method-level faults, FLITSR also substantially outperforms GRACE, a state-of-the-art learning-based fault localizer.","2025-06","2025-11-25 22:29:57","2025-11-25 22:29:57","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Spectrum-based fault localization; Testing and debugging","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CIB29BJX","conferencePaper","2025","Monjezi, Verya; Trivedi, Ashutosh; Kreinovich, Vladik; Tizpaz-Niari, Saeid","Fairness Testing through Extreme Value Theory","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00070","https://doi.org/10.1109/ICSE55347.2025.00070","Data-driven software is increasingly being used as a critical component of automated decision-support systems. Since this class of software learns its logic from historical data, it can encode or amplify discriminatory practices. Previous research on algorithmic fairness has focused on improving ""average-case"" fairness. On the other hand, fairness at the extreme ends of the spectrum, which often signifies lasting and impactful shifts in societal attitudes, has received significantly less emphasis.Leveraging the statistics of extreme value theory (EVT), we propose a novel fairness criterion called extreme counterfactual discrimination (ECD). This criterion estimates the worst-case amounts of disadvantage in outcomes for individuals solely based on their memberships in a protected group. Utilizing tools from search-based software engineering and generative AI, we present a randomized algorithm that samples a statistically significant set of points from the tail of ML outcome distributions even if the input dataset lacks a sufficient number of relevant samples.We conducted several experiments on four ML models (deep neural networks, logistic regression, and random forests) over 10 socially relevant tasks from the literature on algorithmic fairness. First, we evaluate the generative AI methods and find that they generate sufficient samples to infer valid EVT distribution in 95% of cases. Remarkably, we found that the prevalent bias mitigators reduce the average-case discrimination but increase the worst-case discrimination significantly in 35% of cases. We also observed that even the tail-aware mitigation algorithm—MiniMax-Fairness—increased the worst-case discrimination in 30% of cases. We propose a novel ECD-based mitigator that improves fairness in the tail in 90% of cases with no degradation of the average-case discrimination. We hope that the EVT framework serves as a robust tool for evaluating fairness in both average-case and worst-case discrimination.","2025","2025-11-25 22:29:57","2025-11-25 22:47:21","","1501–1513","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","software testing; Software; Software engineering; Generative AI; Testing; Logic; Software algorithms; fairness; Prevention and mitigation; Hands; extreme value theory; Logistic regression; Random forests","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZG3CZRR9","conferencePaper","2025","Gao, Ruohan; Wu, Qiao","BlockGuardian: Intelligent Multi-Dimensional Blockchain Code Analysis Framework","Proceedings of the 2025 International Conference on Generative Artificial Intelligence for Business","979-8-4007-1602-7","","10.1145/3766918.3766926","https://doi.org/10.1145/3766918.3766926","We introduce BlockGuardian, a next-generation framework that synergizes large language models (LLMs) with adaptive parallel processing to deliver precise and scalable analysis of blockchain codebases. Traditional static analyzers rely heavily on rigid rule-based systems, often missing nuanced vulnerabilities and domain-specific inconsistencies. In contrast, BlockGuardian leverages DeepSeek's advanced semantic reasoning across four critical dimensions; structural integrity, code quality, security vulnerabilities, and documentation completeness, to provide a holistic evaluation of blockchain codebases. Extensive benchmarks on leading platforms including Ethereum, FISCO-BCOS, Hyperledger Fabric, and Raychain, demonstrate that BlockGuardian detects 37% more critical vulnerabilities than state-of-the-art tools while maintaining a false positive rate under 8%. Moreover, our adaptive parallelization strategy yields an increase in throughput. By uniting LLM-driven natural language understanding with domain-specific security analysis, BlockGuardian sets a new standard for intelligent, high-performance auditing of decentralized applications.","2025","2025-11-25 22:29:57","2025-11-25 22:29:57","","47–53","","","","","","","GAIB '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Analysis; Blockchain; Vulnerability; Deepseek","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZCP82CFV","journalArticle","2024","Yang, Haoran; Nong, Yu; Zhang, Tao; Luo, Xiapu; Cai, Haipeng","Learning to Detect and Localize Multilingual Bugs","Proc. ACM Softw. Eng.","","","10.1145/3660804","https://doi.org/10.1145/3660804","Increasing studies have shown bugs in multi-language software as a critical loophole in modern software quality assurance, especially those induced by language interactions (i.e., multilingual bugs). Yet existing tool support for bug detection/localization remains largely limited to single-language software, despite the long-standing prevalence of multi-language systems in various real-world software domains. Extant static/dynamic analysis and deep learning (DL) based approaches all face major challenges in addressing multilingual bugs. In this paper, we present xLoc, a DL-based technique/tool for detecting and localizing multilingual bugs. Motivated by results of our bug-characteristics study on top locations of multilingual bugs, xLoc first learns the general knowledge relevant to differentiating various multilingual control-flow structures. This is achieved by pre-training a Transformer model with customized position encoding against novel objectives. Then, xLoc learns task-specific knowledge for the task of multilingual bug detection/localization, through another new position encoding scheme (based on cross-language API vicinity) that allows for the model to attend particularly to control-flow constructs that bear most multilingual bugs during fine-tuning. We have implemented xLoc for Python-C software and curated a dataset of 3,770 buggy and 15,884 non-buggy Python-C samples, which enabled our extensive evaluation of xLoc against two state-of-the-art baselines: fine-tuned CodeT5 and zero-shot ChatGPT. Our results show that xLoc achieved 94.98% F1 and 87.24%@Top-1 accuracy, which are significantly (up to 162.88% and 511.75%) higher than the baselines. Ablation studies further confirmed significant contributions of each of the novel design elements in xLoc. With respective bug-location characteristics and labeled bug datasets for fine-tuning, our design may be applied to other language combinations beyond Python-C.","2024-07","2025-11-25 22:29:57","2025-11-25 22:29:57","","","","FSE","1","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","bug detection; fault localization; Multi-language software; multilingual bugs","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5UG9FGJ8","conferencePaper","2024","Huang, Kai; Meng, Xiangxin; Zhang, Jian; Liu, Yang; Wang, Wenjie; Li, Shuhao; Zhang, Yuqing","An Empirical Study on Fine-Tuning Large Language Models of Code for Automated Program Repair","Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering","979-8-3503-2996-4","","10.1109/ASE56229.2023.00181","https://doi.org/10.1109/ASE56229.2023.00181","The advent of large language models (LLMs) has opened up new opportunities for automated program repair (APR). In particular, some recent studies have explored how to leverage large language models of code (LLMCs) for program repair tasks and show promising results. However, most of them adopt the zero/few-shot learning paradigm for APR, which directly use LLMCs to generate the possibly correct code given its surrounding context. Though effective, the repair capabilities of LLMCs based on the fine-tuning paradigm have yet to be extensively explored. Also, it remains unknown whether LLMCs have the potential to repair more complicated bugs (e.g., multi-hunk bugs). To fill the gap, in this work, we conduct a comprehensive study on the program repair capability of LLMCs in the fine-tuning paradigm. We select 5 popular LLMCs with representative pre-training architectures, including CodeBERT, GraphCodeBERT, PLBART, CodeT5, and UniXcoder. We consider 3 typical program repair scenarios (i.e., bugs, vulnerabilities, and errors) involving 3 programming languages (i.e., Java, C/C++, and JavaScript). Notably, we take both single-hunk and multi-hunk bugs/vulnerabilities into account. We then fine-tune them on widely-used datasets and compare them with existing state-of-the-art APR tools. We also investigate the impact of different design choices, which include code abstractions, code representations, and model evaluation metrics. Our experimental results show that LLMCs in the fine-tuning paradigm can significantly outperform previous state-of-the-art APR tools. Through in-depth analysis, we provide insights into choosing appropriate strategies to guide LLMCs for better performance. Lastly, we reveal several limitations of LLMCs for APR and make suggestions for future research on LLMC-based APR.","2024","2025-11-25 22:29:57","2025-11-25 22:29:57","","1162–1174","","","","","","","ASE '23","","","","IEEE Press","Echternach, Luxembourg","","","","","","","","","","","","automated program repair; fine-tuning; neural machine translation; large language models of code","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WZBLAVKA","conferencePaper","2024","Li, Zhuo; Wu, Xiongfei; Zhu, Derui; Cheng, Mingfei; Chen, Siyuan; Zhang, Fuyuan; Xie, Xiaofei; Ma, Lei; Zhao, Jianjun","Generative Model-Based Testing on Decision-Making Policies","Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering","979-8-3503-2996-4","","10.1109/ASE56229.2023.00153","https://doi.org/10.1109/ASE56229.2023.00153","The reliability of decision-making policies is urgently important today as they have established the fundamentals of many critical applications, such as autonomous driving and robotics. To ensure reliability, there have been a number of research efforts on testing decision-making policies that solve Markov decision processes (MDPs). However, due to the deep neural network (DNN)-based inherit and infinite state space, developing scalable and effective testing frameworks for decision-making policies still remains open and challenging.In this paper, we present an effective testing framework for decision-making policies. The framework adopts a generative diffusion model-based test case generator that can easily adapt to different search spaces, ensuring the practicality and validity of test cases. Then, we propose a termination state novelty-based guidance to diversify agent behaviors and improve the test effectiveness. Finally, we evaluate the framework on five widely used benchmarks, including autonomous driving, aircraft collision avoidance, and gaming scenarios. The results demonstrate that our approach identifies more diverse and influential failure-triggering test cases compared to current state-of-the-art techniques. Moreover, we employ the detected failure cases to repair the evaluated models, achieving better robustness enhancement compared to the baseline method.","2024","2025-11-25 22:29:57","2025-11-25 22:29:57","","243–254","","","","","","","ASE '23","","","","IEEE Press","Echternach, Luxembourg","","","","","","","","","","","","testing; decision-making policies; generative model","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BXPSRVEJ","conferencePaper","2024","Miftah, Samit Shahnawaz; Srivastava, Amisha; Kim, Hyunmin; Basu, Kanad","Assert-O: Context-based Assertion Optimization using LLMs","Proceedings of the Great Lakes Symposium on VLSI 2024","979-8-4007-0605-9","","10.1145/3649476.3660378","https://doi.org/10.1145/3649476.3660378","Modern computing relies on System-on-Chips (SoCs), integrating IP cores for complex functions. However, this integration introduces vulnerabilities, necessitating rigorous hardware security validation. The effectiveness of this validation depends on the security properties embedded in the SoC. Recent studies explore large language models (LLMs) for generating security properties, but these may not be directly optimized for validation. Manual intervention remains necessary to reduce their number. Security validation methods that rely on human expertise are not scalable as they are time-intensive and prone to human error. In order to address these issues, we introduce Assert-O, an automated framework designed to derive security properties from SoC documentation and optimize the generated properties. It also ranks the properties based on the security vulnerabilities they are associated with, thereby streamlining the validation process. Our method leverages hardware documentation to initially create security properties, which are subsequently consolidated and prioritized based on their level of criticality. This approach serves to expedite the validation procedure. Assert-O is trained on documentation of six IPs from OpenTitan. To evaluate our proposed method, Assert-O was assessed on five other modules from OpenTitan. Assert-O was able to generate 183 properties, which was further optimized to reduce them to 138 properties. Subsequently, these properties were ranked based on their impact on the security of the overall system.","2024","2025-11-25 22:29:57","2025-11-25 22:29:57","","233–239","","","","","","","GLSVLSI '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clearwater, FL, USA","","","","Large Language Models; Hardware Security; Hardware Verification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GGGT7H3N","conferencePaper","2025","Cumming, Aedan; Burger, Leon Eldon; Jansen, Gretchen","Siamese Neural Networks for Quantifying Image Similarity in the Context of Copyright","Proceedings of the 2025 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems","979-8-4007-1427-6","","10.1145/3759023.3759110","https://doi.org/10.1145/3759023.3759110","The proliferation of generative artificial intelligence (AI) has introduced challenges to copyright law, particularly regarding the ability of text-to-image models to replicate distinctive artistic styles. Traditional copyright assessment frameworks struggle with the scale and nuance of AI-generated content, creating a need for systematic, quantifiable approaches to evaluate artistic style similarity. This paper proposes using Siamese Neural Networks (SNNs) to quantify stylistic similarities with relevance to legal substantial similarity assessments in copyright cases. A ResNet-18 based SNN architecture with contrastive loss is employed to capture subtle stylistic elements while differentiating between distinct artistic expressions. The SNN is evaluated using controlled test cases generated by fine-tuned Stable Diffusion models trained on specific artists’ styles and an initial validation of our computational metrics through a human perception study is performed. The proposed SNN-based approach achieves superior discrimination between artistic styles compared to traditional perceptual metrics like Learned Perceptual Image Patch Similarity (mean discrimination scores of 0.156 versus 0.008), with performance variations across artists suggesting sensitivity to distinct styles. The observed correlation between computational similarity scores and human judgments (r=0.728, p=0.001) provides preliminary validation of the alignment of the proposed approach with human perception. This research contributes to the evolving discourse on AI and copyright by establishing quantifiable metrics for artistic style similarity assessment that bridge computational methods with legal frameworks, offering potential pathways toward more systematic copyright evaluation in the age of generative AI.","2025","2025-11-25 22:29:57","2025-11-25 22:29:57","","","","","","","","","icABCD '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Artistic Style Similarity; Computational Copyright; Copyright Law; Image Similarity Metrics; Siamese Neural Networks; Substantial Similarity","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BTSC3Y7D","conferencePaper","2024","Yang, Jun; Gilad, Amir; Hu, Yihao; Meng, Hanze; Miao, Zhengjie; Roy, Sudeepa; Stephens-Martinez, Kristin","What Teaching Databases Taught Us about Researching Databases: Extended Talk Abstract","Proceedings of the 3rd International Workshop on Data Systems Education: Bridging Education Practice with Education Research","979-8-4007-0678-3","","10.1145/3663649.3664375","https://doi.org/10.1145/3663649.3664375","Declarative querying is a cornerstone of the success and longevity of database systems, yet it is challenging for novice learners accustomed to different coding paradigms. The transition is further hampered by a lack of query debugging tools compared to the plethora available for programming languages. The paper samples several systems that we build at Duke University to help students learn and debug database queries. These systems have not only helped scale up teaching and improve learning, but also inspired interesting research on databases. Furthermore, with the rise of generative AI, we argue that there is a heightened need for skills in scrutinizing and debugging AI-generated queries, and we outline several ongoing and future work directions aimed at addressing this challenge.","2024","2025-11-25 22:29:58","2025-11-25 22:29:58","","1–6","","","","","","","DataEd '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Santiago, AA, Chile","","","","Database Education; Query Debugging; Query Verification; Relational Algebra; SQL","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VQ4LFHFE","journalArticle","2025","Yan, Jianxin; Ni, Wangze; Chen, Lei; Lin, Xuemin; Cheng, Peng; Qin, Zhan; Ren, Kui","ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models","Proc. VLDB Endow.","","2150-8097","10.14778/3750601.3750679","https://doi.org/10.14778/3750601.3750679","Semantic caching significantly reduces computational costs and improves efficiency by storing and reusing large language model (LLM) responses. However, existing systems rely primarily on matching individual queries, lacking awareness of multi-turn dialogue contexts, which leads to incorrect cache hits when similar queries appear in different conversational settings. This demonstration introduces ContextCache, a context-aware semantic caching system for multi-turn dialogues. ContextCache employs a two-stage retrieval architecture that first executes vector-based retrieval on the current query to identify potential matches and then integrates current and historical dialogue representations through self-attention mechanisms for precise contextual matching. Evaluation of real-world conversations shows that ContextCache improves precision and recall compared to existing methods. Additionally, cached responses exhibit approximately 10 times lower latency than direct LLM invocation, enabling significant computational cost reductions for LLM conversational applications.","2025-08","2025-11-25 22:29:58","2025-11-25 22:29:58","","5391–5394","","12","18","","","","","","","","","","","","","","","","","Publisher: VLDB Endowment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2VTTYGVN","conferencePaper","2022","Zhang, Jialu; Mytkowicz, Todd; Kaufman, Mike; Piskac, Ruzica; Lahiri, Shuvendu K.","Using pre-trained language models to resolve textual and semantic merge conflicts (experience paper)","Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis","978-1-4503-9379-9","","10.1145/3533767.3534396","https://doi.org/10.1145/3533767.3534396","Program merging is standard practice when developers integrate their individual changes to a common code base. When the merge algorithm fails, this is called a merge conflict. The conflict either manifests as a textual merge conflict where the merge fails to produce code, or as a semantic merge conflict where the merged code results in compiler errors or broken tests. Resolving these conflicts for large code projects is expensive because it requires developers to manually identify the sources of conflicts and correct them. In this paper, we explore the feasibility of automatically repairing merge conflicts (both textual and semantic) using k-shot learning with pre-trained large neural language models (LM) such as GPT-3. One of the challenges in leveraging such language models is fitting the examples and the queries within a small prompt (2048 tokens). We evaluate LMs and k-shot learning for both textual and semantic merge conflicts for Microsoft Edge. Our results are mixed: on one-hand, LMs provide the state-of-the-art (SOTA) performance on semantic merge conflict resolution for Edge compared to earlier symbolic approaches; on the other hand, LMs do not yet obviate the benefits of special purpose domain-specific languages (DSL) for restricted patterns for program synthesis.","2022","2025-11-25 22:29:58","2025-11-25 22:29:58","","77–88","","","","","","","ISSTA 2022","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Virtual, South Korea","","","","language model; GPT-3; k-shot learning; Resolving merge conflicts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3WF7Z7IA","conferencePaper","2025","Lee, Hoyoung; Seo, Junhyuk; Park, Suhwan; Lee, Junhyeong; Ahn, Wonbin; Choi, Chanyeol; Lopez-Lira, Alejandro; Lee, Yongjae","Your AI, Not Your View: The Bias of LLMs in Investment Analysis","Proceedings of the 6th ACM International Conference on AI in Finance","979-8-4007-2220-2","","10.1145/3768292.3770375","https://doi.org/10.1145/3768292.3770375","In finance, Large Language Models (LLMs) face frequent knowledge conflicts arising from discrepancies between their pre-trained parametric knowledge and real-time market data. These conflicts are especially problematic in real-world investment services, where a model’s inherent biases can misalign with institutional objectives, leading to unreliable recommendations. Despite this risk, the intrinsic investment biases of LLMs remain underexplored. We propose an experimental framework to investigate emergent behaviors in such conflict scenarios, offering a quantitative analysis of bias in LLM-based investment analysis. Using hypothetical scenarios with balanced and imbalanced arguments, we extract the latent biases of models and measure their persistence. Our analysis, centered on sector, size, and momentum, reveals distinct, model-specific biases. Across most models, a tendency to prefer technology stocks, large-cap stocks, and contrarian strategies is observed. These foundational biases often escalate into confirmation bias, causing models to cling to initial judgments even when faced with increasing counter-evidence. A public leaderboard benchmarking bias across a broader set of models is available at https://linqalpha.com/leaderboard.","2025","2025-11-25 22:29:58","2025-11-25 22:29:58","","150–158","","","","","","","ICAIF '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Large Language Models; Decision-Making; Financial Bias; Investment Analysis; Knowledge Conflict; Preference; Trustworthy AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BKQIPXWH","bookSection","2025","Rao, Nikitha; Vasilescu, Bogdan; Holmes, Reid","From Overload to Insight: Bridging Code Search and Code Review with LLMs","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","","https://doi.org/10.1145/3696630.3728518","The software engineering (SE) research community has developed numerous tools to search and extract actionable insights from software artifacts, ranging from static analysis tools to testing frameworks and continuous integration pipelines (hereafter just ""search tools""). Despite their potential, many of these search tools remain underutilized during code review, a critical process for ensuring software quality. Key challenges include the overwhelming volume of information generated by automated tools, high false-positive rates, and the need for manual configuration or interpretation, which disrupts the flow of review. In this paper, we propose a vision for an LLM-powered conversational agent designed to assist code reviewers by bridging the gap between human reviewers and search tools. This agent would summarize relevant insights, tailor them to the specific code change under review, and facilitate context-aware interactions. By enhancing the human-in-the-loop nature of code review, such a tool has the potential to amplify reviewer effectiveness, streamline the review process, and ultimately improve software quality.","2025","2025-11-25 22:29:58","2025-11-25 22:29:58","","656–660","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WZUN5UB9","conferencePaper","2024","Chen, Xuanzhong; Mao, Xiaohao; Guo, Qihan; Wang, Lun; Zhang, Shuyang; Chen, Ting","RareBench: Can LLMs Serve as Rare Diseases Specialists?","Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining","979-8-4007-0490-1","","10.1145/3637528.3671576","https://doi.org/10.1145/3637528.3671576","Generalist Large Language Models (LLMs), such as GPT-4, have shown considerable promise in various domains, including medical diagnosis. Rare diseases, affecting approximately 300 million people worldwide, often have unsatisfactory clinical diagnosis rates primarily due to a lack of experienced physicians and the complexity of differentiating among many rare diseases. In this context, recent news such as ""ChatGPT correctly diagnosed a 4-year-old's rare disease after 17 doctors failed"" underscore LLMs' potential, yet underexplored, role in clinically diagnosing rare diseases. To bridge this research gap, we introduce RareBench, a pioneering benchmark designed to systematically evaluate the capabilities of LLMs on 4 critical dimensions within the realm of rare diseases. Meanwhile, we have compiled the largest open-source dataset on rare disease patients, establishing a benchmark for future studies in this domain. To facilitate differential diagnosis of rare diseases, we develop a dynamic few-shot prompt methodology, leveraging a comprehensive rare disease knowledge graph synthesized from multiple knowledge bases, significantly enhancing LLMs' diagnostic performance. Moreover, we present an exhaustive comparative study of GPT-4's diagnostic capabilities against those of specialist physicians. Our experimental findings underscore the promising potential of integrating LLMs into the clinical diagnostic process for rare diseases. This paves the way for exciting possibilities in future advancements in this field.","2024","2025-11-25 22:29:58","2025-11-25 22:29:58","","4850–4861","","","","","","","KDD '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Barcelona, Spain","","","","evaluation; benchmark for llms; rare disease diagnosis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7584GDRI","journalArticle","2025","Barboni, Morena; De Angelis, Guglielmo; Morichetta, Andrea; Polini, Andrea","Wielding Blockchain Transactions for Capture-Replay Testing of Upgradeable Smart Contracts","ACM Trans. Internet Technol.","","1533-5399","10.1145/3737699","https://doi.org/10.1145/3737699","Blockchain technology is increasingly adopted in scenarios requiring trust and data integrity. On the Ethereum blockchain, the proxy pattern has become increasingly popular because it allows smart contract code to evolve while preserving stored data. However, a key challenge remains ensuring that such upgrades do not introduce breaking changes or cause disruptions to other contracts and off-chain systems. In this article, we introduce Catana, a framework that leverages historical transactions for Capture-Replay testing of proxy-based Upgradeable Smart Contracts (USCs). Catana assesses the potential impact of an upgrade by comparing the outcomes of replayed transactions with those from the previous version deployed on the main network. Additionally, it extracts and decodes contract state variables, providing deeper insights into how code changes affect the contract state, and helping developers mitigate issues before deployment. Experiments demonstrate that analyzing storage data accounts for the majority (about 86.5%) of detected disruptive upgrades. We also evaluate different policies for building replay test suites from historical transactions. Results identify a strategy that maximizes effectiveness while requiring a small number of replay test executions. Even a test suite containing just one transaction per each invoked method achieved good effectiveness (about 60%) in detecting disruptive upgrades.","2025-08","2025-11-25 22:29:58","2025-11-25 22:29:58","","","","3","25","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","ethereum; blockchain; capture-replay testing; Upgradeable smart contracts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T8CKBI6N","conferencePaper","2024","Gao, Xinyu; Wang, Zhijie; Feng, Yang; Ma, Lei; Chen, Zhenyu; Xu, Baowen","MultiTest: Physical-Aware Object Insertion for Testing Multi-sensor Fusion Perception Systems","Proceedings of the IEEE/ACM 46th International Conference on Software Engineering","979-8-4007-0217-4","","10.1145/3597503.3639191","https://doi.org/10.1145/3597503.3639191","Multi-sensor fusion stands as a pivotal technique in addressing numerous safety-critical tasks and applications, e.g., self-driving cars and automated robotic arms. With the continuous advancement in data-driven artificial intelligence (AI), MSF's potential for sensing and understanding intricate external environments has been further amplified, bringing a profound impact on intelligent systems and specifically on their perception systems. Similar to traditional software, adequate testing is also required for AI-enabled MSF systems. Yet, existing testing methods primarily concentrate on single-sensor perception systems (e.g., image-based and point cloud-based object detection systems). There remains a lack of emphasis on generating multi-modal test cases for MSF systems.To address these limitations, we design and implement MultiTest, a fitness-guided metamorphic testing method for complex MSF perception systems. MultiTest employs a physical-aware approach to synthesize realistic multi-modal object instances and insert them into critical positions of background images and point clouds. A fitness metric is designed to guide and boost the test generation process. We conduct extensive experiments with five SOTA perception systems to evaluate MultiTest from the perspectives of: (1) generated test cases' realism, (2) fault detection capabilities, and (3) performance improvement. The results show that MultiTest can generate realistic and modality-consistent test data and effectively detect hundreds of diverse faults of an MSF system under test. Moreover, retraining an MSF system on the test cases generated by MultiTest can improve the system's robustness. Our replication package and synthesized testing dataset are publicly available at https://sites.google.com/view/msftest.","2024","2025-11-25 22:29:58","2025-11-25 22:29:58","","","","","","","","","ICSE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","testing; multi-sensor fusion; perception systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZTJPI8TF","journalArticle","2024","Hossain, Soneya Binta; Jiang, Nan; Zhou, Qiang; Li, Xiaopeng; Chiang, Wen-Hao; Lyu, Yingjun; Nguyen, Hoan; Tripp, Omer","A Deep Dive into Large Language Models for Automated Bug Localization and Repair","Proc. ACM Softw. Eng.","","","10.1145/3660773","https://doi.org/10.1145/3660773","Large language models (LLMs) have shown impressive effectiveness in various software engineering tasks, including automated program repair (APR). In this study, we take a deep dive into automated bug localization and repair utilizing LLMs. In contrast to many deep learning-based APR methods that assume known bug locations, rely on line-level localization tools, or address bug prediction and fixing in one step, our approach uniquely employs LLMs to predict bug location at the token level and subsequently utilizes them for bug fixing. This methodological separation of bug localization and fixing using different LLMs enables effective integration of diverse contextual information and improved incorporation of inductive biases. We introduce Toggle: Token-Granulated Bug Localization and Repair, a comprehensive program repair framework that integrates a bug localization model, an adjustment model to address tokenizer inconsistencies, and a bug-fixing model. Toggle takes a buggy function as input and generates a complete corrected function. We investigate various styles of prompting to the bug fixing model to identify the most effective prompts that better utilize the inductive bias and significantly outperform others. Toggle achieves the new state-of-the-art (SOTA) performance on the CodeXGLUE code refinement benchmark, and exhibits better and comparable performance on several other widely-used APR datasets, including Defects4J. In the Defects4J benchmark, our approach consistently ranks above other methods, achieving superior results in the Top-10, Top-30, Top-50, and Top-100 metrics. Besides examining Toggle’s generalizability to unseen data, evaluating the effectiveness of various prompts, we also investigate the impact of additional contextual information such as buggy lines and code comments on bug localization, and explore the importance of the adjustment model. Our extensive experiments offer valuable insights and answers to critical research questions.","2024-07","2025-11-25 22:29:58","2025-11-25 22:29:58","","","","FSE","1","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; Automated Bug Localization and Reapir","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PCP4QIXH","journalArticle","2024","Parry, Owain; Gruber, Martin; Henderson, Tim A. D.; McMinn, Phil; Fraser, Gordon","Summary of the 1st International Flaky Test Workshop (FTW 2024)","SIGSOFT Softw. Eng. Notes","","0163-5948","10.1145/3672089.3672100","https://doi.org/10.1145/3672089.3672100","Test flakiness refers to the non-deterministic behavior of software tests. A flaky test may pass or fail without changes to the test case code or the code under test. This leads to decreased developer productivity, expensive and excessive retrying of tests, bugs masked or missed by persistent flakiness, and loss of developer trust in test suites. Despite the problem receiving increasing attention from both practitioners and researchers over the past decade, no workshop dedicated to flakiness had been established.","2024-07","2025-11-25 22:29:58","2025-11-25 22:29:58","","35–36","","3","49","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U3A66RSD","conferencePaper","2024","Frankford, Eduard; Sauerwein, Clemens; Bassner, Patrick; Krusche, Stephan; Breu, Ruth","AI-Tutoring in Software Engineering Education","Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training","979-8-4007-0498-7","","10.1145/3639474.3640061","https://doi.org/10.1145/3639474.3640061","With the rapid advancement of artificial intelligence (AI) in various domains, the education sector is set for transformation. The potential of AI-driven tools in enhancing the learning experience, especially in programming, is immense. However, the scientific evaluation of Large Language Models (LLMs) used in Automated Programming Assessment Systems (APASs) as an AI-Tutor remains largely unexplored. Therefore, there is a need to understand how students interact with such AI-Tutors and to analyze their experiences.In this paper, we conducted an exploratory case study by integrating the GPT-3.5-Turbo model as an AI-Tutor within the APAS Artemis. Through a combination of empirical data collection and an exploratory survey, we identified different user types based on their interaction patterns with the AI-Tutor. Additionally, the findings highlight advantages, such as timely feedback and scalability. However, challenges like generic responses and students' concerns about a learning progress inhibition when using the AI-Tutor were also evident. This research adds to the discourse on AI's role in education.","2024","2025-11-25 22:29:58","2025-11-25 22:29:58","","309–319","","","","","","","ICSE-SEET '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","ChatGPT; artificial intelligence; OpenAI; programming education; automated programming assessment systems; ChatBots","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5YQZWER7","book","2024","","LGM3A '24: Proceedings of the 2nd Workshop on Large Generative Models Meet Multimodal Applications","","979-8-4007-1193-0","","","","On behalf of the organizing committee, it is our distinct pleasure to extend a warm welcome to the LGM3A Workshop. As Chairs of this conference, we are delighted to bring together a community of scholars, researchers, and professionals from diverse backgrounds, all driven by a shared passion for advancing the frontiers of knowledge in our field.This workshop aims to explore the potential of large generative models to revolutionize the way we interact with multimodal information. A Large Language Model (LLM) represents a sophisticated form of artificial intelligence engineered to comprehend and produce natural language text, exemplified by technologies such as GPT, LLaMA, Flan-T5, ChatGLM, and Qwen, etc. These models undergo training on extensive text datasets, exhibiting commendable attributes including robust language generation, zero-shot transfer capabilities, and In-Context Learning (ICL). With the surge in multimodal content encompassing images, videos, audio, and 3D models over the recent period, Large MultiModal Models (LMMs) have seen significant enhancements. These improvements enable the augmentation of conventional LLMs to accommodate multimodal inputs or outputs, as seen in BLIP, Flamingo, KOSMOS, LLaVA, Gemini, GPT-4, etc. Concurrently, certain research initiatives have delved into generating specific modalities, with Kosmos2 and MiniGPT-5 focusing on image generation, and SpeechGPT on speech production. There are also endeavors to integrate LLMs with external tools to achieve a near any-to-any multimodal comprehension and generation capacity, illustrated by projects like Visual-ChatGPT, ViperGPT, MMREACT, HuggingGPT, and AudioGPT. Collectively, these models, spanning not only text and image generation but also other modalities, are referred to as large generative models.","2024","2025-11-25 22:29:58","2025-11-25 22:29:58","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9MCILZTM","book","2024","","LLM4Code '24: Proceedings of the 1st International Workshop on Large Language Models for Code","","979-8-4007-0579-3","","","","Welcome to the first edition of the InternationalWorkshop on Large Language Models for Code (LLM4Code). Large Language Models (LLMs), which are large-scale models being trained on massive textual corpora, have achieved significant advances in various domains, including Software Engineering (SE). Recently, there has been a growing interest in applying LLMs to assist software development and maintenance, such as code generation and comprehension, test generation, and program repair. Although the application of LLMs on code-relevant tasks has shown very promising performance, there is a huge potential to explore this growing domain further. The motivation of the LLM4Code workshop is to provide a platform for academics and practitioners to discuss and share their ideas on applying and developing LLMs to solve code-relevant problems in SE activities.The LLM4Code workshop is concerned with the research on how to better apply LLMs to solve code-relevant tasks, how to design better LLMs for code-relevant tasks, and how to better benchmark LLMs on code-relevant tasks. The workshop aims to achieve multiple goals as follows. Firstly, the workshop aims to provide an opportunity for participants to discuss novel ideas and preliminary results on LLMs for solving code-relevant SE problems, to exchange the latest progress in this domain. Secondly, the workshop aims to encourage participants to discuss the open challenges and problems of LLM4code, to identify important future directions in this domain. Finally, the workshop aims to encourage participants to share infrastructures and benchmarks that are foundational and beneficial for future research in this domain.","2024","2025-11-25 22:29:58","2025-11-25 22:29:58","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5I3887ZW","conferencePaper","2025","Zhang, Tanghaoran; Yu, Yue; Mao, Xinjun; Wang, Shangwen; Yang, Kang; Lu, Yao; Zhang, Zhang; Zhao, Yuxin","Instruct or Interact? Exploring and Eliciting LLMs' Capability in Code Snippet Adaptation through Prompt Engineering","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00104","https://doi.org/10.1109/ICSE55347.2025.00104","Code snippet adaptation is a fundamental activity in the software development process. Unlike code generation, code snippet adaptation is not a ""free creation"", which requires developers to tailor a given code snippet in order to fit specific requirements and the code context. Recently, large language models (LLMs) have confirmed their effectiveness in the code generation task with promising results. However, their performance on code snippet adaptation, a reuse-oriented and context-dependent code change prediction task, is still unclear. To bridge this gap, we conduct an empirical study to investigate the performance and issues of LLMs on the adaptation task. We first evaluate the adaptation performances of three popular LLMs and compare them to the code generation task. Our result indicates that their adaptation ability is weaker than generation, with a nearly 15% decrease on pass@1 and more context-related errors. By manually inspecting 200 cases, we further investigate the causes of LLMs' sub-optimal performance, which can be classified into three categories, i.e., Unclear Requirement, Requirement Misalignment and Context Misapplication. Based on the above empirical research, we propose an interactive prompting approach to eliciting LLMs' ability on the adaptation task. Specifically, we enhance the prompt by enriching the context and decomposing the task, which alleviates context misapplication and improves requirement understanding. Besides, we enable LLMs' reflection by requiring them to interact with a human or a LLM counselor, compensating for unclear requirement. Our experimental result reveals that our approach greatly improve LLMs' adaptation performance. The best-performing Human-LLM interaction successfully solves 159 out of the 202 identified defects and improves the pass@1 and pass@5 by over 40% compared to the initial instruction-based prompt. Considering human efforts, we suggest multi-agent interaction as a tradeoff, which can achieve comparable performance with excellent generalization ability. We deem that our approach could provide methodological assistance for autonomous code snippet reuse and adaptation with LLMs.","2025","2025-11-25 22:29:58","2025-11-25 22:29:58","","566–577","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","prompt engineering; large language models; code snippet adaptation; interactive workflow","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MIAGNV2K","journalArticle","2025","Xu, Zhenyang; Tian, Yongqiang; Zhang, Mengxiao; Sun, Chengnian","Boosting Program Reduction with the Missing Piece of Syntax-Guided Transformations","Proc. ACM Program. Lang.","","","10.1145/3763053","https://doi.org/10.1145/3763053","Program reduction is a widely used technique in testing and debugging language processors. Given a program that triggers a bug in a language processor, program reduction searches for a canonicalized and minimized program that triggers the same bug, thereby facilitating bug deduplication and simplifying the debugging process. To improve reduction performance without sacrificing generality, prior research has leveraged the formal syntax of the programming language as guidance. Two key syntax-guided transformations—Compatible Substructure Hoisting and Quantified Node Reduction—were introduced to enhance this process. While these transformations have proven effective to some extent, their application excessively prunes the search space, preventing the discovery of many smaller results. Consequently, there remains significant potential for further improvement in overall reduction performance. To this end, we propose a novel syntax-guided transformation named Structure Form Conversion (SFC) to complement the aforementioned two transformations. Building on SFC, we introduce three reduction methods: Smaller Structure Replacement, Identifier Elimination, and Structure Canonicalization, designed to effectively and efficiently leverage SFC for program reduction. By integrating these reduction methods to previous language-agnostic program reducers, Perses and Vulcan, we implement two prototypes named SFCPerses and SFCVulcan. Extensive evaluations show that SFCPerses and SFCVulcan significantly outperforms Perses and Vulcan in both minimization and canonicalization. Specifically, compared to Perses, SFCPerses produces programs that are 36.82%, 18.71%, and 41.05% smaller on average on the C, Rust, and SMT-LIBv2 benchmarks at the cost of 3.65×, 16.99×, and 1.42× the time of Perses, respectively. Similarly, SFCVulcan generates programs that are 14.51%, 7.65%, and 7.66% smaller than those produced by Vulcan at the cost of 1.56×, 2.35×, and 1.42× the execution time of Vulcan. Furthermore, in an experiment with a benchmark suite containing 3,796 C programs that trigger 46 unique bugs, SFCPerses and SFCVulcan reduce 442 and 435 more duplicates (programs that trigger the same bug) to identical programs than Perses and Vulcan, respectively.","2025-10","2025-11-25 22:29:58","2025-11-25 22:29:58","","","","OOPSLA2","9","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Program Reduction; Automated Debugging; Test Input Minimization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AN9BZUGW","conferencePaper","2023","Chow, Yiu Wai; Schäfer, Max; Pradel, Michael","Beware of the Unexpected: Bimodal Taint Analysis","Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis","979-8-4007-0221-1","","10.1145/3597926.3598050","https://doi.org/10.1145/3597926.3598050","Static analysis is a powerful tool for detecting security vulnerabilities and other programming problems. Global taint tracking, in particular, can spot vulnerabilities arising from complicated data flow across multiple functions. However, precisely identifying which flows are problematic is challenging, and sometimes depends on factors beyond the reach of pure program analysis, such as conventions and informal knowledge. For example, learning that a parameter name of an API function locale ends up in a file path is surprising and potentially problematic. In contrast, it would be completely unsurprising to find that a parameter command passed to an API function execaCommand is eventually interpreted as part of an operating-system command. This paper presents Fluffy, a bimodal taint analysis that combines static analysis, which reasons about data flow, with machine learning, which probabilistically determines which flows are potentially problematic. The key idea is to let machine learning models predict from natural language information involved in a taint flow, such as API names, whether the flow is expected or unexpected, and to inform developers only about the latter. We present a general framework and instantiate it with four learned models, which offer different trade-offs between the need to annotate training data and the accuracy of predictions. We implement Fluffy on top of the CodeQL analysis framework and apply it to 250K JavaScript projects. Evaluating on five common vulnerability types, we find that Fluffy achieves an F1 score of 0.85 or more on four of them across a variety of datasets.","2023","2025-11-25 22:29:58","2025-11-25 22:29:58","","211–222","","","","","","","ISSTA 2023","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Seattle, WA, USA","","","","software security; AI4SE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XC4EJ358","conferencePaper","2024","Goel, Drishti; Husain, Fiza; Singh, Aditya; Ghosh, Supriyo; Parayil, Anjaly; Bansal, Chetan; Zhang, Xuchao; Rajmohan, Saravan","X-Lifecycle Learning for Cloud Incident Management using LLMs","Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering","979-8-4007-0658-5","","10.1145/3663529.3663861","https://doi.org/10.1145/3663529.3663861","Incident management for large cloud services is a complex and tedious process that requires a significant amount of manual effort from on-call engineers (OCEs). OCEs typically leverage data from different stages of the software development lifecycle [SDLC] (e.g., codes, configuration, monitor data, service properties, service dependencies, trouble-shooting documents, etc.) to generate insights for detection, root cause analysis and mitigation of incidents. Recent advancements in large language models [LLMs] (e.g., ChatGPT, GPT-4, Gemini) have created opportunities to automatically generate contextual recommendations for the OCEs, assisting them in quickly identifying and mitigating critical issues. However, existing research typically takes a silo-ed view of solving a certain task in incident management by leveraging data from a single stage of the SDLC. In this paper, we demonstrate that augmenting additional contextual data from different stages of the SDLC improves the performance of two critically important and practically challenging tasks: (1) automatically generating root cause recommendations for dependency failure related incidents, and (2) identifying the ontology of service monitors used for automatically detecting incidents. By leveraging a dataset of 353 incidents and 260 monitors from Microsoft, we demonstrate that augmenting contextual information from different stages of the SDLC improves the performance over state-of-the-art methods.","2024","2025-11-25 22:29:58","2025-11-25 22:29:58","","417–428","","","","","","","FSE 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","Large language models; Reliability; Cloud Services; Monitor management; Root-cause analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HPYBXBDD","journalArticle","2025","Levin, Kyla H.; van Kempen, Nicolas; Berger, Emery D.; Freund, Stephen N.","ChatDBG: Augmenting Debugging with Large Language Models","Proc. ACM Softw. Eng.","","","10.1145/3729355","https://doi.org/10.1145/3729355","Debugging is a critical but challenging task for programmers. This paper proposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like ""why is x null?"". To handle these queries, ChatDBG grants the LLM autonomy to ""take the wheel"": it can act as an independent agent capable of querying and controlling the debugger to navigate through stacks and inspect program state. It then reports its findings and yields back control to the programmer. By leveraging the real-world knowledge embedded in LLMs, ChatDBG can diagnose issues identifiable only through the use of domain-specific reasoning. Our ChatDBG prototype integrates with standard debuggers including LLDB and GDB for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code including standalone scripts and Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root causes, explain bugs, and generate accurate fixes for a wide range of real-world errors. For the Python programs, a single query led to an actionable bug fix 67% of the time; one additional follow-up query increased the success rate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded more than 75,000 times.","2025-06","2025-11-25 22:29:58","2025-11-25 22:29:58","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Debugging; Software Engineering; Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7EYFBQR7","conferencePaper","2025","Kurniawan, Oka; Chandra, Erick; Poskitt, Christopher M.; Noller, Yannic; Choo, Kenny Tsu Wei; Jegourel, Cyrille","Designing for Novice Debuggers: A Pilot Study on an AI-Assisted Debugging Tool","Proceedings of the 25th Koli Calling International Conference on Computing Education Research","979-8-4007-1599-0","","10.1145/3769994.3769997","https://doi.org/10.1145/3769994.3769997","Debugging is a fundamental skill that novice programmers must develop. Numerous tools have been created to assist novice programmers in this process. Recently, large language models (LLMs) have been integrated with automated program repair techniques to generate fixes for students’ buggy code. However, many of these tools foster an over-reliance on AI and do not actively engage students in the debugging process. In this work, we aim to design an intuitive debugging assistant, CodeHinter, that combines traditional debugging tools with LLM-based techniques to help novice debuggers fix semantic errors while promoting active engagement in the debugging process. We present findings from our second design iteration, which we tested with a group of undergraduate students. Our results indicate that the students found the tool highly effective in resolving semantic errors and significantly easier to use than the first version. Consistent with our previous study, error localization was the most valuable feature. Finally, we conclude that any AI-assisted debugging approach should be personalized based on user profiles to optimize their interactions with the tool.","2025","2025-11-25 22:29:58","2025-11-25 22:29:58","","","","","","","","","Koli Calling '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","large language models; novice programmers; programming education; AI assistants; AI tutoring; Assisted debugging; design guidelines.; intelligent tutoring systems; interactive debugging","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IQ4IXMC3","bookSection","2025","Qiu, Ketai","Ever-Improving Test Suite by Leveraging Large Language Models","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","","https://doi.org/10.1145/3696630.3728614","Augmenting test suites with test cases that reflect the actual usage of the software system is extremely important to sustain the quality of long lasting software systems. In this paper, we propose E-Test, an approach that incrementally augments a test suite with test cases that exercise behaviors that emerge in production and that are not been tested yet. E-Test leverages Large Language Models to identify already-tested, not-yet-tested, and error-prone unit execution scenarios, and augment the test suite accordingly. Our experimental evaluation shows that E-Test outperforms the main state-of-the-art approaches to identify inadequately tested behaviors and optimize test suites.","2025","2025-11-25 22:29:58","2025-11-25 22:29:58","","1010–1012","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JS6VWC53","journalArticle","2024","Bairi, Ramakrishna; Sonwane, Atharv; Kanade, Aditya; C., Vageesh D.; Iyer, Arun; Parthasarathy, Suresh; Rajamani, Sriram; Ashok, B.; Shet, Shashank","CodePlan: Repository-Level Coding using LLMs and Planning","Proc. ACM Softw. Eng.","","","10.1145/3643757","https://doi.org/10.1145/3643757","Software engineering activities such as package migration, fixing error reports from static analysis or testing, and adding type annotations or other specifications to a codebase, involve pervasively editing the entire repository of code. We formulate these activities as repository-level coding tasks. Recent tools like GitHub Copilot, which are powered by Large Language Models (LLMs), have succeeded in offering high-quality solutions to localized coding problems. Repository-level coding tasks are more involved and cannot be solved directly using LLMs, since code within a repository is inter-dependent and the entire repository may be too large to fit into the prompt. We frame repository-level coding as a planning problem and present a task-agnostic, neuro-symbolic framework called CodePlan to solve it. CodePlan synthesizes a multi-step chain-of-edits (plan), where each step results in a call to an LLM on a code location with context derived from the entire repository, previous code changes and task-specific instructions. CodePlan is based on a novel combination of an incremental dependency analysis, a change may-impact analysis and an adaptive planning algorithm (symbolic components) with the neural LLMs. We evaluate the effectiveness of CodePlan on two repository-level tasks: package migration (C#) and temporal code edits (Python). Each task is evaluated on multiple code repositories, each of which requires inter-dependent changes to many files (between 2–97 files). Coding tasks of this level of complexity have not been automated using LLMs before. Our results show that CodePlan has better match with the ground truth compared to baselines. CodePlan is able to get 5/7 repositories to pass the validity checks (i.e., to build without errors and make correct code edits) whereas the baselines (without planning but with the same type of contextual information as CodePlan) cannot get any of the repositories to pass them. We provide our (non-proprietary) data, evaluation scripts and supplementary material at https://github.com/microsoft/codeplan.","2024-07","2025-11-25 22:29:58","2025-11-25 22:29:58","","","","FSE","1","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","static analysis; LLMs; Automated coding; chain of edits; neuro-symbolic AI; plan; repositories","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E62N5ASS","journalArticle","2025","Qin, Yihao; Wang, Shangwen; Lei, Yan; Zhang, Zhuo; Lin, Bo; Peng, Xin; Ma, Jun; Chen, Liqian; Mao, Xiaoguang","Fault Localization from the Semantic Code Search Perspective","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3757915","https://doi.org/10.1145/3757915","The software development process is characterized by an iterative cycle of continuous functionality implementation and debugging, essential for the enhancement of software quality and adaptability to changing requirements. This process incorporates two isolatedly studied tasks: Code Search (CS), which retrieves reference code from a code corpus to aid in code implementation, and Fault Localization (FL), which identifies code entities responsible for bugs within the software project to boost software debugging. The basic observation of this study is that these two tasks exhibit similarities since they both address search problems. Notably, CS techniques have demonstrated greater effectiveness than FL ones, possibly because of the precise semantic details of the required code offered by natural language queries, which are not readily accessible to FL methods. Drawing inspiration from this, we hypothesize that a fault localizer could achieve greater proficiency if semantic information about the buggy methods were made available. Based on this idea, we propose (mathttCosFL) , an FL approach that decomposes the FL task into two steps: query generation, which describes the functionality of the problematic code in natural language, and fault retrieval, which uses CS to find program elements semantically related to the query, allowing for finishing the FL task from a CS perspective. Specifically, to depict the buggy functionalities and generate high-quality queries, (mathttCosFL) extensively harnesses the code analysis, semantic comprehension, text generation, and decision-making capabilities of LLMs. Moreover, to enhance the accuracy of CS, (mathttCosFL) captures varying levels of context information and employs a multi-granularity code search strategy, which facilitates a more precise identification of buggy methods from a holistic view. The evaluation on 835 real bugs from 23 Java projects shows that (mathttCosFL) successfully localizes 324 bugs within Top-1, which significantly outperforms the state-of-the-art approaches by 26.6%-57.3%. The ablation study and sensitivity analysis further validate the importance of different components and the robustness of (mathttCosFL) across different backend models.","2025-08","2025-11-25 22:29:58","2025-11-25 22:29:58","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Debugging; Code search; Fault localization; Language models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FXRX9TTC","book","2024","","AIware 2024: Proceedings of the 1st ACM International Conference on AI-Powered Software","","979-8-4007-0685-1","","","","Welcome to the 1st ACM International Conference on AI-Powered Software (AIware), held on 15th and 16th July 2024 in Porto de Galinhas, Brazil co-located with the ACM International Conference on the Foundations of Software Engineering (FSE 2024). AIware aims to be an annual conference that brings the software engineering community together in anticipation of the upcoming changes driven by Foundation Models (FMs) and looks at them from the perspective of AI-powered software and their evolution. AIware 2024 prioritizes fostering discussions about the latest developments in the interdisciplinary field of AIware rather than solely focusing on the presentation of papers. The emphasis is on engaging conversations from diverse backgrounds to identify emerging research challenges and establish a new research agenda for the community in the Foundation Model era. To present papers and for discussions, the two-day conference will have five sessions themed around AIware Vision, SE for AIware, Human - AI Conversation, Security &amp; Safety and AIware for Software Lifecycle Activities. Furthermore, the conference program will include two keynotes and five industry talks. The final session in the conference program will be dedicated to presenting accepted papers of the AIware challenge track.","2024","2025-11-25 22:29:58","2025-11-25 22:29:58","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RK82YPH7","journalArticle","2025","Pezzè, Mauro; Abrahão, Silvia; Penzenstadler, Birgit; Poshyvanyk, Denys; Roychoudhury, Abhik; Yue, Tao","A 2030 Roadmap for Software Engineering","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3731559","https://doi.org/10.1145/3731559","The landscape of software engineering has dramatically changed in recent years. The impressive advances of artificial intelligence are just the latest and most disruptive innovation that has remarkably changed the software engineering research and practice. This special issue shares a roadmap to guide the software engineering community in this confused era. This roadmap is the outcome of a 2-day intensive discussion at the 2030 Software Engineering workshop. The roadmap spotlights and discusses seven main landmarks in the new software engineering landscape: artificial intelligence for software engineering, human aspects of software engineering, software security, verification and validation, sustainable software engineering, automatic programming, and quantum software engineering. This editorial summarizes the core aspects discussed in the 37 papers that comprise the seven sections of the special issue and guides the interested readers throughout the issue. This roadmap is a living body that we will refine with follow-up workshops that will update the roadmap for a series of forthcoming ACM TOSEM special issues.","2025-05","2025-11-25 22:29:58","2025-11-25 22:29:58","","","","5","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Automatic Programming; A roadmap for software engineering; AI and software engineering; AI for verification and validation; generative AI for software engineering; Human factor in software engineering; Large language models for software engineering; Quantum software engineering; security and software engineering; Sustainable software engineering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XRKBVGUV","conferencePaper","2025","Wang, Chong; Chen, Zhenpeng; Li, Tianlin; Zhang, Yilun; Liu, Yang","Towards Trustworthy LLMs for Code: A Data-Centric Synergistic Auditing Framework","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: New Ideas and Emerging Results","979-8-3315-3711-1","","10.1109/ICSE-NIER66352.2025.00017","https://doi.org/10.1109/ICSE-NIER66352.2025.00017","LLM-powered coding and development assistants have become prevalent to programmers' workflows. However, concerns about the trustworthiness of LLMs for code persist despite their widespread use. Much of the existing research focused on either training or evaluation, raising questions about whether stakeholders In training and evaluation align In their understanding of model trustworthiness and whether they can move toward a unified direction. In this paper, we propose a vision for a unified trustworthiness auditing framework, Data-Trust, which adopts a data-centric approach that synergistically emphasizes both training and evaluation data and their correlations. DataTrust aims to connect model trustworthiness indicators in evaluation with data quality indicators in training. It autonomously inspects training data and evaluates model trustworthiness using synthesized data, attributing potential causes from specific evaluation data to corresponding training data and refining indicator connections. Additionally, a trustworthiness arena powered by DataTrust will engage crowdsourced input and deliver quantitative outcomes. We outline the benefits that various stakeholders can gain from DataTrust and discuss the challenges and opportunities it presents.","2025","2025-11-25 22:29:58","2025-11-25 22:29:58","","56–60","","","","","","","ICSE-NIER '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QLQ8S4XL","bookSection","2025","Nguyen, Guillaume","Automating the conformity assessment of Cyber-Physical Systems software","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","","https://doi.org/10.1145/3696630.3731468","Cyber-physical systems (CPS) are tools used by humans to enhance the way they perform tasks. CPSs make tasks more efficient, more precise, and safer. Those systems are omnipresent in human lives, e.g., in cars with Advanced Driver Assistance Systems (ADAS), in Unmanned Aerial Vehicles (UAV) for self-balancing or even in medical devices. CPSs can read information from the real world, process it, and affect the real world back, considering constraints such as real-time processing. Furthermore, the safety and security of the software controlling the CPS are directly linked with the safety and security of human bystanders. The European Union (EU) has a process to assess the conformity of specific products exchanged within the EU to ensure the safety of its citizens. Recently, regulations and directives such as the Cyber Resilience Act (CRA) pressed European actors to provide compliant software products. Requirements on software started with the Medical Device Regulation (MDR) in 2017. However, technical requirements are challenging to understand from legal texts, and certification processes rely solely on manufacturer documentation. On the one hand, the EU has difficulty monitoring and opening the European market to products deemed compliant. On the other hand, manufacturers have difficulty understanding what is technically required of them when introducing products. This thesis aims to reconcile both parties.","2025","2025-11-25 22:29:58","2025-11-25 22:29:58","","1281–1284","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YHVUJZZ9","conferencePaper","2023","Agarwal, Nimisha; Kumar, Viraj; Raman, Arun; Karkare, Amey","A Bug's New Life: Creating Refute Questions from Filtered CS1 Student Code Snapshots","Proceedings of the ACM Conference on Global Computing Education Vol 1","979-8-4007-0048-4","","10.1145/3576882.3617916","https://doi.org/10.1145/3576882.3617916","In an introductory programming (CS1) context, a Refute question asks students for a counter-example which proves that a given code fragment is an incorrect solution for a given task. Such a question can be used as an assessment item to (formatively) develop or (summatively) demonstrate a student's abilities to comprehend the task and the code well enough to recognize a mismatch. These abilities assume greater significance with the emergence of generative AI technologies capable of writing code that is plausible (at least to novice programmers) but not always correct.Instructors must address three concerns while designing an effective Refute question, each influenced by their specific teaching-learning context: (1) Is the task comprehensible? (2) Is the incorrect code a plausible solution for the task? (3) Is the complexity of finding a counter-example acceptable? While the first concern can often be addressed by reusing tasks from previous code writing questions, addressing the latter concerns may require substantial instructor effort. We therefore investigate whether concerns (2) and (3) can be addressed by buggy student solutions for the corresponding code writing question from a previous course offering. For 6 code writing questions (from a Fall 2015 C programming course), our automated evaluation system logged 13,847 snapshots of executable student code, of which 10,574 were buggy (i.e., they failed at least one instructor-supplied test case). Code selected randomly from this pool rarely addresses these concerns, and manual selection is infeasible. Our paper makes three contributions. First, we propose an automated mechanism to filter this pool to a more manageable number of snapshots from which appropriate code can be selected manually. Second, we evaluate our semi-automated mechanism with respect to concerns (2) and (3) by surveying a diverse set of 56 experienced participants (instructors, tutors, and teaching assistants). Third, we use this mechanism to seed a public repository of Refute questions and provide a template to create additional questions using a public resource (CodeCheck).","2023","2025-11-25 22:29:59","2025-11-25 22:29:59","","7–14","","","","","","","CompEd 2023","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Hyderabad, India","","","","CS1; assessment; refute questions","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WFKAYCDY","conferencePaper","2023","Rastogi, Charvi; Tulio Ribeiro, Marco; King, Nicholas; Nori, Harsha; Amershi, Saleema","Supporting Human-AI Collaboration in Auditing LLMs with LLMs","Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society","979-8-4007-0231-0","","10.1145/3600211.3604712","https://doi.org/10.1145/3600211.3604712","Large language models (LLMs) are increasingly becoming all-powerful and pervasive via deployment in sociotechnical systems. Yet these language models, be it for classification or generation, have been shown to be biased, behave irresponsibly, causing harm to people at scale. It is crucial to audit these language models rigorously before deployment. Existing auditing tools use either or both humans and AI to find failures. In this work, we draw upon literature in human-AI collaboration and sensemaking, and interview research experts in safe and fair AI, to build upon the auditing tool: AdaTest&nbsp;[36], which is powered by a generative LLM. Through the design process we highlight the importance of sensemaking and human-AI communication to leverage complementary strengths of humans and generative models in collaborative auditing. To evaluate the effectiveness of AdaTest++, the augmented tool, we conduct user studies with participants auditing two commercial language models: OpenAI’s GPT-3 and Azure’s sentiment analysis model. Qualitative analysis shows that AdaTest++ effectively leverages human strengths such as schematization, hypothesis testing. Further, with our tool, users identified a variety of failures modes, covering 26 different topics over 2 tasks, that have been shown in formal audits and also those previously under-reported.","2023","2025-11-25 22:29:59","2025-11-25 22:29:59","","913–926","","","","","","","AIES '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Montréal, QC, Canada","","","","language models; auditing; biases; generative models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QL8CWNEF","conferencePaper","2024","Muttillo, Vittoriano; Di Sipio, Claudio; Rubei, Riccardo; Berardinelli, Luca; Dehghani, MohammadHadi","Towards Synthetic Trace Generation of Modeling Operations using In-Context Learning Approach","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695058","https://doi.org/10.1145/3691620.3695058","Producing accurate software models is crucial in model-driven software engineering (MDE). However, modeling complex systems is an error-prone task that requires deep application domain knowledge. In the past decade, several automated techniques have been proposed to support academic and industrial practitioners by providing relevant modeling operations. Nevertheless, those techniques require a huge amount of training data that cannot be available due to several factors, e.g., privacy issues. The advent of large language models (LLMs) can support the generation of synthetic data although state-of-the-art approaches are not yet supporting the generation of modeling operations. To fill the gap, we propose a conceptual framework that combines modeling event logs, intelligent modeling assistants, and the generation of modeling operations using LLMs. In particular, the architecture comprises modeling components that help the designer specify the system, record its operation within a graphical modeling environment, and automatically recommend relevant operations. In addition, we generate a completely new dataset of modeling events by telling on the most prominent LLMs currently available. As a proof of concept, we instantiate the proposed framework using a set of existing modeling tools employed in industrial use cases within different European projects. To assess the proposed methodology, we first evaluate the capability of the examined LLMs to generate realistic modeling operations by relying on well-founded distance metrics. Then, we evaluate the recommended operations by considering real-world industrial modeling artifacts. Our findings demonstrate that LLMs can generate modeling events even though the overall accuracy is higher when considering human-based operations. In this respect, we see generative AI tools as an alternative when the modeling operations are not available to train traditional IMAs specifically conceived to support industrial practitioners.","2024","2025-11-25 22:29:59","2025-11-25 22:29:59","","619–630","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DGRAGFXA","conferencePaper","2025","S, Tejaswi; VN, Sastry; S, Durga Bhavani","Mobile Application Security Assessment through Large Language Models (LLMs) based on User Reviews","Proceedings of the 8th International Conference on Data Science and Management of Data (12th ACM IKDD CODS and 30th COMAD)","979-8-4007-1124-4","","10.1145/3703323.3703733","https://doi.org/10.1145/3703323.3703733","Digital payments via mobile UPI (Unified Payments Interface) applications have recently become increasingly popular due to their ease of use. However, the users face significant security risks during the usage of the mobile applications. Hence security analysis based on user reviews gains importance. The existing literature focuses on bugs, confidentiality and usability issues providing partial insights into application security. In this paper, we present a systematic approach to assess the security of the mobile UPI applications based on user reviews with respect to eleven security goals given by GoI. We model the problem as a Multi-Label Text Categorization problem in which each review is labelled with a subset of the security goals. We developed a zero-shot prompt-based method using Large Language Models (LLMs), which do not need extensive labelled data, to categorize the user reviews. By leveraging expert knowledge and employing group consensus method, we have created an extremely useful annotated dataset of 4,000 mobile application user reviews for ground truth validation. The proposed Multi-Label Text Categorization approach is tested on three datasets including ours and two public datasets from different domains like News and Medical articles. Our approach outperforms the other deep learning methods like SBERT, ROBERTa etc. by showing 14.6% improvement on the best result, on average, with respect to F1 score and 34.1% improvement on Hamming loss. We have collected approximately 1.7 million user reviews on 30 UPI applications from the Google Play Store. We conduct qualitative analysis to demonstrate our approach and offer actionable recommendations for developers with respect to app security.","2025","2025-11-25 22:29:59","2025-11-25 22:29:59","","261–269","","","","","","","CODS-COMAD '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Bibliographic Networks.; Graph Neural Networks; Graph Representation Learning; Heterogeneous Networks","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"64JBRJN8","journalArticle","2025","Langdon, William B.; Blot, Aymeric; Krauss, Oliver","The 14th International Workshop on Genetic Improvement(GI @ ICSE 2025)","SIGSOFT Softw. Eng. Notes","","0163-5948","10.1145/3743095.3743105","https://doi.org/10.1145/3743095.3743105","The GI @ ICSE 2025 workshop, held 27 April, in addition to presentations, contained a keynote on the latest results on automated program generation for testing static program analyzers, which could have huge impact on GI for improving programs for APG, and a tutorial on Magpie, a widely used language independent GI tool. We summarise these, the papers, people, prizes, acknowledgements, discussions and hopes for the future.","2025-07","2025-11-25 22:29:59","2025-11-25 22:29:59","","48–58","","3","50","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VTKTBVNL","conferencePaper","2024","Imran, Mia Mohammad; Chatterjee, Preetha; Damevski, Kostadin","Uncovering the Causes of Emotions in Software Developer Communication Using Zero-shot LLMs","Proceedings of the IEEE/ACM 46th International Conference on Software Engineering","979-8-4007-0217-4","","10.1145/3597503.3639223","https://doi.org/10.1145/3597503.3639223","Understanding and identifying the causes behind developers' emotions (e.g., Frustration caused by 'delays in merging pull requests') can be crucial towards finding solutions to problems and fostering collaboration in open-source communities. Effectively identifying such information in the high volume of communications across the different project channels, such as chats, emails, and issue comments, requires automated recognition of emotions and their causes. To enable this automation, large-scale software engineering-specific datasets that can be used to train accurate machine learning models are required. However, such datasets are expensive to create with the variety and informal nature of software projects' communication channels.In this paper, we explore zero-shot LLMs that are pre-trained on massive datasets but without being fine-tuned specifically for the task of detecting emotion causes in software engineering: ChatGPT, GPT-4, and flan-alpaca. Our evaluation indicates that these recently available models can identify emotion categories when given detailed emotions, although they perform worse than the top-rated models. For emotion cause identification, our results indicate that zero-shot LLMs are effective at recognizing the correct emotion cause with a BLEU-2 score of 0.598. To highlight the potential use of these techniques, we conduct a case study of the causes of Frustration in the last year of development of a popular open-source project, revealing several interesting insights.","2024","2025-11-25 22:29:59","2025-11-25 22:29:59","","","","","","","","","ICSE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SRUI5K6P","journalArticle","2025","Wang, Zhijie; Zhou, Zhehua; Song, Jiayang; Huang, Yuheng; Shu, Zhan; Ma, Lei","VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation","Proc. ACM Softw. Eng.","","","10.1145/3729343","https://doi.org/10.1145/3729343","The rapid advancement of generative AI and multi-modal foundation models has shown significant potential in advancing robotic manipulation. Vision-language-action (VLA) models, in particular, have emerged as a promising approach for visuomotor control by leveraging large-scale vision-language data and robot demonstrations. However, current VLA models are typically evaluated using a limited set of hand-crafted scenes, leaving their general performance and robustness in diverse scenarios largely unexplored. To address this gap, we present VLATest, a fuzzing framework designed to generate robotic manipulation scenes for testing VLA models. Based on VLATest, we conducted an empirical study to assess the performance of seven representative VLA models. Our study results revealed that current VLA models lack the robustness necessary for practical deployment. Additionally, we investigated the impact of various factors, including the number of confounding objects, lighting conditions, camera poses, unseen objects, and task instruction mutations, on the VLA model's performance. Our findings highlight the limitations of existing VLA models, emphasizing the need for further research to develop reliable and trustworthy VLA applications.","2025-06","2025-11-25 22:29:59","2025-11-25 22:29:59","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Empirical Study; Robustness; Robotic Manipulation; Vision-Language-Action Models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EZETH4YX","conferencePaper","2024","Zhang, Haonan; Liao, Lizhi; Ding, Zishuo; Shang, Weiyi; Narula, Nidhi; Sporea, Catalin; Toma, Andrei; Sajedi, Sarah","Towards a Robust Waiting Strategy for Web GUI Testing for an Industrial Software System","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695269","https://doi.org/10.1145/3691620.3695269","Automated web GUI testing has been widely adopted since manual testing is time-consuming and tedious. Waiting strategy plays a vital role in automated web GUI testing since it significantly impacts the testing performance. Though important, little focus has been set on the waiting strategies in web GUI testing. Existing waiting strategies either wait for a predetermined time, which is not reliable in a dynamic environment, or only wait for a specific condition to be verified, which is often not robust enough to handle the complicated testing scenarios. In this work, we introduce a robust waiting strategy. Instead of waiting for a predetermined time or waiting for the availability of a particular element, our approach waits for a desired state to reach. This is achieved by capturing the Document Object Models (DOM) at the desired point, followed by an offline analysis to identify the differences between the DOMs associated with every two consecutive test actions. Such differences are used to determine the appropriate waiting time when automatically generating tests. Evaluation results with an industrial web application indicate that our approach produces more robust tests than the conventional waiting strategies used in web GUI testing. Furthermore, our generated tests are more representative of the recorded usage scenarios and are efficient with low overhead in test execution time.","2024","2025-11-25 22:29:59","2025-11-25 22:29:59","","2065–2076","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","automated web GUI testing; GUI rendering; industrial experience report; waiting strategy","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BFHQGSAW","conferencePaper","2024","Thakkar, Viraj; Sukumar, Madhumitha; Dai, Jiaxin; Singh, Kaushiki; Cao, Zhichao","Can Modern LLMs Tune and Configure LSM-based Key-Value Stores?","Proceedings of the 16th ACM Workshop on Hot Topics in Storage and File Systems","979-8-4007-0630-1","","10.1145/3655038.3665954","https://doi.org/10.1145/3655038.3665954","Log-Structured-Merge tree-based Key-Value Stores (LSM-KVSs) are important data storage building blocks in modern IT infrastructure. However, tuning their performance involves configuring over 100 parameters, a task typically done manually or with limited parameters in auto-tuning mechanisms. This paper explores and answers the following question: can we leverage LLM's understanding of the system and LSM-KVS components for unrestricted parameter-pool tuning of LSM-KVS?LLMs are trained on readily available LSM-KVS source code, research papers, and open materials enabling the machines to have human-like understanding. We investigate integrating Large-Language Models (LLMs) into an automated tuning framework for LSM-KVS to enhance the tuning capability and interactivity. Our framework utilizes LLMs to recommend tailored configurations with calibrated prompts based on hardware, system, and workload information. Initial results demonstrate upto 3X throughput improvements and an upto 9X reduction in p99 latency across various hardware and workloads compared to the out-of-box configuration for the LSM-KVS.","2024","2025-11-25 22:29:59","2025-11-25 22:29:59","","116–123","","","","","","","HotStorage '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Santa Clara, CA, USA","","","","Large Language Models; Automatic Tuning and Configuration; LSM-KVS","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BN8NC478","journalArticle","2025","Pian, Weiguo; Li, Yinghua; Tian, Haoye; Sun, Tiezhu; Song, Yewei; Tang, Xunzhu; Habib, Andrew; Klein, Jacques; Bissyandé, Tegawendé F.","You Don’t Have to Say Where to Edit! jLED—Joint Learning to Localize and Edit Source Code","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3712187","https://doi.org/10.1145/3712187","Learning to edit code automatically is becoming more and more feasible. Thanks to recent advances in Neural Machine Translation (NMT), various case studies are being investigated where patches are automatically produced and assessed either automatically (using test suites) or by developers themselves. An appealing setting remains when the developer must provide a natural language input of the requirement for the code change. A recent proof of concept in the literature showed that it is indeed feasible to translate these natural language requirements into code changes. A recent advancement, MODIT, has shown promising results in code editing by leveraging natural language, code context, and location information as input. However, it struggles when location information is unavailable. While several studies have demonstrated the ability to edit source code without explicitly specifying the edit location, they still tend to generate edits with less accuracy at the line level. In this work, we address the challenge of generating code edits without precise location information, a scenario we consider crucial for the practical adoption of NMT in code development. To that end, we develop a novel joint training approach for both localization and source code editions. Building a benchmark based on over 70k commits (patches and messages), we demonstrate that our joint Localize and EDit (jLED) approach is effective. An ablation study further demonstrates the importance of our design choice in joint training.","2025-07","2025-11-25 22:29:59","2025-11-25 22:29:59","","","","6","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Automated Programming; Joint Learning; Neural Machine Translation; Source Code Edition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W7QVZAYZ","conferencePaper","2025","Qi, Laryn; Zamfirescu-Pereira, J.D.; Kim, Taehan; Hartmann, Bjoern; DeNero, John; Norouzi, Narges","A Knowledge-Component-Based Methodology for Evaluating AI Assistants","Proceedings of the ACM Global on Computing Education Conference 2025 Vol 1","979-8-4007-1929-5","","10.1145/3736181.3747167","https://doi.org/10.1145/3736181.3747167","We present a novel method for evaluating AI assistants and apply our method to a coding assistant for CS1 programming assignments powered by GPT-4. This LLM-based assistant provides natural language guidance on how students can improve their incorrect solutions to programming exercises. A hint can be requested each time a student fails a test case. Our evaluation addresses three research questions: (RQ1) Do the hints help students improve their code? (RQ2) How effectively do the hints capture important issues in student code? (RQ3) Are the issues that students resolve the same as the issues addressed in the hints? To address these research questions quantitatively, we identified a set of fine-grained knowledge components and determined which ones apply to each exercise, incorrect solution, and the generated hint. Comparing data from two large CS1 offerings, we found that access to the hints helps students address problems with their code more quickly, that hints can consistently capture the most pressing errors in students' code, and that hints that address a few issues at once rather than a single bug are more likely to lead to direct student progress.","2025","2025-11-25 22:29:59","2025-11-25 22:29:59","","78–84","","","","","","","CompEd 2025","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Gaborone, Botswana","","","","large language models; ai assistant deployment; ai assistant evaluation; automated tutors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FP5DQ9KQ","conferencePaper","2024","Xie, Danning; Zhang, Zhuo; Jiang, Nan; Xu, Xiangzhe; Tan, Lin; Zhang, Xiangyu","ReSym: Harnessing LLMs to Recover Variable and Data Structure Symbols from Stripped Binaries","Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security","979-8-4007-0636-3","","10.1145/3658644.3670340","https://doi.org/10.1145/3658644.3670340","Decompilation aims to recover a binary executable to the source code form and hence has a wide range of applications in cyber security, such as malware analysis and legacy code hardening. A prominent challenge is to recover variable symbols, including both primitive and complex types such as user-defined data structures, along with their symbol information such as names and types. Existing efforts focus on solving parts of the problem, e.g., recovering only types (without names) or only local variables (without user-defined structures). In this paper, we propose ReSym, a novel hybrid technique that combines Large Language Models (LLMs) and program analysis to recover both names and types for local variables and user-defined data structures. Our method encompasses fine-tuning two LLMs to handle local variables and structures, respectively. To overcome the token limitations inherent in current LLMs, we devise a novel Prolog-based algorithm to aggregate and cross-check results from multiple LLM queries, suppressing uncertainty and hallucinations. Our experiments show that ReSym is effective in recovering variable information and user-defined data structures, substantially outperforming the state-of-the-art methods.","2024","2025-11-25 22:29:59","2025-11-25 22:29:59","","4554–4568","","","","","","","CCS '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Salt Lake City, UT, USA","","","","large language models; program analysis; reverse engineering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VYDPTN93","conferencePaper","2023","Wang, Weishi; Wang, Yue; Joty, Shafiq; Hoi, Steven C.H.","RAP-Gen: Retrieval-Augmented Patch Generation with CodeT5 for Automatic Program Repair","Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","979-8-4007-0327-0","","10.1145/3611643.3616256","https://doi.org/10.1145/3611643.3616256","Automatic program repair (APR) is crucial to reduce manual debugging efforts for developers and improve software reliability. While conventional search-based techniques typically rely on heuristic rules or a redundancy assumption to mine fix patterns, recent years have witnessed the surge of deep learning (DL) based approaches to automate the program repair process in a data-driven manner. However, their performance is often limited by a fixed set of parameters to model the highly complex search space of APR. To ease such burden on the parametric models, in this work, we propose a novel Retrieval-Augmented Patch Generation framework (RAP-Gen) by explicitly leveraging relevant fix patterns retrieved from a codebase of previous bug-fix pairs. Specifically, we build a hybrid patch retriever to account for both lexical and semantic matching based on the raw source code in a language-agnostic manner, which does not rely on any code-specific features. In addition, we adapt a code-aware language model CodeT5 as our foundation model to facilitate both patch retrieval and generation tasks in a unified manner. We adopt a stage-wise approach where the patch retriever first retrieves a relevant external bug-fix pair to augment the buggy input for the CodeT5 patch generator, which synthesizes a ranked list of repair patch candidates. Notably, RAP-Gen is a generic APR framework that can flexibly integrate different patch retrievers and generators to repair various types of bugs. We thoroughly evaluate RAP-Gen on three benchmarks in two programming languages, including the TFix benchmark in JavaScript, and Code Refinement and Defects4J benchmarks in Java, where the bug localization information may or may not be provided. Experimental results show that RAP-Gen significantly outperforms previous state-of-the-art (SoTA) approaches on all benchmarks, e.g., boosting the accuracy of T5-large on TFix from 49.70% to 54.15% (repairing 478 more bugs) and repairing 15 more bugs on 818 Defects4J bugs. Further analysis reveals that our patch retriever can search for relevant fix patterns to guide the APR systems.","2023","2025-11-25 22:29:59","2025-11-25 22:29:59","","146–158","","","","","","","ESEC/FSE 2023","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: San Francisco, CA, USA","","","","Automated program repair; Neural networks; Pretrained language models; Retrieval-augmented generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IBVPZRV8","bookSection","2025","Gao, Shuzheng; Gao, Cuiyun; Gu, Wenchao; Lyu, Michael R.","Search-Based LLMs for Code Optimization","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00021","The code written by developers usually suffers from efficiency problems and contain various performance bugs. These inefficiencies necessitate the research of automated refactoring methods for code optimization. Early research in code optimization employs rule-based methods and focuses on specific inefficiency issues, which are labor-intensive and suffer from the low coverage issue. Recent work regards the task as a sequence generation problem, and resorts to deep learning (DL) techniques such as large language models (LLMs). These methods typically prompt LLMs to directly generate optimized code. Although these methods show state-of-the-art performance, such one-step generation paradigm is hard to achieve an optimal solution. First, complex optimization methods such as combinatorial ones are hard to be captured by LLMs. Second, the one-step generation paradigm poses challenge in precisely infusing the knowledge required for effective code optimization within LLMs, resulting in under-optimized code.To address these problems, we propose to model this task from the search perspective, and propose a search-based LLMs framework named SBLLM that enables iterative refinement and discovery of improved optimization methods. SBLLM synergistically integrate LLMs with evolutionary search and consists of three key components: 1) an execution-based representative sample selection part that evaluates the fitness of each existing optimized code and prioritizes promising ones to pilot the generation of improved code; 2) an adaptive optimization pattern retrieval part that infuses targeted optimization patterns into the model for guiding LLMs towards rectifying and progressively enhancing their optimization methods; and 3) a genetic operator-inspired chain-of-thought prompting part that aids LLMs in combining different optimization methods and generating improved optimization methods. Our evaluation of SBLLM on a dataset of Python and C++ code demonstrates its effectiveness in improving code efficiency. Specifically, the results indicate that SBLLM can improve program execution efficiency by up to 209.59% and consistently outperform all baseline methods by 8.75% 28.06% and 1.15% 9.56% with different LLMs in terms of top-5 speedup rate on Python and C++, respectively.","2025","2025-11-25 22:29:59","2025-11-25 22:29:59","","578–590","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X6HYBSV8","conferencePaper","2025","Ran, Dezhi; Li, Lin; Zhu, Liuchuan; Cao, Yuan; Zhao, Landelong; Tan, Xin; Liang, Guangtai; Wang, Qianxiang; Xie, Tao","Efficient and Robust Security-Patch Localization for Disclosed OSS Vulnerabilities with Fine-Tuned LLMs in an Industrial Setting","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","10.1145/3696630.3728551","https://doi.org/10.1145/3696630.3728551","Security-patch localization, which links disclosed vulnerabilities in open-source software (OSS) to corresponding patches, has become a practical technique to mitigate the risk of OSS vulnerabilities in a timely manner. While existing approaches extensively focus on estimating the correlation between individual patches and Common Vulnerabilities and Exposures (CVEs), they often fail to address two major industrial requirements that make a tool of security-patch localization desirable in industrial settings: (1) efficiency when inspecting an enormous number of commits per vulnerability and (2) robustness to handle confusing patches (related but non-fixing commits). Toward addressing the preceding industrial requirements, in this paper, we report our experiences of developing and deploying Taper, a two-stage approach for efficiently and robustly locating security patches via mining the temporal relations among commits and CVEs. In the first stage, Taper extracts the information of the fixed version and the affected version from CVE descriptions to narrow down the inspection scope of commits, thus significantly improving the efficiency. In the second stage, Taper collects temporally co-located patches around the genuine security-patch commit as hard negative examples for security-patch localization. By fine-tuning a language model with these hard negative samples, Taper avoids recognizing confusing patches as security patches, thus improving patch-localization precision and robustness. We evaluate Taper against 2,128 CVEs from 978 OSS projects, which have a balanced distribution of programming languages and are consistent with industrial settings. Evaluation results show that Taper substantially outperforms a state-of-the-art approach named PatchFinder, improving the absolute MRR and Recall@1 by 0.422 and 0.541, respectively. Taper has been deployed at Huawei Cloud since October 2024. During 800 hours of operation, Taper helps locate over 52,140 security patches, providing daily service of security-patch localization for the Huawei company and Huawei Cloud users. We summarize three major lessons learned from developing and deploying Taper.","2025","2025-11-25 22:29:59","2025-11-25 22:29:59","","262–273","","","","","","","FSE Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","large language models; software supply chain; vulnerability detection; software security; cybersecurity; patch localization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H8H848VS","conferencePaper","2025","Winstanley, Curtis; Wainer, Gabriel","Testing Methodology for DEVS Models in Cadmium","Proceedings of the Winter Simulation Conference","979-8-3315-3420-2","","","","The practice of testing in modeling and simulation software development can be a very lengthy and tedious process but is arguably one of the most important phases in the software development lifecycle. As the complexity of a simulation model increases, so does the amount of testing to thoroughly verify and validate and to achieve adequate quality assurance of the software. This paper introduces a testing framework that is used to assist in proving the validity of DEVS atomic models in the open-source simulation tool Cadmium. Furthermore, this framework utilizes the ChatGPT Application Programming Interface (API) to help lighten the workload involved in testing those DEVS atomic models. We show how to use the framework using the Cadmium simulator to show the effectiveness of the framework.","2025","2025-11-25 22:29:59","2025-11-25 22:47:59","","1260–1270","","","","","","","WSC '24","","","","IEEE Press","Orlando, Florida, USA","","","","","","","","","","","","Software; Chatbots; Software development management; Testing; Artificial intelligence; Quality assurance; Generators; Error correction; Application programming interfaces; Cadmium","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GTLBVCUC","conferencePaper","2025","Tamura, Raiki; Kotani, Daisuke; Shudo, Kazuyuki; Okabe, Yasuo","Bringing Together Cross-ISA Checkpoint/Restoration and AOT Compilation of WebAssembly Programs","Proceedings of the 22nd ACM SIGPLAN International Conference on Managed Programming Languages and Runtimes","979-8-4007-2149-6","","10.1145/3759426.3760985","https://doi.org/10.1145/3759426.3760985","Cross-instruction set architecture (ISA) checkpoint/restoration is becoming increasingly important for live migration in heterogeneous computing environments, where applications need to move seamlessly between ARM, x86, and other processor architectures. While existing approaches either require compilation without Control-flow Integrity (CFI) or suffer from significant performance overhead through interpreter-based execution, this paper presents a novel approach that enables efficient cross-ISA migration using instrumentation during ahead-of-time (AOT) compilation. Our key insight is that on-stack replacement (OSR) enables cross-ISA checkpoint/restoration. OSR is a technique for JIT compilers, and we leverage it to transform between ISA-dependent machine states and ISA-independent WebAssembly states. Our other notable contribution is a technique enabling checkpointing without disabling modern CPU security features such as CFI. We implement the proposed techniques in Wanco, a WebAssembly AOT compiler supporting Linux on ARM-v8 and x86-64 architectures. Our evaluation demonstrates that Wanco achieves efficient cross-ISA migration compared to CRIU, a standard Linux process migration tool. Wanco reduces checkpoint time by 1.0–5.1 times and snapshot size by 1.1–25 times, while incurring an average execution-time overhead of 36 %.","2025","2025-11-25 22:29:59","2025-11-25 22:29:59","","114–124","","","","","","","MPLR '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Singapore, Singapore","","","","WebAssembly; Compilers; Live Migration; Virtual Machine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H7CLNZ48","conferencePaper","2024","Velez, Xavier","Understanding Algorithmic Problem Solving using LLMs","Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 2","979-8-4007-0604-2","","10.1145/3649409.3691086","https://doi.org/10.1145/3649409.3691086","With the rapid advancement of Large Language Models (LLMs) many instructors for Computer Science courses have begun to opt to allow students to use them as an additional educational resource but often warn that the output may be unreliable. Recent research on LLMs has demonstrated their ability to interpret commands in natural language and produce code in a variety of programming languages. However, it is not clear how well LLMs fair in tackling more complex problem set ups, like those typically seen in Algorithms courses in which students are provided natural language descriptions of an ambiguous problem and use what they learn to map the problem to an algorithmic solution. In this paper, we explore use of LLMs, such as OpenAI's GPT-4o, as tools for assisting students with complex Computer Science curricula, such as algorithmic problem solving. We specifically aim to see if using prompt refinement techniques, LLMs are capable of taking a problem statement in plain English and performing the following tasks: providing both a natural language description and code solution in the Python programming language, producing an analytical argument for the solutions correctness, and finally providing runtime analysis for the produced solution. Our experiments show that GPT-4o is well suited to solving problems like LeetCode 75 that have been seen during training, and prompt-refinement helps with those that have not been seen.","2024","2025-11-25 22:29:59","2025-11-25 22:29:59","","327–328","","","","","","","SIGCSE Virtual 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Virtual Event, NC, USA","","","","large language models; GPT-4o; algorithms","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VJJ6D2HX","conferencePaper","2025","Dong, Juechu; Rosenblum, Jonah; Narayanasamy, Satish","Toleo: Scaling Freshness to Tera-scale Memory Using CXL and PIM","Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4","979-8-4007-0391-1","","10.1145/3622781.3674180","https://doi.org/10.1145/3622781.3674180","Trusted hardware's freshness guarantee ensures that an adversary cannot replay an old value in response to a memory read request. They rely on maintaining a version number for each cache block and ensuring their integrity using a Merkle tree. However, these existing solutions protect only a small amount of main memory (few MBs), as the extraneous memory accesses to the Merkle tree increase prohibitively with the protected memory size. We present Toleo, which uses trusted smart memory connected through a secure CXL IDE network to safely store version numbers. Toleo eliminates the need for an unscalable Merkle tree to protect the integrity of version numbers by instead using smart memory as the root of trust. Additionally, Toleo ensures version confidentiality which enables stealth versions that reduce the version storage overhead in half.Furthermore, in the absence of Merkle tree imposed constraints, we effectively exploit version locality at page granularity to compress version number by a factor of 240. These space optimizations make it feasible for one 168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded main memory pool in a rack server for a negligible performance overhead. We analyze the benefits of Toleo using several privacy-sensitive genomics, graph, generative AI, and database workloads.","2025","2025-11-25 22:29:59","2025-11-25 22:29:59","","313–328","","","","","","","ASPLOS '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Hilton La Jolla Torrey Pines, La Jolla, CA, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EL24ATBV","conferencePaper","2025","Nagakalyani, Goda; Chaudhary, Saurav; Apte, Varsha; Ramakrishnan, Ganesh; Tamilselvam, Srikanth","Design and Evaluation of an AI-Assisted Grading Tool for Introductory Programming Assignments: An Experience Report","Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1","979-8-4007-0531-1","","10.1145/3641554.3701913","https://doi.org/10.1145/3641554.3701913","In a typical introductory programming course, grading student-submitted programs involves an autograder which compiles and runs the programs and tests their functionality with predefined test cases, with no attention to the source code. However, in an educational setting, grading based on inspection of the source code is required for two main reasons (1) awarding partial marks to 'partially correct' code that may be failing the testcase check (2) awarding marks (or penalties) based on source code quality or specific criteria that the instructor may have laid out in the problem statement (e.g. 'implement sorting using bubble-sort'). However, grading based on studying the source code can be highly time consuming when the course has a large enrollment. In this paper we present the design and evaluation of an AI Assistant for source code grading, which we have named TA Buddy. TA Buddy is powered by Code Llama, a large language model especially trained for code related tasks, which we fine-tuned using a graded programs dataset. Given a problem statement, student code submissions and a grading rubric, TA Buddy can be asked to generate suggested grades, i.e. ratings for the various rubric criteria, for each submission. The human teaching assistant (TA) can then accept or overrule these grades. We evaluated the TA Buddy-assisted manual grading against 'pure' manual grading and found that the time taken to grade reduced by 24% while maintaining grade agreement in the two cases at 90%.","2025","2025-11-25 22:29:59","2025-11-25 22:29:59","","805–811","","","","","","","SIGCSETS 2025","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Pittsburgh, PA, USA","","","","llms; cs education; grading; ai-assisted grading; programming assignments; rubric; source code evaluation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"83HGAXGN","conferencePaper","2024","Padhye, Rohan","Software Engineering Methods for AI-Driven Deductive Legal Reasoning","Proceedings of the 2024 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software","979-8-4007-1215-9","","10.1145/3689492.3690050","https://doi.org/10.1145/3689492.3690050","The recent proliferation of generative artificial intelligence (AI) technologies such as pre-trained large language models (LLMs) has opened up new frontiers in computational law. An exciting area of development is the use of AI to automate the deductive rule-based reasoning inherent in statutory and contract law. This paper argues that such automated deductive legal reasoning can now be viewed from the lens of software engineering, treating LLMs as interpreters of natural-language programs with natural-language inputs. We show how it is possible to apply principled software engineering techniques to enhance AI-driven legal reasoning of complex statutes and to unlock new applications in automated meta-reasoning such as mutation-guided example generation and metamorphic property-based testing.","2024","2025-11-25 22:29:59","2025-11-25 22:29:59","","85–95","","","","","","","Onward! '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Pasadena, CA, USA","","","","large language models; generative artificial intelligence; software engineering; mutation testing; computational law; example generation; legal reasoning; property-based testing; statutory reasoning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LRBQPTIP","journalArticle","2025","Wang, Jiyuan; Qiu, Yuxin; Limpanukorn, Ben; Kang, Hong Jin; Zhang, Qian; Kim, Miryung","DuoReduce: Bug Isolation for Multi-layer Extensible Compilation","Proc. ACM Softw. Eng.","","","10.1145/3715747","https://doi.org/10.1145/3715747","In recent years, the MLIR framework has had explosive growth due to the need for extensible deep learning compilers for hardware accelerators. Such examples include Triton, CIRCT, and ONNX-MLIR. MLIR compilers introduce significant complexities in localizing bugs or inefficiencies because of their layered optimization and transformation process with compilation passes. While existing delta debugging techniques can be used to identify a minimum subset of IR code that reproduces a given bug symptom, their naive application to MLIR is time-consuming because real-world MLIR compilers usually involve a large number of compilation passes. Compiler developers must identify a minimized set of relevant compilation passes to reduce the footprint of MLIR compiler code to be inspected for a bug fix. We propose DuoReduce, a dual-dimensional reduction approach for MLIR bug localization. DuoReduce leverages three key ideas in tandem to design an efficient MLIR delta debugger. First, DuoReduce reduces compiler passes that are irrelevant to the bug by identifying ordering dependencies among the different compilation passes. Second, DuoReduce uses MLIR-semantics-aware transformations to expedite IR code reduction. Finally, DuoReduce leverages cross-dependence between the IR code dimension and the compilation pass dimension by accounting for which IR code segments are related to which compilation passes to reduce unused passes. Experiments with three large-scale MLIR compiler projects find that DuoReduce outperforms syntax-aware reducers such as Perses and Vulcan in terms of IR code reduction by 31.6% and 21.5% respectively. If one uses these reducers by enumerating all possible compilation passes (on average 18 passes), it could take up to 145 hours. By identifying ordering dependencies among compilation passes, DuoReduce reduces this time to 9.5 minutes. By identifying which compilation passes are unused for compiling reduced IR code, DuoReduce reduces the number of passes by 14.6%. This translates to not needing to examine 281 lines of MLIR compiler code on average to fix the bugs. DuoReduce has the potential to significantly reduce debugging effort in MLIR compilers, which serves as the foundation for the current landscape of machine learning and hardware accelerators.","2025-06","2025-11-25 22:29:59","2025-11-25 22:29:59","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Fault Localization; MLIR; Multi-Layer Extensible Compilation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M58YQ3ZT","journalArticle","2025","Sovrano, Francesco; Bauer, Adam; Bacchelli, Alberto","Large Language Models for In-File Vulnerability Localization Can Be “Lost in the End”","Proc. ACM Softw. Eng.","","","10.1145/3715758","https://doi.org/10.1145/3715758","Traditionally, software vulnerability detection research has focused on individual small functions due to earlier language processing technologies’ limitations in handling larger inputs. However, this function-level approach may miss bugs that span multiple functions and code blocks. Recent advancements in artificial intelligence have enabled processing of larger inputs, leading everyday software developers to increasingly rely on chat-based large language models (LLMs) like GPT-3.5 and GPT-4 to detect vulnerabilities across entire files, not just within functions. This new development practice requires researchers to urgently investigate whether commonly used LLMs can effectively analyze large file-sized inputs, in order to provide timely insights for software developers and engineers about the pros and cons of this emerging technological trend. Hence, the goal of this paper is to evaluate the effectiveness of several state-of-the-art chat-based LLMs, including the GPT models, in detecting in-file vulnerabilities. We conducted a costly investigation into how the performance of LLMs varies based on vulnerability type, input size, and vulnerability location within the file. To give enough statistical power (β ≥ .8) to our study, we could only focus on the three most common (as well as dangerous) vulnerabilities: XSS, SQL injection, and path traversal. Our findings indicate that the effectiveness of LLMs in detecting these vulnerabilities is strongly influenced by both the location of the vulnerability and the overall size of the input. Specifically, regardless of the vulnerability type, LLMs tend to significantly (p &lt; .05) underperform when detecting vulnerabilities located toward the end of larger files—a pattern we call the lost-in-the-end effect. Finally, to further support software developers and practitioners, we also explored the optimal input size for these LLMs and presented a simple strategy for identifying it, which can be applied to other models and vulnerability types. Eventually, we show how adjusting the input size can lead to significant improvements in LLM-based vulnerability detection, with an average recall increase of over 37% across all models.","2025-06","2025-11-25 22:29:59","2025-11-25 22:29:59","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; ‘Lost-in-the-End’ Issue; Code Context; In-File Vulnerability Detection; Path Traversal; SQL Injection; XSS","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AFNDQQMT","conferencePaper","2024","Gohar, Usman; Hunter, Michael C.; Lutz, Robyn R.; Cohen, Myra B.","CoDefeater: Using LLMs To Find Defeaters in Assurance Cases","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695296","https://doi.org/10.1145/3691620.3695296","Constructing assurance cases is a widely used and sometimes required process toward demonstrating that safety-critical systems will operate safely in their planned environment. To mitigate the risk of errors and missing edge cases, the concept of defeaters - challenges to claims in an assurance case - has been introduced. Defeaters can detect weaknesses in the arguments, prompting further investigation and timely mitigations. However, capturing defeaters relies on expert judgment, experience, and creativity and must be done iteratively due to evolving requirements and regulations. In this paper, we propose CoDefeater, an automated process to leverage large language models (LLMs) for finding defeaters. Initial results on two systems show that LLMs can efficiently find known and unforeseen feasible defeaters to support safety analysts in enhancing the completeness and confidence of assurance cases.","2024","2025-11-25 22:29:59","2025-11-25 22:29:59","","2262–2267","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","large language models; sUAS; assurance case; assurance defeaters","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JZIFFFRH","conferencePaper","2025","Donato, Benedetta; Mariani, Leonardo; Micucci, Daniela; Riganelli, Oliviero; Somaschini, Marco","MultiMind: A Plug-in for the Implementation of Development Tasks Aided by AI Assistants","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","10.1145/3696630.3730564","https://doi.org/10.1145/3696630.3730564","The integration of AI assistants into software development workflows is rapidly evolving, shifting from automation-assisted tasks to collaborative interactions between developers and AI. Large Language Models (LLMs) have demonstrated their effectiveness in several development activities, including code completion, test case generation, and documentation production. However, embedding AI-assisted tasks within Integrated Development Environments (IDEs) presents significant challenges. It requires designing mechanisms to invoke AI assistants at the appropriate time, coordinate interactions with multiple assistants, process the generated outputs, and present feedback in a way that seamlessly integrates with the development workflow. To address these issues, we introduce MultiMind, a Visual Studio Code plug-in that streamlines the creation of AI-assisted development tasks. MultiMind provides a modular and extensible framework, enabling developers to cost-effectively implement and experiment with new AI-powered interactions without the need for complex IDE customizations. MultiMind has been tested in two use cases: one for the automatic generation of code comments and the other about the definition of AI-powered chat.","2025","2025-11-25 22:29:59","2025-11-25 22:29:59","","1310–1317","","","","","","","FSE Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","IDE; AI-agents; multi LLM; VSCode","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WTVPTSR3","conferencePaper","2024","Tang, Shuncheng; Zhang, Zhenya; Zhou, Jixiang; Lei, Lei; Zhou, Yuan; Xue, Yinxing","LeGEND: A Top-Down Approach to Scenario Generation of Autonomous Driving Systems Assisted by Large Language Models","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695520","https://doi.org/10.1145/3691620.3695520","Autonomous driving systems (ADS) are safety-critical and require comprehensive testing before their deployment on public roads. While existing testing approaches primarily aim at the criticality of scenarios, they often overlook the diversity of the generated scenarios that is also important to reflect system defects in different aspects. To bridge the gap, we propose LeGEND, that features a top-down fashion of scenario generation: it starts with abstract functional scenarios, and then steps downwards to logical and concrete scenarios, such that scenario diversity can be controlled at the functional level. However, unlike logical scenarios that can be formally described, functional scenarios are often documented in natural languages (e.g., accident reports) and thus cannot be precisely parsed and processed by computers. To tackle that issue, LeGEND leverages the recent advances of large language models (LLMs) to transform textual functional scenarios to formal logical scenarios. To mitigate the distraction of useless information in functional scenario description, we devise a two-phase transformation that features the use of an intermediate language; consequently, we adopt two LLMs in LeGEND, one for extracting information from functional scenarios, the other for converting the extracted information to formal logical scenarios. We experimentally evaluate LeGEND on Apollo, an industry-grade ADS from Baidu. Evaluation results show that LeGEND can effectively identify critical scenarios, and compared to baseline approaches, LeGEND exhibits evident superiority in diversity of generated scenarios. Moreover, we also demonstrate the advantages of our two-phase transformation framework, and the accuracy of the adopted LLMs.","2024","2025-11-25 22:29:59","2025-11-25 22:29:59","","1497–1508","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","large language models; autonomous driving systems; critical scenario generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VZYDVSBE","bookSection","2025","Jiang, Xue; Dong, Yihong; Tao, Yongding; Liu, Huanyu; Jin, Zhi; Li, Ge","ROCODE: Integrating Backtracking Mechanism and Program Analysis in Large Language Models for Code Generation","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00133","Large language models (LLMs) have achieved impressive performance in code generation recently, offering programmers revolutionary assistance in software development. However, due to the auto-regressive nature of LLMs, they are susceptible to error accumulation during code generation. Once an error is produced, LLMs can merely continue to generate the subsequent code conditioned on it, given their inability to adjust previous outputs. Existing LLM-based approaches typically consider post-revising after code generation, leading to the challenging resolution of accumulated errors and the significant wastage of resources. Ideally, LLMs should rollback and resolve the occurred error in time during code generation, rather than proceed on the basis of the error and wait for post-revising after generation. In this paper, we propose RoCode, which integrates the backtracking mechanism and program analysis into LLMs for code generation. Specifically, we employ program analysis to perform incremental error detection during the generation process. When an error is detected, the backtracking mechanism is triggered to priming rollback strategies and constraint regeneration, thereby eliminating the error early and ensuring continued generation on the correct basis. Experiments on multiple code generation benchmarks show that RoCode can significantly reduce the errors generated by LLMs, with a compilation pass rate of 99.1%. The test pass rate is relatively improved by up to 23.8% compared to the best baseline approach. Compared to the post-revising baseline, the token cost is reduced by 19.3%. Moreover, our approach is model-agnostic and achieves consistent improvements across nine representative LLMs.","2025","2025-11-25 22:30:00","2025-11-25 22:30:00","","334–346","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PGQQ54MK","journalArticle","2025","Weber, Max; Mailach, Alina; Apel, Sven; Siegmund, Janet; Dachselt, Raimund; Siegmund, Norbert","Understanding Debugging as Episodes: A Case Study on Performance Bugs in Configurable Software Systems","Proc. ACM Softw. Eng.","","","10.1145/3717523","https://doi.org/10.1145/3717523","Debugging performance bugs in configurable software systems is a complex and time-consuming task that requires not only fixing a bug, but also understanding its root cause. While there is a vast body of literature on debugging strategies, there is no consensus on general debugging. This makes it difficult to provide concrete guidance for developers, especially for configuration-dependent performance bugs. The goal of our work is to alleviate this situation by providing an framework to describe debugging strategies in a more general, unifying way. We conducted a user study with 12 professional developers who debugged a performance bug in a real-world configurable system. To observe developers in an unobstructive way, we provided an immersive virtual reality tool, SoftVR, giving them a large degree of freedom to choose the preferred debugging strategy. The results show that the existing documentation of strategies is too coarse-grained and intermixed to identify successful approaches. In a subsequent qualitative analysis, we devised a coding framework to reason about debugging approaches. With this framework, we identified five goal-oriented episodes that developers employ, which they also confirmed in subsequent interviews. Our work provides a unified description of debugging strategies, allowing researchers a common foundation to study debugging and practitioners and teachers guidance on successful debugging strategies.","2025-06","2025-11-25 22:30:00","2025-11-25 22:30:00","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Human Factors; Immersive Environment; Program Debugging","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"98RNCH2X","conferencePaper","2025","Krishna Vajjala, Arun; Krishna Vajjala, Ajay; Badea, Carmen; Bird, Christian; D'Souza, Jade; DeLine, Robert; Demyanyuk, Mikhail O; Entenmann, Jason; Forsgren, Nicole; Hramadski, Aliaksandr; Mohammad, Haris; Sanyal, Sandeepan; Surmachev, Oleg; Zimmermann, Thomas","Using Large Language Models to Support the Workflow of Differential Testing","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","10.1145/3696630.3728559","https://doi.org/10.1145/3696630.3728559","Many software development teams use differential testing as a quality gate in their release process. Differential testing—namely, comparing behavioral differences between a system in production and a system in test—is a laborious process to label changes as regressions, expected changes, or incidental changes (e.g. those due to nondeterminism or timing). This manual process involves inspecting large textual artifacts, like logs, pull requests, and team discussions, which suggests that Large Language Models (LLMs) could be helpful. In this paper, we engage with the team developing a central Azure service to understand their work practice for differential testing. We used a design probe method to elicit feedback about several ways to use LLMs to improve their work practice, including automatically labeling behavior differences and providing summaries of various artifacts and discussions. Release engineers on the team report that predicting a difference's label would save them effort, but they want an explicit rationale to improve their trust in the prediction; they found the generated summaries to be informative, if a bit wordy.","2025","2025-11-25 22:30:00","2025-11-25 22:30:00","","355–365","","","","","","","FSE Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","large language models; AI for SE; differential testing; developer productivity; human-computer interaction; release engineering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LURLG7MP","conferencePaper","2025","Hu, Yage; Zhang, Wen; Xiao, Botang; Kong, Qingchen; Yi, Boyang; Ji, Suxin; Wang, Songlan; Wang, Wenwen","WASIT: Deep and Continuous Differential Testing of WebAssembly System Interface Implementations","Proceedings of the ACM SIGOPS 31st Symposium on Operating Systems Principles","979-8-4007-1870-0","","10.1145/3731569.3764819","https://doi.org/10.1145/3731569.3764819","This paper presents WASIT, a powerful specification-driven differential testing framework for WebAssembly (Wasm) system interface (WASI) implementations. WASIT invents several innovative techniques to address the challenges facing state-of-the-art testing approaches when applied to WASI implementations. Specifically, it introduces real-time resource abstraction and tracking to facilitate the generation of meaningful and dependent WASI function calls. It also creates a domain-specific language to automatically filter out uninteresting WASI function argument values by augmenting the WASI specification. Finally, it adopts a decoupled system architecture to achieve smooth co-evolution with WASI. Our evaluation shows that WASIT successfully found 48 new WASI-specific bugs in six popular Wasm runtimes, with 41 confirmed, 37 fixed, and three CVEs assigned.","2025","2025-11-25 22:30:00","2025-11-25 22:30:00","","719–735","","","","","","","SOSP '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lotte Hotel World, Seoul, Republic of Korea","","","","differential testing; specification-driven testing; system state abstraction and tracking; WASI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8N642UP8","journalArticle","2025","Ma, Yingwei; Cao, Rongyu; Cao, Yongchang; Zhang, Yue; Chen, Jue; Liu, Yibo; Liu, Yuchen; Li, Binhua; Huang, Fei; Li, Yongbin","SWE-GPT: A Process-Centric Language Model for Automated Software Improvement","Proc. ACM Softw. Eng.","","","10.1145/3728981","https://doi.org/10.1145/3728981","Large language models (LLMs) have demonstrated remarkable performance in code generation, significantly enhancing the coding efficiency of developers. Recent advancements in LLM-based agents have led to significant progress in end-to-end automatic software engineering (ASE), particularly in software maintenance (e.g., fixing software issues) and evolution (e.g., adding new features). Despite these encouraging advances, current research faces two major challenges. First, state-of-the-art performance primarily depends on closed-source models like GPT-4, which significantly limits the technology’s accessibility, and potential for customization in diverse software engineering tasks. This dependence also raises concerns about data privacy, particularly when handling sensitive codebases. Second, these models are predominantly trained on static code data, lacking a deep understanding of the dynamic interactions, iterative problem-solving processes, and evolutionary characteristics inherent in software development. Consequently, they may face challenges in navigating complex project structures and generating contextually relevant solutions, which can affect their practical utility in real-world scenarios. To address these challenges, our study adopts a software engineering perspective. We recognize that real-world software maintenance and evolution processes encompass not only static code data but also developers’ thought processes, utilization of external tools, and the interaction between different functional personnel. Our objective is to develop an open-source large language model specifically optimized for software improvement, aiming to match the performance of closed-source alternatives while offering greater accessibility and customization potential. Consequently, we introduce the Lingma SWE-GPT series, comprising Lingma SWE-GPT 7B and Lingma SWE-GPT 72B. By learning from and simulating real-world code submission activities, Lingma SWE-GPT systematically incorporates the dynamic interactions and iterative problem-solving inherent in software development process—such as repository understanding, fault localization, and patch generation—thereby achieving a more comprehensive understanding of software improvement processes. We conducted experimental evaluations using SWE-bench-Verified benchmark (comprising 500 real GitHub issues), recently proposed by OpenAI. The results demonstrate that Lingma SWE-GPT 72B successfully resolves 30.20% of the GitHub issues, marking a significant improvement in automatic issue resolution (22.76% relative improvement compared to Llama 3.1 405B), approaching the performance of closed-source models (31.80% issues of GPT-4o resolved). Notably, Lingma SWE-GPT 7B resolves 18.20% of the issues, surpassing the 17.20% resolution rate of Llama 3.1 70B, highlighting the potential for applying smaller models to ASE tasks.","2025-06","2025-11-25 22:30:00","2025-11-25 22:30:00","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models (LLMs); Automated Program Repair; Fault Localization; Automatic Software Engineering (ASE); Software Engineering Agents","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5AAZN4LA","conferencePaper","2024","Pan, Rangeet; Ibrahimzada, Ali Reza; Krishna, Rahul; Sankar, Divya; Wassi, Lambert Pouguem; Merler, Michele; Sobolev, Boris; Pavuluri, Raju; Sinha, Saurabh; Jabbarvand, Reyhaneh","Lost in Translation: A Study of Bugs Introduced by Large Language Models while Translating Code","Proceedings of the IEEE/ACM 46th International Conference on Software Engineering","979-8-4007-0217-4","","10.1145/3597503.3639226","https://doi.org/10.1145/3597503.3639226","Code translation aims to convert source code from one programming language (PL) to another. Given the promising abilities of large language models (LLMs) in code synthesis, researchers are exploring their potential to automate code translation. The prerequisite for advancing the state of LLM-based code translation is to understand their promises and limitations over existing techniques. To that end, we present a large-scale empirical study to investigate the ability of general LLMs and code LLMs for code translation across pairs of different languages, including C, C++, Go, Java, and Python. Our study, which involves the translation of 1,700 code samples from three benchmarks and two real-world projects, reveals that LLMs are yet to be reliably used to automate code translation—with correct translations ranging from 2.1% to 47.3% for the studied LLMs. Further manual investigation of unsuccessful translations identifies 15 categories of translation bugs. We also compare LLM-based code translation with traditional non-LLM-based approaches. Our analysis shows that these two classes of techniques have their own strengths and weaknesses. Finally, insights from our study suggest that providing more context to LLMs during translation can help them produce better results. To that end, we propose a prompt-crafting approach based on the symptoms of erroneous translations; this improves the performance of LLM-based code translation by 5.5% on average. Our study is the first of its kind, in terms of scale and breadth, that provides insights into the current limitations of LLMs in code translation and opportunities for improving them. Our dataset—consisting of 1,700 code samples in five PLs with 10K+ tests, 43K+ translated code, 1,748 manually labeled bugs, and 1,365 bug-fix pairs—can help drive research in this area.","2024","2025-11-25 22:30:00","2025-11-25 22:30:00","","","","","","","","","ICSE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","llm; code translation; bug taxonomy","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YVEAJF96","journalArticle","2024","Lou, Yiling; Yang, Jun; Benton, Samuel; Hao, Dan; Tan, Lin; Chen, Zhenpeng; Zhang, Lu; Zhang, Lingming","When Automated Program Repair Meets Regression Testing—An Extensive Study on Two Million Patches","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3672450","https://doi.org/10.1145/3672450","In recent years, Automated Program Repair (APR) has been extensively studied in academia and even drawn wide attention from the industry. However, APR techniques can be extremely time consuming since (1) a large number of patches can be generated for a given bug, and (2) each patch needs to be executed on the original tests to ensure its correctness. In the literature, various techniques (e.g., based on learning, mining, and constraint solving) have been proposed/studied to reduce the number of patches. Intuitively, every patch can be treated as a software revision during regression testing; thus, traditional Regression Test Selection (RTS) techniques can be leveraged to only execute the tests affected by each patch (as the other tests would keep the same outcomes) to further reduce patch execution time. However, few APR systems actually adopt RTS and there is still a lack of systematic studies demonstrating the benefits of RTS and the impact of different RTS strategies on APR. To this end, this article presents the first extensive study of widely used RTS techniques at different levels (i.e., class/method/statement levels) for 12 state-of-the-art APR systems on over 2M patches. Our study reveals various practical guidelines for bridging the gap between APR and regression testing, including: (1) the number of patches widely used for measuring APR efficiency can incur skewed conclusions, and the use of inconsistent RTS configurations can further skew the conclusions; (2) all studied RTS techniques can substantially improve APR efficiency and should be considered in future APR work; (3) method- and statement-level RTS outperform class-level RTS substantially and should be preferred; (4) RTS techniques can substantially outperform state-of-the-art test prioritization techniques for APR, and combining them can further improve APR efficiency; and (5) traditional Regression Test Prioritization (RTP) widely studied in regression testing performs even better than APR-specific test prioritization when combined with most RTS techniques. Furthermore, we also present the detailed impact of different patch categories and patch validation strategies on our findings.","2024-09","2025-11-25 22:30:00","2025-11-25 22:30:00","","","","7","33","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","program repair; patch validation; Test selection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CXYGZ3WP","bookSection","2025","Jahan, Sigma; Rahman, Mohammad Masudur","Can Hessian-Based Insights Support Fault Diagnosis in Attention-based Models?","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","","https://doi.org/10.1145/3696630.3728522","As attention-based deep learning models scale in size and complexity, diagnosing their faults becomes increasingly challenging. In this work, we conduct an empirical study to evaluate the potential of Hessian-based analysis for diagnosing faults in attention-based models. Specifically, we use Hessian-derived insights to identify fragile regions (via curvature analysis) and parameter interdependencies (via parameter interaction analysis) within attention mechanisms. Through experiments on three diverse models (HAN, 3D-CNN, DistilBERT), we show that Hessian-based metrics can localize instability and pinpoint fault sources more effectively than gradients alone. Our empirical findings suggest that these metrics could significantly improve fault diagnosis in complex neural architectures, potentially improving software debugging practices.","2025","2025-11-25 22:30:00","2025-11-25 22:30:00","","676–680","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SK8V2RXM","journalArticle","2024","Coppola, Riccardo; Ardito, Luca; Leotta, Maurizio","Gamify: Gamification in Software Development, Verification,and Validation","SIGSOFT Softw. Eng. Notes","","0163-5948","10.1145/3650142.3650151","https://doi.org/10.1145/3650142.3650151","In this paper we report the outcomes of the 1st and 2nd edition of the International Workshop on Gamification in Software Development, Verification, and Validation (Gamify 2022 and Gamify 2023) which were held as part of the 30th and 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2022, in Singapore, November 17, 2022 and ESEC/FSE 2023, online workshop, December 4, 2023).","2024-04","2025-11-25 22:30:00","2025-11-25 22:30:00","","27–30","","2","49","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZK5VUHIF","journalArticle","2023","Biagiola, Matteo; Cardozo, Nicolás; Shin, Donghwan; Khomh, Foutse; Stocco, Andrea; Riccio, Vincenzo","Summary of the Fourth International Workshop on Deep Learning for Testing and Testing for Deep Learning (DeepTest 2023)","SIGSOFT Softw. Eng. Notes","","0163-5948","10.1145/3617946.3617953","https://doi.org/10.1145/3617946.3617953","Deep Learning (DL) techniques help software developers thanks to their ability to learn from historical information which is useful in several program analysis and testing tasks (e.g., malware detection, fuzz testing, bug-finding, and type-checking). DL-based software systems are also increasingly adopted in safety-critical domains, such as autonomous driving, medical diagnosis, and aircraft collision avoidance systems. In particular, testing the correctness and reliability of DL-based systems is paramount, since a failure of such systems would cause a significant safety risk for the involved people and/or environment. The 4th International Workshop on Deep Learning for Testing and Testing for Deep Learning (DeepTest 2023) was co-located with the 45th International Conference on Software Engineering (ICSE), with the goal of targeting research at the intersection of software engineering and deep learning and devise novel approaches and tools to ensure the interpretability and dependability of software systems that depends on DL components.","2023-10","2025-11-25 22:30:00","2025-11-25 22:30:00","","39–40","","4","48","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"53PJMUUZ","journalArticle","2025","Sun, Weifeng; Huang, Naiqi; Yan, Meng; Liu, Zhongxin; Li, Hongyan; Lei, Yan; Lo, David","On-the-fly Generation-Quality Enhancement of Deep Code Models via Model Collaboration","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3765752","https://doi.org/10.1145/3765752","The growing prominence of deep code models in automating software engineering tasks is undeniable. However, their deployment encounters significant challenges in on-the-fly performance enhancement, which refers to dynamically improving the performance of deep code models during real-time execution. Conventional techniques, such as retraining or fine-tuning, are effective in controlled pre-deployment scenarios but fall short when adapting to on-the-fly adjustments post-deployment. CodeDenoise, a notable on-the-fly performance enhancement technology, leverages uncertainty-based methods to identify misclassified inputs and applies an input modification strategy to rectify classification errors. While effective for classification tasks, this approach is inapplicable to generative tasks due to two key challenges: ❶ Uncertainty-based methods are unsuitable for identifying challenging inputs, especially in generative tasks with diverse and open-ended outputs. Challenging inputs refers to a class of inputs where, due to the inherent complexity of the task or insufficient context in the input samples, the model struggles to generate high-quality outputs. ❷ Input modification strategies cannot be applied to generative tasks, as modifying the input can unpredictably affect the entire sequence of generated outputs. These limitations highlight the need for novel techniques that can enhance the generation quality of deep code models in real-time.To bridge this gap, we propose CodEn, a framework designed to enhance the generation quality of deployed deep code models through model collaboration and real-time output repair. CodEn&nbsp;employs an ensemble learning approach, integrating multiple generic output quality assessment metrics to identify challenging inputs. By combining these diverse metrics, CodEn&nbsp;overcomes the limitations of uncertainty-based methods, making it effective across various generative tasks. Additionally, we introduce an elaborate on-the-fly repair method for the outputs of challenging inputs, leveraging a large language model (LLM) and a novel dual-prompt strategy. This strategy utilizes both generation and selection-based prompts to provide potential fixes and employs an adaptive mechanism to select the optimal output. Our experiments, conducted on 12 deep code models across three pre-trained code models, three popular code-related generation tasks, and four datasets, demonstrate the effectiveness of CodEn. For example, in the assertion generation task, CodEn&nbsp;enhances the SAM (Semantic Accuracy Match) of baseline models with improvements ranging from 12.14% to 21.65%. In the bug fixing task, CodEn&nbsp;achieves exact match gains ranging from 17.51% to 30.64% on TFix dataset. For the code summarization task, CodEn&nbsp;significantly boosts performance across key metrics: BLEU scores improved by 5.72% (sim) 11.79%, ROUGE-L by 4.41% (sim) 7.70%, METEOR by 7.51% (sim) 12.29%, and CIDEr by 8.09% (sim) 15.80%. Besides, we conduct experiments of CodEn&nbsp;on different open-source LLMs and demonstrate that CodEn&nbsp;can still achieve significant improvements.","2025-09","2025-11-25 22:30:00","2025-11-25 22:30:00","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Deep Code Models; In-Context Learning; Model Collaboration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DUWISDKR","journalArticle","2025","Schweikl, Sebastian; Fraser, Gordon","RePurr: Automated Repair of Block-Based Learners’ Programs","Proc. ACM Softw. Eng.","","","10.1145/3715786","https://doi.org/10.1145/3715786","Programming is increasingly taught using dedicated block-based programming environments such as Scratch. While the use of blocks instead of text prevents syntax errors, learners can still make semantic mistakes implying a need for feedback and help. Since teachers may be overwhelmed by help requests in a classroom, may not have the required programming education themselves, and may simply not be available in independent learning scenarios, automated hint generation is desirable. Automated program repair can provide the foundation for automated hints, but relies on multiple assumptions: (1) Program repair usually aims to produce localized patches for fixing single bugs, but learners may fundamentally misunderstand programming concepts and tasks or request help for substantially incomplete programs. (2) Software tests are required to guide the search and to localize broken statements, but test suites for block-based programs are different to those considered in past research on fault localization and repair: They consist of system tests, where very few tests are sufficient to fully cover the code. At the same time, these tests have vastly longer execution times caused by the use of animations and interactions on Scratch programs, thus inhibiting the applicability of metaheuristic search. (3) The plastic surgery hypothesis assumes that the code necessary for repairs already exists in the codebase. Block-based programs tend to be small and may lack this necessary redundancy. In order to study whether automated program repair of block-based programs is nevertheless feasible, in this paper we introduce, to the best of our knowledge, the first automated program repair approach for Scratch programs based on evolutionary search. Our RePurr prototype includes novel refinements of fault localization to improve the lack of guidance of the test suites, recovers the plastic surgery hypothesis by exploiting that a learning scenario provides model and student solutions as alternatives, and uses parallelization and accelerated executions to reduce the costs of fitness evaluations. Empirical evaluation of RePurr on a set of real learners' programs confirms the anticipated challenges, but also demonstrates that the repair can nonetheless effectively improve and fix learners' programs, thus enabling automated generation of hints and feedback for learners.","2025-06","2025-11-25 22:30:00","2025-11-25 22:30:00","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Program repair; Block-based programming; Hints; Scratch","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WRB7XEUH","bookSection","2025","Wang, Jue; Chen, Shuxiang; Liu, Yu; Deng, Yuan; Zhang, Lei; Fu, Yuanchang; Liu, Bo","TestGPT-Server: Automatically Testing Microservices with Large Language Models at ByteDance","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","","https://doi.org/10.1145/3696630.3728545","Despite the advantages of the microservice architecture, there is an urgent need for automated testing microservice APIs, ensuring that each API functions correctly and reliably without exhaustive manual effort. This paper introduces TestGPT-Server, a novel, fully automated testing system for microservice APIs developed and deployed at ByteDance, leveraging the capabilities of large language models (LLMs) to enhance test request generation and bug detection. TestGPT-Server utilizes an LLM-enhanced interface analyzer and a code analyzer to extract syntactic and semantic information of the API under test. Based on the extracted information, it employs a targeted request generator and a guided stochastic fuzzer to produce comprehensive test requests. After sending generated test requests to the API, an LLM-assisted response analyzer identifies potential service crashes, and an assertion miner automatically mines functional assertions to detect functional bugs. Deployed at ByteDance, the evaluation results demonstrate that TestGPT-Server is effective, achieving a 40.52% improvement over existing test requests and identifying 50 crash bugs and 2 functional bugs across 14,366 microservice APIs, with minimal manual effort.","2025","2025-11-25 22:30:00","2025-11-25 22:30:00","","192–203","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DPKMUF3Y","journalArticle","2024","Li, Yuechen; Pei, Hanyu; Huang, Linzhi; Yin, Beibei; Cai, Kai-Yuan","Automatic Repair of Quantum Programs via Unitary Operation","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3664604","https://doi.org/10.1145/3664604","With the continuous advancement of quantum computing (QC), the demand for high-quality quantum programs (QPs) is growing. To avoid program failure, in software engineering, the technology of automatic program repair (APR) employs appropriate patches to remove potential bugs without the intervention of a human. However, the method tailored for repairing defective QPs is still absent. This article proposes, to the best of our knowledge, a new APR method named UnitAR that can repair QPs via unitary operation automatically. Based on the characteristics of superposition and entanglement in QC, the article constructs an algebraic model and adopts a generate-and-validate approach for the repair procedure. Furthermore, the article presents two schemes that can respectively promote the efficiency of generating patches and guarantee the effectiveness of applying patches. For the purpose of evaluating the proposed method, the article selects 29 mutated versions as well as five real-world buggy programs as the objects and introduces two traditional APR approaches GenProg and TBar as baselines. According to the experiments, UnitAR can fix 23 buggy programs, and this method demonstrates the highest efficiency and effectiveness among three APR approaches. Besides, the experimental results further manifest the crucial roles of two constituents involved in the framework of UnitAR.","2024-06","2025-11-25 22:30:00","2025-11-25 22:30:00","","","","6","33","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","automatic program repair; Quantum computing; quantum software engineering; S-ADA; software cybernetics; unitary operation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ESCCA9JT","journalArticle","2024","Liu, Yue; Tantithamthavorn, Chakkrit; Liu, Yonghui; Thongtanunam, Patanamon; Li, Li","Automatically Recommend Code Updates: Are We There Yet?","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3678167","https://doi.org/10.1145/3678167","In recent years, large pre-trained Language Models of Code (CodeLMs) have shown promising results on various software engineering tasks. One such task is automatic code update recommendation, which transforms outdated code snippets into their approved and revised counterparts. Although many CodeLM-based approaches have been proposed, claiming high accuracy, their effectiveness and reliability on real-world code update tasks remain questionable. In this article, we present the first extensive evaluation of state-of-the-art CodeLMs for automatically recommending code updates. We assess their performance on two diverse datasets of paired updated methods, considering factors such as temporal evolution, project specificity, method size, and update complexity. Our results reveal that while CodeLMs exhibit higher performance in settings that ignore temporal information, they struggle in more realistic time-wise scenarios and generalize poorly to new projects. Furthermore, CodeLM performance decreases significantly for larger methods and more complex updates. Furthermore, we observe that many CodeLM-generated “updates” are actually null, especially in time-wise settings, and meaningful edits remain challenging. Our findings highlight the significant gap between the perceived and actual effectiveness of CodeLMs for real-world code update recommendation and emphasize the need for more research on improving their practicality, robustness, and generalizability.","2024-12","2025-11-25 22:30:00","2025-11-25 22:30:00","","","","8","33","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","neural machine translation; Code updates","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6GAZQ87K","journalArticle","2025","Zhao, Beibei; Feng, Wenjie; Guo, Qingli; Sun, Yingli; Gu, Fangming; Zhang, Bolun; Gong, Xiaorui; Li, Hong","FAVDisco: Modeling and Discovering File Access Vulnerabilities","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3744901","https://doi.org/10.1145/3744901","File access vulnerabilities (FAVs) are one type of security weakness arising from adversary manipulations of file access inputs, posing significant threats to system integrity. Despite their prevalence, FAVs remain underexplored due to limited understanding, complex triggering scenarios, and stealthy and diverse manifestations; these challenges render current detection approaches incomplete and inaccurate.To this end, we conducted an in-depth empirical study across 204 file-related CVEs, uncovering the root cause and trigger mechanisms of FAVs. Based on these findings, we propose an exhaustive accessing model and a specialized threat model that define the Adversary and Attack Surface for FAVs, enabling systematic attribution and analysis of file operations. Furthermore, we propose FAVDisco, a novel framework for discovering FAVs by mutating, triggering, and analyzing file operations. It employs a File Mutator to simulate diverse execution scenarios and a FAV Checker that integrates a model-based adversary controllable checker with pattern-based detection rules to identify FAVs. Implemented on Windows, FAVDisco achieves remarkable performance with 92.1% precision and 83.3% recall on the disclosed FAV detection task, outperforming state-of-the-art methods. Moreover, it uncovers 13 zero-day FAVs in 10 widely-used services, with 6 assigned new CVEs and earning a reward of $29,000 from Microsoft Security Response Center.","2025-06","2025-11-25 22:30:00","2025-11-25 22:30:00","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Confused Deputy; File Access Vulnerability; Privilege Escalation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B7HBQBRQ","conferencePaper","2024","Burgueño, Lola; Keet, Maria; Kienzle, Jörg; Michael, Judith; Babur, Önder","A Human Behavior Exploration Approach Using LLMs for Cyber-Physical Systems","Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems","979-8-4007-0622-6","","10.1145/3652620.3687806","https://doi.org/10.1145/3652620.3687806","In the early phases of Cyber-Physical Systems (CPS) development, scoping human behavior plays a significant role, especially when interactions extend beyond expected behavior. Here, it is especially challenging to develop cases that capture the full spectrum of human behavior. Up to now, identifying such behavior of humans remains a task for domain experts. We explore how one can use Large Languages Models (LLMs) in the design phase of systems to provide additional information about human-CPS interaction. Our approach proposes a preliminary ontology describing a hierarchy of types of behavior and relevant CPS components as input for prompt templates. It uses them to generate parts of human behavior descriptions, as well as a canned prompt with one variable about behavior. For demonstration, we take a smart building with a Home Energy System as the use case.An initial user evaluation shows that the behavior descriptions generated with standard and ontology-driven prompts complement each other and are useful when assisting humans. The discovered uncommon behaviors can be used to complete interaction scenarios that eventually result in a more robust CPS implementation.","2024","2025-11-25 22:30:00","2025-11-25 22:30:00","","578–586","","","","","","","MODELS Companion '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Linz, Austria","","","","large language models; cyber-physical systems; digital twin; human behavior; user scenario","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M2TJQ2AY","conferencePaper","2025","Rawal, Ruchit; Pădurean, Victor-Alexandru; Apel, Sven; Singla, Adish; Toneva, Mariya","Hints Help Finding and Fixing Bugs Differently in Python and Text-Based Program Representations","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00192","https://doi.org/10.1109/ICSE55347.2025.00192","With the recent advances in AI programming assistants such as GitHub Copilot, programming is not limited to classical programming languages anymore-programming tasks can also be expressed and solved by end-users in natural text. Despite the availability of this new programming modality, users still face difficulties with algorithmic understanding and program debugging. One promising approach to support end-users is to provide hints to help them find and fix bugs while forming and improving their programming capabilities. While it is plausible that hints can help, it is unclear which type of hint is helpful and how this depends on program representations (classic source code or a textual representation) and the user's capability of understanding the algorithmic task. To understand the role of hints in this space, we conduct a large-scale crowd-sourced study involving 753 participants investigating the effect of three types of hints (test cases, conceptual, and detailed), across two program representations (Python and text-based), and two groups of users (with clear understanding or confusion about the algorithmic task). We find that the program representation (Python vs. text) has a significant influence on the users' accuracy at finding and fixing bugs. Surprisingly, users are more accurate at finding and fixing bugs when they see the program in natural text. Hints are generally helpful in improving accuracy, but different hints help differently depending on the program representation and the user's understanding of the algorithmic task. These findings have implications for designing next-generation programming tools that provide personalized support to users, for example, by adapting the programming modality and providing hints with respect to the user's skill level and understanding.","2025","2025-11-25 22:30:00","2025-11-25 22:30:00","","1230–1242","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","debugging; crowd-sourced study; hints; program comprehension; programming modalities","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ABMBKE7B","conferencePaper","2025","Jiang, Yue; Schoop, Eldon; Swearngin, Amanda; Nichols, Jeffrey","ILuvUI: Instruction-tuned LangUage-Vision modeling of UIs from Machine Conversations","Proceedings of the 30th International Conference on Intelligent User Interfaces","979-8-4007-1306-4","","10.1145/3708359.3712129","https://doi.org/10.1145/3708359.3712129","Multimodal Vision-Language Models (VLMs) enable powerful applications from their fused understanding of images and language, but many perform poorly on UI tasks due to the lack of UI training data. In this paper, we adapt a recipe for generating paired text-image training data for VLMs to the UI domain by combining existing pixel-based methods with a Large Language Model (LLM). Unlike prior art, our method requires no human-provided annotations, and it can be applied to any dataset of UI screenshots. We generate a dataset of 353K conversational examples paired with UIs that cover Q&amp;A, UI descriptions, and planning, and use it to fine-tune a conversational VLM for UI tasks. To assess the performance of our model, we benchmark it on UI element detection tasks, evaluate response quality, and showcase its applicability to UI verification.","2025","2025-11-25 22:30:00","2025-11-25 22:30:00","","861–877","","","","","","","IUI '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","User Interface; UI Automation; Vision Language Model","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I6WC5I9Q","conferencePaper","2025","Shihab, Md Istiak Hossain; Hundhausen, Christopher; Tariq, Ahsun; Haque, Summit; Qiao, Yunhan; Mulanda, Brian Wise","The Effects of GitHub Copilot on Computing Students' Programming Effectiveness, Efficiency, and Processes in Brownfield Coding Tasks","Proceedings of the 2025 ACM Conference on International Computing Education Research V.1","979-8-4007-1340-8","","10.1145/3702652.3744219","https://doi.org/10.1145/3702652.3744219","When graduates of computing degree programs enter the software industry, they will most likely join teams working on legacy code bases developed by people other than themselves. In these so-called brownfield software development settings, generative artificial intelligence (GenAI) coding assistants like GitHub Copilot are rapidly transforming software development practices, yet the impact of GenAI on student programmers performing brownfield development tasks remains underexplored. This paper investigates how GitHub Copilot influences undergraduate students’ programming performance, behaviors, and understanding when completing brownfield programming tasks in which they add new code to an unfamiliar code base. We conducted a controlled experiment in which 10 undergraduate computer science students completed highly similar brownfield development tasks with and without Copilot in a legacy web application. Using a mixed-methods approach combining performance analysis, behavioral analysis, and exit interviews, we found that students completed tasks 34.9% faster (p &lt; 0.05) and made 50.0% more solution progress (p &lt; 0.05) when using Copilot. Moreover, our analysis revealed that, when using Copilot, students spent 10.6% less time manually writing code (p &lt; 0.05), and 11.6% less time conducting web searches (p &lt; 0.05), providing evidence of a fundamental shift in how they engaged in programming. In exit interviews, students reported concerns about not understanding how or why Copilot suggestions work. This research suggests the need for computing educators to develop new pedagogical approaches that leverage GenAI assistants’ benefits while fostering reflection on how and why GenAI suggestions address brownfield programming tasks. Complete study results and analysis are presented at ghcopilot-icer.github.io.","2025","2025-11-25 22:30:00","2025-11-25 22:30:00","","407–420","","","","","","","ICER '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","large language models; software engineering education; GitHub Copilot; AI-assisted programming; brownfield software development; empirical studies of programming; Generative AI code assistants; legacy code; undergraduate programming","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HMVNB9Q5","journalArticle","2025","Martinez, Matias; Martínez-Fernández, Silverio; Franch, Xavier","The Sustainability Face of Automated Program Repair Tools","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3744900","https://doi.org/10.1145/3744900","Automated program repair (APR) aims to automatize the process of repairing software bugs in order to reduce the cost of maintaining software programs. While APR accuracy has significantly improved in recent years, its energy impact remains unstudied. The field of green software research aims to measure the energy consumption required to develop, maintain, and use software products. Our main goal is to define the foundation for measuring the energy consumption of the APR activity. We state that an environmentally sustainable (or green) APR tool achieves a good balance between the ability to correctly repair bugs and the amount of energy consumed during such process. We measure the energy consumption of 10 traditional APR tools for Java and 11 fine-tuned large-language models (LLM) trying to repair real bugs from Defects4J. The results of this study show the existing tradeoff between energy consumption and repairability. In particular, APR tools such as TBar and RepairLlama repair more bugs than other approaches at the expense of a higher energy consumption. Other tools, such as SimFix and the LLM CodeT5-large, provide a good tradeoff between energy consumption and repairability. We also present guidelines consisting of a set of recommendations for developing greener APR.","2025-10","2025-11-25 22:30:00","2025-11-25 22:30:00","","","","8","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Automated Program Repair; Energy Consumption of Software Tools; Green Computing; Software Sustainability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y683PWLL","journalArticle","2025","Peng, Yun; Wan, Jun; Li, Yichen; Ren, Xiaoxue","COFFE: A Code Efficiency Benchmark for Code Generation","Proc. ACM Softw. Eng.","","","10.1145/3715727","https://doi.org/10.1145/3715727","Code generation has largely improved development efficiency in the era of large language models (LLMs). With the ability to follow instructions, current LLMs can be prompted to generate code solutions given detailed descriptions in natural language. Many research efforts are being devoted to improving the correctness of LLM-generated code, and many benchmarks are proposed to evaluate the correctness comprehensively. Despite the focus on correctness, the time efficiency of LLM-generated code solutions is under-explored. Current correctness benchmarks are not suitable for time efficiency evaluation since their test cases cannot well distinguish the time efficiency of different code solutions. Besides, the current execution time measurement is not stable and comprehensive, threatening the validity of the time efficiency evaluation. To address the challenges in the time efficiency evaluation of code generation, we propose COFFE, a code generation benchmark for evaluating the time efficiency of LLM-generated code solutions. COFFE contains 398 and 358 problems for function-level and file-level code generation, respectively. To improve the distinguishability, we design a novel stressful test case generation approach with contracts and two new formats of test cases to improve the accuracy of generation. For the time evaluation metric, we propose efficienct@k based on CPU instruction count to ensure a stable and solid comparison between different solutions. We evaluate 14 popular LLMs on COFFE and identify four findings. Based on the findings, we draw some implications for LLM researchers and software practitioners to facilitate future research and usage of LLMs in code generation.","2025-06","2025-11-25 22:30:00","2025-11-25 22:30:00","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Code Generation; Benchmark; Code Efficiency; Time","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"76E99SCD","conferencePaper","2025","Tian, Zhao; Chen, Junjie; Zhang, Xiangyu","Fixing Large Language Models' Specification Misunderstanding for Better Code Generation","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00108","https://doi.org/10.1109/ICSE55347.2025.00108","Code generation is to automatically generate source code conforming to a given programming specification, which has received extensive attention especially with the development of large language models (LLMs). Due to the inherent difficulty of code generation, the code generated by LLMs may not be aligned with the specification. Although thought-eliciting prompting techniques have been proposed to enhance the code generation performance of LLMs, producing correct understanding for complicated programming problems remains challenging, resulting in unsatisfactory performance. Also, some feedback-based prompting techniques have been proposed to fix incorrect code using error messages produced by test execution. However, when the generated code deviates significantly from the ground truth, they encounter difficulties in improving performance based on such coarse-grained information.In this work, we propose a novel prompting technique, called μFiX, to improve the code generation performance of LLMs by devising both sophisticated thought-eliciting prompting and feedback-based prompting and making the first exploration on their synergy. It first exploits test case analysis to obtain specification understanding and enables a self-improvement process to identify and refine the misunderstanding in the thought-eliciting prompting phase. μFiX further fixes the specification understanding towards the direction reducing the gap between the provided understanding (from the first phase) and the actual understanding implicitly utilized by LLMs for code generation in the feedback-based prompting phase. By improving the understanding with μFiX, the code generation performance of LLMs can be largely improved. Our evaluation on two advanced LLMs (ChatGPT and DeepSeek-Coder) with six widely-used benchmarks by comparing with 15 baselines, demonstrates the effectiveness of μFiX. For example, μFiX outperforms the most effective baseline with an average improvement of 35.62% in terms of Pass@1 across all subjects.","2025","2025-11-25 22:30:00","2025-11-25 22:47:24","","1514–1526","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","Large language models; Software engineering; Accuracy; Chatbots; Codes; Source coding; large language models; Large Language Models; Benchmark testing; code generation; Code Generation; Programming; prompting engineering; Prompting Engineering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H63J84GT","conferencePaper","2025","Saha, Diptikalyan; Sondhi, Devika; Haldar, Swagatam; Sinha, Saurabh","REST API Functional Tester","Proceedings of the 18th Innovations in Software Engineering Conference","979-8-4007-1424-5","","10.1145/3717383.3717388","https://doi.org/10.1145/3717383.3717388","Software developers nowadays routinely publish REST APIs to make their applications accessible to clients. To support the development of REST APIs, many testing techniques and tools have been developed that automatically explore the API endpoints, using various strategies, intending to increase coverage and detect bugs. However, these techniques can produce test cases that may not be suitable for creating functional test suites. In this paper, we address the problem of automatically generating realistic functional test cases for REST APIs. Our work is guided by interactions with practitioners and the approaches they follow for performing functional testing of REST APIs. Our technique first groups the API operations by the resources they manipulate; it then employs a novel approach for inferring producer-consumer relations between operations, fine-grained subtyping of HTTP methods, and language-model-based sequencing to construct operation sequences that correspond to functional scenarios. We evaluated the effectiveness of our technique and compared it against state-of-the-art REST API testing tools. Our results show that the technique produces test cases that can better support functional testing of REST APIs than the other tools.","2025","2025-11-25 22:30:00","2025-11-25 22:30:00","","","","","","","","","ISEC '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UR8GLXHX","journalArticle","2025","Frankford, Eduard; Antensteiner, Tobias; Vierhauser, Michael; Sauerwein, Clemens; Wallner, Vivien; Groher, Iris; Plösch, Reinhold; Breu, Ruth","A Survey on Feedback Types in Automated Programming Assessment Systems","ACM Trans. Comput. Educ.","","","10.1145/3773911","https://doi.org/10.1145/3773911","With the recent rapid increase in digitization across all major industries, acquiring programming skills has increased the demand for introductory programming courses. This has further resulted in universities integrating programming courses into a wide range of curricula, including not only technical studies but also business and management fields of study. Consequently, additional resources are needed for teaching, grading, and tutoring students with diverse educational backgrounds and skills. As part of this, Automated Programming Assessment Systems (APASs) have emerged, providing scalable and high-quality assessment systems with efficient evaluation and instant feedback. Commonly, APASs heavily rely on predefined unit tests for generating feedback, often limiting the scope and level of detail of feedback that can be provided to students. With the rise of Large Language Models (LLMs) in recent years, new opportunities have emerged as these technologies can enhance feedback quality and personalization. To investigate how different feedback mechanisms in APASs are perceived by students, and how effective they are in supporting problem-solving, we have conducted a large-scale study with over 200 students from two different universities. Specifically, we compare baseline Compiler Feedback, standard Unit Test Feedback, and advanced LLM-based Feedback regarding perceived quality and impact on student performance. Results indicate that while students rate unit test feedback as the most helpful, AI-generated feedback leads to significantly better performances. These findings suggest combining unit tests and AI-driven guidance to optimize automated feedback mechanisms and improve learning outcomes in programming education.","2025-10","2025-11-25 22:30:00","2025-11-25 22:30:00","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","ChatGPT; Large Language Models; GenAI; AI Feedback; Automated Programming Assessment Systems; Programming Feedback; Unit Test Feedback","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y7SGG5P3","conferencePaper","2025","Sinha, Yashaswi; Shanmugam, Shravan; Sahu, Yash Kumar; Mukhopadhyay, Abhishek; Biswas, Pradipta","Diffuse Your Data Blues: Augmenting Low-Resource Datasets via User-Assisted Diffusion","Proceedings of the 30th International Conference on Intelligent User Interfaces","979-8-4007-1306-4","","10.1145/3708359.3712163","https://doi.org/10.1145/3708359.3712163","Mixed reality applications in industrial contexts necessitate extensive and varied datasets for training object detection models, yet actual data gathering may be obstructed by logistical or cost issues. This study investigates the implementation of generative AI methods to work on this issue for mixed reality applications, with an emphasis on assembly and disassembly tasks. The novel objects found in industrial settings are difficult to describe using words, making text-based models less effective. In this study, a diffusion model is used to generate images by combining novel objects with various backgrounds. The backgrounds are selected where object detection in specific applications has been ineffective. This approach efficiently produces a diverse range of training samples. We compare three approaches: traditional augmentation methods, GAN-based augmentation, and Diffusion-based augmentation. Results show that the diffusion model significantly improved detection metrics. For instance, applying diffusion models to the dataset containing mechanical components of a pneumatic cylinder raised the F1 Score from 69.77 to 84.21 and the mAP@50 from 76.48 to 88.77, resulting in an (11%) increase in object detection performance, with a 67% less dataset size compared to the traditional augmented dataset. The proposed image composition diffusion model and user-friendly interface further simplify dataset enrichment, proving effective for augmenting data and improving the robustness of detection models.","2025","2025-11-25 22:30:01","2025-11-25 22:30:01","","538–552","","","","","","","IUI '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Diffusion Models; Efficient Augmentation; Image Composition; Object Detection; Synthetic Dataset","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q37NAX7T","conferencePaper","2023","Mondal, Rajdeep; Tang, Alan; Beckett, Ryan; Millstein, Todd; Varghese, George","What do LLMs need to Synthesize Correct Router Configurations?","Proceedings of the 22nd ACM Workshop on Hot Topics in Networks","979-8-4007-0415-4","","10.1145/3626111.3628194","https://doi.org/10.1145/3626111.3628194","We investigate whether Large Language Models (e.g., GPT-4) can synthesize correct router configurations with reduced manual effort. We find GPT-4 works very badly by itself, producing promising draft configurations but with egregious errors in topology, syntax, and semantics. Our strategy, that we call Verified Prompt Programming, is to combine GPT-4 with verifiers, and use localized feedback from the verifier to automatically correct errors. Verification requires a specification and actionable localized feedback to be effective. We show results for two use cases: translating from Cisco to Juniper configurations on a single router, and implementing a no-transit policy on multiple routers. While human input is still required, if we define the leverage as the number of automated prompts to the number of human prompts, our experiments show a leverage of 10X for Juniper translation, and 6X for implementing the no-transit policy, ending with verified configurations.","2023","2025-11-25 22:30:01","2025-11-25 22:30:01","","189–195","","","","","","","HotNets '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Cambridge, MA, USA","","","","large language models (LLMs); CoSynth; network verification and synthesis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PP4FFT22","conferencePaper","2024","Rivera, Juan-Pablo; Mukobi, Gabriel; Reuel, Anka; Lamparth, Max; Smith, Chandler; Schneider, Jacquelyn","Escalation Risks from Language Models in Military and Diplomatic Decision-Making","Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency","979-8-4007-0450-5","","10.1145/3630106.3658942","https://doi.org/10.1145/3630106.3658942","Governments are increasingly considering integrating autonomous AI agents in high-stakes military and foreign-policy decision-making, especially with the emergence of advanced generative AI models like GPT-4. Our work aims to scrutinize the behavior of multiple AI agents in simulated wargames, specifically focusing on their predilection to take escalatory actions that may exacerbate multilateral conflicts. Drawing on political science and international relations literature about escalation dynamics, we design a novel wargame simulation and scoring framework to assess the escalation risks of actions taken by these agents in different scenarios. Contrary to prior studies, our research provides both qualitative and quantitative insights and focuses on large language models (LLMs). We find that all five studied off-the-shelf LLMs show forms of escalation and difficult-to-predict escalation patterns. We observe that models tend to develop arms-race dynamics, leading to greater conflict, and in rare cases, even to the deployment of nuclear weapons. Qualitatively, we also collect the models’ reported reasoning for chosen actions and observe worrying justifications based on deterrence and first-strike tactics. Given the high stakes of military and foreign-policy contexts, we recommend further examination and cautious consideration before deploying autonomous language model agents for strategic military or diplomatic decision-making.","2024","2025-11-25 22:30:01","2025-11-25 22:30:01","","836–898","","","","","","","FAccT '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Rio de Janeiro, Brazil","","","","Natural Language Processing; Evaluation; Language Model Agents; Military Applications; Multi-Agent Security; Safety; Socio-Technical Impact","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JDQVBBI3","conferencePaper","2025","Lin, Zhihao; Zhou, Mingyi; Ma, Wei; Chen, Chi; Yang, Yun; Wang, Jun; Hu, Chunming; Li, Li","HapRepair: Learn to Repair OpenHarmony Apps","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","10.1145/3696630.3728556","https://doi.org/10.1145/3696630.3728556","Software defect detection and repair are essential software engineering tasks that mitigate potential risks in the early development stages. Large Language Models (LLMs) have demonstrated significant capabilities in software defect detection and repair. However, it is hard for LLMs to handle the new programming languages such as ArkTS (which is predominantly used in the OpenHarmony platform) due to training data shortage. Additionally, LLM-based multi-defect repair suffers from the limitation of the context window of LLMs. These issues significantly affect the performance of LLM-based defect repair in new programming languages. To address the above challenges, we propose HapRepair, a defect repair framework that integrates static analysis tools with retrieval-augmented generation (RAG) to improve the effectiveness of the defect repair. Specifically, we integrate CodeLinter into our iterative defect repair framework for defect detection, which is the basis of defect repair, and utilize RAG together with ArkAnalyzer to improve the quality of our repair solutions. To overcome the context window limitation of LLMs, we propose the Surrounding Context Extractor and the Context Combination Tool. Experiment results show that HapRepair effectively repairs defects in OpenHarmony Apps, demonstrating high reliability and efficiency in addressing code issues, achieving a defect repair rate of about 99% on the test set, compared to only about 37% when directly using LLMs for defect repair based on the defect information. Our approach demonstrates a robust solution for defect repair on new programming languages that have limited code corpus.","2025","2025-11-25 22:30:01","2025-11-25 22:30:01","","319–330","","","","","","","FSE Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WMH8NCTX","conferencePaper","2024","Hussain, Aftab; Rabin, Md Rafiqul Islam; Alipour, Mohammad Amin","Measuring Impacts of Poisoning on Model Parameters and Embeddings for Large Language Models of Code","Proceedings of the 1st ACM International Conference on AI-Powered Software","979-8-4007-0685-1","","10.1145/3664646.3664764","https://doi.org/10.1145/3664646.3664764","Large language models (LLMs) have revolutionized software development practices, yet concerns about their safety have arisen, particularly regarding hidden backdoors, aka trojans. Backdoor attacks involve the insertion of triggers into training data, allowing attackers to manipulate the behavior of the model maliciously. In this paper, we focus on analyzing the model parameters to detect potential backdoor signals in code models. Specifically, we examine attention weights and biases, and context embeddings of the clean and poisoned CodeBERT and CodeT5 models. Our results suggest noticeable patterns in context embeddings of poisoned samples for both the poisoned models; however, attention weights and biases do not show any significant differences. This work contributes to ongoing efforts in white-box detection of backdoor signals in LLMs of code through the analysis of parameters and embeddings.","2024","2025-11-25 22:30:01","2025-11-25 22:30:01","","59–64","","","","","","","AIware 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","LLMs of Code; Safe AI; Trojan Detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CJC96M85","conferencePaper","2024","Gómez-Abajo, Pablo; Pérez-Soler, Sara; Cañizares, Pablo C.; Guerra, Esther; de Lara, Juan","Mutation Testing for Task-Oriented Chatbots","Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering","979-8-4007-1701-7","","10.1145/3661167.3661220","https://doi.org/10.1145/3661167.3661220","Conversational agents, or chatbots, are increasingly used to access all sorts of services using natural language. While open-domain chatbots – like ChatGPT – can converse on any topic, task-oriented chatbots – the focus of this paper – are designed for specific tasks, like booking a flight, obtaining customer support, or setting an appointment. Like any other software, task-oriented chatbots need to be properly tested, usually by defining and executing test scenarios (i.e., sequences of user-chatbot interactions). However, there is currently a lack of methods to quantify the completeness and strength of such test scenarios, which can lead to low-quality tests, and hence to buggy chatbots. To fill this gap, we propose adapting mutation testing (MuT) for task-oriented chatbots. To this end, we introduce a set of mutation operators that emulate faults in chatbot designs, an architecture that enables MuT on chatbots built using heterogeneous technologies, and a practical realisation as an Eclipse plugin. Moreover, we evaluate the applicability, effectiveness and efficiency of our approach on open-source chatbots, with promising results.","2024","2025-11-25 22:30:01","2025-11-25 22:30:01","","232–241","","","","","","","EASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Salerno, Italy","","","","Mutation testing; Botium; Dialogflow; Rasa; Task-oriented chatbots","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UNH3UKV4","journalArticle","2023","Milic-Frayling, Natasa","On the Cusp: Computing Thrills and Perils and Professional Awakening","Proc. VLDB Endow.","","2150-8097","10.14778/3611540.3611640","https://doi.org/10.14778/3611540.3611640","Over the past eight decades, computer science has advanced as a field, and the computing profession has matured by establishing professional codes of conduct, fostering best practices, and establishing industry standards to support the proliferation of technologies and services. Research and applications of digital computation continue to change all aspects of human endeavor through new waves of innovation. While it is clear that different research advances fuel innovation, the ways they come together to make an impact vary. In contrast to highly regulated sectors such as pharma, medicine and law, the process of transforming research into widely deployed technologies is not regulated. We reflect on collective practices, from discovery by scientists and engineers to market delivery by entrepreneurs, industry leaders, and practitioners. We consider ecosystem changes that are required to sustain the transformational effects of new technologies and enable new practices to take root. Every such transformation ruptures in the existing socio-technical fabric and requires a concerted effort to remedy this through effective policies and regulations. Computing experts are involved in all phases and must match the transformational power of their innovation with the highest standard of professional conduct. We highlight the principles of responsible innovation and discuss three waves of digital innovation. We use wide and uncontrolled generative AI deployments to illustrate risks from the implosion of digital media due to contamination of digital records, removal of human agency, and risk to an individual's personhood.","2023-08","2025-11-25 22:30:01","2025-11-25 22:30:01","","4152–4159","","12","16","","","","","","","","","","","","","","","","","Publisher: VLDB Endowment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NGXAZDKK","journalArticle","2025","Shariffdeen, Ridwan; Timperley, Christopher S.; Noller, Yannic; Le Goues, Claire; Roychoudhury, Abhik","Vulnerability Repair via Concolic Execution and Code Mutations","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3707454","https://doi.org/10.1145/3707454","Security vulnerabilities detected via techniques like greybox fuzzing are often fixed with a significant time lag. This increases the exposure of the software to vulnerabilities. Automated fixing of vulnerabilities where a tool can generate fix suggestions is thus of value. In this work, we present such a tool, called CrashRepair, to automatically generate fix suggestions using concolic execution, specification inference, and search techniques. Our approach avoids generating fix suggestions merely at the crash location because such fixes often disable the manifestation of the error instead of fixing the error. Instead, based on sanitizer-guided concolic execution, we infer desired constraints at specific program locations and then opportunistically search for code mutations that help respect those constraints. Our technique only requires a single detected vulnerability or exploit as input; it does not require any user-provided properties. Evaluation results on a wide variety of CVEs in the VulnLoc benchmark, show CrashRepair achieves greater efficacy than state-of-the-art vulnerability repair tools like Senx. The repairs suggested come in the form of a ranked set of patches at different locations, and we show that on most occasions, the desired fix is among the top-3 fixes reported by CrashRepair.","2025-04","2025-11-25 22:30:01","2025-11-25 22:30:01","","","","4","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Automated Program Repair; Vulnerability Repair; Concolic Execution; Semantic Program Analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"APVKMDZD","conferencePaper","2024","Schaef, Martin; Cirisci, Berk; Luo, Linghui; Mansur, Muhammad Numair; Tripp, Omer; Sanchez, Daniel; Zhou, Qiang; Zafar, Muhammad Bilal","Understanding Developer-Analyzer Interactions in Code Reviews","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695257","https://doi.org/10.1145/3691620.3695257","Static code analyzers are now a common part of the codereview process. These automated tools integrate into the code review process by commenting on code changes and suggesting improvements, in the same way as human reviewers. The comments made by static analyzers often trigger a conversation between developers to align on if and how the issue should be fixed. Because developers rarely give feedback directly to the tool, understanding the sentiment and intent in the conversation triggered by the tool comments can be used to measure the usefulness of the static analyzer.In this paper, we report on an experiment where we use large language models to automatically label and categorize the sentiment and intent of such conversations triggered by static analyzer comments. Our experiment demonstrates that LLMs not only classify and interpret complex developer-analyzer conversations, but can be more accurate than human experts.","2024","2025-11-25 22:30:01","2025-11-25 22:30:01","","1945–1955","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F9I7ANGF","conferencePaper","2023","Morbidoni, Christian","Poster: LLMs for online customer reviews analysis: oracles or tools? Experiments with GPT 3.5","Proceedings of the 15th Biannual Conference of the Italian SIGCHI Chapter","979-8-4007-0806-0","","10.1145/3605390.3610810","https://doi.org/10.1145/3605390.3610810","Generative Large Language Models, pre-trained on a huge amount of human authored text, are showing emergent capabilities in understanding and accomplishing a variety of NLP and text comprehension tasks. Recently, interest is growing in understanding to what extent LLMs can support humans, or even replace them, in accomplishing non trivial data analysis tasks. In this paper, we explore OpenAI’s GPT capabilities in online customer review analysis, a multi-step analysis activity which typically involves both human knowledge and predictive data analysis techniques (e.g. topic extraction, aspect-based sentiment analysis). We explore different interaction modalities where the LLM covers all or part of the analysis process, and provide a preliminary evaluation against human annotation outcomes.","2023","2025-11-25 22:30:01","2025-11-25 22:30:01","","","","","","","","","CHItaly '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Torino, Italy","","","","Large Language Models; GPT; NLP; zero-shot; Online customer reviews analysis; topic-level sentiment analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3RNJGN7T","journalArticle","2025","Ye, He; Yang, Aidan Z.H.; Hu, Chang; Wang, Yanlin; Zhang, Tao; Le Goues, Claire","AdverIntent-Agent: Adversarial Reasoning for Repair Based on Inferred Program Intent","Proc. ACM Softw. Eng.","","","10.1145/3728939","https://doi.org/10.1145/3728939","Automated program repair (APR) has shown promising results, particularly with the use of neural networks. Currently, most APR tools focus on code transformations specified by test suites, rather than reasoning about the program’s intent and the high-level bug specification. Without a proper understanding of program intent, these tools tend to generate patches that overfit incomplete test suites and fail to reflect the developer’s intentions. However, reasoning about program intent is challenging. In our work, we propose an approach called AdverIntent-Agent, based on critique and adversarial reasoning. Our approach is novel to shift the focus from generating multiple APR patches to inferring multiple potential program intents. Ideally, we aim to infer intents that are, to some extent, adversarial to each other, maximizing the probability that at least one aligns closely with the developer’s original intent. AdverIntent-Agent is a multi-agent approach consisting of three agents: a reasoning agent, a test agent, and a repair agent. First, the reasoning agent generates adversarial program intents along with the corresponding faulty statements. Next, the test agent produces adversarial test cases that align with each inferred intent, constructing oracles that use the same inputs but have different expected outputs. Finally, the repair agent uses dynamic and precise LLM prompts to generate patches that satisfy both the inferred program intent and the generated tests. AdverIntent-Agent was evaluated on two benchmarks: Defects4J 2.0 and HumanEval-Java. AdverIntentAgent correctly repaired 77 and 105 bugs in both benchmarks, respectively. Our work helps reduce the effort required to review patches by enabling developers to assess program intent in natural language, rather than reviewing code patches.","2025-06","2025-11-25 22:30:01","2025-11-25 22:30:01","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; Program Repair","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IK5SHIMW","bookSection","2025","Liu, Changshu; Jabbarvand, Reyhan","A Tool for In-depth Analysis of Code Execution Reasoning of Large Language Models","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","","https://doi.org/10.1145/3696630.3728605","Code Executing Reasoning is becoming a new non-functional metric that assesses the ability of large language models (LLMs) in programming tasks. State-of-the-art frameworks and benchmarks (CruxEval) usually focus on LLM's prediction of a given code's input/output or intermediate variable states/values on limited programs. However, there is no tool for more in-depth analysis of the results. Without such a tool, the observations about LLM's code execution reasoning cannot be generalized to more datasets, preventing the research community and practitioners from devising the next generation of LLMs with better code execution reasoning abilities. This paper introduces ExeRScope, a series of tools and heuristics to analyze the result of code execution reasoning frameworks to understand the impact of code properties in the studied benchmarks. With such tooling, analysis can be generalized to code with similar properties without the urgent need to design more benchmarks, which is a cumbersome effort.The implementation of ExeRScope is publicly available, and it currently assesses the impact of different (1) program constructs, (2) program complexities, (3) dynamic programming properties such as recursion length, and (4) variable types on code execution reasoning abilities of LLMs. Evaluation of ExeRScope on four programming benchmarks (1450 Python programs) and six LLMs from three existing code execution reasoning techniques shows its effectiveness and practicality in providing important insights, highlighting the strengths and limitations of LLMs concerning code execution reasoning. We released our artifact and demo in [36].","2025","2025-11-25 22:30:01","2025-11-25 22:30:01","","1178–1182","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XTIFE6XH","journalArticle","2024","Ngo, Bao; Formato, Jack; May, James J.; Ho, Nguyen; Bui, Hoang; Ngo, Linh B.","FACE: A Framework for AI-Driven Coding Generation Evaluation","J. Comput. Sci. Coll.","","1937-4771","","","Previous work on evaluation code generation solutions is limited to static test cases due to difficulty in manual acquisition of test data. This paper presents a framework that enables the automated study of various code generation solutions using the entirety of an online competitive programming platform. To evaluate the capability of this framework, we exhaustively tested solutions generated from ChatGPT and Gemini for all programming questions on this platform. The resulting statistical and textual analysis highlights the difference between these two platforms and demonstrates the contribution of this framework in enabling researchers to collect and analyze a massive amount of data.","2024-10","2025-11-25 22:30:01","2025-11-25 22:30:01","","263–276","","3","40","","","","","","","","","","","","","","","","","Place: Evansville, IN, USA Publisher: Consortium for Computing Sciences in Colleges","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SWLCFLF5","journalArticle","2025","Rahman, Shanto; Dutta, Saikat; Shi, August","Understanding and Improving Flaky Test Classification","Proc. ACM Program. Lang.","","","10.1145/3763098","https://doi.org/10.1145/3763098","Regression testing is an essential part of software development, but it suffers from the presence of flaky tests – tests that pass and fail non-deterministically when run on the same code. These unpredictable failures waste developers’ time and often hide real bugs. Prior work showed that fine-tuned large language models (LLMs) can classify flaky tests into different categories with very high accuracy. However, we find that prior approaches over-estimated the accuracy of the models due to incorrect experimental design and unrealistic datasets – making the flaky test classification problem seem simpler than it is. In this paper, we first show how prior flaky test classifiers over-estimate the prediction accuracy due to 1) flawed experiment design and 2) mis-representation of the real distribution of flaky (and non-flaky) tests in their datasets. After we fix the experimental design and construct a more realistic dataset (which we name FlakeBench), the prior state-of-the-art model shows a steep drop in F1-score, from 81.82% down to 56.62%. Motivated by these observations, we develop a new training strategy to fine-tune a flaky test classifier, FlakyLens, that improves the classification F1-score to 65.79% (9.17pp higher than the state-of-the-art). We also compare FlakyLens against recent pre-trained LLMs, such as CodeLlama and DeepSeekCoder, on the same classification task. Our results show that FlakyLens consistently outperforms these models, highlighting that general-purpose LLMs still fall short on this specialized task. Using our improved flaky test classifier, we identify the important tokens in the test code that influence the models in making correct or incorrect predictions. By leveraging attribution scores computed per code token in each test, we investigate the tokens that have higher impact on the flaky test classifier’s decision-making per flaky test category. To assess the influence of these important tokens, we introduce adversarial perturbation using these important tokens into the tests and observe whether the model’s predictions change. Our findings show that, when introducing perturbations using the most important tokens, the classification accuracy can change by as much as -18.37pp. These results highlight that these models still struggle to generalize beyond their training data and rely on identifying category-specific tokens (instead of understanding their semantic context), calling for further research into more robust training methodologies.","2025-10","2025-11-25 22:30:01","2025-11-25 22:30:01","","","","OOPSLA2","9","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","large language models; flaky test classification; flaky tests","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M2LDE3WG","journalArticle","2025","Autili, Marco; De Sanctis, Martina; Inverardi, Paola; Pelliccione, Patrizio","Engineering Digital Systems for Humanity: A Research Roadmap","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3712006","https://doi.org/10.1145/3712006","As testified by new regulations like the European AI Act, worries about the human and societal impact of (autonomous) software technologies are becoming of public concern. Human, societal, and environmental values, alongside traditional software quality, are increasingly recognized as essential for sustainability and long-term well-being. Traditionally, systems are engineered taking into account business goals and technology drivers. Considering the growing awareness in the community, in this article, we argue that engineering of systems should also consider human, societal, and environmental drivers. Then, we identify the macro and technological challenges by focusing on humans and their role while co-existing with digital systems. The first challenge considers humans in a proactive role when interacting with digital systems, i.e., taking initiative in making things happen instead of reacting to events. The second concerns humans having a reactive role in interacting with digital systems, i.e., humans interacting with digital systems as a reaction to events. The third challenge focuses on humans with a passive role, i.e., they experience, enjoy or even suffer the decisions and/or actions of digital systems. The fourth challenge concerns the duality of trust and trustworthiness, with humans playing any role. Building on the new human, societal, and environmental drivers and the macro and technological challenges, we identify a research roadmap of digital systems for humanity. The research roadmap is concretized in a number of research directions organized into four groups: development process, requirements engineering, software architecture and design, and verification and validation.","2025-05","2025-11-25 22:30:01","2025-11-25 22:30:01","","","","5","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Software engineering; Environmental values; Human values; Research directions; Research roadmap; Societal values","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CGZN36HS","conferencePaper","2025","Zhou, Yi; Shan, Zhenyu; Dai, Zeyao; Zhang, Zhao","CodeEduBench: A Multidimensional Benchmark for Evaluating AI-Driven Code Generation Models in C Programming Education","Proceedings of the 2025 International Conference on Artificial Intelligence and Educational Systems","979-8-4007-1506-8","","10.1145/3744367.3744402","https://doi.org/10.1145/3744367.3744402","Large language models (LLMs) are a core technology that can facilitate change in programming education. However, the educational applications and impact of LLMs have not yet been thoroughly evaluated. In this research, we develop the CodeEduBench three-dimensional evaluation framework, which assesses the applicability of LLMs for C programming instruction across three aspects: test case creation, code synthesis, and bug fixing. Five widely-used open-source models are chosen for comparative study. The results of this study show that Qwen2.5-coder-32b has excellent overall performance, although its shortcomings in code generation accuracy and error correction. In addition, this study proposes model selection strategies and optimization pathways to provide methodological support for the scientific application of LLM in programming education.","2025","2025-11-25 22:30:01","2025-11-25 22:30:01","","217–222","","","","","","","ICAIES '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","large language models; C language; multidimensional benchmark; programming education evaluation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EMFIKDCG","conferencePaper","2025","Fan, Stephen; Liao, Lizhi; Li, Zhenhao","LTB25: 13th International Workshop on Load Testing and Benchmarking of Software Systems","Companion of the 16th ACM/SPEC International Conference on Performance Engineering","979-8-4007-1130-5","","10.1145/3680256.3721301","https://doi.org/10.1145/3680256.3721301","It is our great pleasure to welcome you to the 13th edition of the International Workshop on Load Testing and Benchmarking of Software Systems - LTB 2025, (https://ltb2025.github.io). This workshop brings together software testing and software performance researchers, practitioners, and tool developers to discuss the challenges and opportunities of conducting research on load testing and benchmarking software systems, including theory, applications, and experiences. LTB 2025 includes 1 keynote talk and 4 research papers. The topics cover AIOps, performance and load testing, workload tracing, benchmarking, and performance verification.","2025","2025-11-25 22:30:01","2025-11-25 22:30:01","","121","","","","","","","ICPE '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Toronto ON, Canada","","","","benchmark; load testing; ltb2025; software performance","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EKQL9EWF","conferencePaper","2024","Khajezade, Mohamad; Wu, Jie JW; Fard, Fatemeh Hendijani; Rodriguez-Perez, Gema; Shehata, Mohamed Sami","Investigating the Efficacy of Large Language Models for Code Clone Detection","Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension","979-8-4007-0586-1","","10.1145/3643916.3645030","https://doi.org/10.1145/3643916.3645030","Large Language Models (LLMs) have demonstrated remarkable success in various natural language processing and software engineering tasks, such as code generation. The LLMs are mainly utilized in the prompt-based zero/few-shot paradigm to guide the model in accomplishing the task. GPT-based models are one of the popular ones studied for tasks such as code comment generation or test generation. These tasks are 'generative' tasks. However, there is limited research on the usage of LLMs for 'non-generative' tasks such as classification using the prompt-based paradigm. In this preliminary exploratory study, we investigated the applicability of LLMs for Code Clone Detection (CCD), a non-generative task. By building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we first investigated two different prompts using ChatGPT to detect Type-4 code clones in Java-Java and Java-Ruby pairs in a zero-shot setting. We then conducted an analysis to understand the strengths and weaknesses of ChatGPT in CCD. ChatGPT surpasses the baselines in cross-language CCD attaining an F1-score of 0.877 and achieves comparable performance to fully fine-tuned models for mono-lingual CCD, with an F1-score of 0.878. Also, the prompt and the difficulty level of the problems has an impact on the performance of ChatGPT. Finally, we provide insights and future directions based on our initial analysis1.","2024","2025-11-25 22:30:01","2025-11-25 22:47:28","","161–165","","","","","","","ICPC '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","Test pattern generators; Chatbots; Codes; large language models; Large Language Models; Buildings; code clone detection; few-shot learning; zero-shot learning; Data models; Few-shot Learning; Charge coupled devices; Cloning; Code Clone Detection; Zero-shot Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"69LD3R23","journalArticle","2025","Le-Cong, Thanh; Nguyen, Dat; Le, Bach; Murray, Toby","Towards Reliable Evaluation of Neural Program Repair with Natural Robustness Testing","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3716167","https://doi.org/10.1145/3716167","Automated program repair (APR) has recently gained ground, with numerous research efforts being conducted in the area that have been adopted in the industry. One notable class of APR is neural program repair (NPR), which typically employs deep learning techniques that are trained on vast amounts of historical data to fix bugs that have not been seen in the past. To study the true effectiveness of NPR on existing limited datasets, recent work augments the evaluation data by employing semantics-preserving transformations to convert original buggy programs to semantically equivalent ones. Experiments show that NPR techniques are not robust; e.g., NPR cannot repair semantically equivalent counterparts of 20%–35% of bugs that they can repair in the original dataset. However, we found that many of these transformations are unnatural, that are unlikely to occur in real-world scenarios, leading to misleading conclusions about NPR effectiveness and misguide the improvement on unrobust behaviors, which have minimal real-world impact.In this article, we propose shifting the focus of robustness evaluation for NPR techniques towards naturally occurring data transformations. To accomplish this, we first examine the naturalness of semantic-preserving transformations through a two-stage human study. This study includes: (i) interviews with senior software developers to establish concrete criteria for evaluating the naturalness of these transformations and (ii) a survey involving 10 developers to assess the naturalness of 1,178 transformations, i.e., pairs of original and transformed programs, applied to 225 real-world bugs. Our findings show that only 60% of these transformations are considered natural, while 20% are considered unnatural, with strong agreement among the annotators. Moreover, the unnaturalness of these transformations significantly impacts both their applicability to benchmarks and the conclusions drawn from robustness testing.Next, we conduct natural robustness tests on NPR techniques to assess their true effectiveness against real-world data variations. Our experimental results reveal a substantial number of prediction changes in NPR techniques, leading to significant reductions in both plausible and correct patch rates when comparing performance on the original and transformed datasets. Furthermore, we observe notable differences in performance improvements between NPR techniques, suggesting potential biases in the evaluation of NPR introduced by limited datasets. Finally, we explore automating the assessment of transformation naturalness by developing a new naturalness metric, namely RNC, using large language models. This metric effectively evaluates naturalness with an AUC of 0.7, offering a promising direction for automating the naturalness assessment of code transformations.","2025-08","2025-11-25 22:30:01","2025-11-25 22:30:01","","","","7","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Automated Program Repair; Code Naturalness; Code Transformations; Natural Robustness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M37X9M64","conferencePaper","2025","Peng, Yiteng; Xiao, Dongwei; Liu, Zhibo; Ji, Zhenlan; Wu, Daoyuan; Wang, Shuai; Rahmel, Juergen","The Phantom Menace in Crypto-Based PET-Hardened Deep Learning Models: Invisible Configuration-Induced Attacks","Proceedings of the 2025 ACM SIGSAC Conference on Computer and Communications Security","979-8-4007-1525-9","","10.1145/3719027.3765107","https://doi.org/10.1145/3719027.3765107","The increasing use of deep learning (DL) models has given rise to significant privacy concerns regarding training and inference data. To address these concerns, the community has increasingly adopted crypto-based privacy-enhancing technologies (CPET) like homomorphic encryption (HE), secure multi-party computation (MPC), and zero-knowledge proofs (ZKP). The integration of CPET with DL, often referred to as CPET-DL, is commonly facilitated by specialized frameworks like CrypTen, TenSEAL, and EZKL. These frameworks offer configurable parameters to balance model accuracy and computational efficiency during privacy-preserving operations. However, these configurations, while seemingly harmless, can introduce subtle vulnerabilities. The stealthy attacks induced by misconfigurations are hard to detect because 1) the plaintext models remain vulnerability-free, and 2) existing auditing tools are hardly applicable to CPET-hardened models. This creates a paradox: tools intended to protect privacy can be undermined through configuration manipulation. We present ConPETro, the first attack on CPET-hardened models by manipulating the CPET-DL framework configurations. We show that well-crafted configurations allow attackers to create CPET-hardened models that function similarly to benign plaintext models under normal inputs, but exhibit significantly reduced robustness for malicious inputs embedded with triggers. ConPETro strategically selects triggers to maximize behavioral deviations with benign models and uses gradient consistency to guide configuration exploration, effectively finding malicious configurations that bypass standard plaintext model auditing. Evaluations across three mainstream CPET-DL frameworks (HE, MPC, and ZKP) demonstrate ConPETro's effectiveness in both semantic and non-semantic triggers. ConPETro achieves an average maximum attack success rate (ASR) of 72.27% in CPET-hardened models with non-semantic triggers; the accuracy only drops by 4%, thus maintaining stealthiness. It also achieves a maximum ASR of 94.74% with semantic triggers across three datasets. We also demonstrate that our stealthy attacks can bypass advanced defense and detection tools.","2025","2025-11-25 22:30:01","2025-11-25 22:30:01","","4379–4393","","","","","","","CCS '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Taipei, Taiwan","","","","deep learning; configuration attacks; model auditing; privacy-preserving protocols; security testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T6BMNKBE","bookSection","2025","Rinard, Martin C.","Research in Program Repair and Approximate Computing: A Retrospective","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00257","This paper and accompanying talk trace the trajectory of my research in program repair and approximate computing. The prevailing value system in the field at the time focused on program correctness as a fundamental goal. This research, in contrast, was driven by a new perspective that emphasized acceptable (but not necessarily fully correct) survival through errors and the automatic identification and exploitation of performance versus accuracy tradeoff spaces implicitly present in computations coded to operate at only a single point in this space.Because the research challenged the prevailing value system at the time, it met with some skepticism despite empirical results highlighting its effectiveness. The following quote from an anonymous reviewer may give some idea of the reaction:""The basic idea—to assist incorrect programs in their efforts to emit incorrect output—is an abomination and if adopted would likely usher in a new dark age.""As the research progressed, we gained a deeper understanding of the reasons behind the surprising — at least to us — phenomena we observed. We were able to formalize this understanding to generate source code patches and obtain performance, accuracy, and acceptability guarantees for computations that leveraged our techniques, bringing the research full circle to once again focus on reasoning statically about program behavior but with different reasoning techniques and guarantees.Finally, I discuss lessons learned and future relevance of the principles, perspectives, and concepts that this research pioneered.","2025","2025-11-25 22:30:01","2025-11-25 22:30:01","","1–15","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ALBWCFAH","bookSection","2025","Shen, Qingchao; Tian, Yongqiang; Ma, Haoyang; Chen, Junjie; Huang, Lili; Fu, Ruifeng; Cheung, Shing-Chi; Wang, Zan","A Tale of Two DL Cities: When Library Tests Meet Compiler","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00025","Deep Learning (DL) compilers typically load a DL model and optimize it with intermediate representation. Existing DL compiler testing techniques mainly focus on model optimization stages, but rarely explore bug detection at the model loading stage. Effectively testing the model loading stage requires covering diverse usages of each DL operator from various DL libraries, which shares a common objective with DL library testing, indicating that the embedded knowledge in DL library tests is beneficial for testing the model loading stage of DL compilers. With this idea, we propose Opera to migrate the knowledge embedded in DL library tests to test the model loading stage. Opera constructs diverse tests from various tests for DL libraries (including the tests documented in DL libraries and those generated by recent fuzzers). In total, we considered three sources of tests in DL libraries for migration. In addition, it incorporates a diversity-based test prioritization strategy to migrate and execute those tests that are more likely to detect diverse bugs earlier. We then used eight frontends from three DL compilers (e.g., TVM, TensorRT, and OpenVINO) for evaluation. Opera detected 170 previously unknown bugs in total, 90 of which have been confirmed/fixed by developers, demonstrating the effectiveness of such the migration-based idea. The test prioritization strategy in Opera improves testing efficiency with migrated tests by 11.9% 47.4% on average compared to general test prioritization strategies.","2025","2025-11-25 22:30:01","2025-11-25 22:30:01","","2201–2212","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R5ILTBQV","conferencePaper","2025","Ahmed, Umair Z.; Sahai, Shubham; Leong, Ben; Karkare, Amey","Feasibility Study of Augmenting Teaching Assistants with AI for CS1 Programming Feedback","Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1","979-8-4007-0531-1","","10.1145/3641554.3701972","https://doi.org/10.1145/3641554.3701972","With the increasing adoption of Large Language Models (LLMs), there are proposals to replace human Teaching Assistants (TAs) with LLM-based AI agents for providing feedback to students. In this paper, we explore a new hybrid model where human TAs receive AI-generated feedback for CS1 programming exercises, which they can then review and modify as needed. We conducted a large-scale randomized intervention with 185 CS1 undergraduate students, comparing the efficacy of this hybrid approach against manual feedback and direct AI-generated feedback.Our initial hypothesis predicted that AI-augmented feedback would improve TA efficiency and increase the accuracy of guidance to students. However, our findings revealed mixed results. Although students perceived improvements in feedback quality, the hybrid model did not consistently translate to better student performance. We also observed complacency among some TAs who over-relied on LLM generated feedback and failed to identify and correct inaccuracies. These results suggest that augmenting human tutors with AI may not always result in improved teaching outcomes, and further research is needed to ensure it is truly effective.","2025","2025-11-25 22:30:01","2025-11-25 22:30:01","","11–17","","","","","","","SIGCSETS 2025","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Pittsburgh, PA, USA","","","","llm; cs1; gpt; programming; hint; randomized trial; ta","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7QCGBXAN","journalArticle","2025","Cao, Shaoheng; Pan, Minxue; Lan, Yuanhong; Li, Xuandong","Intention-Based GUI Test Migration for Mobile Apps using Large Language Models","Proc. ACM Softw. Eng.","","","10.1145/3728978","https://doi.org/10.1145/3728978","Graphical User Interface (GUI) testing is one of the primary quality assurance methods for mobile apps. Manually constructing high-quality test cases for GUI testing is costly and labor-intensive, leading to the development of various automated approaches that migrate test cases from a source app to a target app. Existing approaches predominantly treat this test migration task as a widget-matching problem, which performs well when the interaction logic between apps remains consistent. However, they struggle with variations in interaction logic for specific functionalities, a common scenario across different apps. To address this limitation, a novel approach named ITeM is introduced in this paper for the test migration task. Unlike existing works that model the problem as a widget-matching task, ITeM seeks a novel pathway by adopting a two-stage framework with the comprehension and reasoning capability of Large Language Models: first, a transition-aware mechanism for generating test intentions; and second, a dynamic reasoning-based mechanism for fulfilling these intentions. This approach maintains effectiveness regardless of variations across the source and target apps' interaction logic. Experimental results on 35 real-world Android apps across 280 test migration tasks demonstrate the superior effectiveness and efficiency of ITeM compared to state-of-the-art approaches.","2025-06","2025-11-25 22:30:01","2025-11-25 22:30:01","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Android Testing; GUI Test Migration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FZV7686D","book","2024","","MM '24: Proceedings of the 32nd ACM International Conference on Multimedia","","979-8-4007-0686-8","","","","We are delighted to welcome you to Melbourne, Australia for ACM Multimedia 2024, the 32nd ACM International Conference on Multimedia. ACM Multimedia is the premier international conference series in the area of multimedia within the field of computer science. Since 1993, ACM Multimedia has been bringing together worldwide researchers and practitioners from academia and industry to present their innovative research and to discuss recent advancements in multimedia.For the first time since the end of the COVID-19 pandemic, this year's conference returns to the Asia-Pacific region and resumes as a full-fledged, inperson event. With no travel restrictions or significant visa challenges, we are excited to once again experience the warmth of face-to-face gatherings, where we can reconnect with colleagues and friends.The enthusiasm and support from the community have been incredible. ACM Multimedia 2024 received over 4,300 main conference submissions, accepting more than 1,100 papers (please refer to the TPC Chairs' message for details). In addition, 10 Grand Challenges were selected from 22 submissions, 18 workshops from 30 submissions, and 8 tutorials from 13 proposals. We've prepared an exciting five-day program: workshops, grand challenges, and tutorials will be held on the 1st and 5th days, with the main conference occupying the middle three days. All accepted papers will be accessible online prior to the conference, and we are working to ensure proceedings are available through the ACM Digital Library around the conference period.This year's conference features three distinguished academic keynote speeches, several prestigious SIGMM award talks, a panel discussion on Generative AI in Multimedia, a refreshed Brave New Idea (BNI) session, and our inaugural industry program.The opening keynote will be delivered by Prof. Pascale Fung from HKUST, a Fellow of AAAI, ACL, and IEEE. Her talk will explore the pressing topic of Agents in the Large Language Model (LLM) Era. Prof. Judy Kay from the University of Sydney, a renowned expert in HCI, user modeling, and ubiquitous computing, will give the second keynote on how to empower individuals to harness and control their multimodal data. The final academic keynote will be presented by Prof. Jiebo Luo from the University of Rochester, a Fellow of ACM, AAAI, IEEE, SPIE, and IAPR, as well as a member of Academia Europaea and the US National Academy of Inventors. He will discuss leveraging LLMs as social multimedia analysis engines.This year, we continue using OpenReview to ensure an open and transparent review process. Thanks to the exceptional efforts of the technical program committee, every paper received at least three reviews before the review announcement. The BNI track has also revamped its review process to align with the main conference, promoting visionary papers. Additionally, we are excited to introduce the industry program to ACM Multimedia for the first time, featuring industry keynote speeches, expert talks, and demonstrations (please refer to the industry chairs' message for further details).We are also committed to making the conference inclusive and accessible. To support students with financial constraints, we have awarded travel grants to at least 25 students from the ACM Multimedia 2024 budget, with an additional 20+ students receiving SIGMM travel grants. Over 20 local students have also been recruited as volunteers, benefiting from complimentary registration. Furthermore, we have arranged childcare facilities to accommodate attendees with young children. A welcome reception will take place on the 2nd day of the conference, followed by a gala dinner on the 3rd day, featuring exciting cultural performances.We hope you find this year's program engaging and thought-provoking and that it offers valuable opportunities to exchange ideas with fellow researchers and practitioners from around the globe. We also encourage you to take time to explore the beautiful city of Melbourne and its surrounding regions.","2024","2025-11-25 22:30:01","2025-11-25 22:30:01","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SPEUBR74","conferencePaper","2024","Mechtaev, Sergey; Tan, Shin Hwei","F1X at APR-COMP 2024","Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair","979-8-4007-0577-9","","10.1145/3643788.3648018","https://doi.org/10.1145/3643788.3648018","Automated program repair aims to generate patches for buggy programs, a task often hindered by the cost of test executions in large projects. F1X introduces a novel methodology relying on test-equivalence relations, defining if two programs yield indistinguishable results for a specific test. By leveraging two test-equivalence relations based on runtime values and dependencies, F1X' algorithm categorises patches into test-equivalence classes, which helps to significantly reduce the number of required test execution to generate a patch without any information loss. Experiments on real-world programs from the ManyBugs benchmark demonstrated a substantial reduction in test executions, leading to efficiency gains over the previous methods, while retaining the patch quality. The efficiency and effectiveness of F1X was further shown in APR-COMP 2024, where it received the highest score in the Functional-C track.","2024","2025-11-25 22:30:02","2025-11-25 22:30:02","","56–57","","","","","","","APR '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YYH4NKRS","conferencePaper","2025","Challen, Geoffrey; Nordick, Ben","Accelerating Accurate Assignment Authoring Using Solution-Generated Autograders","Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1","979-8-4007-0531-1","","10.1145/3641554.3701862","https://doi.org/10.1145/3641554.3701862","Students learning to program benefit from access to large numbers of practice problems. Autograders are commonly used to support programming questions by providing quick feedback on submissions. But authoring accurate autograders remains challenging. Autograders are frequently created by enumerating test cases-a tedious process that can produce inaccurate autograders that fail to correctly classify submissions. When authoring accurate autograders is slow, it is difficult to create large banks of practice problems to support beginning programmers.We present solution-generated autograding: a faster, more accurate, and more enjoyable way to create autograders. Our approach leverages a key difference between software testing and autograding: The question author can provide a solution. By starting with a solution, we can eliminate the need to manually enumerate test cases, validate the autograder's accuracy, and evaluate other aspects of submission code quality beyond behavioral correctness. We describe Questioner, an implementation of solution-generated autograding for Java and Kotlin, and share experiences from four years using Questioner to support a large CS1 course: authoring nearly 800 programming questions used by thousands of students to evaluate millions of submissions.","2025","2025-11-25 22:30:02","2025-11-25 22:30:02","","227–233","","","","","","","SIGCSETS 2025","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Pittsburgh, PA, USA","","","","autograding; code quality evaluation; problem authoring","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7VM8Z5Y7","conferencePaper","2024","Zhang, Jian; Wang, Chong; Li, Anran; Wang, Wenhan; Li, Tianlin; Liu, Yang","VulAdvisor: Natural Language Suggestion Generation for Software Vulnerability Repair","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695555","https://doi.org/10.1145/3691620.3695555","Software vulnerabilities pose serious threats to the security of modern software systems. Deep Learning-based Automated Vulnerability Repair (AVR) has gained attention as a potential solution to accelerate the remediation of vulnerabilities. However, recent studies indicate that existing AVR approaches often only generate patches, which may not align with developers' current repair practices or expectations. In this paper, we introduce VulAdvisor, an automated approach that generates natural language suggestions to guide developers or AVR tools in repairing vulnerabilities. VulAdvisor comprises two main components: oracle extraction and suggestion learning. To address the challenge of limited historical data, we propose an oracle extraction method facilitating ChatGPT to construct a comprehensive and high-quality dataset. For suggestion learning, we take the supervised fine-tuning CodeT5 model as the basis, integrating local context into Multi-Head Attention and introducing a repair action loss, to improve the relevance and meaningfulness of the generated suggestions. Extensive experiments on a large-scale dataset from real-world C/C++ projects demonstrate the effectiveness of VulAdvisor, surpassing several alternatives in terms of both lexical and semantic metrics. Moreover, we show that the generated suggestions enhance the patch generation capabilities of existing AVR tools. Human evaluations further validate the quality and utility of VulAdvisor's suggestions, confirming their potential to improve software vulnerability repair practices.","2024","2025-11-25 22:30:02","2025-11-25 22:48:14","","1932–1944","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","Semantics; Software testing; Software engineering; Software systems; Chatbots; Source coding; large language models; program repair; Security; suggestion generation; vulnerability repair; Natural languages; Maintenance engineering; Software maintenance","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"38F4R4MA","conferencePaper","2023","Karakas, Umutcan; Tosun, Ayse","Automated Fairness Testing with Representative Sampling","Proceedings of the 19th International Conference on Predictive Models and Data Analytics in Software Engineering","979-8-4007-0375-1","","10.1145/3617555.3617871","https://doi.org/10.1145/3617555.3617871","The issue of fairness testing in machine learning models has become popular due to rising concerns about potential bias and discrimination, as these models continue to permeate end-user applications. However, achieving an accurate and reliable measurement of the fairness performance of machine learning models remains a substantial challenge. Representative sampling plays a pivotal role in ensuring accurate fairness assessments and providing insight into the underlying dynamics of data, unlike biased or random sampling approaches. In our study, we introduce our approach, namely RSFair, which adopts the representative sampling method to comprehensively evaluate the fairness performance of a trained machine learning model. Our research findings on two datasets indicate that RSFair yields more accurate and reliable results, thus improving the efficiency of subsequent search steps, and ultimately the fairness performance of the model. With the usage of Orthogonal Matching Pursuit (OMP) and K-Singular Value Decomposition (K-SVD) algorithms for representative sampling, RSFair significantly improves the detection of discriminatory inputs by 76% and the fairness performance by 53% compared to other search-based approaches in the literature.","2023","2025-11-25 22:30:02","2025-11-25 22:30:02","","54–63","","","","","","","PROMISE 2023","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: San Francisco, CA, USA","","","","machine learning; fairness testing; representative sampling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2978WMR7","book","2023","","Gamify 2023: Proceedings of the 2nd International Workshop on Gamification in Software Development, Verification, and Validation","","979-8-4007-0373-7","","","","On behalf of the Program Committee, we are pleased to present the proceedings of the 2nd International Workshop on Gamification in Software Development, Verification, and Validation (Gamify 2023). The workshop is virtually co-located with the 2023 edition of the ESEC/FSE conference, held in San Francisco (CA, USA). The workshop will be held online only the 4th of December 2023.","2023","2025-11-25 22:30:02","2025-11-25 22:30:02","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ULKLE5VZ","conferencePaper","2024","Ahmed, Toufique; Pai, Kunal Suresh; Devanbu, Premkumar; Barr, Earl","Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)","Proceedings of the IEEE/ACM 46th International Conference on Software Engineering","979-8-4007-0217-4","","10.1145/3597503.3639183","https://doi.org/10.1145/3597503.3639183","Large Language Models (LLM) are a new class of computation engines, ""programmed"" via prompt engineering. Researchers are still learning how to best ""program"" these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously collect semantics facts, from the code, while working. Mostly these are shallow, simple facts arising from a quick read. For a function, such facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc.One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them implicitly capable of doing this simple level of ""code analysis"" and extracting such information, while processing code: but are they, really? If they aren't, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether automatically augmenting an LLM's prompt with semantic facts explicitly, actually helps.Prior work shows that LLM performance on code summarization benefits from embedding a few code &amp; summary exemplars in the prompt, before the code to be summarized. While summarization performance has steadily progressed since the early days, there is still room for improvement: LLM performance on code summarization still lags its performance on natural-language tasks like translation and text summarization.We find that adding semantic facts to the code in the prompt actually does help! This approach improves performance in several different settings suggested by prior work, including for three different Large Language Models. In most cases, we see improvements, as measured by a range of commonly-used metrics; for the PHP language in the challenging CodeSearchNet dataset, this augmentation actually yields performance surpassing 30 BLEU1. In addition, we have also found that including semantic facts yields a substantial enhancement in LLMs' line completion performance.","2024","2025-11-25 22:30:02","2025-11-25 22:30:02","","","","","","","","","ICSE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","LLM; prompt engineering; program analysis; code summarization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3IZU8BDG","conferencePaper","2024","Corso, Vincenzo; Mariani, Leonardo; Micucci, Daniela; Riganelli, Oliviero","Generating Java Methods: An Empirical Assessment of Four AI-Based Code Assistants","Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension","979-8-4007-0586-1","","10.1145/3643916.3644402","https://doi.org/10.1145/3643916.3644402","AI-based code assistants are promising tools that can facilitate and speed up code development. They exploit machine learning algorithms and natural language processing to interact with developers, suggesting code snippets (e.g., method implementations) that can be incorporated into projects. Recent studies empirically investigated the effectiveness of code assistants using simple exemplary problems (e.g., the re-implementation of well-known algorithms), which fail to capture the spectrum and nature of the tasks actually faced by developers.In this paper, we expand the knowledge in the area by comparatively assessing four popular AI-based code assistants, namely GitHub Copilot, Tabnine, ChatGPT, and Google Bard, with a dataset of 100 methods that we constructed from real-life open-source Java projects, considering a variety of cases for complexity and dependency from contextual elements. Results show that Copilot is often more accurate than other techniques, yet none of the assistants is completely subsumed by the rest of the approaches. Interestingly, the effectiveness of these solutions dramatically decreases when dealing with dependencies outside the boundaries of single classes.","2024","2025-11-25 22:30:02","2025-11-25 22:30:02","","13–23","","","","","","","ICPC '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","ChatGPT; empirical study; copilot; AI-based code assistants; bard; code completion; tabnine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EYHC3ADD","book","2024","","APR '24: Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair","","979-8-4007-0577-9","","","","Welcome to the fifth International Workshop on Automated Program Repair (APR 2024), hosted by International Conference on Software Engineering (ICSE) 2024. Since its inception in 2020, APR has become a central event of the program repair community, reflecting a growing interest in the field among the software engineering, programming language, machine learning and formal methods communities.APR 2024 continues the tradition of fostering interaction among researchers in program repair. As always, we are particularly focused on narrowing the divide between academic research and real-world industry applications.","2024","2025-11-25 22:30:02","2025-11-25 22:30:02","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DXULDFCD","conferencePaper","2025","Hoffmann, Michael; Fillies, Jan; Paschke, Adrian","Malinowski in the Age of AI: Can large language models create a text game based on an anthropological classic?","Proceedings of the 21st International Conference on Culture and Computer Science: From Humanism to Digital Humanities","979-8-4007-1032-2","","10.1145/3719236.3719240","https://doi.org/10.1145/3719236.3719240","Recent advancements in Large Language Models (LLMs) like ChatGPT and GPT-4 have shown remarkable abilities in a wide range of tasks such as summarizing texts and assisting in coding. Scientific research has demonstrated that these models can also play text-adventure games. This study aims to explore whether LLMs can autonomously create text-based games based on anthropological classics, evaluating also their effectiveness in communicating knowledge. To achieve this, the study engaged anthropologists in discussions to gather their expectations and design inputs for an anthropologically themed game. Through iterative processes following the established HCI principle of ’design thinking’, the prompts and the conceptual framework for crafting these games were refined. Leveraging GPT3.5, the study created three prototypes of games centered around the seminal anthropological work of the social anthropologist’s Bronislaw Malinowski’s ""Argonauts of the Western Pacific“ (1922). Subsequently, evaluations were conducted by inviting senior anthropologists to playtest these games, and based on their inputs, the game designs were refined. The tests revealed promising outcomes but also highlighted key challenges: the models encountered difficulties in providing in-depth thematic understandings, showed suspectibility to misinformation, tended towards monotonic responses after an extended period of play, and struggled to offer detailed biographical information. Despite these limitations, the study’s findings open up new research avenues at the crossroads of artificial intelligence, machine learning, LLMs, ethnography, anthropology and human-computer interaction.","2025","2025-11-25 22:30:02","2025-11-25 22:30:02","","","","","","","","","KUI '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Machine Learning; Prompt Engineering; Big Data; Computational Anthropology.; Design Thinking; Educational Games","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FCQG5FNQ","journalArticle","2025","Fan, Fu; Jiang, Yanjie; Chen, Tianyi; Zhang, Hengshun; Zhang, Yuxia; Niu, Nan; Liu, Hui","An Empirical Study on Common Sense-Violating Bugs in Mobile Apps","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3709356","https://doi.org/10.1145/3709356","Mobile applications are widely used by billions of users in their daily work and life. Such GUI software is prone to bugs, potentially degrading user experience. Notably, many bugs in mobile apps are reported by end-users who cannot access the requirements of the app or test cases accompanied by explicitly specified test oracles. It may suggest that such bugs are not identified in the traditional way, i.e., by comparing the actual behaviors of the apps against their expected behaviors explicitly specified in the requirements or test cases. Instead, such bugs are often identified by comparing the actual behaviors against users’ common knowledge of apps, noted as common sense. We refer to such bugs as common sense-violating bugs. Although it is well-known that common sense-violating bugs are common in mobile apps, it remains unclear how popular they are and what kind of common sense principles are violated by them, let alone the relationship among the violated common sense principles. To this end, in this paper, we conduct the first large-scale empirical study on common sense-violating bugs in open-source mobile apps. We manually analyzed 2,808 real-world bug reports across 948 open-sourced mobile apps on GitHub. Our analysis results suggest that 1,006 (35.8%) out of the 2,808 bugs pertain to common sense-violating bugs. From those common sense-violating bugs, we identified a set of common sense principles violated by the buggy behaviors, and built a taxonomy for the common sense principles. Such principles fall into three categories: UI content-related common sense principles, UI layout-related common sense principles, and interaction-related common sense principles. By analyzing the frequency of the common sense principles being violated, we observed that a small set of common sense principles were frequently violated by the majority of common sense-violating bugs: 18 common sense principles, accounting for only 5% of the violated common sense principles, were violated by more than half of the common sense-violating bugs. These findings suggest that identifying the most frequent common sense-violating bugs could be achieved by using a small set of critical common sense principles, which may significantly reduce the cost of common sense-based bug detection. Finally, to demonstrate the feasibility of automated bug detection with common sense-based test oracles, we propose an automated approach to validating whether a given test run violates the most frequently violated common sense principle: No raw error message. Our evaluation results suggest that the automated approach is accurate, whose precision and recall are 91.3% and 91.6%, respectively.","2025-07","2025-11-25 22:30:02","2025-11-25 22:30:02","","","","6","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Empirical study; Android applications; Mobile applications; UI testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UN27DCWW","book","2023","","FUZZING 2023: Proceedings of the 2nd International Fuzzing Workshop","","979-8-4007-0247-1","","","","It is our great pleasure to welcome you to the 2nd International Workshop on Fuzzing (FUZZING 2023), co-located with ISSTA in Seattle, Washington, USA on 17 July 2023. This workshop is the continuation of last year's successful inauguration workshop that introduced a preregistration-based publication process to our community. Similar to last year, this workshop hosts the presentations of the accepted drafts of the registered reports that were accepted as part of the first stage in a two-stage publication process. In the first stage, the program committee (PC) evaluates all submissions based on: (i) the significance and novelty of the hypotheses or techniques and (ii) the soundness and reproducibility of the methodology specified to validate the claims or hypotheses – but explicitly not based on the strength of the (preliminary) results. These draft registered reports are presented and improved at the FUZZING 2023 workshop in Seattle.","2023","2025-11-25 22:30:02","2025-11-25 22:30:02","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U5M446XZ","journalArticle","2024","He, Yibo; Huang, Jiaming; Yu, Hao; Xie, Tao","An Empirical Study on Focal Methods in Deep-Learning-Based Approaches for Assertion Generation","Proc. ACM Softw. Eng.","","","10.1145/3660785","https://doi.org/10.1145/3660785","Unit testing is widely recognized as an essential aspect of the software development process. Generating high-quality assertions automatically is one of the most important and challenging problems in automatic unit test generation. To generate high-quality assertions, deep-learning-based approaches have been proposed in recent years. For state-of-the-art deep-learning-based approaches for assertion generation (DLAGs), the focal method (i.e., the main method under test) for a unit test case plays an important role of being a required part of the input to these approaches. To use DLAGs in practice, there are two main ways to provide a focal method for these approaches: (1) manually providing a developer-intended focal method or (2) identifying a likely focal method from the given test prefix (i.e., complete unit test code excluding assertions) with test-to-code traceability techniques. However, the state-of-the-art DLAGs are all evaluated on the ATLAS dataset, where the focal method for a test case is assumed as the last non-JUnit-API method invoked in the complete unit test code (i.e., code from both the test prefix and assertion portion). There exist two issues of the existing empirical evaluations of DLAGs, causing inaccurate assessment of DLAGs toward adoption in practice. First, it is unclear whether the last method call before assertions (LCBA) technique can accurately reflect developer-intended focal methods. Second, when applying DLAGs in practice, the assertion portion of a unit test is not available as a part of the input to DLAGs (actually being the output of DLAGs); thus, the assumption made by the ATLAS dataset does not hold in practical scenarios of applying DLAGs. To address the first issue, we conduct a study of seven test-to-code traceability techniques in the scenario of assertion generation. We find that the LCBA technique is not the best among the seven techniques and can accurately identify focal methods with only 43.38% precision and 38.42% recall; thus, the LCBA technique cannot accurately reflect developer-intended focal methods, raising a concern on using the ATLAS dataset for evaluation. To address the second issue along with the concern raised by the preceding finding, we apply all seven test-to-code traceability techniques, respectively, to identify focal methods automatically from only test prefixes and construct a new dataset named ATLAS+ by replacing the existing focal methods in the ATLAS dataset with the focal methods identified by the seven traceability techniques, respectively. On a test set from new ATLAS+, we evaluate four state-of-the-art DLAGs trained on a training set from the ATLAS dataset. We find that all of the four DLAGs achieve lower accuracy on a test set in ATLAS+ than the corresponding test set in the ATLAS dataset, indicating that DLAGs should be (re)evaluated with a test set in ATLAS+, which better reflects practical scenarios of providing focal methods than the ATLAS dataset. In addition, we evaluate state-of-the-art DLAGs trained on training sets in ATLAS+. We find that using training sets in ATLAS+ helps effectively improve the accuracy of the ATLAS approach and T5 approach over these approaches trained using the corresponding training set from the ATLAS dataset.","2024-07","2025-11-25 22:30:02","2025-11-25 22:30:02","","","","FSE","1","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Unit Testing; Traceability; Assertion Generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ISNYAIHI","conferencePaper","2024","Ton, Khiem; Nguyen, Nhi; Nazzal, Mahmoud; Khreishah, Abdallah; Borcea, Cristian; Phan, NhatHai; Jin, Ruoming; Khalil, Issa; Shen, Yelong","Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code","Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security","979-8-4007-0636-3","","10.1145/3658644.3691367","https://doi.org/10.1145/3658644.3691367","This paper introduces SGCode, a flexible prompt-optimizing system to generate secure code with large language models (LLMs). SGCode integrates recent prompt-optimization approaches with LLMs in a unified system accessible through front-end and back-end APIs, enabling users to 1) generate secure code, which is free of vulnerabilities, 2) review and share security analysis, and 3) easily switch from one prompt optimization approach to another, while providing insights on model and system performance. We populated SGCode on an AWS server with PromSec, an approach that optimizes prompts by combining an LLM and security tools with a lightweight generative adversarial graph neural network to detect and fix security vulnerabilities in the generated code. Extensive experiments show that SGCode is practical as a public tool to gain insights into the trade-offs between model utility, secure code generation, and system cost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is available at: http://3.131.141.63:8501/.","2024","2025-11-25 22:30:02","2025-11-25 22:30:02","","5078–5080","","","","","","","CCS '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Salt Lake City, UT, USA","","","","llms; demonstration system; prompt optimization; secure code","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KHCKCQZQ","book","2025","","MPLR '25: Proceedings of the 22nd ACM SIGPLAN International Conference on Managed Programming Languages and Runtimes","","979-8-4007-2149-6","","","","Welcome to MPLR 2025, the 22nd ACM SIGPLAN International Conference on Managed Programming Languages and Runtimes, held in Singapore on Tuesday, October 14, 2025, co- located with ICFP/SPLASH 2025. MPLR is a successor to the conference series on Managed Languages and Runtimes (ManLang, 2017 and 2018) which in turn is a successor to the conference series on Principles and Practice of Programming in Java (PPPJ, 2002 through 2016). MPLR is a premier forum for presenting and discussing novel results in all aspects of managed programming languages and runtime systems, which serve as building blocks for some of the most important computing systems around, ranging from small-scale (embedded and real-time systems) to large- scale (cloud-computing and big-data platforms) and anything in between (mobile, IoT, and wearable applications).","2025","2025-11-25 22:30:02","2025-11-25 22:30:02","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WB4TMA6T","book","2023","","VORTEX 2023: Proceedings of the 6th International Workshop on Verification and Monitoring at Runtime Execution","","979-8-4007-0249-5","","","","Welcome to the 6th Workshop on Verification and Monitoring at Runtime Execution (VORTEX 2023), hosted in Seattle (WA), USA, July 18, 2023, co-located with ECOOP/ISSTA 2023.","2023","2025-11-25 22:30:02","2025-11-25 22:30:02","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PTSHLW5V","conferencePaper","2025","Zhou, Shide; Li, Tianlin; Wang, Kailong; Huang, Yihao; Shi, Ling; Liu, Yang; Wang, Haoyu","Understanding the Effectiveness of Coverage Criteria for Large Language Models: A Special Angle from Jailbreak Attacks","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00209","https://doi.org/10.1109/ICSE55347.2025.00209","Large language models (LLMs) have revolutionized artificial intelligence, but their increasing deployment across critical domains has raised concerns about their abnormal behaviors when faced with malicious attacks. Such vulnerability alerts the widespread inadequacy of pre-release testing. In this paper, we conduct a comprehensive empirical study to evaluate the effectiveness of traditional coverage criteria in identifying such inadequacies, exemplified by the significant security concern of jailbreak attacks. Our study begins with a clustering analysis of the hidden states of LLMs, revealing that the embedded characteristics effectively distinguish between different query types. We then systematically evaluate the performance of these criteria across three key dimensions: criterion level, layer level, and token level.Our research uncovers significant differences in neuron coverage when LLMs process normal versus jailbreak queries, aligning with our clustering experiments. Leveraging these findings, we propose three practical applications of coverage criteria in the context of LLM security testing. Specifically, we develop a real-time jailbreak detection mechanism that achieves high accuracy (93.61% on average) in classifying queries as normal or jailbreak. Furthermore, we explore the use of coverage levels to prioritize test cases, improving testing efficiency by focusing on high-risk interactions and removing redundant tests. Lastly, we introduce a coverage-guided approach for generating jailbreak attack examples, enabling systematic refinement of prompts to uncover vulnerabilities. This study improves our understanding of LLM security testing, enhances their safety, and provides a foundation for developing more robust AI applications.","2025","2025-11-25 22:30:02","2025-11-25 22:30:02","","730–742","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6X9VCB8K","conferencePaper","2024","Blanco Lambruschini, Braulio C.; Brorsson, Mats","Transforming Unstructured Sensitive Information into Structured Knowledge","Proceedings of the 5th ACM International Conference on AI in Finance","979-8-4007-1081-0","","10.1145/3677052.3698602","https://doi.org/10.1145/3677052.3698602","Information is crucial in today’s context, yet less than 20% of companies utilize their unstructured data due to its complexity. Information Extraction (IE) is vital for effective data use, but current IE models face four major issues. First, they often provide limited information, such as a simple entity-attribute relation. Second, they struggle with multiple languages. Models like GPT, Mistral, and Llama3 show promise but face a third issue: output reliability due to hallucinations. Fourth, there is a challenge in reducing sensitive data leakage after fine-tuning models. This study introduces an enhanced approach for fine-tuning GPT-based models, designed to extract and assess information involving multiple entities and attributes, performing both multientity extraction (MEE) and multirelation extraction (MRE), and presenting results in a JSON format. Our methodology evaluates the impact of using synthetic data for fine-tuning to ensure reliable outcomes. Applied to legal documents from the Luxembourg Business Registers (LBR), our findings show that replacing sensitive data with synthetic data significantly improves the fine-tuning of Llama3-based models, though not for Mistral-based models. Our top models outperform Mistral in various scenarios, requiring only 500 samples for fine-tuning and running efficiently on modest servers. This approach is suitable for multilingual Information Extraction in any domain.","2024","2025-11-25 22:30:02","2025-11-25 22:30:02","","831–838","","","","","","","ICAIF '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Brooklyn, NY, USA","","","","LLM; Information Extraction; Finance","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XARN4Z4S","journalArticle","2024","Biringa, Chidera; Kul, Gökhan","PACE: A Program Analysis Framework for Continuous Performance Prediction","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3637230","https://doi.org/10.1145/3637230","Software development teams establish elaborate continuous integration pipelines containing automated test cases to accelerate the development process of software. Automated tests help to verify the correctness of code modifications decreasing the response time to changing requirements. However, when the software teams do not track the performance impact of pending modifications, they may need to spend considerable time refactoring existing code. This article presents PACE, a program analysis framework that provides continuous feedback on the performance impact of pending code updates. We design performance microbenchmarks by mapping the execution time of functional test cases given a code update. We map microbenchmarks to code stylometry features and feed them to predictors for performance predictions. Our experiments achieved significant performance in predicting code performance, outperforming current state-of-the-art by 75% on neural-represented code stylometry features.","2024-04","2025-11-25 22:30:02","2025-11-25 22:30:02","","","","4","33","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Code Stylometry Features; Current Code State; Microbenchmarking","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7KKB2W4A","bookSection","2025","Shabani, Taha; Nashid, Noor; Alian, Parsa; Mesbah, Ali","Dockerfile Flakiness: Characterization and Repair","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00238","Dockerfile flakiness—unpredictable temporal build failures caused by external dependencies and evolving environments—undermines deployment reliability and increases debugging overhead. Unlike traditional Dockerfile issues, flakiness occurs without modifications to the Dockerfile itself, complicating its resolution. In this work, we present the first comprehensive study of Dockerfile flakiness, featuring a nine-month analysis of 8,132 Dockerized projects, revealing that around 10% exhibit flaky behavior. We propose a taxonomy categorizing common flakiness causes, including dependency errors and server connectivity issues. Existing tools fail to effectively address these challenges due to their reliance on pre-defined rules and limited generalizability. To overcome these limitations, we introduce FlakiDock, a novel repair framework combining static and dynamic analysis, similarity retrieval, and an iterative feedback loop powered by Large Language Models (LLMs). Our evaluation demonstrates that FlakiDock achieves a repair accuracy of 73.55%, significantly surpassing state-of-the-art tools and baselines.","2025","2025-11-25 22:30:02","2025-11-25 22:30:02","","1793–1805","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WKVPRIMV","conferencePaper","2025","Hueckelheim, Jan; Sathia, Vimarsh; Qian, Siyuan Brant","Data Race Detection through Vibe Translation","Proceedings of the SC '25 Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis","979-8-4007-1871-7","","10.1145/3731599.3767363","https://doi.org/10.1145/3731599.3767363","We propose a data race detection approach for code written in a source programming language, by means of AI-agent translation to a target language, followed by conventional tool-based detection in the target language. We evaluate this translate-then-check approach by translating the C/Fortran+OpenMP programs in DataRaceBench to the Go programming language, and using the Go data race detector to check for races. The translation is controlled through natural language prompts, similar to approaches popularized as vibe coding. Translate-then-check achieves 92.8% accuracy and 9 false negatives for the C programs in DataRaceBench, compared to 89.9% accuracy and 17 false negatives for Clang with ThreadSanitizer and Archer applied to the original C programs. We discuss the approach and its overall accuracy, and show examples where translate-then-check leads to false negatives or positives due to limitations of the Go data race checker or, in some cases, limitations of the translation.","2025","2025-11-25 22:30:02","2025-11-25 22:30:02","","198–206","","","","","","","SC Workshops '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","OpenMP; Data Race Detection; Go; Vibe Coding","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IPRZB5EY","conferencePaper","2023","Adam, Julien; Besnard, Jean-Baptiste; Canat, Paul; Taboada, Hugo; Roussel, Adrien; Pérache, Marc; Jaeger, Julien; Shende, Sameer","Generating and Scaling a Multi-Language Test-Suite for MPI","Proceedings of the 30th European MPI Users' Group Meeting","979-8-4007-0913-5","","10.1145/3615318.3615329","https://doi.org/10.1145/3615318.3615329","High-Performance Computing (HPC) is currently facing significant challenges. The hardware pressure has become increasingly difficult to manage due to the lack of parallel abstractions in applications. As a result, parallel programs must undergo drastic evolution to effectively exploit underlying hardware parallelism. Failure to do so results in inefficient code. In this constrained environment, parallel runtimes play a critical role, and their testing becomes crucial. This paper focuses on the MPI interface and leverages the MPI binding tools to develop a multi-language test suite for MPI. By doing so and building on previous work from the Forum document editors, we implement a systematic testing of MPI symbols in the context of the Parallel Computing Validation System (PCVS), which is an HPC validation platform dedicated to running and managing test suites at scale. We first describe PCVS, then outline the process of generating the MPI API test suite, and finally, run these tests at scale. All data sets, code generators, and implementations are made available in open-source to the community. We also set up a dedicated website showcasing the results, which self-updates thanks to the Spack package manager.","2023","2025-11-25 22:30:02","2025-11-25 22:30:02","","","","","","","","","EuroMPI '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Bristol, United Kingdom","","","","HPC; api; MPI; test; validation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HUSJJ7AN","journalArticle","2025","Zhang, Shenglin; Xia, Sibo; Fan, Wenzhao; Shi, Binpeng; Xiong, Xiao; Zhong, Zhenyu; Ma, Minghua; Sun, Yongqian; Pei, Dan","Failure Diagnosis in Microservice Systems: A Comprehensive Survey and Analysis","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3715005","https://doi.org/10.1145/3715005","Widely adopted for their scalability and flexibility, modern microservice systems present unique failure diagnosis challenges due to their independent deployment and dynamic interactions. This complexity can lead to cascading failures that negatively impact operational efficiency and user experience. Recognizing the critical role of fault diagnosis in improving the stability and reliability of microservice systems, researchers have conducted extensive studies and achieved a number of significant results. This survey provides an exhaustive review of 98 scientific papers from 2003 to the present, including a thorough examination and elucidation of the fundamental concepts, system architecture, and problem statement. It also includes a qualitative analysis of the dimensions, providing an in-depth discussion of current best practices and future directions, aiming to further its development and application. In addition, this survey compiles publicly available datasets, toolkits, and evaluation metrics to facilitate the selection and validation of techniques for practitioners.","2025-01","2025-11-25 22:30:02","2025-11-25 22:30:02","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","multimodal data; failure classification; failure diagnosis; Microservice; root cause localization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8NYBH7FJ","conferencePaper","2024","Ngaruiya, Njeri; Donner, Jonathan; Baru, Joshua Kinuthia; Chege, Babra Wanjiku","The domestication of AI by Kenyan digital creators","Proceedings of the 4th African Human Computer Interaction Conference","979-8-4007-0887-9","","10.1145/3628096.3628753","https://doi.org/10.1145/3628096.3628753","This note explores the adoption and use of artificial intelligence (AI) in Kenya's digital creative sectors. Guided by a lens of domestication theory, and informed by interviews with 21 practitioners, the study documents ways in which AI is transforming traditional workflows, job roles, and skill requirements, enabling increased efficiency, automation, and creativity possibilities. Digital marketers leverage AI-powered analytics tools for data-driven insights and personalized marketing campaigns. Coders utilize AI algorithms to optimize code development, enhance software testing, and streamline debugging processes. Graphic designers incorporate AI tools for image recognition, automated design generation, and enhanced visual effects. Ghostwriters embrace AI-based writing assistants for generating content, improving productivity, and meeting client demands. Importantly, the study identifies concerns among professionals regarding job security, ethical implications, and the need for upskilling to effectively collaborate with AI technologies.","2024","2025-11-25 22:30:02","2025-11-25 22:30:02","","71–75","","","","","","","AfriCHI '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: East London, South Africa","","","","AI; Domestication; ICT4D; Livelihoods","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BLPRU23V","journalArticle","2025","Assres, Gebremariam; Bhandari, Guru; Shalaginov, Andrii; Gronli, Tor-Morten; Ghinea, Gheorghita","State-of-the-Art and Challenges of Engineering ML- Enabled Software Systems in the Deep Learning Era","ACM Comput. Surv.","","0360-0300","10.1145/3731597","https://doi.org/10.1145/3731597","Emerging from the software crisis of the 1960s, conventional software systems have vastly improved through Software Engineering (SE) practices. Simultaneously, Artificial Intelligence (AI) endeavors to augment or replace human decision-making. In the contemporary landscape, Machine Learning (ML), a subset of AI, leverages extensive data from diverse sources, fostering the development of ML-enabled (intelligent) software systems. While ML is increasingly utilized in conventional software development, the integration of SE practices in developing ML-enabled systems, especially across typical Software Development Life Cycle (SDLC) phases and methodologies in the post-2010 Deep Learning (DL) era, remains underexplored. Our survey of existing literature unveils insights into current practices, emphasizing the interdisciplinary collaboration challenges of developing ML-enabled software, including data quality, ethics, explainability, continuous monitoring and adaptation, and security. The study underscores the imperative for ongoing research and development with focus on data-driven hypotheses, non-functional requirements, established design principles, ML-first integration, automation, specialized testing, and use of agile methods.","2025-05","2025-11-25 22:30:02","2025-11-25 22:30:02","","","","10","57","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Conventional software; ML-enabled software; ML-powered systems; process areas; SDLC phases; software development models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DK6P95WS","journalArticle","2025","Molinelli, Davide; Martin-Lopez, Alberto; Zackrone, Elliott; Eken, Beyza; Ernst, Michael D.; Pezzè, Mauro","Tratto: A Neuro-Symbolic Approach to Deriving Axiomatic Test Oracles","Proc. ACM Softw. Eng.","","","10.1145/3728960","https://doi.org/10.1145/3728960","This paper presents Tratto, a neuro-symbolic approach that generates assertions (boolean expressions) that can serve as axiomatic oracles, from source code and documentation. The symbolic module of Tratto takes advantage of the grammar of the programming language, the unit under test, and the context of the unit (its class and available APIs) to restrict the search space of the tokens that can be successfully used to generate valid oracles. The neural module of Tratto uses transformers fine-tuned for both deciding whether to output an oracle or not and selecting the next lexical token to incrementally build the oracle from the set of tokens returned by the symbolic module. Our experiments show that Tratto outperforms the state-of-the-art axiomatic oracle generation approaches, with 73% accuracy, 72% precision, and 61% F1-score, largely higher than the best results of the symbolic and neural approaches considered in our study (61%, 62%, and 37%, respectively). Tratto can generate three times more axiomatic oracles than current symbolic approaches, while generating 10 times less false positives than GPT4 complemented with few-shot learning and Chain-of-Thought prompting.","2025-06","2025-11-25 22:30:02","2025-11-25 22:30:02","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","deep learning; test oracle; automated oracle generation; transfer learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"79XH2DTF","conferencePaper","2024","Dipongkor, Atish Kumar","Towards Interpreting the Behavior of Large Language Models on Software Engineering Tasks","Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings","979-8-4007-0502-1","","10.1145/3639478.3639798","https://doi.org/10.1145/3639478.3639798","Large Language Models (LLMs) have ushered in a significant breakthrough within the field of Natural Language Processing. Building upon this achievement, analogous language models have been developed specifically for code-related tasks, commonly referred to as Large Language Models for Code (LLMsC). Notable examples of LLMsC include CodeBERT, UnixCoder, CoPilot, among others. These models have demonstrated exceptional performance across various Software Engineering (SE) tasks, encompassing code summarization, test case generation, natural language to code conversion, bug triaging, malware detection, program repair, and more.Despite the promising results achieved by LLMsC in SE tasks, there remains fundamental questions regarding their decision-making processes. Understanding these model decision mechanisms is crucial for further enhancing the performance of LLMsC. In pursuit of this objective, my PhD dissertation aims to pioneer novel methodologies for interpreting and comprehending the behavior of LLMsC.","2024","2025-11-25 22:30:02","2025-11-25 22:30:02","","255–257","","","","","","","ICSE-Companion '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KUQI7DK5","journalArticle","2025","Kim, Donguk; Jeon, Minseok; Hwang, Doha; Oh, Hakjoo","PAFL: Enhancing Fault Localizers by Leveraging Project-Specific Fault Patterns","Proc. ACM Program. Lang.","","","10.1145/3720526","https://doi.org/10.1145/3720526","We present PAFL, a new technique for enhancing existing fault localization methods by leveraging project-specific fault patterns. We observed that each software project has its own challenges and suffers from recurring fault patterns associated with those challenges. However, existing fault localization techniques use a universal localization strategy without considering those repetitive faults. To address this limitation, our technique, called project-aware fault localization (PAFL), enables existing fault localizers to leverage project-specific fault patterns. Given a buggy version of a project and a baseline fault localizer, PAFL first mines the fault patterns from past buggy versions of the project. Then, it uses the mined fault patterns to update the suspiciousness scores of statements computed by the baseline fault localizer. To this end, we use two novel ideas. First, we design a domain-specific fault pattern-description language to represent various fault patterns. An instance, called crossword, in our language describes a project-specific fault pattern and how it affects the suspiciousness scores of statements. Second, we develop an algorithm that synthesizes crosswords (i.e., fault patterns) from past buggy versions of the project. Evaluation using seven baseline fault localizers and 12 real-world C/C++ and Python projects demonstrates that PAFL effectively, robustly, and efficiently improves the performance of the baseline fault localization techniques.","2025-04","2025-11-25 22:30:02","2025-11-25 22:30:02","","","","OOPSLA1","9","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Program Synthesis; Fault Localization; Domain-Specific Language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G9LBSY2Y","conferencePaper","2024","Leung, Mira; Murphy, Gail","On Automated Assistants for Software Development: The Role of LLMs","Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering","979-8-3503-2996-4","","10.1109/ASE56229.2023.00035","https://doi.org/10.1109/ASE56229.2023.00035","Software developers handle many complex tasks that include gathering and applying domain knowledge, coordinating subtasks, designing interfaces, turning ideas into elegant code, and more. They must switch contexts between these tasks, incurring more cognitive costs. Recent advances in large language models (LLMs) open up new possibilities for moving beyond the support provided by automated assistants (AAs) available today. In this paper, we explore if a human memory model can provide a framework for the systematic investigation of AAs for software development based on LLMs and other new technologies.","2024","2025-11-25 22:30:02","2025-11-25 22:30:02","","1737–1741","","","","","","","ASE '23","","","","IEEE Press","Echternach, Luxembourg","","","","","","","","","","","","large language models; artificial intelligence; machine learning; automation; software development productivity","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S4UULA6D","conferencePaper","2024","Xu, Qingyao; Yang, Dingkang; Zhang, Lihua","Code Optimization Chain-of-Thought: Structured Understanding and Self-Checking","Proceedings of the 2024 4th International Conference on Artificial Intelligence, Big Data and Algorithms","979-8-4007-1024-7","","10.1145/3690407.3690479","https://doi.org/10.1145/3690407.3690479","In recent years, significant advancements have been made in the field of LLMs (large language models), particularly within the domain of code optimization. This paper explores the realm of code optimization in LLMs and presents comprehensive approaches to enhance the model's abilities to generate and correct code through fine-tuning, training, and and applying Chain-of-Thought techniques during the inference phase. Novel strategies are introduced to augment the model's understanding of coded structures during the fine-tuning phase by integrating structured code information, providing a more robust grasp of core principles. This knowledge augmentation reflects a significant improvement in the model's structured comprehension of code and lays the foundations for a more effective generation and revision of code. Furthermore, a unique Chain-of-thought technique is applied during the inference phase to generate core coding principles and several sets of unit test data. The large language model is empowered to utilize these testing datasets for an active self-check and modification process. This novel methodology fosters the model's ability to autonomously adjust and fix the produced code, thereby enhancing the overall robustness and reliability of the generated code. The concepts and techniques elucidated in this paper aim to carve a path for future research and advancements in large language model code optimization.","2024","2025-11-25 22:30:03","2025-11-25 22:30:03","","425–430","","","","","","","CAIBDA '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Large Language Models; Artificial Intelligence; Chain of Thought; Fine-tuning; Code Optimization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B9PEMAKI","conferencePaper","2024","Arkko, Jari; Lindbo, Dag; Klitte, Martin","Do Large Language Models Dream of Sockets?","Proceedings of the 2024 Applied Networking Research Workshop","979-8-4007-0723-0","","10.1145/3673422.3674900","https://doi.org/10.1145/3673422.3674900","This paper discusses the concept of protocol Large Language Models (LLMs). These are models capable of conversing in native protocol messages. These models could potentially be used to help understand protocols, e.g., diagnose errors from packet traces. Our ongoing research investigates the feasibility of these models, their applications, and limitations.We present our preliminary results, including how LLMs understand the behavior of example systems such as web servers. Our contribution focuses on testing how and how well an LLM can diagnose protocol traces, receive and send protocol messages over sockets, and handle complex protocol fields and interfaces.","2024","2025-11-25 22:30:03","2025-11-25 22:30:03","","103–105","","","","","","","ANRW '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Vancouver, AA, Canada","","","","AI; LLMs; packet traces; protocols; simulation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5BFF2ISP","conferencePaper","2024","Pan, Wei Hung; Chok, Ming Jie; Wong, Jonathan Leong Shan; Shin, Yung Xin; Poon, Yeong Shian; Yang, Zhou; Chong, Chun Yong; Lo, David; Lim, Mei Kuan","Assessing AI Detectors in Identifying AI-Generated Code: Implications for Education","Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training","979-8-4007-0498-7","","10.1145/3639474.3640068","https://doi.org/10.1145/3639474.3640068","Educators are increasingly concerned about the usage of Large Language Models (LLMs) such as ChatGPT in programming education, particularly regarding the potential exploitation of imperfections in Artificial Intelligence Generated Content (AIGC) Detectors for academic misconduct.In this paper, we present an empirical study where the LLM is examined for its attempts to bypass detection by AIGC Detectors. This is achieved by generating code in response to a given question using different variants. We collected a dataset comprising 5,069 samples, with each sample consisting of a textual description of a coding problem and its corresponding human-written Python solution codes. These samples were obtained from various sources, including 80 from Quescol, 3,264 from Kaggle, and 1,725 from Leet-Code. From the dataset, we created 13 sets of code problem variant prompts, which were used to instruct ChatGPT to generate the outputs. Subsequently, we assessed the performance of five AIGC detectors. Our results demonstrate that existing AIGC Detectors perform poorly in distinguishing between human-written code and AI-generated code.","2024","2025-11-25 22:30:03","2025-11-25 22:30:03","","1–11","","","","","","","ICSE-SEET '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","software engineering education; AI-generated code; AI-generated code detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZJ2TNNFX","conferencePaper","2023","Jain, Ridhi; Gervasoni, Nicole; Ndhlovu, Mthandazo; Rawat, Sanjay","A Code Centric Evaluation of C/C++ Vulnerability Datasets for Deep Learning Based Vulnerability Detection Techniques","Proceedings of the 16th Innovations in Software Engineering Conference","979-8-4007-0064-4","","10.1145/3578527.3578530","https://doi.org/10.1145/3578527.3578530","Recent years have witnessed tremendous progress in NLP-based code comprehension via deep neural networks (DNN) learning, especially Large Language Models (LLMs). While the original application of LLMs is focused on code generation, there have been attempts to extend the application to more specialized tasks, like code similarity, author attribution, code repairs, and so on. As data plays an important role in the success of any machine learning approach, researchers have also proposed several benchmarks which are coupled with a specific task at hand. It is well known in the machine learning (ML) community that the presence of biases in the dataset affects the quality of the ML algorithm in a real-world scenario. This paper evaluates several existing datasets from DNN’s application perspective. We specifically focus on training datasets of C/C++ language code. Our choice of language stems from the fact that while LLM-based techniques have been applied and evaluated on programming languages like Python, JavaScript, and Ruby, there is not much LLM research for C/C++. As a result, datasets generated synthetically or from real-world codes are in individual research work. Consequently, in the absence of a uniform dataset, such works are hard to compare with each other. In this work, we aim to achieve two main objectives– 1. propose code-centric features that are relevant to security program analysis tasks like vulnerability detection; 2. a thorough (qualitative and quantitative) examination of the existing code datasets that demonstrate the main characteristics of the individual datasets to have a clear comparison. Our evaluation finds exciting facts about existing datasets highlighting gaps that need to be addressed.","2023","2025-11-25 22:30:03","2025-11-25 22:30:03","","","","","","","","","ISEC '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Allahabad, India","","","","datasets; program graphs; software metrics; software vulnerability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SIG8ZPA5","journalArticle","2025","Dong, Yihong; Ding, Jiazheng; Jiang, Xue; Li, Ge; Li, Zhuo; Jin, Zhi","CodeScore: Evaluating Code Generation by Learning Code Execution","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3695991","https://doi.org/10.1145/3695991","A proper code evaluation metric (CEM) profoundly impacts the evolution of code generation, which is an important research field in NLP and software engineering. Prevailing match-based CEMs (e.g., BLEU, Accuracy, and CodeBLEU) suffer from two significant drawbacks. 1. They primarily measure the surface differences between codes without considering their functional equivalence. However, functional equivalence is pivotal in evaluating the effectiveness of code generation, as different codes can perform identical operations. 2. They are predominantly designed for the Ref-only input format. However, code evaluation necessitates versatility in input formats. Aside from Ref-only, there are NL-only and Ref and NL formats, which existing match-based CEMs cannot effectively accommodate. In this article, we propose CodeScore, a large language model (LLM)-based CEM, which estimates the functional correctness of generated code on three input types. To acquire CodeScore, we present UniCE, a unified code generation learning framework, for LLMs to learn code execution (i.e., learning PassRatio and Executability of generated code) with unified input. Extensive experimental results on multiple code evaluation datasets demonstrate that CodeScore absolutely improves up to 58.87% correlation with functional correctness compared to other CEMs, achieves state-of-the-art performance, and effectively handles three input formats.","2025-02","2025-11-25 22:30:03","2025-11-25 22:30:03","","","","3","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Code Generation; Code Evaluation; Code Pre-trained Language Model","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VL349AX6","bookSection","2025","Zhou, Chijin; Zhang, Quan; Qian, Bingzhou; Jiang, Yu","Janus: Detecting Rendering Bugs in Web Browsers via Visual Delta Consistency","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00013","Rendering lies at the heart of our modern web experience. However, the correctness of browser rendering is not always guaranteed, often leading to rendering bugs. Traditional differential testing, while successful in various domains, falls short when applied to rendering bug detection because an HTML file is likely yield different rendered outcomes across different browsers. This paper introduces Visual Delta Consistency, a test oracle to detect rendering bugs in web browsers, aiming to make rendered pages across browsers comparable. Our key insight is that any modifications made to an HTML file should uniformly influence rendering outcomes across browsers. Specifically, when presented with two HTML files that differ only by minor modifications, the reaction of all browsers should be consistent, i.e., either all browsers render them identically or all render them differently. Based on this insight, We implemented it as a practical fuzzer named JANUS. It constructs pairs of slightly modified HTML files and observes the change statuses of the corresponding rendered pages across browsers for bug detection. We evaluated it on three widely-used browsers, i.e., Chrome, Safari, and Firefox. In total, JANUS detected 31 non-crash rendering bugs, out of which 24 confirmed with 8 fixed.","2025","2025-11-25 22:30:03","2025-11-25 22:30:03","","2702–2713","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZSWWLZNE","conferencePaper","2024","Xiong, Yiheng; Su, Ting; Wang, Jue; Sun, Jingling; Pu, Geguang; Su, Zhendong","General and Practical Property-based Testing for Android Apps","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3694986","https://doi.org/10.1145/3691620.3694986","Finding non-crashing functional bugs for Android apps is challenging for both manual testing and automated GUI testing techniques. This paper introduces and designs a general and practical testing technique based on the idea of property-based testing for finding such bugs. Specifically, our technique incorporates (1) a property description language (PDL) to allow specifying desired app properties, and (2) two exploration strategies as the input generators for effectively validating the properties. We implemented our technique as a tool named Kea and evaluated it on 124 historical bugs from eight real-world, popular Android apps. Our evaluation shows that our PDL can specify all the app properties violated by these historical bugs, demonstrating its generability for finding functional bugs. Kea successfully found 66 (68.0%) and 92 (94.8%) of the 97 historical bugs in scope under the two exploration strategies, demonstrating its practicability. Moreover, Kea found 25 new functional bugs on the latest versions of these eight apps, given the specified properties. To date, all these bugs have been confirmed, and 21 have been fixed. In comparison, prior state-of-the-art techniques found only 13 (13.4%) historical bugs and 1 new bug. We have made all the artifacts publicly available at https://github.com/ecnusse/Kea.","2024","2025-11-25 22:30:03","2025-11-25 22:30:03","","53–64","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","property-based testing; Android app testing; non-crashing functional bugs","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4XXELVNH","bookSection","2025","Li, Yichen; Wu, Yulun; Liu, Jinyang; Jiang, Zhihan; Chen, Zhuangbin; Yu, Guangba; Lyu, Michael R.","COCA: Generative Root Cause Analysis for Distributed Systems with Code Knowledge","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00234","Runtime failures are commonplace in modern distributed systems. When such issues arise, users often turn to platforms such as Github or JIRA to report them and request assistance. Automatically identifying the root cause of these failures is critical for ensuring high reliability and availability. However, prevailing automatic root cause analysis (RCA) approaches rely significantly on comprehensive runtime monitoring data, which is often not fully available in issue platforms. Recent methods leverage large language models (LLMs) to analyze issue reports, but their effectiveness is limited by incomplete or ambiguous user-provided information.To obtain more accurate and comprehensive RCA results, the core idea of this work is to extract additional diagnostic clues from code to supplement data-limited issue reports. Specifically, we propose COCA, a code knowledge enhanced root cause analysis approach for issue reports. Based on the data within issue reports, COCA intelligently extracts relevant code snippets and reconstructs execution paths, providing a comprehensive execution context for further RCA. Subsequently, COCA constructs a prompt combining historical issue reports along with profiled code knowledge, enabling the LLMs to generate detailed root cause summaries and localize responsible components. Our evaluation on datasets from five real-world distributed systems demonstrates that COCA significantly outperforms existing methods, achieving a 28.3% improvement in root cause localization and a 22.0% improvement in root cause summarization. Furthermore, COCA's performance consistency across various LLMs underscores its robust generalizability.","2025","2025-11-25 22:30:03","2025-11-25 22:30:03","","1346–1358","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9UXAL3WW","conferencePaper","2024","Meyer, Bertrand; Kananchuk, Viktoryia; Huang, Li","BUGFIX: towards a common language and framework for the Automatic Program Repair community","Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair","979-8-4007-0577-9","","10.1145/3643788.3648007","https://doi.org/10.1145/3643788.3648007","Techniques of Automatic Program Repair (APR) have the potential of thoroughly facilitating the task of producing quality software. After a promising start, however, progress in making APR practical has been hindered by the lack of a common framework to support the multiplicity of APR ideas and tools, and of target programming languages and environments. In this position paper we outline a general framework to enable the APR community to benefit from each other's advances, in particular through a standard language for describing bugs and their fixes. Such a common framework — which is also applicable to work on fault seeding — could be a tremendous benefit to researchers and developers of Interactive Development Environments (IDEs) who are working to make APR an effective part of the software developer's practical experience.","2024","2025-11-25 22:30:03","2025-11-25 22:30:03","","9–13","","","","","","","APR '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","automatic program repair; debugging; software quality; bug seeding; integrated development environments; program transformation; software tools","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"435M3NT8","journalArticle","2024","Drosos, Georgios-Petros; Sotiropoulos, Thodoris; Alexopoulos, Georgios; Mitropoulos, Dimitris; Su, Zhendong","When Your Infrastructure Is a Buggy Program: Understanding Faults in Infrastructure as Code Ecosystems","Proc. ACM Program. Lang.","","","10.1145/3689799","https://doi.org/10.1145/3689799","Modern applications have become increasingly complex and their manual installation and configuration is no longer practical. Instead, IT organizations heavily rely on Infrastructure as Code (IaC) technologies, to automate the provisioning, configuration, and maintenance of computing infrastructures and systems. IaC systems typically offer declarative, domain-specific languages (DSLs) that allow system administrators and developers to write high-level programs that specify the desired state of their infrastructure in a reliable, predictable, and documented fashion. Just like traditional programs, IaC software is not immune to faults, with issues ranging from deployment failures to critical misconfigurations that often impact production systems used by millions of end users. Surprisingly, despite its crucial role in global infrastructure management, the tooling and techniques for ensuring IaC reliability still have room for improvement. In this work, we conduct a comprehensive analysis of 360 bugs identified in IaC software within prominent IaC ecosystems including Ansible, Puppet, and Chef. Our work is the first in-depth exploration of bug characteristics in these widely-used IaC environments. Through our analysis we aim to understand: (1) how these bugs manifest, (2) their underlying root causes, (3) their reproduction requirements in terms of system state (e.g., operating system versions) or input characteristics, and (4) how these bugs are fixed. Based on our findings, we evaluate the state-of-the-art techniques for IaC reliability, identify their limitations, and provide a set of recommendations for future research. We believe that our study helps researchers to (1) better understand the complexity and peculiarities of IaC software, and (2) develop advanced tooling for more reliable and robust system configurations.","2024-10","2025-11-25 22:30:03","2025-11-25 22:30:03","","","","OOPSLA2","8","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","testing; infrastructure as code; Ansible; bug; Chef; deployment; IaC; Puppet","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HIZNIZXM","bookSection","2025","Yu, Jiazhao; Tu, Yanlun; Zhang, Zhanlei; Zhang, Tiehua; Xu, Cheng; Wu, Weigang; Kang, Hong Jin; Zheng, Xi","Grey-Box Fuzzing in Constrained Ultra-Large Systems: Lessons for SE Community","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","","https://doi.org/10.1145/3696630.3728546","Testing ultra-large microservices-based FinTech systems presents significant challenges, including restricted access to production environments, complex dependencies, and stringent security constraints. We propose SandBoxFuzz, a scalable grey-box fuzzing technique that addresses these limitations by leveraging aspect-oriented programming and runtime reflection to enable dynamic specification mining, generating targeted inputs for constrained environments. SandBoxFuzz also introduces a log-based coverage mechanism, seamlessly integrated into the build pipeline, eliminating the need for runtime coverage agents that are often infeasible in industrial settings. SandBoxFuzz has been successfully deployed to Ant Group's production line and, compared to an initial solution built on a state-of-the-art fuzzing framework, it demonstrates superior performance in their microservices software. SandBoxFuzz achieves a 7.5% increase in branch coverage, identifies 1,850 additional exceptions, and reduces setup time from hours to minutes, highlighting its effectiveness and practical utility in a real-world industrial environment. By open-sourcing SandBoxFuzz1, we provide a practical and effective tool for researchers and practitioners to test large-scale microservices systems.","2025","2025-11-25 22:30:03","2025-11-25 22:30:03","","204–214","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QLEQUWCT","conferencePaper","2024","Sonkar, Shashank; Chen, Xinghe; Le, Myco; Liu, Naiming; Basu Mallick, Debshila; Baraniuk, Richard","Code Soliloquies for Accurate Calculations in Large Language Models","Proceedings of the 14th Learning Analytics and Knowledge Conference","979-8-4007-1618-8","","10.1145/3636555.3636889","https://doi.org/10.1145/3636555.3636889","High-quality conversational datasets are crucial for the successful development of Intelligent Tutoring Systems (ITS) that utilize a Large Language Model (LLM) backend. Synthetic student-teacher dialogues, generated using advanced GPT-4 models, are a common strategy for creating these datasets. However, subjects like physics that entail complex calculations pose a challenge. While GPT-4 presents impressive language processing capabilities, its limitations in fundamental mathematical reasoning curtail its efficacy for such subjects. To tackle this limitation, we introduce in this paper an innovative stateful prompt design. Our design orchestrates a mock conversation where both student and tutorbot roles are simulated by GPT-4. Each student response triggers an internal monologue, or ‘code soliloquy’ in the GPT-tutorbot, which assesses whether its subsequent response would necessitate calculations. If a calculation is deemed necessary, it scripts the relevant Python code and uses the Python output to construct a response to the student. Our approach notably enhances the quality of synthetic conversation datasets, especially for subjects that are calculation-intensive. The preliminary Subject Matter Expert evaluations reveal that our Higgs model, a fine-tuned LLaMA model, effectively uses Python for computations, which significantly enhances the accuracy and computational reliability of Higgs’ responses.","2024","2025-11-25 22:30:03","2025-11-25 22:30:03","","828–835","","","","","","","LAK '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Kyoto, Japan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"633JE7DV","bookSection","2025","Plein, Laura","Predicting Software Changes from Desired Behavior Changes","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","","https://doi.org/10.1145/3696630.3728617","In the past years, we have seen an explosion of Machine Learning techniques applied to software engineering tasks [1, 10, 12]. The vast majority of these approaches is applied to program code, using Large Language Models (LLMs) to predict token sequences in specific contexts [11]. However, the dynamic nature of programs is hardly exploited; on the contrary, interpreting and predicting the semantics of code remains challenging for LLMs.","2025","2025-11-25 22:30:03","2025-11-25 22:30:03","","1019–1021","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IWNEJKCL","conferencePaper","2025","Sun, Yang; Poskitt, Christopher M.; Wang, Kun; Sun, Jun","FixDrive: Automatically Repairing Autonomous Vehicle Driving Behaviour for $0.08 per Violation","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00216","https://doi.org/10.1109/ICSE55347.2025.00216","Autonomous Vehicles (AVs) are advancing rapidly, with Level-4 AVs already operating in real-world conditions. Current AVs, however, still lag behind human drivers in adaptability and performance, often exhibiting overly conservative behaviours and occasionally violating traffic laws. Existing solutions, such as runtime enforcement, mitigate this by automatically repairing the AV's planned trajectory at runtime, but such approaches lack transparency and should be a measure of last resort. It would be preferable for AV repairs to generalise beyond specific incidents and to be interpretable for users. In this work, we propose FixDrive, a framework that analyses driving records from near-misses or law violations to generate AV driving strategy repairs that reduce the chance of such incidents occurring again. These repairs are captured in μDrive, a high-level domain-specific language for specifying driving behaviours in response to event-based triggers. Implemented for the state-of-the-art autonomous driving system Apollo, FixDrive identifies and visualises critical moments from driving records, then uses a Multimodal Large Language Model (MLLM) with zero-shot learning to generate μDrive programs. We tested FixDrive on various benchmark scenarios, and found that the generated repairs improved the AV's performance with respect to following traffic laws, avoiding collisions, and successfully reaching destinations. Furthermore, the direct costs of repairing an AV—15 minutes of offline analysis and $0.08 per violation—are reasonable in practice.","2025","2025-11-25 22:30:03","2025-11-25 22:30:03","","1921–1933","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","autonomous driving systems; autonomous vehicles; driving compliance; multimodal large language models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"933ZGQUK","conferencePaper","2024","Baral, Talank; Rahman, Shanto; Chanumolu, Bala Naren; Balcı, Başak; Tuncer, Tuna; Shi, August; Lam, Wing","Optimizing Continuous Development By Detecting and Preventing Unnecessary Content Generation","Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering","979-8-3503-2996-4","","10.1109/ASE56229.2023.00216","https://doi.org/10.1109/ASE56229.2023.00216","Continuous development (CD) helps developers quickly release and update their software. To enact CD, developers customize their CD builds to perform several tasks, including compiling, testing, static analysis checks, etc. However, as developers add more tasks to their builds, the builds take longer to run, therefore slowing down the entire CD process. Furthermore, developers may unknowingly include tasks into their builds whose results are not used (e.g., generating coverage files that are never read or uploaded anywhere), therefore wasting build runtime doing unnecessary tasks.We propose OptCD, a technique to dynamically detect unnecessary work within CD builds. Our intuition is that unnecessary work can be identified by the generation of files that are not used by any other task within the build. OptCD runs alongside a CD build, tracking the generated files during the build and which files are read/written. Files that are written to but are never read from are unnecessary content from a build. Based on the names of the unnecessary files, OptCD then maps the files to the specific build tasks responsible for generating or writing to those files. Finally, OptCD leverages ChatGPT to suggest changing the build configuration to disable generating these unnecessary files. Our evaluation of OptCD on 22 open-source projects finds that 95.6% of projects generate at least one unused directory, a directory whose contents are all unnecessarily generated. OptCD identifies the correct task that generates 92.0% of the unused directories. Further, OptCD can produce a patch for the CD configuration file to prevent generating 72.0% of the unused directories. Using the patches, we reduce the runtime by 7.0% on average for the projects we studied. We submitted 26 pull requests for the unused directories that we could disable. Developers have accepted 12 of them, with five rejected, and nine still pending.","2024","2025-11-25 22:30:03","2025-11-25 22:30:03","","901–913","","","","","","","ASE '23","","","","IEEE Press","Echternach, Luxembourg","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F9WS2V6E","conferencePaper","2024","Van Praet, Lucas; Hoobergs, Jesse; Schrijvers, Tom","ASSIST: Automated Feedback Generation for Syntax and Logical Errors in Programming Exercises","Proceedings of the 2024 ACM SIGPLAN International Symposium on SPLASH-E","979-8-4007-1216-6","","10.1145/3689493.3689981","https://doi.org/10.1145/3689493.3689981","Introductory programming courses often rely on numerous exercises to help students practice and reinforce their skills. Commonly used automated tests fall short by merely identifying the issues without offering guidance on how to resolve them and manual reviews are too resource-intensive to use in large classes. To address these challenges, we present ASSIST—a tool designed to provide automated, detailed feedback on how to resolve issues in programming exercise submissions with both syntactic and logical errors. ASSIST combines fault-tolerant parsing with fixes based on the context of error nodes to resolve syntactic errors and give feedback. ASSIST feeds this valid program to the Sketch program synthesis tool to determine the needed changes from a set of potential changes induced by rewrite rules, and generates feedback on logic errors based on the needed changes. This dual approach allows ASSIST to offer actionable feedback on both syntax and logic issues in student submissions. We evaluated ASSIST on submissions from an online platform for secondary education. Our findings reveal that, for submissions with syntax errors, ASSIST delivers feedback on all syntax errors in 71% of cases and extends its feedback to cover logical errors in 34% of these submissions. When evaluating all incorrect submissions, ASSIST is able to give feedback on logical errors in 64% of cases. These results indicate that ASSIST can significantly enhance the feedback process in large-scale programming courses, offering a feasible and efficient alternative to current methods.","2024","2025-11-25 22:30:03","2025-11-25 22:30:03","","66–76","","","","","","","SPLASH-E '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Pasadena, CA, USA","","","","Program Repair; Computer Science Education; Automated Feedback","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T9MKLB4W","conferencePaper","2024","Liu, Changshu; Cetin, Pelin; Patodia, Yogesh; Ray, Baishakhi; Chakraborty, Saikat; Ding, Yangruibo","Automated Code Editing with Search-Generate-Modify","Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings","979-8-4007-0502-1","","10.1145/3639478.3643124","https://doi.org/10.1145/3639478.3643124","Code editing is essential in evolving software development. In literature, several automated code editing tools are proposed, which leverage Information Retrieval-based techniques and Machine Learning-based code generation and code editing models.A patch that is obtained by search &amp; retrieval, even if incorrect, can provide helpful guidance to a code generation model. However, a retrieval-guided patch produced by a code generation model can still be a few tokens off from the intended patch. Such generated patches can be slightly modified to create the intended patches. We propose SarGaM which mimics a developer's behavior - search for related patches, generate or write code and then modify to adapt it to the right context. Our evaluation of SarGaM on edit generation shows superior performance w.r.t. the current state-of-the-art techniques. SarGaM also shows its effectiveness on automated program repair tasks.","2024","2025-11-25 22:30:03","2025-11-25 22:30:03","","398–399","","","","","","","ICSE-Companion '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","automated program repair; bug fixing; edit-based neural network","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V5PWNFHU","conferencePaper","2025","You, Hanmo; Wang, Zan; Lin, Bin; Chen, Junjie","Navigating the Testing of Evolving Deep Learning Systems: An Exploratory Interview Study","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00106","https://doi.org/10.1109/ICSE55347.2025.00106","Deep Learning (DL) systems have been widely adopted across various industrial domains such as autonomous driving and intelligent healthcare. As with traditional software, DL systems also need to constantly evolve to meet ever-changing user requirements. However, ensuring the quality of these continuously evolving systems presents significant challenges, especially in the context of testing. Understanding how industry developers address these challenges and what extra obstacles they are facing could provide valuable insights for further safeguarding the quality of DL systems. To reach this goal, we conducted semi-structured interviews with 22 DL developers from diverse domains and backgrounds. More specifically, our study focuses on exploring the challenges developers encounter in testing evolving DL systems, the practical solutions they employ, and their expectations for extra support. Our results highlight the difficulties in testing evolving DL systems (e.g., regression faults, online-offline differences, and test data collection) and identify the best practices for DL developers to address these challenges. Additionally, we pinpoint potential future research directions to enhance testing effectiveness in evolving DL systems.","2025","2025-11-25 22:30:03","2025-11-25 22:30:03","","2726–2738","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","testing; deep learning; interview study; software evolution","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7IMBWYIH","journalArticle","2025","Zhou, Yupeng; Cui, Can; Peng, Juntong; Yang, Zichong; Lu, Juanwu; Panchal, Jitesh; Yao, Bin; Wang, Ziran","A Hierarchical Test Platform for Vision Language Model (VLM)-Integrated Real-World Autonomous Driving","ACM Trans. Internet Things","","","10.1145/3769867","https://doi.org/10.1145/3769867","Vision-Language Models (VLMs) have demonstrated significant promise for autonomous driving due to their powerful multimodal reasoning capabilities. However, adapting VLMs from generic data to safety-critical driving contexts introduces a notable challenge known as domain shift. Existing simulation-based and dataset-driven evaluation approaches struggle to accurately replicate real-world complexities, lacking repeatable closed-loop evaluation and flexible scenario manipulation. Furthermore, current real-world testing platforms typically focus on isolated modules and do not support comprehensive interaction with VLM-based systems. Consequently, there is a critical need for a holistic testing architecture capable of integrating perception, planning, and control modules, accommodating VLM-based systems, and supporting configurable real-world testing scenarios. In this paper, we address this critical gap by proposing a hierarchical real-world test platform specialized in the rigorous evaluation of VLM-integrated autonomous driving systems. Specifically, our platform features have: a lightweight, structured, and low-latency middleware pipeline specialized for seamless VLM integration; a hierarchical modular architecture enabling flexible substitution between conventional and VLM-based autonomy components, providing exceptional deployment flexibility for rapid experimentation; and sophisticated closed-loop scenario-based testing capabilities on a controlled test track, facilitating comprehensive evaluation of the entire full-stack VLM-integrated autonomous driving pipeline, from perception, reasoning, decision-making, and planning to final vehicle maneuvers. Through an extensive real-world case study, we demonstrate the effectiveness of our platform in evaluating the performance and robustness of VLM-integrated autonomous driving under diverse realistic conditions. Project page and codes: https://github.com/YupengZhouPurdue/VLMTest","2025-09","2025-11-25 22:30:03","2025-11-25 22:30:03","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","verification; testing; and validation.; autonomous driving; Vision-Language Models (VLMs)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AULUXA8I","conferencePaper","2025","Sheng, Guangming; Zhang, Chi; Ye, Zilingfeng; Wu, Xibin; Zhang, Wang; Zhang, Ru; Peng, Yanghua; Lin, Haibin; Wu, Chuan","HybridFlow: A Flexible and Efficient RLHF Framework","Proceedings of the Twentieth European Conference on Computer Systems","979-8-4007-1196-1","","10.1145/3689031.3696075","https://doi.org/10.1145/3689031.3696075","Reinforcement Learning from Human Feedback (RLHF) is widely used in Large Language Model (LLM) alignment. Traditional RL can be modeled as a dataflow, where each node represents computation of a neural network (NN) and each edge denotes data dependencies between the NNs. RLHF complicates the dataflow by expanding each node into a distributed LLM training or generation program, and each edge into a many-to-many multicast. Traditional RL frameworks execute the dataflow using a single controller to instruct both intra-node computation and inter-node communication, which can be inefficient in RLHF due to large control dispatch overhead for distributed intra-node computation. Existing RLHF systems adopt a multi-controller paradigm, which can be inflexible due to nesting distributed computation and data communication. We propose HybridFlow, which combines single-controller and multi-controller paradigms in a hybrid manner to enable flexible representation and efficient execution of the RLHF data flow. We carefully design a set of hierarchical APIs that decouple and encapsulate computation and data dependencies in the complex RLHF dataflow, allowing efficient operation orchestration to implement RLHF algorithms and flexible mapping of the computation onto various devices. We further design a 3D-HybridEngine for efficient actor model resharding between training and generation phases, with zero memory redundancy and significantly reduced communication overhead. Our experimental results demonstrate 1.53x 20.57× throughput improvement when running various RLHF algorithms using HybridFlow, as compared with state-of-the-art baselines. HybridFlow source code is available at https://github.com/volcengine/verl","2025","2025-11-25 22:30:03","2025-11-25 22:30:03","","1279–1297","","","","","","","EuroSys '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Rotterdam, Netherlands","","","","Distributed systems; Reinforcement Learning from Human Feedback","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9F4L7GFH","conferencePaper","2024","Kim, Myeongsoo; Stennett, Tyler; Shah, Dhruv; Sinha, Saurabh; Orso, Alessandro","Leveraging Large Language Models to Improve REST API Testing","Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results","979-8-4007-0500-7","","10.1145/3639476.3639769","https://doi.org/10.1145/3639476.3639769","The widespread adoption of REST APIs, coupled with their growing complexity and size, has led to the need for automated REST API testing tools. Current tools focus on the structured data in REST API specifications but often neglect valuable insights available in unstructured natural-language descriptions in the specifications, which leads to suboptimal test coverage. Recently, to address this gap, researchers have developed techniques that extract rules from these human-readable descriptions and query knowledge bases to derive meaningful input values. However, these techniques are limited in the types of rules they can extract and prone to produce inaccurate results. This paper presents RESTGPT, an innovative approach that leverages the power and intrinsic context-awareness of Large Language Models (LLMs) to improve REST API testing. RESTGPT takes as input an API specification, extracts machine-interpretable rules, and generates example parameter values from natural-language descriptions in the specification. It then augments the original specification with these rules and values. Our evaluations indicate that RESTGPT outperforms existing techniques in both rule extraction and value generation. Given these promising results, we outline future research directions for advancing REST API testing through LLMs.","2024","2025-11-25 22:30:03","2025-11-25 22:30:03","","37–41","","","","","","","ICSE-NIER'24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","large language models for testing; OpenAPI specification analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4AUM7KH2","journalArticle","2024","Haroon, Sabaat; Brown, Chris; Gulzar, Muhammad Ali","DeSQL: Interactive Debugging of SQL in Data-Intensive Scalable Computing","Proc. ACM Softw. Eng.","","","10.1145/3643761","https://doi.org/10.1145/3643761","SQL is the most commonly used front-end language for data-intensive scalable computing (DISC) applications due to its broad presence in new and legacy workflows and shallow learning curve. However, DISC-backed SQL introduces several layers of abstraction that significantly reduce the visibility and transparency of workflows, making it challenging for developers to find and fix errors in a query. When a query returns incorrect outputs, it takes a non-trivial effort to comprehend every stage of the query execution and find the root cause among the input data and complex SQL query. We aim to bring the benefits of step-through interactive debugging to DISC-powered SQL with DeSQL. Due to the declarative nature of SQL, there are no ordered atomic statements to place a breakpoint to monitor the flow of data. DeSQL’s automated query decomposition breaks a SQL query into its constituent sub queries, offering natural locations for setting breakpoints and monitoring intermediate data. However, due to advanced query optimization and translation in DISC systems, a user query rarely matches the physical execution, making it challenging to associate subqueries with their intermediate data. DeSQL performs fine-grained taint analysis to dynamically map the subqueries to their intermediate data, while also recognizing subqueries removed by the optimizers. For such subqueries, DeSQL efficiently regenerates the intermediate data from a nearby subquery’s data. On the popular TPC-DC benchmark, DeSQL provides a complete debugging view in 13% less time than the original job time while incurring an average overhead of 10% in addition to retaining Apache Spark’s scalability. In a user study comprising 15 participants engaged in two debugging tasks, we find that participants utilizing DeSQL identify the root cause behind a wrong query output in 74% less time than the de-facto, manual debugging.","2024-07","2025-11-25 22:30:03","2025-11-25 22:30:03","","","","FSE","1","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Debugging; SQL; data-intensive scalable computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YTKLJYUM","conferencePaper","2025","Wu, Zhiyong; Liang, Jie; Fu, Jingzhou; Wang, Mingzhe; Jiang, Yu","Puppy: Finding Performance Degradation Bugs in DBMSs via Limited-Optimization Plan Construction","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00045","https://doi.org/10.1109/ICSE55347.2025.00045","Database management systems (DBMSs) consistently strive for enhanced performance. For a given query, the optimizer of a DBMS aims to construct an optimal execution plan that incorporates multiple optimization operations. However, the resulting plan may sometimes perform worse than even if no optimizations were applied. This occurs because the interactions between optimizations are complex and some situations might be overlooked in the implementation. We refer to these issues as Performance Degradation Bugs (PDBs). PDBs can result in significant consequences from decreased system efficiency and prolonged query processing times to potential disruptions in critical business operations.In this paper, we present Puppy, an automated approach for detecting PDBs in DBMSs using limited-optimization plan construction. The key idea is to compare the performance with the plan generated with all optimization operations enabled, against the plan generated with only a subset of optimization operations in the same DBMS. If the response time of the plan with the limited optimization set is shorter than that of the fully optimized plan, it indicates a potential PDB. Specifically, Puppy first generates queries that incorporate multiple optimization sequences, guided by optimization operation sequence coverage. Secondly, Puppy analyzes the query plan and selectively disables specific optimizations to construct the limited optimization plan. We evaluate Puppy on five widely-used DBMSs, namely MySQL, Percona, TiDB, PolarDB, and PostgreSQL against the state-of-the-art DBMS performance testing tools Apollo and Amoeba. More importantly, Puppy reports 62 PDBs, with 54 anomalies confirmed as previously unknown bugs.","2025","2025-11-25 22:30:03","2025-11-25 22:30:03","","679–690","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CHAWXVD4","conferencePaper","2024","Lajko, Mark; Csuvik, Viktor; Gyimothy, Tibor; Vidacs, Laszlo","Automated Program Repair with the GPT Family, including GPT-2, GPT-3 and CodeX","Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair","979-8-4007-0577-9","","10.1145/3643788.3648021","https://doi.org/10.1145/3643788.3648021","Automated Program Repair (APR) is a promising approach for addressing software defects and improving software reliability. There are various approaches to APR, including using Machine Learning (ML) techniques such as neural networks and evolutionary algorithms, as well as more traditional methods such as static analysis and symbolic execution. In recent years, there has been growing interest in using ML techniques for APR, including the use of large language models such as GPT-2 and GPT-3. These models have the ability to generate human-like text and code, making them well-suited for tasks such as generating repair patches for defective programs. In this paper, we explore the use of the GPT family (including GPT-2, GPT-J-6B, GPT-3 and Codex) for APR of JavaScript programs and evaluate their performance in terms of the number and quality of repair patches generated. Our results show that these state-of-the-art language models are able to generate repair patches that successfully fix the defects in the JavaScript programs, with Codex performing slightly better overall. To be precise, in our self-assembled dataset, Codex was able to generate 108 repair patches that are exactly the same as the developer fix for the first try. If we consider multiple patch generations, up to 201 buggy programs are being repaired automatically from the 1559 evaluation dataset (12.89%).","2024","2025-11-25 22:30:03","2025-11-25 22:30:03","","34–41","","","","","","","APR '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","automated program repair; codex; JavaScript; GPT-3; transformers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SNKY9ADE","journalArticle","2025","Nguyen, Loi Ngo Duc; Islam, Tahiatul; Wang, Theron; Lenz, Sam; Kellogg, Martin","Static Program Reduction via Type-Directed Slicing","Proc. ACM Softw. Eng.","","","10.1145/3728968","https://doi.org/10.1145/3728968","A traditional program slicer constructs a smaller variant of a target program that computes the same result with respect to some target variable—that is, program slicing preserves the original program’s run-time semantics. We propose type-directed slicing, which constructs a smaller program that guarantees that a typechecker will produce the same result on the sliced program when considering only a target program location—that is, a type-directed slicer preserves the target program’s compile-time semantics, from the view of a specific typechecker, with respect to some location. Type-directed slicing is a useful debugging aid for designers and maintainers of typecheckers. When a typechecker produces an unexpected result (a crash, a false positive warning, a missed warning, etc.) on a large codebase, the user typically reports a bug to the maintainers of the typechecker without an accompanying test case. State-of-the-art approaches to this program reduction problem are dynamic: they require repeatedly running the typechecker to validate minimizations. A type-directed slicer solves this problem statically, without rerunning the typechecker, by exploiting the modularity inherent in a typechecker’s type rules. Our prototype type-directed slicer for Java is fully automatic, can operate on incomplete programs, and is fast. It produces a small test case that preserves typechecker misbehavior for 25 of 28 (89%) historical bugs from the issue trackers of three widely-used typecheckers: the Java compiler itself, NullAway, and the Checker Framework; in each of these 25 cases, it preserved the typechecker’s behavior even without the classpath of the target program. And, it runs in under a minute on each benchmark, whose size ranges up to millions of lines of code, on a free-tier CI runner.","2025-06","2025-11-25 22:30:03","2025-11-25 22:30:03","","","","ISSTA","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","program minimization; program reduction; type systems; typechecker","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SUQNS4PC","bookSection","2025","Mai, Yubo; Gao, Zhipeng; Wang, Haoye; Bi, Tingting; Hu, Xing; Xia, Xin; Sun, Jianling","Towards Better Answers: Automated Stack Overflow Post Updating","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00024","Utilizing code snippets on Stack Overflow (SO) is a common practice among developers for problem-solving. Although SO code snippets serve as valuable resources, it is important to acknowledge their imperfections, reusing problematic code snippets can lead to the introduction of suboptimal or buggy code into software projects. SO comments often point out weaknesses of a post and provide valuable insights to improve the quality of answers, while SO comments are usually missed and/or ignored, leaving these problematic code snippets untouched. In this work, we first investigate the task of automatic SO posts updating based on their associated comments. We introduce a novel framework, named Soup (Stack Overflow Updator for Post) for this task. Soup addresses two key tasks: Valid Comment-Edit Prediction (VCP) and Automatic Post Updating (APU). We fine-tuned a large language model, CodeLlama, using low-rank adaptation techniques to complete the VCP task, and constructed a dataset containing 78k valid comment-edit pairs for the APU task. Subsequently, we tested the performance of multiple large language models on the APU task. Extensive experimental results show the promising performance of our model over a set of benchmarks. Moreover, we also perform an in-the-wild evaluation on Stack Overflow, we submitted 50 edits generated by our approach to Stack Overflow posts and 21 of them have been verified and accepted by SO maintainers, further proving the practical value of Soup.","2025","2025-11-25 22:30:03","2025-11-25 22:30:03","","591–603","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TLMD23DT","conferencePaper","2023","Finnie-Ansley, James; Denny, Paul; Luxton-Reilly, Andrew; Santos, Eddie Antonio; Prather, James; Becker, Brett A.","My AI Wants to Know if This Will Be on the Exam: Testing OpenAI’s Codex on CS2 Programming Exercises","Proceedings of the 25th Australasian Computing Education Conference","978-1-4503-9941-8","","10.1145/3576123.3576134","https://doi.org/10.1145/3576123.3576134","The introduction of OpenAI Codex sparked a surge of interest in the impact of generative AI models on computing education practices. Codex is also the underlying model for GitHub Copilot, a plugin which makes AI-generated code accessible to students through auto-completion in popular code editors. Research in this area, particularly on the educational implications, is nascent and has focused almost exclusively on introductory programming (or CS1) questions. Very recent work has shown that Codex performs considerably better on typical CS1 exam questions than most students. It is not clear, however, what Codex’s limits are with regard to more complex programming assignments and exams. In this paper, we present results detailing how Codex performs on more advanced CS2 (data structures and algorithms) exam questions taken from past exams. We compare these results to those of students who took the same exams under normal conditions, demonstrating that Codex outscores most students. We consider the implications of such tools for the future of undergraduate computing education.","2023","2025-11-25 22:30:03","2025-11-25 22:30:03","","97–104","","","","","","","ACE '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Melbourne, VIC, Australia","","","","artificial intelligence; AI; machine learning; Codex; CS1; GitHub; OpenAI; code generation; copilot; novice programming; AlphaCode; academic integrity; deep learning; neural networks; introductory programming; GPT-3; algorithms; CS2; data structures; DeepMind","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2Z6G4RD4","journalArticle","2025","Lyu, Michael R.; Ray, Baishakhi; Roychoudhury, Abhik; Tan, Shin Hwei; Thongtanunam, Patanamon","Automatic Programming: Large Language Models and Beyond","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3708519","https://doi.org/10.1145/3708519","Automatic programming has seen increasing popularity due to the emergence of tools like GitHub Copilot which rely on Large Language Models (LLMs). At the same time, automatically generated code faces challenges during deployment due to concerns around quality and trust. In this article, we study automated coding in a general sense and study the concerns around code quality, security, and related issues of programmer responsibility. These are key issues for organizations while deciding on the usage of automatically generated code. We discuss how advances in software engineering such as program repair and analysis can enable automatic programming. We conclude with a forward looking view, focusing on the programming environment of the near future, where programmers may need to switch to different roles to fully utilize the power of automatic programming. Automated repair of automatically generated programs from LLMs can help produce higher assurance code from LLMs, along with evidence of assurance.","2025-05","2025-11-25 22:30:04","2025-11-25 22:30:04","","","","5","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Automated Program Repair; AI-based coding; Trustworthy Software","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HNETDQAI","conferencePaper","2024","Li, Zhong; Zhang, Chong; Pan, Minxue; Zhang, Tian; Li, Xuandong","AACEGEN: Attention Guided Adversarial Code Example Generation for Deep Code Models","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695500","https://doi.org/10.1145/3691620.3695500","Adversarial code examples are important to investigate the robustness of deep code models. Existing work on adversarial code example generation has shown promising results yet still falls short in practical applications due to either the high number of model invocations or the limited naturalness of generated examples. In this paper, we propose AaceGEN, an attention-guided adversarial code example generation method for deep code models. The key idea of AaceGEN is to utilize the attention distributions behind deep code models to guide the generation of adversarial code examples. As such, the code elements critical for model predictions could be prioritized for exploration, enhancing the effectiveness and efficiency of adversarial code example generation. In addition, AaceGEN implements a code transformation library providing diverse semantic-preserving code transformations for various code elements, and further conducts a search under the constraint of a maximum number of allowable code transformations to generate adversarial code examples with subtlety and stealth. Our extensive experiments on 9 diverse subjects, taking into account different software engineering tasks and varied deep code models, demonstrate that AaceGEN outperforms 3 baseline approaches under comprehensive evaluation.","2024","2025-11-25 22:30:04","2025-11-25 22:30:04","","1245–1257","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","adversarial example; code transformation; deep code model; search-based testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5GIC6CNQ","journalArticle","2025","Li, Ao; Kang, Byeongjee; Vikram, Vasudev; Laybourn, Isabella; Dharanikota, Samvid; Tiwari, Shrey; Padhye, Rohan","Fray: An Efficient General-Purpose Concurrency Testing Platform for the JVM","Proc. ACM Program. Lang.","","","10.1145/3764119","https://doi.org/10.1145/3764119","Concurrency bugs are hard to discover and reproduce, even in well-synchronized programs that are free of data races. Thankfully, prior work on controlled concurrency testing (CCT) has developed sophisticated algorithms—such as partial-order based and selectively uniform sampling—to effectively search over the space of thread interleavings. Unfortunately, in practice, these techniques cannot easily be applied to real-world Java programs due to the difficulties of controlling concurrency in the presence of the managed runtime and complex synchronization primitives. So, mature Java projects that make heavy use of concurrency still rely on naive repeated stress testing in a loop. In this paper, we take a first-principles approach for elucidating the requirements and design space to enable CCT on arbitrary real-world JVM applications. We identify practical challenges with classical design choices described in prior work—such as concurrency mocking, VM hacking, and OS-level scheduling—that affect bug-finding effectiveness and/or the scope of target applications that can be easily supported. Based on these insights, we present Fray, a new platform for performing push-button concurrency testing (beyond data races) of JVM programs. The key design principle behind Fray is to orchestrate thread interleavings without replacing existing concurrency primitives, using a concurrency control mechanism called shadow locking for faithfully expressing the set of all possible program behaviors. With full concurrency control, Fray can test applications using a number of search algorithms from a simple random walk to sophisticated techniques like PCT, POS, and SURW. In an empirical evaluation on 53 benchmark programs with known bugs (SCTBench and JaConTeBe), Fray with random walk finds 70% more bugs than JPF and 77% more bugs than RR's chaos mode. We also demonstrate Fray's push-button applicability on 2,664 tests from Apache Kafka, Lucene, and Google Guava. In these mature projects, Fray successfully discovered 18 real-world concurrency bugs that can cause 371 of the existing tests to fail under specific interleavings. We believe that Fray serves as a bridge between classical academic research and industrial practice— empowering developers with advanced concurrency testing algorithms that demonstrably uncover more bugs, while simultaneously providing researchers a platform for large-scale evaluation of search techniques.","2025-10","2025-11-25 22:30:04","2025-11-25 22:30:04","","","","OOPSLA2","9","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Debugging; Controlled Concurrency Testing; JVM","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RY2V3B5I","conferencePaper","2025","Chen, Hao; Schelter, Sebastian","Towards Automated Task-Aware Data Validation","Proceedings of the Workshop on Data Management for End-to-End Machine Learning","979-8-4007-1924-0","","10.1145/3735654.3735939","https://doi.org/10.1145/3735654.3735939","Data is a central resource for modern enterprises and institutions, and data validation is essential for ensuring the reliability of downstream applications. However, a major limitation of existing automated data unit testing frameworks is that they ignore the specific requirements of the tasks that consume the data. This paper introduces a task-aware approach to data validation that leverages large language models to generate customized data unit tests based on the semantics of downstream code. We present tadv, a prototype system that analyzes task code and dataset profiles to identify data access patterns, infer implicit data assumptions, and produce executable code for data unit tests. We evaluate our prototype with a novel benchmark comprising over 100 downstream tasks across two datasets, including annotations of their column access patterns and support for assessing the impact of synthetically injected data errors. We demonstrate that tadv outperforms task-agnostic baselines in detecting the data columns accessed by downstream tasks and generating data unit tests that account for the end-to-end impact of data errors. We make our benchmark and prototype code publicly available.","2025","2025-11-25 22:30:04","2025-11-25 22:30:04","","","","","","","","","DEEM '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Berlin, Germany","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4SW9JU25","journalArticle","2025","He, Yumeng; Nandi, Chandrakana; Pai, Sreepathi","Formalizing Linear Motion G-Code for Invariant Checking and Differential Testing of Fabrication Tools","Proc. ACM Program. Lang.","","","10.1145/3763106","https://doi.org/10.1145/3763106","The computational fabrication pipeline for 3D printing is much like a compiler — users design models in Computer Aided Design (CAD) tools that are lowered to polygon meshes to be ultimately compiled to machine code by 3D slicers. For traditional compilers and programming languages, techniques for checking program invariants are well-established. Similarly, methods like differential testing are frequently used to uncover bugs in compilers themselves, which makes them more reliable. The fabrication pipeline would benefit from similar techniques but traditional approaches do not directly apply to the representations used in this domain. Unlike traditional programs, 3D models exist both as geometric objects (a CAD model or a polygon mesh) as well as machine code that ultimately runs on the hardware. The machine code, like in traditional compiling, is affected by many factors like the model, the slicer being used, and numerous user-configurable parameters that control the slicing process. In this work, we propose a new algorithm for lifting G-code (a common language used in many fabrication pipelines) by denoting a G-code program to a set of cuboids, and then defining an approximate point cloud representation for efficiently operating on these cuboids. Our algorithm opens up new opportunities: we show three use cases that demonstrate how it enables (1) error localization in CAD models through invariant checking, (2) quantitative comparisons between slicers, and (3) evaluating the efficacy of mesh repair tools. We present a prototype implementation of our algorithm in a tool, GlitchFinder, and evaluate it on 58 real-world CAD models. Our results show that GlitchFinder is particularly effective in identifying slicing issues due to small features, can highlight differences in how popular slicers (Cura and PrusaSlicer) slice the same model, and can identify cases where mesh repair tools (MeshLab and Meshmixer) introduce new errors during repair.","2025-10","2025-11-25 22:30:04","2025-11-25 22:30:04","","","","OOPSLA2","9","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","differential testing; G-code; invariant checking; operational semantics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SKT9L9WH","conferencePaper","2024","Hu, Yongxiang; Jin, Hailiang; Wang, Xuan; Gu, Jiazhen; Guo, Shiyu; Chen, Chaoyi; Wang, Xin; Zhou, Yangfan","AutoConsis: Automatic GUI-driven Data Inconsistency Detection of Mobile Apps","Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice","979-8-4007-0501-4","","10.1145/3639477.3639748","https://doi.org/10.1145/3639477.3639748","In industrial practice, many bugs in commercial mobile apps manifest as self-conflicts of data presented in the GUI (Graphical User Interface). Such data inconsistency bugs can bring confusion to the users and deteriorate user experiences. They are a major target of industrial testing practice. However, due to the complication and diversity of GUI implementation and data presentation (e.g., the ways to present the data in natural language), detecting data inconsistency bugs is a very challenging task. It still largely relies on manual efforts. To reduce such human efforts, we proposed AutoConsis, an automated data inconsistency testing tool we designed for Meituan. one of the largest E-commerce providers with over 600 million transacting users. AutoConsis can automatically analyze GUI pages via a multi-modal deep-learning model and extract target data from textual phrases leveraging LLMs (Large Language Models). With these extracted data, their inconsistencies can then be detected. We evaluate the design of AutoConsis via a set of ablation experiments. Moreover, we demonstrate the effectiveness of AutoConsis when applying it to real-world commercial mobile apps with eight representative cases.","2024","2025-11-25 22:30:04","2025-11-25 22:30:04","","137–146","","","","","","","ICSE-SEIP '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","in-context learning; automatic testing; functional bug; mobile apps","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M55B82UF","conferencePaper","2024","Wei, Moshi; Harzevili, Nima Shiri; Huang, Yuekai; Yang, Jinqiu; Wang, Junjie; Wang, Song","Demystifying and Detecting Misuses of Deep Learning APIs","Proceedings of the IEEE/ACM 46th International Conference on Software Engineering","979-8-4007-0217-4","","10.1145/3597503.3639177","https://doi.org/10.1145/3597503.3639177","Deep Learning (DL) libraries have significantly impacted various domains in computer science over the last decade. However, developers often face challenges when using the DL APIs, as the development paradigm of DL applications differs greatly from traditional software development. Existing studies on API misuse mainly focus on traditional software, leaving a gap in understanding API misuse within DL APIs. To address this gap, we present the first comprehensive study of DL API misuse in TensorFlow and PyTorch. Specifically, we first collected a dataset of 4,224 commits from the top 200 most-starred projects using these two libraries and manually identified 891 API misuses. We then investigated the characteristics of these misuses from three perspectives, i.e., types, root causes, and symptoms. We have also conducted an evaluation to assess the effectiveness of the current state-of-the-art API misuse detector on our 891 confirmed API misuses. Our results confirmed that the state-of-the-art API misuse detector is ineffective in detecting DL API misuses. To address the limitations of existing API misuse detection for DL APIs, we propose LLMAPIDet, which leverages Large Language Models (LLMs) for DL API misuse detection and repair. We build LLMAPIDet by prompt-tuning a chain of ChatGPT prompts on 600 out of 891 confirmed API misuses and reserve the rest 291 API misuses as the testing dataset. Our evaluation shows that LLMAPIDet can detect 48 out of the 291 DL API misuses while none of them can be detected by the existing API misuse detector. We further evaluate LLMAPIDet on the latest versions of 10 GitHub projects. The evaluation shows that LLMAPIDet can identify 119 previously unknown API misuses and successfully fix 46 of them.","2024","2025-11-25 22:30:04","2025-11-25 22:30:04","","","","","","","","","ICSE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","empirical study; API misuse; deep learning APIs; detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3GAGK4LK","conferencePaper","2024","Zheng, Xi; Mok, Aloysius K.; Piskac, Ruzica; Lee, Yong Jae; Krishnamachari, Bhaskar; Zhu, Dakai; Sokolsky, Oleg; Lee, Insup","Testing Learning-Enabled Cyber-Physical Systems with Large-Language Models: A Formal Approach","Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering","979-8-4007-0658-5","","10.1145/3663529.3663779","https://doi.org/10.1145/3663529.3663779","The integration of machine learning into cyber-physical systems (CPS) promises enhanced efficiency and autonomous capabilities, revolutionizing fields like autonomous vehicles and telemedicine. This evolution necessitates a shift in the software development life cycle, where data and learning are pivotal. Traditional verification and validation methods are inadequate for these AI-driven systems. This study focuses on the challenges in ensuring safety in learning-enabled CPS. It emphasizes the role of testing as a primary method for verification and validation, critiques current methodologies, and advocates for a more rigorous approach to assure formal safety.","2024","2025-11-25 22:30:04","2025-11-25 22:30:04","","467–471","","","","","","","FSE 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","AI-based Systems; automata-learning; LLM-based Testing; model-based testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JA35VCJJ","conferencePaper","2025","Raihan, Nishat; Siddiq, Mohammed Latif; Santos, Joanna C.S.; Zampieri, Marcos","Large Language Models in Computer Science Education: A Systematic Literature Review","Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1","979-8-4007-0531-1","","10.1145/3641554.3701863","https://doi.org/10.1145/3641554.3701863","Large language models (LLMs) are becoming increasingly better at a wide range of Natural Language Processing tasks (NLP), such as text generation and understanding. Recently, these models have extended their capabilities to coding tasks, bridging the gap between natural languages (NL) and programming languages (PL). Foundational models such as the Generative Pre-trained Transformer (GPT) and LLaMA series have set strong baseline performances in various NL and PL tasks. Additionally, several models have been fine-tuned specifically for code generation, showing significant improvements in code-related applications. Both foundational and fine-tuned models are increasingly used in education, helping students write, debug, and understand code. We present a comprehensive systematic literature review to examine the impact of LLMs in computer science and computer engineering education. We analyze their effectiveness in enhancing the learning experience, supporting personalized education, and aiding educators in curriculum development. We address five research questions to uncover insights into how LLMs contribute to educational outcomes, identify challenges, and suggest directions for future research.","2025","2025-11-25 22:30:04","2025-11-25 22:30:04","","938–944","","","","","","","SIGCSETS 2025","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Pittsburgh, PA, USA","","","","large language models; code generation; cs education","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7IJ643PG","conferencePaper","2025","Hundal, Rajdeep Singh; Xiao, Yan; Cao, Xiaochun; Dong, Jin Song; Rigger, Manuel","On the Mistaken Assumption of Interchangeable Deep Reinforcement Learning Implementations","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00222","https://doi.org/10.1109/ICSE55347.2025.00222","Deep Reinforcement Learning (DRL) is a paradigm of artificial intelligence where an agent uses a neural network to learn which actions to take in a given environment. DRL has recently gained traction from being able to solve complex environments like driving simulators, 3D robotic control, and multiplayer-online-battle-arena video games. Numerous implementations of the state-of-the-art algorithms responsible for training these agents, like the Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) algorithms, currently exist. However, studies make the mistake of assuming implementations of the same algorithm to be consistent and thus, interchangeable. In this paper, through a differential testing lens, we present the results of studying the extent of implementation inconsistencies, their effect on the implementations' performance, as well as their impact on the conclusions of prior studies under the assumption of interchangeable implementations. The outcomes of our differential tests showed significant discrepancies between the tested algorithm implementations, indicating that they are not interchangeable. In particular, out of the five PPO implementations tested on 56 games, three implementations achieved superhuman performance for 50% of their total trials while the other two implementations only achieved superhuman performance for less than 15% of their total trials. Furthermore, the performance among the high-performing PPO implementations was found to differ significantly in nine games. As part of a meticulous manual analysis of the implementations' source code, we analyzed implementation discrepancies and determined that code-level inconsistencies primarily caused these discrepancies. Lastly, we replicated a study and showed that this assumption of implementation interchangeability was sufficient to flip experiment outcomes. Therefore, this calls for a shift in how implementations are being used. In addition, we recommend for (1) replicability studies for studies mistakenly assuming implementation inter-changeability, (2) DRL researchers and practitioners to adopt the differential testing methodology proposed in this paper to combat implementation inconsistencies, and (3) the use of large environment suites.","2025","2025-11-25 22:30:04","2025-11-25 22:30:04","","2225–2237","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","differential testing; reinforcement learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PQJ2RI6X","conferencePaper","2025","Fakhoury, Sarah; Kuppe, Markus; Lahiri, Shuvendu K.; Ramananandro, Tahina; Swamy, Nikhil","3DGen: AI-Assisted Generation of Provably Correct Binary Format Parsers","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00173","https://doi.org/10.1109/ICSE55347.2025.00173","Improper parsing of attacker-controlled input is a leading source of software security vulnerabilities, especially when programmers transcribe informal format descriptions in RFCs into efficient parsing logic in low-level, memory unsafe languages. Several researchers have proposed formal specification languages for data formats from which efficient code can be extracted. However, distilling informal requirements into formal specifications is challenging and, despite their benefits, new, formal languages are hard for people to learn and use.In this work, we present 3DGen, a framework that makes use of AI agents to transform mixed informal input, including natural language documents (i.e., RFCs) and example inputs into format specifications in a language called 3D. To support humans in understanding and trusting the generated specifications, 3DGen uses symbolic methods to also synthesize test inputs that can be validated against an external oracle. Symbolic test generation also helps in distinguishing multiple plausible solutions. Through a process of repeated refinement, 3DGen produces a 3D specification that conforms to a test suite, and which yields safe, efficient, provably correct, parsing code in C.We have evaluated 3DGen on 20 Internet standard formats, demonstrating the potential for AI-agents to produce formally verified C code at a non-trivial scale. A key enabler is the use of a domain-specific language to limit AI outputs to a class for which automated, symbolic analysis is tractable.","2025","2025-11-25 22:30:04","2025-11-25 22:30:04","","2535–2547","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","code generation; agentic ai systems; trustworthy AI programming","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"32Z5VDAI","conferencePaper","2025","Luo, Han; Tang, Yixuan; Zhang, Jingxuan; Zhu, Xuanyan; Li, Jiayi; Zou, Weiqin","SRLRF: Fine-Grained Root Cause Analysis and Prediction for Compiler Optimization Defects","Proceedings of the 16th International Conference on Internetware","979-8-4007-1926-4","","10.1145/3755881.3755906","https://doi.org/10.1145/3755881.3755906","Compiler defects, especially optimization defects, pose significant threats to software systems. Diagnosing their root causes is crucial for efficient debugging and fixing but remains highly challenging due to complicated interactions of optimizations and ambiguity in root cause prediction. To address these challenges, in this paper, we propose a novel approach, which leverages a data-driven approach and the strengths of Large Language Models (LLMs) for automated root cause analysis and prediction. Specifically, we first extract and iteratively summarize the debugging information from historically fixed optimization defects, and ultimately construct a systematic taxonomy with eight root cause categories. Then, based on this taxonomy, we propose SRLRF, which leverages a domain-specific LLM (i.e., RTA) and a general-purpose LLM (i.e., Llama3.1) to achieve multiple root cause categories’ prediction. Finally, SRLRF integrates stacking ensemble learning to train the prediction model to improve prediction performance. The experimental results on 5,573 GCC optimization defects show that SRLRF is able to accurately predict the root causes of 63.11&nbsp;% optimization defects, and significantly outperforms four baselines with the average improvements ranging from 16.98&nbsp;% to 179.99&nbsp;%.","2025","2025-11-25 22:30:04","2025-11-25 22:30:04","","108–118","","","","","","","Internetware '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Compiler Optimization Defects; Empirical Software Engineering; Root Cause Classification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2RQLCD8M","journalArticle","2024","Zhou, Chijin; Qian, Bingzhou; Go, Gwihwan; Zhang, Quan; Li, Shanshan; Jiang, Yu","PolyJuice: Detecting Mis-compilation Bugs in Tensor Compilers with Equality Saturation Based Rewriting","Proc. ACM Program. Lang.","","","10.1145/3689757","https://doi.org/10.1145/3689757","Tensor compilers are essential for deploying deep learning applications across various hardware platforms. While powerful, they are inherently complex and present significant challenges in ensuring correctness. This paper introduces PolyJuice, an automatic detection tool for identifying mis-compilation bugs in tensor compilers. Its basic idea is to construct semantically-equivalent computation graphs to validate the correctness of tensor compilers. The main challenge is to construct equivalent graphs capable of efficiently exploring the diverse optimization logic during compilation. We approach it from two dimensions. First, we propose arithmetic and structural equivalent rewrite rules to modify the dataflow of a tensor program. Second, we design an efficient equality saturation based rewriting framework to identify the most simplified and the most complex equivalent computation graphs for an input graph. After that, the outcome computation graphs have different dataflow and will likely experience different optimization processes during compilation. We applied it to five well-tested industrial tensor compilers, namely PyTorch Inductor, OnnxRuntime, TVM, TensorRT, and XLA, as well as two well-maintained academic tensor compilers, EinNet and Hidet. In total, PolyJuice detected 84 non-crash mis-compilation bugs, out of which 49 were confirmed with 20 fixed.","2024-10","2025-11-25 22:30:04","2025-11-25 22:30:04","","","","OOPSLA2","8","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Fuzzing; Equality Saturation; ML System; Tensor Compiler Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GLCXEPEK","journalArticle","2025","Fedchin, Aleksandr; Bai, Alexander Y.; Foster, Jeffrey S.","Metamorph: Synthesizing Large Objects from Dafny Specifications","Proc. ACM Program. Lang.","","","10.1145/3720448","https://doi.org/10.1145/3720448","Program synthesis aims to produce code that adheres to user-provided specifications. In this work, we focus on synthesizing sequences of calls to formally specified APIs to generate objects that satisfy certain properties. This problem is particularly relevant in automated test generation, where a test engine may need an object with specific properties to trigger a given execution path. Constructing instances of complex data structures may require dozens of method calls, but reasoning about consecutive calls is computationally expensive, and existing work typically limits the number of calls in the solution. In this paper, we focus on synthesizing such long sequences of method calls in the Dafny programming language. To that end, we introduce Metamorph, a synthesis tool that uses counterexamples returned by the Dafny verifier to reason about the effects of method calls one at a time, limiting the complexity of solver queries. We also aim to limit the overall number of SMT queries by comparing the counterexamples using two distance metrics we develop for guiding the synthesis process. In particular, we introduce a novel piecewise distance metric, which puts a provably correct lower bound on the number of method calls in the solution and allows us to frame the synthesis problem as weighted A* search. When computing piecewise distance, we view object states as conjunctions of atomic constraints, identify constraints that each method call can satisfy, and combine this information using integer programming. We evaluate Metamorph’s ability to generate large objects on six benchmarks defining key data structures: linked lists, queues, arrays, binary trees, and graphs. Metamorph can successfully construct programs that require up to 57 method calls per instance and compares favorably to an alternative baseline approach. Additionally, we integrate Metamorph with DTest, Dafny’s automated test generation toolkit, and show that Metamorph can synthesize test inputs for parts of the AWS Cryptographic Material Providers Library that DTest alone is not able to cover. Finally, we use Metamorph to generate executable bytecode for a simple virtual machine, demonstrating that the techniques described here are more broadly applicable in the context of specification-guided synthesis.","2025-04","2025-11-25 22:30:04","2025-11-25 22:30:04","","","","OOPSLA1","9","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Counterexamples; Dafny; Synthesis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G3TPGLQT","conferencePaper","2024","Kim, Myeongsoo; Sinha, Saurabh; Orso, Alessandro","Adaptive REST API Testing with Reinforcement Learning","Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering","979-8-3503-2996-4","","10.1109/ASE56229.2023.00218","https://doi.org/10.1109/ASE56229.2023.00218","Modern web services increasingly rely on REST APIs. Effectively testing these APIs is challenging due to the vast search space to be explored, which involves selecting API operations for sequence creation, choosing parameters for each operation from a potentially large set of parameters, and sampling values from the virtually infinite parameter input space. Current testing tools lack efficient exploration mechanisms, treating all operations and parameters equally (i.e., not considering their importance or complexity) and lacking prioritization strategies. Furthermore, these tools struggle when response schemas are absent in the specification or exhibit variants. To address these limitations, we present an adaptive REST API testing technique that incorporates reinforcement learning to prioritize operations and parameters during exploration. Our approach dynamically analyzes request and response data to inform dependent parameters and adopts a sampling-based strategy for efficient processing of dynamic API feedback. We evaluated our technique on ten RESTful services, comparing it against state-of-the-art REST testing tools with respect to code coverage achieved, requests generated, operations covered, and service failures triggered. Additionally, we performed an ablation study on prioritization, dynamic feedback analysis, and sampling to assess their individual effects. Our findings demonstrate that our approach outperforms existing REST API testing tools in terms of effectiveness, efficiency, and fault-finding ability.","2024","2025-11-25 22:30:04","2025-11-25 22:30:04","","446–458","","","","","","","ASE '23","","","","IEEE Press","Echternach, Luxembourg","","","","","","","","","","","","automated rest API testing; reinforcement learning for testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5QFFDAAF","conferencePaper","2025","Spiess, Claudio; Gros, David; Pai, Kunal Suresh; Pradel, Michael; Rabin, Md Rafiqul Islam; Alipour, Amin; Jha, Susmit; Devanbu, Prem; Ahmed, Toufique","Calibration and Correctness of Language Models for Code","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00040","https://doi.org/10.1109/ICSE55347.2025.00040","Machine learning models are widely used, but can also often be wrong. Users would benefit from a reliable indication of whether a given output from a given model should be trusted, so a rational decision can be made whether to use the output or not. For example, outputs can be associated with a confidence measure; if this confidence measure is strongly associated with likelihood of correctness, then the model is said to be well-calibrated.A well-calibrated confidence measure can serve as a basis for rational, graduated decision-making on how much review and care is needed when using generated code. Calibration has so far been studied in mostly non-generative (e.g., classification) settings, especially in software engineering. However, generated code can quite often be wrong: Given generated code, developers must decide whether to use directly, use after varying intensity of careful review, or discard model-generated code. Thus, calibration is vital in generative settings.We make several contributions. We develop a framework for evaluating the calibration of code-generating models. We consider several tasks, correctness criteria, datasets, and approaches, and find that, by and large, generative code models we test are not well-calibrated out of the box. We then show how calibration can be improved using standard methods, such as Platt scaling. Since Platt scaling relies on the prior availability of correctness data, we evaluate the applicability and generalizability of Platt scaling in software engineering, discuss settings where it has good potential for practical use, and settings where it does not. Our contributions will lead to better-calibrated decision-making in the current use of code generated by language models, and offers a framework for future research to further improve calibration methods for generative models in software engineering.","2025","2025-11-25 22:30:04","2025-11-25 22:30:04","","540–552","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","LLM; calibration; confidence measure","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"56C6IG7T","conferencePaper","2025","Liu, Hanzhi; Jiang, Yanyan; Xu, Chang","Understanding the Linux Kernel, Visually","Proceedings of the Twentieth European Conference on Computer Systems","979-8-4007-1196-1","","10.1145/3689031.3696095","https://doi.org/10.1145/3689031.3696095","Understanding the Linux kernel is challenging due to its large and complex program state. While existing kernel debugging tools provide full access to kernel states at arbitrary levels of detail, developers often spend a significant amount of time sifting through redundant information to find what is truly useful. Additionally, the textual results provided by traditional debuggers are often insufficient for expressing high-dimensional information in a readable manner.This paper presents Visualinux, the first debugging framework that can simplify the program state of the Linux kernel to a level that can be visually understood with low programming complexity and effort. Visualinux includes a domain-specific language for specifying simplifications of a kernel object graph, an SQL-like domain-specific language for customizing the simplified object graph, and a panel-based interactive debugger. Evaluation results show that Visualinux can visualize various complex kernel components and efficiently assist developers in diagnosing sophisticated kernel bugs.","2025","2025-11-25 22:30:04","2025-11-25 22:30:04","","1044–1060","","","","","","","EuroSys '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Rotterdam, Netherlands","","","","Debugging; Linux Kernel; Software Visualization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YAPSLE4B","journalArticle","2024","Mazurczyk, Wojciech; Lee, Dongwon; Vlachos, Andreas","Disinformation 2.0 in the Age of AI: A Cybersecurity Perspective","Commun. ACM","","0001-0782","10.1145/3624721","https://doi.org/10.1145/3624721","Why disinformation is a cyber threat.","2024-02","2025-11-25 22:30:04","2025-11-25 22:30:04","","36–39","","3","67","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UGC657JK","journalArticle","2023","Soldani, Jacopo","An Interview with Chunyang Chen - 2023 SIGSOFT Awardee","SIGSOFT Softw. Eng. Notes","","0163-5948","10.1145/3599975.3599981","https://doi.org/10.1145/3599975.3599981","Chunyang Chen received the 2023 SIGSOFT Early Career Researcher Award for outstanding contributions to the study of intelligent software development automation including automated mobile application development, software testing, migration, and accessibility. He received a Ph.D. in computer science from School of Computer Science and Engineering, Nanyang Technological University (Singapore). He is now a senior lecturer in the Faculty of Information Technology of the Monash University (Australia).","2023-06","2025-11-25 22:30:04","2025-11-25 22:30:04","","16–17","","3","48","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6SDAQY2I","conferencePaper","2025","Valle, Pablo","Automated Repair of Cyber-Physical Systems","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings","979-8-3315-3683-1","","10.1109/ICSE-Companion66252.2025.00060","https://doi.org/10.1109/ICSE-Companion66252.2025.00060","Cyber-Physical Systems (CPS) integrate digital technologies with physical processes and are common in different domains and industries, such as robotic systems, autonomous vehicles or satellites. Debugging and verification of CPS software consumes much of the development budget as it is often purely manual. To speed up this process, Automated Program Repair (APR) has been targeted for a long time. Although there have been advances in software APR and CPS verification techniques, research specifically on APR for CPSs is limited. This Ph.D. research project aims to develop scalable APR techniques for CPSs, addressing problems of fault localization, long test execution times, and fitness function limitations. A new method combining spectrum-based fault localization (SBFL) with patch generation and advanced artificial intelligence techniques will be investigated. The approach will be validated by empirical studies on open and industrial code bases of CPSs.","2025","2025-11-25 22:30:04","2025-11-25 22:30:04","","199–201","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","automated program repair; fault localization; cyber-physical systems; test input minimization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GUMWMQKG","conferencePaper","2024","Liu, Pei; Lin, Bo; Qin, Yihao; Weng, Cheng; Chen, Liqian","T-RAP: A Template-guided Retrieval-Augmented Vulnerability Patch Generation Approach","Proceedings of the 15th Asia-Pacific Symposium on Internetware","979-8-4007-0705-6","","10.1145/3671016.3672506","https://doi.org/10.1145/3671016.3672506","Vulnerabilities exert great burden on developers in terms of debugging and maintenance. Automated Vulnerability Repair(AVR) is considered as a promising approach to alleviate the burden of developers. Template-based automated program repair techniques have shown their effectiveness in fixing general bugs. However, due to the diverse root causes of vulnerabilities, it is challenging to construct sufficient repair templates to cover various vulnerabilities. In this paper, we introduce a Template-guided Retrieval-Augmented Patch generation approach, named T-RAP. Inspired by retrieval-augmented techniques that effectively utilize historical data, our approach leverages repair templates to extract similar vulnerability repair patches from the codebase. These patches then guide the process of generating vulnerability patches. To extract similar patches, we also propose a matching algorithm specifically designed for the retrieval-augmented vulnerability repair. This involves identifying similarities between numerous templates and vulnerabilities during the template-guided stage. Experimental results demonstrate that T-RAP outperforms all the studied AVR approaches, repairing 56.8% more vulnerabilities than VulRepair and 30.24% more than VulMaster. It can also accurately repair more types of real-world vulnerabilities than VulMaster. Additionally, we evaluated the effectiveness of our patch retriever. The results indicate that our template-guided retriever, which is based on our matching algorithm, outperforms the retrieval algorithm proposed in the recent retrieval-augmented patch generation approach RAP-Gen.","2024","2025-11-25 22:30:04","2025-11-25 22:30:04","","105–114","","","","","","","Internetware '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Macau, China","","","","Deep Learning; Automated Vulnerability Repair; Repair Template; Software Vulnerability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SLC9MGUS","conferencePaper","2024","Hu, Yuchen; Xu, Ke; Sun, Jialin; Fang, Xinwei; Shan, Weiwei; Wang, Xi; Jiang, Zhe","Make Each Iteration Count","Proceedings of the ACM Turing Award Celebration Conference - China 2024","979-8-4007-1011-7","","10.1145/3674399.3674482","https://doi.org/10.1145/3674399.3674482","Large Language Models (LLMs) is widely used for code debugging (e.g., C and Python) but face limitations in debugging Register Transfer Level (RTL) code largely due to data scarcity. This paper presents Make Each Iteration Count (MEIC), a novel framework for RTL debugging. Unlike traditional approaches heavily relying on prompt engineering, model tuning, and model training, MEIC employs an iterative process with LLM to address syntax and function errors efficiently. We also introduce an open-source dataset with 178 RTL errors for evaluation. Results demonstrate a 93% fix rate for syntax errors and a 78% fix rate for function errors.","2024","2025-11-25 22:30:04","2025-11-25 22:30:04","","236–238","","","","","","","ACM-TURC '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Changsha, China","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5CU99HGC","journalArticle","2024","Wu, Nan; Li, Yingjie; Yang, Hang; Chen, Hanqiu; Dai, Steve; Hao, Cong; Yu, Cunxi; Xie, Yuan","Survey of Machine Learning for Software-assisted Hardware Design Verification: Past, Present, and Prospect","ACM Trans. Des. Autom. Electron. Syst.","","1084-4309","10.1145/3661308","https://doi.org/10.1145/3661308","With the ever-increasing hardware design complexity comes the realization that efforts required for hardware verification increase at an even faster rate. Driven by the push from the desired verification productivity boost and the pull from leap-ahead capabilities of machine learning (ML), recent years have witnessed the emergence of exploiting ML-based techniques to improve the efficiency of hardware verification. In this article, we present a panoramic view of how ML-based techniques are embraced in hardware design verification, from formal verification to simulation-based verification, from academia to industry, and from current progress to future prospects. We envision that the adoption of ML-based techniques will pave the road for more scalable, more intelligent, and more productive hardware verification.","2024-06","2025-11-25 22:30:04","2025-11-25 22:30:04","","","","4","29","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","machine learning; formal verification; Hardware verification; simulation-based verification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XMEIE3J7","journalArticle","2024","Ni, Shengquan; Huang, Yicong; Wang, Zuozhi; Li, Chen","IcedTea: Efficient and Responsive Time-Travel Debugging in Dataflow Systems","Proc. VLDB Endow.","","2150-8097","10.14778/3712221.3712251","https://doi.org/10.14778/3712221.3712251","Dataflow systems have an increasing need to support a wide range of tasks in data-centric applications using latest techniques such as machine learning. These tasks often involve custom functions with complex internal states. Consequently, users need enhanced debugging support to understand runtime behaviors and investigate internal states of dataflows. Traditional forward debuggers allow users to follow the chronological order of operations in an execution. Therefore, a user cannot easily identify a past runtime behavior after an unexpected result is produced. In this paper, we present a novel time-travel debugging paradigm called IcedTea, which supports reverse debugging. In particular, in a dataflow's execution, which is inherently distributed across multiple operators, the user can periodically interact with the job and retrieve the global states of the operators. After the execution, the system allows the user to roll back the dataflow state to any past interactions. The user can use step instructions to repeat the past execution to understand how data was processed in the original execution. We give a full specification of this powerful paradigm, study how to reduce its runtime overhead and develop techniques to support debugging instructions responsively. Our experiments on real-world datasets and workflows show that IcedTea can support responsive time-travel debugging with low time and space overhead.","2024-11","2025-11-25 22:30:04","2025-11-25 22:30:04","","902–914","","3","18","","","","","","","","","","","","","","","","","Publisher: VLDB Endowment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3YSWQAHV","conferencePaper","2025","Ahmmed, Jobayer; Mahmud, Quazi I.; Shim, Junhyung; Li, Liyi; Jannesari, Ali; Cohen, Myra B.","Differential Testing for Sequential to Parallel Transformations","Proceedings of the SC '25 Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis","979-8-4007-1871-7","","10.1145/3731599.3767365","https://doi.org/10.1145/3731599.3767365","Many high-performance programs can benefit from parallelism creating orders of magnitude speedups in their performance. However, translating code into its parallel equivalent is challenging, time-consuming, and error-prone. In recent years there has been a move to automate this process, creating algorithms to perform translations. While automation removes the manual effort, it needs to be accompanied by strong validation to ensure the parallel program has the same behavior as the sequential one. Incorrect translation can lead to data races, poor performance, rounding problems, or unexpected behavior. In this paper, we present a dynamic validation approach called Seq2ParDiff&nbsp;that uses differential testing to check conformance of the parallel program to the original sequential program version. We evaluate Seq2ParDiff&nbsp;on two sets of benchmarks for OpenMP programs. In the first, we find 20 new faults, outperforming state of the art static techniques. In the second, we find many faults that other tools miss, however we are not as effective in finding some types of data races.","2025","2025-11-25 22:30:04","2025-11-25 22:30:04","","207–216","","","","","","","SC Workshops '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","OpenMP; Differential Testing; Parallel Computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C3CKV2TA","journalArticle","2025","Gu, Xiaodong; Chen, Meng; Lin, Yalan; Hu, Yuhan; Zhang, Hongyu; Wan, Chengcheng; Wei, Zhao; Xu, Yong; Wang, Juhong","On the Effectiveness of Large Language Models in Domain-Specific Code Generation","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3697012","https://doi.org/10.1145/3697012","Large language models (LLMs) such as ChatGPT have shown remarkable capabilities in code generation. Despite significant achievements, they rely on enormous training data to acquire a broad spectrum of open-domain knowledge. Besides, their evaluation revolves around open-domain benchmarks like HumanEval, which primarily consist of programming contests. Therefore, it is hard to fully characterize the intricacies and challenges associated with particular domains (e.g., Web, game, and math). In this article, we conduct an in-depth study of the LLMs in domain-specific code generation. Our results demonstrate that LLMs exhibit sub-optimal performance in generating domain-specific code, due to their limited proficiency in utilizing domain-specific libraries. We further observe that incorporating API knowledge as prompts can empower LLMs to generate more professional code. Based on these findings, we further investigate how to effectively incorporate API knowledge into the code generation process. We experiment with three strategies for incorporating domain knowledge, namely, external knowledge inquirer, chain-of-thought prompting, and chain-of-thought fine-tuning. We refer to these strategies as a new code generation approach called DomCoder. Experimental results show that all strategies of DomCoder improve the effectiveness of domain-specific code generation under certain settings.","2025-02","2025-11-25 22:30:04","2025-11-25 22:30:04","","","","3","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","large language models; code generation; domain-specific program generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DNAE8ZJK","conferencePaper","2024","Nichols, Daniel; Davis, Joshua H.; Xie, Zhaojun; Rajaram, Arjun; Bhatele, Abhinav","Can Large Language Models Write Parallel Code?","Proceedings of the 33rd International Symposium on High-Performance Parallel and Distributed Computing","979-8-4007-0413-0","","10.1145/3625549.3658689","https://doi.org/10.1145/3625549.3658689","Large language models are increasingly becoming a popular tool for software development. Their ability to model and generate source code has been demonstrated in a variety of contexts, including code completion, summarization, translation, and lookup. However, they often struggle to generate code for complex programs. In this paper, we study the capabilities of state-of-the-art language models to generate parallel code. In order to evaluate language models,we create a benchmark, ParEval, consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing. We use ParEval to evaluate the effectiveness of several state-of-the-art open- and closed-source language models on these tasks. We introduce novel metrics for evaluating the performance of generated code, and use them to explore how well each large language model performs for 12 different computational problem types and six different parallel programming models.","2024","2025-11-25 22:30:04","2025-11-25 22:30:04","","281–294","","","","","","","HPDC '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Pisa, Italy","","","","large language models; benchmarking; HPC; parallel code generation; performance evaluation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MSPHS6ED","journalArticle","2025","Zhong, Wenkang; Li, Chuanyi; Liu, Kui; Ge, Jidong; Luo, Bin; Bissyandé, Tegawendé F.; Ng, Vincent","Benchmarking and Categorizing the Performance of Neural Program Repair Systems for Java","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3688834","https://doi.org/10.1145/3688834","Recent years have seen a rise in Neural Program Repair (NPR) systems in the software engineering community, which adopt advanced deep learning techniques to automatically fix bugs. Having a comprehensive understanding of existing systems can facilitate new improvements in this area and provide practical instructions for users. However, we observe two potential weaknesses in the current evaluation of NPR systems: ① published systems are trained with varying data, and ② NPR systems are roughly evaluated through the number of totally fixed bugs. Questions such as what types of bugs are repairable for current systems cannot be answered yet. Consequently, researchers cannot make target improvements in this area and users have no idea of the real affair of existing systems. In this article, we perform a systematic evaluation of the existing nine state-of-the-art NPR systems. To perform a fair and detailed comparison, we (1) build a new benchmark and framework that supports training and validating the nine systems with unified data and (2) evaluate re-trained systems with detailed performance analysis, especially on the effectiveness and the efficiency. We believe our benchmark tool and evaluation results could offer practitioners the real affairs of current NPR systems and the implications of further facilitating the improvements of NPR.","2025-12","2025-11-25 22:30:05","2025-11-25 22:30:05","","","","1","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","empirical study; datasets; benchmark; program repair","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8IA5PA8K","bookSection","2025","Kim, Sehoon; Kim, Yonghyeon; Park, Dahyeon; Jeon, Yuseok; Yi, Jooyong; Kim, Mijung","Lightweight Concolic Testing via Path-Condition Synthesis for Deep Learning Libraries","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00202","Many techniques have been recently developed for testing deep learning (DL) libraries. Although these techniques have effectively improved API and code coverage and detected unknown bugs, they rely on blackbox fuzzing for input generation. Concolic testing (also known as dynamic symbolic execution) can be more effective in exploring diverse execution paths, but applying it to DL libraries is extremely challenging due to their inherent complexity. In this paper, we introduce the first concolic testing technique for DL libraries. Our technique offers a lightweight approach that significantly reduces the heavy overhead associated with traditional concolic testing. While symbolic execution maintains symbolic expressions for every variable with non-concrete values to build a path condition, our technique computes approximate path conditions by inferring branch conditions via inductive program synthesis. Despite potential imprecision from approximation, our method's light overhead allows for effective exploration of diverse execution paths within the complex implementations of DL libraries. We have implemented our tool, PathFinder, and evaluated it on PyTorch and TensorFlow. Our results show that PathFinder outperforms existing API-level DL library fuzzers by achieving 67% more branch coverage on average; up to 63% higher than TitanFuzz and 120% higher than FreeFuzz. PathFinder is also effective in bug detection, uncovering 61 crash bugs, 59 of which were confirmed by developers as previously unknown, with 32 already fixed.","2025","2025-11-25 22:30:05","2025-11-25 22:30:05","","2957–2969","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NCZRGJH5","conferencePaper","2023","Hu, Yongxiang; Gu, Jiazhen; Hu, Shuqing; Zhang, Yu; Tian, Wenjie; Guo, Shiyu; Chen, Chaoyi; Zhou, Yangfan","Appaction: Automatic GUI Interaction for Mobile Apps via Holistic Widget Perception","Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","979-8-4007-0327-0","","10.1145/3611643.3613885","https://doi.org/10.1145/3611643.3613885","In industrial practice, GUI (Graphic User Interface) testing of mobile apps still inevitably relies on huge manual efforts. The major efforts are those on understanding the GUIs, so that testing scripts can be written accordingly. Quality assurance could therefore be very labor-intensive, especially for modern commercial mobile apps, where one may include tremendous, diverse, and complex GUIs, e.g., those for placing orders of different commercial items. To reduce such human efforts, we propose Appaction, a learning-based automatic GUI interaction approach we developed for Meituan, one of the largest E-commerce providers with over 600 million users. Appaction can automatically analyze the target GUI and understand what each input of the GUI is about, so that corresponding valid inputs can be entered accordingly. To this end, Appaction adopts a multi-modal model to learn from human experiences in perceiving a GUI. This allows it to infer corresponding valid input events that can properly interact with the GUI. In this way, the target app can be effectively exercised. We present our experiences in Meituan on applying Appaction to popular commercial apps. We demonstrate the effectiveness of Appaction in GUI analysis, and it can perform correct interactions for numerous form pages.","2023","2025-11-25 22:30:05","2025-11-25 22:30:05","","1786–1797","","","","","","","ESEC/FSE 2023","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: San Francisco, CA, USA","","","","Testing; GUI Interaction; Mobile Apps","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FCAYFA78","conferencePaper","2025","Zhang, Yuxin; Chen, Sen; Xie, Xiaofei; Liu, Zibo; Fan, Lingling","Scenario-Driven and Context-Aware Automated Accessibility Testing for Android Apps","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00093","https://doi.org/10.1109/ICSE55347.2025.00093","Mobile accessibility is increasingly important nowadays as it enables people with disabilities to use mobile applications to perform daily tasks. Ensuring mobile accessibility not only benefits those with disabilities but also enhances the user experience for all users, making applications more intuitive and user-friendly. Although numerous tools are available for testing and detecting accessibility issues in Android applications, a large number of false negatives and false positives persist due to limitations in the existing approaches, i.e., low coverage of UI scenarios and lack of consideration of runtime context. To address these problems, in this paper, we propose a scenario-driven exploration method for improving the coverage of UI scenarios, thereby detecting accessibility issues within the application, and ultimately reducing false negatives. Furthermore, to reduce false positives caused by not considering the runtime context, we propose a context-aware detection method that provides a more fine-grained detection capability.Our experimental results reveal that A11yScan can detect 1.7X more issues surpassing current state-of-the-art approaches like Xbot (3,991 vs. 2,321), thereby reducing the false negative rate by 41.84%. Additionally, it outperforms established UI exploration techniques such as SceneDroid (952 vs. 661 UI scenarios), while achieving comparable activity coverage to recent leading GUI testing tools like GPTDroid on the available dataset (73% vs. 71%). Meanwhile, with the context-aware detection method, A11yScan effectively reduces the false positive rate by 21%, validated with a 90.56% accuracy rate through a user study.","2025","2025-11-25 22:30:05","2025-11-25 22:30:05","","2777–2789","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","android app; accessibility testing; context-aware analysis; mobile accessibility; UI exploration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DPXXNQBR","conferencePaper","2024","Aljohani, Ahmed; Do, Hyunsook","From Fine-tuning to Output: An Empirical Investigation of Test Smells in Transformer-Based Test Code Generation","Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing","979-8-4007-0243-3","","10.1145/3605098.3636058","https://doi.org/10.1145/3605098.3636058","Researchers have recently leveraged transformer-based test code generation models to improve testing-related tasks (e.g., assert completion and test method generation). One such model, AthenaTest, has been recently introduced, and it has been well accepted by developers due to its ability to generate test cases similar to the ones written by developers. While the AthenaTest model provides adequate test coverage and improves test code readability, concerns remain regarding the quality of its generated test code, particularly the presence of test smells, which could degrade the test code's comprehension, readability, performance, and maintainability. In this paper, we investigated whether test cases generated by a transformer-based test code generation model - AthenaTest, contain test smells, including the presence of test smells in its fine-tuning dataset (Methods2Test). We evaluated seven test smells in AthenaTest's and Methods2Test's test cases. Our results reveal that 65% of Methods2Test's test cases contain test smells, which influence the output of AthenaTest, where 62% of its test cases contain at least one test smell. We also examined the design (test size and assertion frequency) of AthenaTest and Methods2Test test cases. Our findings show that AthenaTest tends to generate more assertions than the Methods2Test test case, which influenced the model to increase the occurrence rate of Assertion Roulette and Duplicate Assert smells.","2024","2025-11-25 22:30:05","2025-11-25 22:30:05","","1282–1291","","","","","","","SAC '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Avila, Spain","","","","test smells; athenatest; Methods2Test; transformer-based test code; unit test","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TCKVV4IM","conferencePaper","2023","Schiller, Nico; Xu, Xinyi; Bernhard, Lukas; Bars, Nils; Schloegel, Moritz; Holz, Thorsten","Novelty Not Found: Adaptive Fuzzer Restarts to Improve Input Space Coverage (Registered Report)","Proceedings of the 2nd International Fuzzing Workshop","979-8-4007-0247-1","","10.1145/3605157.3605171","https://doi.org/10.1145/3605157.3605171","Feedback-driven greybox fuzzing is one of the cornerstones of modern bug detection techniques. Its flexibility, automated nature, and effectiveness render it an indispensable tool for making software more secure. A key feature that enables its impressive performance is coverage feedback, which guides the fuzzer to explore different parts of the program. The most prominent way to use this feedback is novelty search, in which the fuzzer generates new inputs and only keeps those that have exercised a new program edge. This is grounded in the assumption that novel coverage is a proxy for interestingness. Bolstered by its widespread success, it is easy to overlook its limitations. Particularly the phenomenon of input shadowing, situations in which an “interesting” input is discarded because it does not contribute novel coverage, needs to be considered. This phenomenon limits the explorable input space and risks missing bugs when shadowed inputs are more amenable to mutations that would trigger bugs. In this work, we analyze input shadowing in more detail and find that multiple fuzzing runs of the same target exhibit a different basic block hit frequency despite overlapping code coverage. In other words, different fuzzing runs may find the same set of basic blocks but one might exercise specific basic blocks significantly more often than the other, and vice versa. To better distribute the frequency, we propose restarting the fuzzer to reset the fuzzing state, diversifying the fuzzer’s attention across basic blocks. Our preliminary evaluation of three Fuzzbench targets finds that fuzzer restarts effectively distribute the basic block hit frequencies and boost the achieved coverage by up to 9.3%.","2023","2025-11-25 22:30:05","2025-11-25 22:30:05","","12–20","","","","","","","FUZZING 2023","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Seattle, WA, USA","","","","Fuzzing; Software Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7YZ6VEBT","journalArticle","2024","Misu, Md Rakib Hossain; Lopes, Cristina V.; Ma, Iris; Noble, James","Towards AI-Assisted Synthesis of Verified Dafny Methods","Proc. ACM Softw. Eng.","","","10.1145/3643763","https://doi.org/10.1145/3643763","Large language models show great promise in many domains, including programming. A promise is easy to make but hard to keep, and language models often fail to keep their promises, generating erroneous code. A promising avenue to keep models honest is to incorporate formal verification: generating programs’ specifications as well as code so that the code can be proved correct with respect to the specifications. Unfortunately, existing large language models show a severe lack of proficiency in verified programming. In this paper, we demonstrate how to improve two pretrained models’ proficiency in the Dafny verification-aware language. Using 178 problems from the MBPP dataset, we prompt two contemporary models (GPT-4 and PaLM-2) to synthesize Dafny methods. We use three different types of prompts: a direct Contextless prompt; a Signature prompt that includes a method signature and test cases, and a Chain of Thought (CoT) prompt that decomposes the problem into steps and includes retrieval augmentation generated example problems and solutions. Our results show that GPT-4 performs better than PaLM-2 on these tasks and that both models perform best with the retrieval augmentation generated CoT prompt. GPT-4 was able to generate verified, human-evaluated, Dafny methods for 58% of the problems, however, GPT-4 managed only 19% of the problems with the Contextless prompt, and even fewer (10%) for the Signature prompt. We are thus able to contribute 153 verified Dafny solutions to MBPP problems, 50 that we wrote manually, and 103 synthesized by GPT-4. Our results demonstrate that the benefits of formal program verification are now within reach of code generating large language models. Likewise, program verification systems can benefit from large language models, whether to synthesize code wholesale, to generate specifications, or to act as a ""programmer’s verification apprentice"", to construct annotations such as loop invariants which are hard for programmers to write or verification tools to find. Finally, we expect that the approach we have pioneered here — generating candidate solutions that are subsequently formally checked for correctness — should transfer to other domains (e.g., legal arguments, transport signaling, structural engineering) where solutions must be correct, where that correctness must be demonstrated, explained and understood by designers and end-users.","2024-07","2025-11-25 22:30:05","2025-11-25 22:30:05","","","","FSE","1","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","LLM; Program Synthesis; Dafny; Program Verification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BBIULMD8","conferencePaper","2025","Yang, Aidan Z. H.; Kolak, Sophia; Hellendoorn, Vincent; Martins, Ruben; Goues, Claire Le","Revisiting Unnaturalness for Automated Program Repair in the Era of Large Language Models","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00089","https://doi.org/10.1109/ICSE55347.2025.00089","The problem of software quality has motivated the development of a variety of techniques for Automatic Program Repair (APR). Meanwhile, recent advances in AI and Large Language Models (LLMs) have produced orders of magnitude performance improvements over previous code generation techniques, affording promising opportunities for program repair and its constituent subproblems (e.g., fault localization, patch generation). Because models are trained on large volumes of code in which defects are relatively rare, they tend to both simultaneously perceive faulty code as unlikely (or ""unnatural"") and to produce generally correct code (which is more ""natural""). This paper comprehensively revisits the idea of (un)naturalness for program repair. We argue that, fundamentally, LLMs can only go so far on their own in reasoning about and fixing buggy code. This motivates the incorporation of traditional tools, which compress useful contextual and analysis information, as a complement to LLMs for repair. We interrogate the role of entropy at every stage of traditional repair, and show that it is indeed usefully complementary to classic techniques. We show that combining measures of naturalness with class Spectrum-Based Fault Localization (SBFL) approaches improves Top-5 scoring by 50% over SBFL alone. We show that entropy delta, or change in entropy induced by a candidate patch, can improve patch generation efficiency by 24 test suite executions per repair, on average, on our dataset. Finally, we show compelling results that entropy delta for patch classification is highly effective at distinguishing correct from overfitting patches. Overall, our results suggest that LLMs can effectively complement classic techniques for analysis and transformation, producing more efficient and effective automated repair techniques overall.","2025","2025-11-25 22:30:05","2025-11-25 22:30:05","","2561–2573","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","large language models; program repair; deep learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N6SJTTZZ","journalArticle","2025","Kwon, Jaeseong; Jang, Bongjun; Lee, Juneyoung; Heo, Kihong","Optimization-Directed Compiler Fuzzing for Continuous Translation Validation","Proc. ACM Program. Lang.","","","10.1145/3729275","https://doi.org/10.1145/3729275","Incorrect compiler optimizations can lead to unintended program behavior and security vulnerabilities. However, the enormous size and complexity of modern compilers make it challenging to ensure the correctness of optimizations. The problem becomes more severe as compiler engineers continuously add new optimizations to improve performance and support new language features. In this paper, we propose Optimuzz, a framework to effectively detect incorrect optimization bugs in such continuously changing compilers. The key idea is to combine two complementary techniques: directed grey-box fuzzing and translation validation. We design a novel optimization-directed fuzzing framework that efficiently generates input programs to trigger specific compiler optimizations. Optimuzz then use existing translation validation tools to verify the correctness of the optimizations on the input programs. We instantiate our approach for two major compilers, LLVM and TurboFan. The results show that Optimuzz can effectively detect miscompilation bugs in these compilers compared to the state-of-the-art tools. We also applied Optimuzz to the latest version of LLVM and discovered 55 new miscompilation bugs.","2025-06","2025-11-25 22:30:05","2025-11-25 22:30:05","","","","PLDI","9","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Compiler Optimization; Compiler Defect; Compiler Testing; Directed Fuzzing; Miscompilation; Translation Validation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K4682LRU","journalArticle","2024","Wang, Xu; Yu, Hongwei; Meng, Xiangxin; Cao, Hongliang; Zhang, Hongyu; Sun, Hailong; Liu, Xudong; Hu, Chunming","MTL-TRANSFER: Leveraging Multi-task Learning and Transferred Knowledge for Improving Fault Localization and Program Repair","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3654441","https://doi.org/10.1145/3654441","Fault localization (FL) and automated program repair (APR) are two main tasks of automatic software debugging. Compared with traditional methods, deep learning-based approaches have been demonstrated to achieve better performance in FL and APR tasks. However, the existing deep learning-based FL methods ignore the deep semantic features or only consider simple code representations. And for APR tasks, existing template-based APR methods are weak in selecting the correct fix templates for more effective program repair, which are also not able to synthesize patches via the embedded end-to-end code modification knowledge obtained by training models on large-scale bug-fix code pairs. Moreover, in most of FL and APR methods, the model designs and training phases are performed separately, leading to ineffective sharing of updated parameters and extracted knowledge during the training process. This limitation hinders the further improvement in the performance of FL and APR tasks. To solve the above problems, we propose a novel approach called MTL-TRANSFER, which leverages a multi-task learning strategy to extract deep semantic features and transferred knowledge from different perspectives. First, we construct a large-scale open-source bug datasets and implement 11 multi-task learning models for bug detection and patch generation sub-tasks on 11 commonly used bug types, as well as one multi-classifier to learn the relevant semantics for the subsequent fix template selection task. Second, an MLP-based ranking model is leveraged to fuse spectrum-based, mutation-based and semantic-based features to generate a sorted list of suspicious statements. Third, we combine the patches generated by the neural patch generation sub-task from the multi-task learning strategy with the optimized fix template selecting order gained from the multi-classifier mentioned above. Finally, the more accurate FL results, the optimized fix template selecting order, and the expanded patch candidates are combined together to further enhance the overall performance of APR tasks. Our extensive experiments on widely-used benchmark Defects4J show that MTL-TRANSFER outperforms all baselines in FL and APR tasks, proving the effectiveness of our approach. Compared with our previously proposed FL method TRANSFER-FL (which is also the state-of-the-art statement-level FL method), MTL-TRANSFER increases the faults hit by 8/11/12 on Top-1/3/5 metrics (92/159/183 in total). And on APR tasks, the number of successfully repaired bugs of MTL-TRANSFER under the perfect localization setting reaches 75, which is 8 more than our previous APR method TRANSFER-PR. Furthermore, another experiment to simulate the actual repair scenarios shows that MTL-TRANSFER can successfully repair 15 and 9 more bugs (56 in total) compared with TBar and TRANSFER, which demonstrates the effectiveness of the combination of our optimized FL and APR components.","2024-06","2025-11-25 22:30:05","2025-11-25 22:30:05","","","","6","33","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","automated program repair; multi-task learning; neural machine translation; Fault localization; transfer learning; deep neural networks","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CJ52SA75","bookSection","2025","Rong, Yuyang; Yu, Zhanghan; Weng, Zhenkai; Neuendorffer, Stephen; Chen, Hao","IRFuzzer: Specialized Fuzzing for LLVM Backend Code Generation","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00130","Modern compilers, such as LLVM, are complex. Due to their complexity, manual testing is unlikely to suffice, yet formal verification is difficult to scale. End-to-end fuzzing can be used, but it has difficulties in discovering LLVM backend problems for two reasons. First, frontend preprocessing and middle optimization shield the backend from seeing diverse inputs. Second, branch coverage cannot provide effective feedback as LLVM backend contains much reusable code.In this paper, we implement IRFuzzer to investigate the need of specialized fuzzing of the LLVM compiler backend. We focus on two approaches to improve the fuzzer: guaranteed input validity using constrained mutations to improve input diversity and new metrics to improve feedback quality. The mutator in IRFuzzer can generate a wide range of LLVM IR inputs, including structured control flow, vector types, and function definitions. The system instruments coding patterns in the compiler to monitor the execution status of instruction selection. The instrumentation not only provides new coverage feedback on the matcher table but also guides the mutator on architecture-specific intrinsics.We ran IRFuzzer on 29 mature LLVM backend targets. IRFuzzer discovered 78 new, confirmed bugs in LLVM upstream, none of which existing fuzzers could discover. This demonstrates that IRFuzzer is far more effective than existing fuzzers. Upon receiving our bug report, the developers have fixed 57 bugs and back-ported five fixes to LLVM 15, which shows that specialized fuzzing provides actionable insights to LLVM developers.","2025","2025-11-25 22:30:05","2025-11-25 22:30:05","","1986–1998","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U23XE3NK","journalArticle","2025","Gao, Cuiyun; Hu, Xing; Gao, Shan; Xia, Xin; Jin, Zhi","The Current Challenges of Software Engineering in the Era of Large Language Models","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3712005","https://doi.org/10.1145/3712005","With the advent of large language models (LLMs) in the AI area, the field of software engineering (SE) has also witnessed a paradigm shift. These models, by leveraging the power of deep learning and massive amounts of data, have demonstrated an unprecedented capacity to understand, generate, and operate programming languages. They can assist developers in completing a broad spectrum of software development activities, encompassing software design, automated programming, and maintenance, which potentially reduces huge human efforts. Integrating LLMs within the SE landscape (LLM4SE) has become a burgeoning trend, necessitating exploring this emergent landscape’s challenges and opportunities.The article aims at revisiting the software development lifecycle (SDLC) under LLMs, and highlighting challenges and opportunities of the new paradigm. The article first summarizes the overall process of LLM4SE, and then elaborates on the current challenges based on a through discussion. The discussion was held among more than 20 participants from academia and industry, specializing in fields such as SE and artificial intelligence. Specifically, we achieve 26 key challenges from seven aspects, including software requirement and design, coding assistance, testing code generation, code review, code maintenance, software vulnerability management, and data, training, and evaluation. We hope the achieved challenges would benefit future research in the LLM4SE field.","2025-05","2025-11-25 22:30:05","2025-11-25 22:30:05","","","","5","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; Challenges; LLM4SE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FBRLX3ML","conferencePaper","2024","Kim, YoungJae; Park, Yechan; Han, Seungheon; Yi, Jooyong","Enhancing the Efficiency of Automated Program Repair via Greybox Analysis","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695602","https://doi.org/10.1145/3691620.3695602","In this paper, we pay attention to the efficiency of automated program repair (APR). Recently, an efficient patch scheduling algorithm, Casino, has been proposed to improve APR efficiency. Inspired by fuzzing, Casino adaptively chooses the next patch candidate to evaluate based on the results of previous evaluations. However, we observe that Casino utilizes only the test results, treating the patched program as a black box. Inspired by greybox fuzzing, we propose a novel patch-scheduling algorithm, Gresino, which leverages the internal state of the program to further enhance APR efficiency. Specifically, Gresino monitors the hit counts of branches observed during the execution of the program and uses them to guide the search for a valid patch. Our experimental evaluation on the Defects4J benchmark and eight APR tools demonstrates the efficacy of our approach.","2024","2025-11-25 22:30:05","2025-11-25 22:30:05","","1719–1731","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","automated program repair; greybox analysis; patch scheduling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P5S7X68E","conferencePaper","2024","Suri, Samdyuti; Das, Sankar Narayan; Singi, Kapil; Dey, Kuntal; Sharma, Vibhu Saujanya; Kaulgud, Vikrant","Software Engineering Using Autonomous Agents: Are We There Yet?","Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering","979-8-3503-2996-4","","10.1109/ASE56229.2023.00174","https://doi.org/10.1109/ASE56229.2023.00174","Autonomous agents equipped with Large Language Models (LLMs) are rapidly gaining prominence as a revolutionary technology within the realm of Software Engineering. These intelligent and autonomous systems demonstrate the capacity to perform tasks and make independent decisions, leveraging their intrinsic reasoning and decision-making abilities.This paper delves into the current state of autonomous agents, their capabilities, challenges, and opportunities in Software Engineering practices. By employing different prompts (with or without context), we conclude the advantages of context-rich prompts for autonomous agents. Prompts with context enhance user requirement understanding, avoiding irrelevant details that could hinder task comprehension and degrade model performance, particularly when dealing with complex frameworks such as Spring Boot, Django, Flask, etc.This exploration is conducted using Auto-GPT (v0.3.0), an open-source application powered by GPT-3.5 and GPT-4 which intelligently connects the ""thoughts"" of Large Language Models (LLMs) to independently accomplish the assigned goals or tasks.","2024","2025-11-25 22:30:05","2025-11-25 22:30:05","","1855–1857","","","","","","","ASE '23","","","","IEEE Press","Echternach, Luxembourg","","","","","","","","","","","","large language models (LLMs); autonomous agents; SDLC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R442F96V","journalArticle","2023","Elran, Sharan R.; Harel, Yuval; Zoran, Amit R.","Fine Motor Skills: Operating standard robotic fabrication as a generative system","Proc. ACM Comput. Graph. Interact. Tech.","","","10.1145/3597627","https://doi.org/10.1145/3597627","Our concept of a digital spirit refers to the agency of technology in the creative process and the potential to use it as an autonomous agent that both influences and participates in the artistic process. By attributing a digital spirit, we are able to have a more collaborative and interactive relationship with technology, where the role of the artist is to guide and direct the design rather than fully control it. In this work, we extend the autonomy of this spirit to allow the machine and the material to interact freely and generate forms independently from our will. In this new approach to digital fabrication, we transform a deterministic mechanical procedure into a creative exploration. Rather than designing traditional machine operations, we propose machine-material operations tailored for a specific material context. As a test case, we show a digital agent that manipulates clay while allowing the material's internal dynamics to play a dominant role in the finalization of the design. Through this machine-material interaction, we are able to generate an abundance of forms and complexity that would be impossible to create by hand or through traditional programming. It exposes an untapped expressive potential of digital tools that is made possible by operating digital machines outside the paradigm of tight control.","2023-08","2025-11-25 22:30:05","2025-11-25 22:30:05","","","","2","6","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Ceramics; Digital Craft; Fabrication; Hybrid","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LPBL5WDX","conferencePaper","2024","Shariffdeen, Ridwan; Noller, Yannic; Mirchev, Martin; Ruan, Haifeng; Xiang, Gao; Costa, Andreea; Duck, Gregory J; Roychoudhury, Abhik","APR Competition 2024","Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair","979-8-4007-0577-9","","10.1145/3643788.3648015","https://doi.org/10.1145/3643788.3648015","This report outlines the objectives, methodology, challenges, and results of the first Automated Program Repair Competition held at the APR Workshop 2024. The competition utilized Cerberus, a program repair framework, to evaluate the program repair tools using different repair configurations for each track in the competition. The competition was organized in three phases: first the participants integrated their tools with Cerberus, second the integrated tools were tested using public benchmarks and participants were able to fix any identified issues. In the last phase, the submitted tools and baseline comparison tools were evaluated against private benchmark programs.","2024","2025-11-25 22:30:05","2025-11-25 22:30:05","","46–49","","","","","","","APR '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BN3I3AJH","conferencePaper","2025","Kottom, Luke; Stefan-Zavala, Alejandro; Gharib, Morteza; Humml, Julian","Augmented Reality and Vision-Language Models to Guide Humans Across Manual Tasks","Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Immersive Pavilion","979-8-4007-1547-1","","10.1145/3721245.3734043","https://doi.org/10.1145/3721245.3734043","We present a novel mixed-reality pipeline for assisted assembly verification that enhances productivity, skill development, reliability and operational efficiency in manual tasks such as manufacturing, maintenance and DIY end-user assembly.Our system incorporates modern augmented reality (AR) headsets, vision-language models (VLM) and object detection models to construct an immersive assembly guidance platform.In real time, our system parses the state of the assembly and location of components from the user’s point of view, providing labels in three-dimensional space, as well as visual and textual feedback, through an AR interface. Each assembly step is validated using the VLM and a state-space encoding of all sub-assemblies, such that user mistakes are detected and clear feedback is given on how to revert to a valid state.In this work, we present a proof-of-concept instance of this platform for the test case of guiding users through building a LEGO set. Our proof-of-concept uses an Apple VisionPro AR headset for spatial computing and user interfacing, a CNN-based foundational object detection model to label and track individual LEGO pieces, and a foundational vision-language model to validate assemblies and provide textual feedback.Users with no prior knowledge of the LEGO set’s assembly are guided step-by-step throughout the process, detecting mistakes and giving textual and visual guidance on corrective measures, until the final assembly is validated. We present this test case as proof of a new paradigm in human-machine integration, where human judgment, perception and skill are augmented by artificial intelligence, seamlessly, in the human’s domain.","2025","2025-11-25 22:30:05","2025-11-25 22:30:05","","","","","","","","","SIGGRAPH Immersive Pavilion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Active Learning; Augmented Reality; Environmental Sampling; Human-Machine Teaming; Sequential Design; Spatial Sensing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MR8UPEEY","book","2025","","UKICER '25: Proceedings of the 2025 Conference on UK and Ireland Computing Education Research","","979-8-4007-2078-9","","","","","2025","2025-11-25 22:30:05","2025-11-25 22:30:05","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MLIRDTDK","journalArticle","2025","Fowler, Max; Emeka, Chinedu; Chen, Binglin; IV, David H. Smith; West, Matthew; Zilles, Craig","Evaluating AI models for Autograding Explain in Plain English Questions: Challenges and Considerations","ACM Trans. Interact. Intell. Syst.","","2160-6455","10.1145/3774752","https://doi.org/10.1145/3774752","Code reading ability has traditionally been under-emphasized in assessments as it is difficult to assess at scale. Prior research has shown that code reading and code writing are intimately related skills; thus being able to assess and train code reading skills may be necessary for student learning. One way to assess code reading ability is using Explain in Plain English (EiPE) questions, which ask students to describe what a piece of code does with natural language. Previous research deployed a binary (correct/incorrect) autograder using bigram models that performed comparably with human teaching assistants on student responses. With a data set of 3,064 student responses from 17 EiPE questions, we investigated multiple autograders for EiPE questions. We evaluated methods as simple as logistic regression trained on bigram features, to more complicated support vector machines (SVMs) trained on embeddings from large language models (LLMs), to GPT-4. We found multiple useful autograders, most with accuracies in the (86-88) % range, with different advantages. SVMs trained on LLM embeddings had the highest accuracy; few-shot chat completion with GPT-4 required minimal human effort; pipelines with multiple autograders for specific dimensions (what we call 3D autograders) can provide fine-grained feedback; and code generation with GPT-4 to leverage automatic code testing as a grading mechanism in exchange for slightly more lenient grading standards. While piloting these autograders in a non-major introductory Python course, students had largely similar views of all autograders, although they more often found the GPT-based grader and code generation graders more helpful and liked the code generation grader the most.","2025-11","2025-11-25 22:30:05","2025-11-25 22:30:05","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","large language models; LLMs; autograding; EiPE; Explain in Plain English","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F3SKJQFR","conferencePaper","2024","Zhu, Taohong; Newton, William; Embury, Suzanne; Sun, Youcheng","TAIiST CPS-UAV at the SBFT Tool Competition 2024","Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing","979-8-4007-0562-5","","10.1145/3643659.3643936","https://doi.org/10.1145/3643659.3643936","This paper presents an innovative approach to testing Cyber-Physical Systems (CPS) with a specific focus on Unmanned Aerial Vehicles (UAVs) using Large Language Models (LLMs). In the rapidly evolving field of UAV technology, ensuring the reliability and safety of these systems is of utmost importance. Traditional testing methods often fall short in addressing the complex, dynamic, and stateful environments in which UAVs operate. To bridge this gap, we propose the use of state-of-the-art LLMs.Our methodology leverages the capabilities of LLMs to ""intelligently"" simulate a wide range of real-world scenarios and interactions that UAVs may encounter. This includes interpreting and responding to dynamic environmental changes, unexpected obstacles, and real-time decision-making processes. By integrating LLMs into the testing framework, we can create more comprehensive, realistic, and efficient testing scenarios for CPS-UAVs.We demonstrate the effectiveness of our approach through a series of experiments using popular UAV platforms including PX4 and Ardupilot. Our code and implementation are made publicly available in our project page https://github.com/Trusted-AI-in-System-Test.","2024","2025-11-25 22:30:05","2025-11-25 22:30:05","","51–52","","","","","","","SBFT '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2BLDNSJC","conferencePaper","2025","Lyu, Tao; Dwivedi, Kumar Kartikeya; Bourgeat, Thomas; Payer, Mathias; Xu, Meng; Kashyap, Sanidhya","eBPF Misbehavior Detection: Fuzzing with a Specification-Based Oracle","Proceedings of the ACM SIGOPS 31st Symposium on Operating Systems Principles","979-8-4007-1870-0","","10.1145/3731569.3764797","https://doi.org/10.1145/3731569.3764797","Bugs in the Linux eBPF verifier may cause it to mistakenly accept unsafe eBPF programs or reject safe ones, causing either security or usability issues. While prior works on fuzzing the eBPF verifier have been effective, their bug oracles only hint at the existence of bugs indirectly (e.g., when a memory error occurs in downstream execution) instead of showing the root cause, confining them to uncover a narrow range of security bugs only with no detection of usability issues.In this paper, we propose SpecCheck, a specification-based oracle integrated with our fuzzer Veritas, to detect a wide range of bugs in the eBPF verifier. SpecCheck encodes eBPF instruction semantics and safety properties as a specification and turns the claim of whether a concrete eBPF program is safe into checking the satisfiability of the corresponding safety constraints, which can be reasoned automatically without abstraction. The output from the oracle will be crosschecked with the eBPF verifier for any discrepancies. Using SpecCheck, Veritas uncovered 13 bugs in the Linux eBPF verifier, including severe bugs that can cause privilege escalation or information leakage, as well as bugs that cause frustration in even experienced kernel developers.","2025","2025-11-25 22:30:05","2025-11-25 22:30:05","","701–718","","","","","","","SOSP '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lotte Hotel World, Seoul, Republic of Korea","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E7T9M7FJ","conferencePaper","2025","Vogelsang, Andreas; Korn, Alexander; Broccia, Giovanna; Ferrari, Alessio; Fischbach, Jannik; Arora, Chetan","On the Impact of Requirements Smells in Prompts: The Case of Automated Traceability","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: New Ideas and Emerging Results","979-8-3315-3711-1","","10.1109/ICSE-NIER66352.2025.00016","https://doi.org/10.1109/ICSE-NIER66352.2025.00016","Large language models (LLMs) are increasingly used to generate software artifacts, such as source code, tests, and trace links. Requirements play a central role in shaping the input prompts that guide LLMs, as they are often used as part of the prompts to synthesize the artifacts. However, the impact of requirements formulation on LLM performance remains unclear. In this paper, we investigate the role of requirements smells—indicators of potential issues like ambiguity and inconsistency—when used in prompts for LLMs. We conducted experiments using two LLMs focusing on automated trace link generation between requirements and code. Our results show mixed outcomes: while requirements smells had a small but significant effect when predicting whether a requirement was implemented in a piece of code (i.e., a trace link exists), no significant effect was observed when tracing the requirements with the associated lines of code. These findings suggest that requirements smells can affect LLM performance in certain SE tasks but may not uniformly impact all tasks. We highlight the need for further research to understand these nuances and propose future work toward developing guidelines for mitigating the negative effects of requirements smells in AI-driven SE processes.","2025","2025-11-25 22:30:05","2025-11-25 22:30:05","","51–55","","","","","","","ICSE-NIER '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","LLMs; requirements eng.; smells; traceability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NEIPNQ3F","conferencePaper","2024","Yang, Wenzhang; Gao, Cuifeng; Liu, Xiaoyuan; Li, Yuekang; Xue, Yinxing","Rust-twins: Automatic Rust Compiler Testing through Program Mutation and Dual Macros Generation","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695059","https://doi.org/10.1145/3691620.3695059","Rust is a relatively new programming language known for its memory safety and numerous advanced features. It has been widely used in system software in recent years. Thus, ensuring the reliability and robustness of the only implementation of the Rust compiler, rustc, is critical. However, compiler testing, as one of the most effective techniques to detect bugs, faces difficulties in generating valid Rust programs with sufficient diversity due to its stringent memory safety mechanisms. Furthermore, existing research primarily focuses on testing rustc to trigger crash errors, neglecting incorrect compilation results - miscompilation. Detecting miscompilation remains a challenge in the absence of multiple implementations of the Rust compiler to serve as a test oracle.This paper tackles these challenges by introducing rust-twins, a novel and effective approach to performing automated differential testing for rustc to detect both crashes and miscompilations. We devise four Rust-specific mutators and adapt fourteen general mutators for Rust, each intends to produce a syntax and semantic valid Rust program to trigger rustc crashes. Additionally, we develop a macroize approach to rewrite a regular Rust program into dual macros with equivalent behaviors but in different implementations. Furthermore, we design an assessment component to check the equivalence by comparing the expansion results with a simple macro input. Finally, rust-twins attempts to expand the two macros with numerous complex inputs to detect differences. Due to the macro expansion mechanism, the root causes of differences can arise not only from the macro expansion part but also from any other mis-implemented compiler code.We have evaluated rust-twins on the latest version of rustc. Our experimental results indicate that rust-twins achieves twice the total line coverage and identifies more crashes and differences than the best baseline technique, rustsmith, after 24 hours of testing. In total, rust-twins triggered 10 rustc crashes, and 229 of the generated macros exposed rustc differences. We analyzed and reported 12 previously unknown bugs, of which 8 have already been confirmed and fixed.","2024","2025-11-25 22:30:05","2025-11-25 22:30:05","","631–642","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","rust; differential testing; compiler testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L6AN2VKP","journalArticle","2025","Chen, Junjie; Fan, Xingyu; Yang, Chen; Liu, Shuang; Sun, Jun","De-duplicating Silent Compiler Bugs via Deep Semantic Representation","Proc. ACM Softw. Eng.","","","10.1145/3729375","https://doi.org/10.1145/3729375","The compiler bug duplication problem (where many test failures are caused by the same compiler bug) can lead to huge waste of time and resource in diagnosing test failures produced by compiler testing. It is particularly challenging with regard to the silent compiler bugs that do not produce any error messages. To address this problem, multiple white-box techniques were proposed, but they are inapplicable in many practical scenarios. Black-box techniques are more practical, but the existing ones are less effective as they often rely on irrelevant syntactic information. To bridge this gap, we propose a novel black-box technique (BLADE), which aims to improve the effectiveness of black-box de-duplication by extracting failure-relevant semantic information from failure-triggering test programs in a black-box manner. It first learns failure-relevant semantic information based on intermediate representation learning by employing the classification of failure-triggering and failure-free test programs as the auxiliary objective, and then extracts such information based on model interpretation. Our experiments on four widely-used datasets (collected from GCC and LLVM) show that BLADE significantly outperforms the two existing black-box techniques with an average improvement of 36% and 12% in identifying unique silent compiler bugs when analyzing the same number of test failures respectively, and achieves competitive effectiveness with the state-of-the-art white-box techniques.","2025-06","2025-11-25 22:30:05","2025-11-25 22:30:05","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Compiler Testing; Bug Deduplication","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T59R9RIS","conferencePaper","2024","Zhang, Quanjun; Fang, Chunrong; Zhang, Tongke; Yu, Bowen; Sun, Weisong; Chen, Zhenyu","Gamma: Revisiting Template-based Automated Program Repair via Mask Prediction","Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering","979-8-3503-2996-4","","10.1109/ASE56229.2023.00063","https://doi.org/10.1109/ASE56229.2023.00063","Automated program repair (APR) aims to fix software bugs without manual debugging efforts and plays a crucial role in software development and maintenance. Template-based APR has been widely investigated and shown promising results. However, it is challenging for template-based APR to select the appropriate donor code, which is an important repair ingredient for generating candidate patches. Inappropriate donor code may cause plausible but incorrect patch generation even with correct fix patterns, limiting the repair performance.In this paper, we aim to revisit template-based APR, and propose Gamma, to directly leverage large pre-trained language models for donor code generation. Our main insight is that instead of retrieving donor code in the local buggy file, we can directly predict the correct code tokens based on the context code snippets and repair patterns by a cloze task. Specifically, (1) Gamma revises a variety of fix templates from state-of-the-art template-based APR techniques (i.e., TBar) and transforms them into mask patterns. (2) Gamma adopts a pre-trained language model to predict the correct code for masked code as a fill-in-the-blank task. Although our idea is general and can be built on various existing pre-trained language models, we have implemented Gamma as a practical APR tool based on the recent UniXcoder model. The experimental results demonstrate that Gamma correctly repairs 82 bugs on Defects4J-v1.2, which achieves 20.59% (14 bugs) and 26.15% (17 bugs) improvement over the previous state-of-the-art template-based approach TBar and learning-based one Recoder. Furthermore, Gamma repairs 45 bugs and 22 bugs from the additional Defects4J-v2.0 and QuixBugs, indicating the generalizability of Gamma in addressing the dataset overfitting issue. We also prove that adopting other pre-trained language models can provide substantial advancement, e.g., CodeBERT-based and ChatGPT-based Gamma is able to fix 80 and 67 bugs on Defects4J-v1.2, indicating the scalability of Gamma. Overall, our study highlights the promising future of adopting pre-trained models to generate correct patches on top of fix patterns in practice.","2024","2025-11-25 22:30:06","2025-11-25 22:30:06","","535–547","","","","","","","ASE '23","","","","IEEE Press","Echternach, Luxembourg","","","","","","","","","","","","automated program repair; LLM4SE; fix pattern; pre-trained model","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"THQEPDGZ","journalArticle","2025","Tony, Catherine; Díaz Ferreyra, Nicolás E.; Mutas, Markus; Dhif, Salem; Scandariato, Riccardo","Prompting Techniques for Secure Code Generation: A Systematic Investigation","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3722108","https://doi.org/10.1145/3722108","Large Language Models (LLMs) are gaining momentum in software development with prompt-driven programming enabling developers to create code from Natural Language (NL) instructions. However, studies have questioned their ability to produce secure code and, thereby, the quality of prompt-generated software. Alongside, various prompting techniques that carefully tailor prompts have emerged to elicit optimal responses from LLMs. Still, the interplay between such prompting strategies and secure code generation remains under-explored and calls for further investigations. Objective: In this study, we investigate the impact of different prompting techniques on the security of code generated from NL instructions by LLMs. Method: First, we perform a systematic literature review to identify the existing prompting techniques that can be used for code generation tasks. A subset of these techniques are evaluated on GPT-3, GPT-3.5, and GPT-4 models for secure code generation. For this, we used an existing dataset consisting of 150 NL security-relevant code generation prompts. Results: Our work (i) classifies potential prompting techniques for code generation (ii) adapts and evaluates a subset of the identified techniques for secure code generation tasks, and (iii) observes a reduction in security weaknesses across the tested LLMs, especially after using an existing technique called Recursive Criticism and Improvement (RCI), contributing valuable insights to the ongoing discourse on LLM-generated code security.","2025-10","2025-11-25 22:30:06","2025-11-25 22:30:06","","","","8","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","prompt engineering; LLMs; secure code generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"72LKQ9JN","journalArticle","2025","Suo, Chenyao; Wang, Jianrong; Wang, Yongjia; Jiang, Jiajun; Shen, Qingchao; Chen, Junjie","DESIL: Detecting Silent Bugs in MLIR Compiler Infrastructure","Proc. ACM Program. Lang.","","","10.1145/3763161","https://doi.org/10.1145/3763161","MLIR (Multi-Level Intermediate Representation) compiler infrastructure provides an efficient framework for introducing a new abstraction level for programming languages and domain-specific languages. It has attracted widespread attention in recent years and has been applied in various domains, such as deep learning compiler construction. Recently, several MLIR compiler fuzzing techniques, such as MLIRSmith and MLIRod, have been proposed. However, none of them can detect silent bugs, i.e., bugs that incorrectly optimize code silently. The difficulty in detecting silent bugs arises from two main aspects: (1) UB-Free Program Generation: Generates programs that are free from undefined behaviors to suit the non-UB assumptions required by compiler optimizations. (2) Lowering Support: Converts the given MLIR program into an executable form with a suitable lowering path that reduces redundant lowering passes and improves the efficiency of fuzzing. To address the above issues, we propose DESIL. DESIL enables silent bug detection by defining a set of UB-elimination rules based on the MLIR documentation and applying them to input programs. To convert dialects in the MLIR program into executable form, DESIL designs a lowering path optimization strategy to convert the dialects in the given MLIR program into executable form. Furthermore, DESIL incorporates the differential testing for silent bug detection. It introduces an operation-aware optimization recommendation strategy into the compilation process to generate diverse executable files. We applied DESIL to the latest revisions of the MLIR compiler infrastructure. It detected 23 silent bugs and 19 crash bugs, of which 17/16 have been confirmed or fixed.","2025-10","2025-11-25 22:30:06","2025-11-25 22:30:06","","","","OOPSLA2","9","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Compiler Fuzzing; MLIR Compiler Infrastructure; Undefined Behavior","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WNBKBLT4","journalArticle","2025","Jiang, Juyong; Wang, Fan; Shen, Jiasi; Kim, Sungju; Kim, Sunghun","A Survey on Large Language Models for Code Generation","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3747588","https://doi.org/10.1145/3747588","Large Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks, known as Code LLMs, particularly in code generation that generates source code with LLM from natural language descriptions. This burgeoning field has captured significant interest from both academic researchers and industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language processing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive and up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge this gap by providing a systematic literature review that serves as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize and discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest advances, performance evaluation, ethical implications, environmental impact, and real-world applications. In addition, we present a historical overview of the evolution of LLMs for code generation and provide a quantitative and qualitative comparative analysis of experimental results of code LLMs, sourced from their original papers to ensure a fair comparison on the HumanEval, MBPP, and BigCodeBench benchmarks, across various levels of difficulty and types of programming tasks, to highlight the progressive enhancements in LLM capabilities for code generation. We identify critical challenges and promising opportunities regarding the gap between academia and practical development. Furthermore, we have established a dedicated resource GitHub page () to continuously document and disseminate the most recent advances in the field.","2025-07","2025-11-25 22:30:06","2025-11-25 22:30:06","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; Code Generation; Code Large Language Models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QU62MZGH","conferencePaper","2025","Hao, Qiang; Liu, Ruohan","Towards Integrating Behavior-Driven Development in Mobile Development: An Experience Report","Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1","979-8-4007-0531-1","","10.1145/3641554.3701875","https://doi.org/10.1145/3641554.3701875","Testing is an important yet often neglected skill in learning and teaching of computing science at the college level. Prior studies explored integrating test-driven development (TDD) into computer science courses with some degree of success, but also observed issues such as students' lack of appreciation, expressed frustration, and inconsistent adherence to TDD. TDD is a software development methodology that emphasizes writing low-level unit test cases prior to writing the corresponding portion of implementation. Behavior-driven development (BDD) was proposed as an evolution of TDD to emphasize software behavior from users' perspective. BDD has been widely adopted in industry, and holds great potential in addressing the issues in using TDD to improve students' learning of testing. However, BDD was rarely explored in enhancing students' mastery of testing. Informed by the literature, this experience report explored the integration of BDD into a mobile development course. Students' performance, attitude and feedback on BDD was examined, and potential improvement on the integration of BDD was discussed. The results of this report sheds light on how to effectively integrate BDD into computer science courses.","2025","2025-11-25 22:30:06","2025-11-25 22:30:06","","450–456","","","","","","","SIGCSETS 2025","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Pittsburgh, PA, USA","","","","testing; software engineering education; project-based learning; continuous integration; behavior-driven development; mobile development; test-driven development","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BQPGWYYW","conferencePaper","2023","Du, Xiaohu; Wen, Ming; Wei, Zichao; Wang, Shangwen; Jin, Hai","An Extensive Study on Adversarial Attack against Pre-trained Models of Code","Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","979-8-4007-0327-0","","10.1145/3611643.3616356","https://doi.org/10.1145/3611643.3616356","Transformer-based pre-trained models of code (PTMC) have been widely utilized and have achieved state-of-the-art performance in many mission-critical applications. However, they can be vulnerable to adversarial attacks through identifier substitution or coding style transformation, which can significantly degrade accuracy and may further incur security concerns. Although several approaches have been proposed to generate adversarial examples for PTMC, the effectiveness and efficiency of such approaches, especially on different code intelligence tasks, has not been well understood. To bridge this gap, this study systematically analyzes five state-of-the-art adversarial attack approaches from three perspectives: effectiveness, efficiency, and the quality of generated examples. The results show that none of the five approaches balances all these perspectives. Particularly, approaches with a high attack success rate tend to be time-consuming; the adversarial code they generate often lack naturalness, and vice versa. To address this limitation, we explore the impact of perturbing identifiers under different contexts and find that identifier substitution within for and if statements is the most effective. Based on these findings, we propose a new approach that prioritizes different types of statements for various tasks and further utilizes beam search to generate adversarial examples. Evaluation results show that it outperforms the state-of-the-art ALERT in terms of both effectiveness and efficiency while preserving the naturalness of the generated adversarial examples.","2023","2025-11-25 22:30:06","2025-11-25 22:30:06","","489–501","","","","","","","ESEC/FSE 2023","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: San Francisco, CA, USA","","","","Deep Learning; Adversarial Attack; Pre-Trained Model","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DCG3FKL6","book","2025","","ISEC '25: Proceedings of the 18th Innovations in Software Engineering Conference","","979-8-4007-1424-5","","","","","2025","2025-11-25 22:30:06","2025-11-25 22:30:06","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"38DDJSV6","journalArticle","2023","Zhang, Quanjun; Fang, Chunrong; Ma, Yuxiang; Sun, Weisong; Chen, Zhenyu","A Survey of Learning-based Automated Program Repair","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3631974","https://doi.org/10.1145/3631974","Automated program repair (APR) aims to fix software bugs automatically and plays a crucial role in software development and maintenance. With the recent advances in deep learning (DL), an increasing number of APR techniques have been proposed to leverage neural networks to learn bug-fixing patterns from massive open-source code repositories. Such learning-based techniques usually treat APR as a neural machine translation (NMT) task, where buggy code snippets (i.e., source language) are translated into fixed code snippets (i.e., target language) automatically. Benefiting from the powerful capability of DL to learn hidden relationships from previous bug-fixing datasets, learning-based APR techniques have achieved remarkable performance.In this article, we provide a systematic survey to summarize the current state-of-the-art research in the learning-based APR community. We illustrate the general workflow of learning-based APR techniques and detail the crucial components, including fault localization, patch generation, patch ranking, patch validation, and patch correctness phases. We then discuss the widely adopted datasets and evaluation metrics and outline existing empirical studies. We discuss several critical aspects of learning-based APR techniques, such as repair domains, industrial deployment, and the open science issue. We highlight several practical guidelines on applying DL techniques for future APR studies, such as exploring explainable patch generation and utilizing code features. Overall, our article can help researchers gain a comprehensive understanding about the achievements of the existing learning-based APR techniques and promote the practical application of these techniques. Our artifacts are publicly available at the repository: .","2023-12","2025-11-25 22:30:06","2025-11-25 22:30:06","","","","2","33","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","deep learning; neural machine translation; AI and software engineering; Automatic program repair","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"THZCKX6J","journalArticle","2025","Wang, Bo; Chen, Chong; Deng, Ming; Chen, Junjie; Zhang, Xing; Lin, Youfang; Hao, Dan; Sun, Jun","Fuzzing C++ Compilers via Type-Driven Mutation","Proc. ACM Program. Lang.","","","10.1145/3763094","https://doi.org/10.1145/3763094","C++ is a system-level programming language for modern software development, which supports multiple programming paradigms, including object-oriented, generic, and functional programming. The intrinsic complexity of these paradigms and their interactions grants C++ powerful expressiveness while posing significant challenges for compilers in correctly implementing its type system. A type system encompasses various aspects such as type inference, type checking, subtyping, type conversions, generics, scoping, and binding. However, systematic testing of the type systems of C++ compilers remains largely underexplored in existing studies. In this work, we present TyMut, the first approach specifically designed to test the C++ type system. TyMut is a mutation-based compiler fuzzer equipped with advanced type-driven mutation operators, carefully crafted to target intricate type-related features such as template generics, type conversions, and inheritance. Beyond differential testing, TyMut introduces enhanced test oracles through a must analysis that partially confirms the validity of generated programs. Specifically, mutation operators are classified into well-formed and not-well-formed: Programs generated by well-formed mutation operators are valid and must be accepted by compilers. Programs generated by not-well-formed operators are validated against a set of well-formedness rules. Any violation indicates the program is invalid and must be rejected. For programs that pass the rules but lack a definitive oracle, TyMut applies differential testing to identify behavioral inconsistencies across compilers. The testing campaign took about 32 hours to generate and test 250584 programs. The must analysis provides definite test oracles for nearly 80% of all generated programs. TyMut uncovered 102 bugs in the recent versions of GCC and Clang, with 56 confirmed as new bugs by compiler developers. Among the confirmed bugs, 26 of them cause compiler crashes, and more than 50% cause miscompilation. Additionally, 7 of them had remained hidden for over 20 years, 22 for over 10 years, and 39 for over 5 years. One long-standing bug discovered by TyMut was later confirmed as the root cause of a real-world issue in TensorFlow. Before submitting this paper, 13 bugs were fixed, most of which were fixed within 60 days. Notably, some unconfirmed bugs have led to in-depth discussions among developers. For instance, one bug led a compiler developer to submit a new issue to the C++ language standard, showing that we uncovered ambiguities in the language specification.","2025-10","2025-11-25 22:30:06","2025-11-25 22:30:06","","","","OOPSLA2","9","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","OOP; Compiler Testing; C++; Compiler Bugs; Generics; Type System","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PSMITPIK","book","2025","","TDIS '25: Proceedings of the 3rd International Workshop on Testing Distributed Internet of Things Systems","","979-8-4007-1526-6","","","","","2025","2025-11-25 22:30:06","2025-11-25 22:30:06","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SSINEDCS","bookSection","2025","Dai, Xing; Yao, Weifang","Research on Development Strategies for Edge AI-Based Smart Home Devices","Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence","979-8-4007-1279-1","","","https://doi.org/10.1145/3745238.3745469","In today's information era, the rapid development of IoT (Internet of Things) as well as AI (artificial intelligence) has shifted household scenes into smart living modes. Most of the traditional smart household devices rely on the cloud or local end to process data. On the one hand, for technical reasons, computing might contain risks, including data privacy leaks as well as net lag. On the other hand, local ends often fail to meet household needs of human-machine interaction because of their core chips' inherent computing calculation limitation. The booming of edge AI has provided new solutions for household end devices. Through case studies of different intelligent locks from the ends, this paper has investigated the developing strategies of smart household devices and helped researchers with suitable methodologies.","2025","2025-11-25 22:30:06","2025-11-25 22:30:06","","1474–1481","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AMEM8AA7","conferencePaper","2024","Liu, Tianyang; Ji, Weixing; Dong, Xiaohui; Yao, Wuhuang; Wang, Yizhuo; Liu, Hui; Peng, Haiyang; Wang, Yuxuan","JLeaks: A Featured Resource Leak Repository Collected From Hundreds of Open-Source Java Projects","Proceedings of the IEEE/ACM 46th International Conference on Software Engineering","979-8-4007-0217-4","","10.1145/3597503.3639162","https://doi.org/10.1145/3597503.3639162","High-quality defect repositories are vital in defect detection, localization, and repair. However, existing repositories collected from open-source projects are either small-scale or inadequately labeled and packed. This paper systematically summarizes the programming APIs of system resources (i.e., file, socket, and thread) in Java. Additionally, this paper demonstrates the exceptions that may cause resource leaks in the chained and nested streaming operations. A semi-automatic toolchain is built to improve the efficiency of defect extraction, including automatic building for large legacy Java projects. Accordingly, 1,094 resource leaks were collected from 321 open-source projects on GitHub. This repository, named JLeaks, was built by round-by-round filtering and cross-validation, involving the review of approximately 3,185 commits from hundreds of projects. JLeaks is currently the largest resource leak repository, and each defect in JLeaks is well-labeled and packed, including causes, locations, patches, source files, and compiled bytecode files for 254 defects. We have conducted a detailed analysis of JLeaks for defect distribution, root causes, and fix approaches. We compare JLeaks with two well-known resource leak repositories, and the results show that JLeaks is more informative and complete, with high availability, uniqueness, and consistency. Additionally, we show the usability of JLeaks in two application scenarios. Future studies can leverage our repository to encourage better design and implementation of defect-related algorithms and tools.","2024","2025-11-25 22:30:06","2025-11-25 22:30:06","","","","","","","","","ICSE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Lisbon, Portugal","","","","defect repository; java language; open-source projects; resource leak","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NUWD5ZX7","conferencePaper","2025","Tang, Zhenghan","TTFact: A Fact-Checking Framework Based on Task-Transform and Template-Transform","Proceedings of the 2024 2nd International Conference on Electronics, Computers and Communication Technology","979-8-4007-1019-3","","10.1145/3705754.3705757","https://doi.org/10.1145/3705754.3705757","Nowadays, many works have attempted to introduce the large language models (LLM) into the fact-checking task by in-context learning (ICL), which has three labels: real, fake, and not enough information (NEI). The vanilla ICL method used by these works lets the LLM do the 3-label classification task and performs badly. We conducted experiments on multiple LLMs and found that LLMs can not identify the NEI label well, but can perform very well while only needing to classify real and fake labels. So based on the characteristics of the fact-checking task, we propose a framework named TTFact, which has two main ideas:Task-Transform and Template-Transform. The Task-Transform extracts the relevance scores of retrieved article sentences as features to judge whether the test case is the NEI label. The Template-Transform treats the authoritative retrieved sentences as news text with the real label to get a better-performed prompt template. Besides, we propose a contrastive learning method to finetune the retrieval model and get better relevance scores to improve the classification of the NEI label. The experiment on the widely used fact-checking dataset CHEF shows that our TTFact achieves the best LLM-based fact-checking method. When introducing the finetuned small language model (SLM) into our TTFact, our enhanced method TTFact-S achieves SOTA.","2025","2025-11-25 22:30:06","2025-11-25 22:30:06","","14–21","","","","","","","CECCT '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","Large Language Models; In-context Learning; Fact-checking","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6BNB7EMH","conferencePaper","2024","Liu, Haoran; Jia, Zhouyang; Zhou, Huiping; Zhou, Haifang; Li, Shanshan","Go the Extra Mile: Fixing Propagated Error-Handling Bugs","Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering","979-8-4007-0658-5","","10.1145/3663529.3663868","https://doi.org/10.1145/3663529.3663868","Error handling bugs are widespread in software, compromising its reliability. In C/C++ environments, error-handling bugs are often propagated to multiple functions through return values. This paper introduces EH-Fixer, a conversation-based automated method for fixing propagated error-handling (PEH) bugs. EH-Fixer employs LLM in a conversation style, utilizing information retrieval to address PEH bugs. We constructed a dataset containing 30 PEH bugs and evaluated EH-Fixer against two state-of-the-art approaches. Preliminary results indicate that EH-Fixer successfully fixed 19 more PEH bugs than existing approaches.","2024","2025-11-25 22:30:06","2025-11-25 22:30:06","","661–662","","","","","","","FSE 2024","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Porto de Galinhas, Brazil","","","","Automatic Program Repair; Error-Handling Bug","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TKEX8C8E","book","2024","","Koli Calling '24: Proceedings of the 24th Koli Calling International Conference on Computing Education Research","","979-8-4007-1038-4","","","","","2024","2025-11-25 22:30:06","2025-11-25 22:30:06","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R3V2K3KI","conferencePaper","2025","Wang, Xinchen; Gao, Pengfei; Meng, Xiangxin; Peng, Chao; Hu, Ruida; Lin, Yun; Gao, Cuiyun","AEGIS: An Agent-based Framework for Bug Reproduction from Issue Descriptions","Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering","979-8-4007-1276-0","","10.1145/3696630.3728557","https://doi.org/10.1145/3696630.3728557","Automatically reproducing bugs in issue descriptions helps developers pinpoint and fix bugs timely, greatly facilitating the software development and maintenance. Built upon the powerful understanding capabilities of large language models (LLMs), agent-based approaches have achieved the state-of-the-art performance in the task. They generally leverage LLMs as the central controller to first retrieve bug-related information as context and then generate bug reproduction scripts. During the script modification process, agent-based approaches modify the script iteratively until the execution information reflects the bug accurately or the iterative turns are exhausted. Nevertheless, the agent-based approaches still face the following challenges: (1) Lengthy retrieved bug-related information: The retrieved bug-related information is usually long in length and contains irrelevant snippets, which is hard to be well comprehended by LLMs. (2) Lack of guidance in bug reproduction script generation: They generally modify bug reproduction scripts randomly and tend to generate repeated or spurious modifications, leading to bug reproduction failure.To address the above challenges, in this paper, we propose an automated bug reproduction script generation framework named AEGIS. AEGIS consists of two main modules: (1) Bug-related context summarization module, aiming at condensing the retrieved information into structural context through further reranking and summarization. (2) Finite state machine (FSM)-guided script generation module, which aims at guiding the script modification process with proposed FSM which contains predefined modification rules. Extensive experiments on SWE-Bench, one public benchmark dataset, and six baseline methods show that AEGIS achieves the best performance in the task, exceeding the best baseline by 19.0% with respect to the bug reproduction rate. Besides, we deploy AEGIS in five internal repositories of ByteDance. During the three-month deployment period, it successfully reproduces 12 bugs and assists developers in implementing fixes.","2025","2025-11-25 22:30:06","2025-11-25 22:30:06","","331–342","","","","","","","FSE Companion '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Clarion Hotel Trondheim, Trondheim, Norway","","","","large language models; AI agent; bug reproduction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DBYEEG56","conferencePaper","2025","Yan, Yanfu; Duong, Viet; Shao, Huajie; Poshyvanyk, Denys","Towards More Trustworthy Deep Code Models by Enabling Out-of-Distribution Detection","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","10.1109/ICSE55347.2025.00177","https://doi.org/10.1109/ICSE55347.2025.00177","Numerous machine learning (ML) models have been developed, including those for software engineering (SE) tasks, under the assumption that training and testing data come from the same distribution. However, training and testing distributions often differ, as training datasets rarely encompass the entire distribution, while testing distribution tends to shift over time. Hence, when confronted with out-of-distribution (OOD) instances that differ from the training data, a reliable and trustworthy SE ML model must be capable of detecting them to either abstain from making predictions, or potentially forward these OODs to appropriate models handling other categories or tasks.In this paper, we develop two types of SE-specific OOD detection models, unsupervised and weakly-supervised OOD detection for code. The unsupervised OOD detection approach is trained solely on in-distribution samples while the weakly-supervised approach utilizes a tiny number of OOD samples to further enhance the detection performance in various OOD scenarios. Extensive experimental results demonstrate that our proposed methods significantly outperform the baselines in detecting OOD samples from four different scenarios simultaneously and also positively impact a main code understanding task.","2025","2025-11-25 22:30:06","2025-11-25 22:30:06","","769–781","","","","","","","ICSE '25","","","","IEEE Press","Ottawa, Ontario, Canada","","","","","","","","","","","","code models; contrastive learning; OOD detection; trustworthy ML","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CB2PPVNJ","conferencePaper","2025","Li, Mingjia; Tong, Junkai; Huang, Yiyang; Ding, Yifei; Qian, Hong; Zhou, Aimin","Paper-Level Computerized Adaptive Testing for High-Stakes Examination via Multi-Objective Optimization","Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2","979-8-4007-1454-2","","10.1145/3711896.3737073","https://doi.org/10.1145/3711896.3737073","Computerized Adaptive Testing (CAT) is a testing technique that accurately infers students' proficiency levels using a relatively small number of questions. Most existing CAT systems operate on a question-level adaptive paradigm, which is suitable for practice scenarios. However, in computerized standardized high-stakes examinations such as the GRE and GMAT, this paradigm faces several challenges: (1) the lack of comparability in exam results(2) high implementation costs due to the reliance on real-time interactions and the financial burden of maintaining CAT testing system, and (3) the difficulty in balancing multiple factors of diagnosis quality, attribute coverage, and question exposure. To address these challenges, we propose a Paper-level Computerized Adaptive Testing (PCAT) and its corresponding evaluation method. PCAT divides an exam into multiple testing stages, where examinees adaptively receive test papers of varying difficulty based on their performance in previous stages. The paper assembly problem in PCAT is solved using a population-based multi-objective optimization (MOO) approach. PCAT offers several advantages: First, the paper-level adaptive mechanism ensures that the questions faced by examinees depend solely on their performance in the earlier stages, maintaining adaptability while enhancing the comparability of results across different examinees. Second, PCAT replaces the selection strategy module in traditional CAT with an assembly module, allowing computationally intensive tasks such as cognitive diagnosis and paper assembly to be completed offline before the exam, eliminating the need for real-time interactions. Additionally, the population-based MOO approach generates a set of high-quality solutions in one run, meeting the demands of frequent administration of standardized high-stakes exams like the GRE and reducing the financial burden of maintaining a large-scale CAT system. Finally, MOO naturally models multiple factors as separate objectives, enabling a balanced consideration of these factors and allowing exam administrators to customize the exam based on specific needs. Extensive experiments on four real-world datasets show that PCAT outperforms state-of-the-art (SOTA) CAT methods in terms of diagnosis quality, attribute coverage, and question exposure, while maintaining the same number of questions answered by examinees. These results highlight PCAT's potential in high-stakes examination settings.","2025","2025-11-25 22:30:06","2025-11-25 22:30:06","","1435–1446","","","","","","","KDD '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Toronto ON, Canada","","","","computerized adaptive testing; high-stakes examination; multi-objective optimization; paper assembly","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LEANYI5B","conferencePaper","2025","Denny, Paul; Kumar, Viraj; MacNeil, Stephen; Prather, James; Leinonen, Juho","Probing the Unknown: Exploring Student Interactions with Probeable Problems at Scale in Introductory Programming","Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 1","979-8-4007-1567-9","","10.1145/3724363.3729093","https://doi.org/10.1145/3724363.3729093","Introductory programming courses often rely on small code-writing exercises that have clearly specified problem statements. This limits opportunities for students to practice how to clarify ambiguous requirements - a critical skill in real-world programming. In addition, the emerging capabilities of large language models (LLMs) to produce code from well-defined specifications may harm student engagement with traditional programming exercises. This study explores the use of ”Probeable Problems”, automatically gradable tasks that have deliberately vague or incomplete specifications. Such problems require students to submit test inputs, or 'probes', to clarify requirements before implementation. Through analysis of over 40,000 probes in an introductory course, we identify patterns linking probing behaviors to task success. Systematic strategies, such as thoroughly exploring expected behavior before coding, resulted in fewer incorrect code submissions and correlated with course success. Feedback from nearly 1,000 participants highlighted the challenges and real-world relevance of these tasks, as well as benefits to critical thinking and metacognitive skills. Probeable Problems are easy to set up and deploy at scale, and help students recognize and resolve uncertainties in programming problems.","2025","2025-11-25 22:30:06","2025-11-25 22:30:06","","618–624","","","","","","","ITiCSE 2025","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Nijmegen, Netherlands","","","","test cases; cs1; ambiguity; probeable problems; requirements","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3LI9NILA","book","2025","","ACE '25: Proceedings of the 27th Australasian Computing Education Conference","","979-8-4007-1425-2","","","","","2025","2025-11-25 22:30:06","2025-11-25 22:30:06","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D5ADSAEH","journalArticle","2025","Murillo, Juan Manuel; Garcia-Alonso, Jose; Moguel, Enrique; Barzen, Johanna; Leymann, Frank; Ali, Shaukat; Yue, Tao; Arcaini, Paolo; Pérez-Castillo, Ricardo; García-Rodríguez de Guzmán, Ignacio; Piattini, Mario; Ruiz-Cortés, Antonio; Brogi, Antonio; Zhao, Jianjun; Miranskyy, Andriy; Wimmer, Manuel","Quantum Software Engineering: Roadmap and Challenges Ahead","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3712002","https://doi.org/10.1145/3712002","As quantum computers advance, the complexity of the software they can execute increases as well. To ensure this software is efficient, maintainable, reusable, and cost-effective—key qualities of any industry-grade software—mature software engineering practices must be applied throughout its design, development, and operation. However, the significant differences between classical and quantum software make it challenging to directly apply classical software engineering methods to quantum systems. This challenge has led to the emergence of Quantum Software Engineering (QSE) as a distinct field within the broader software engineering landscape. In this work, a group of active researchers analyze in depth the current state of QSE research. From this analysis, the key areas of QSE are identified and explored in order to determine the most relevant open challenges that should be addressed in the next years. These challenges help identify necessary breakthroughs and future research directions for advancing QSE.","2025-05","2025-11-25 22:30:06","2025-11-25 22:30:06","","","","5","34","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Quantum Software Engineering; open challenges; QSE; Quantum Computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3SYSH6TD","journalArticle","2025","Ni, Yunbo; Liu, Zixi; Feng, Yang; Chen, Runtao; Xu, Baowen","PanicFI: An Infrastructure for Fixing Panic Bugs in Real-World Rust Programs","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3773991","https://doi.org/10.1145/3773991","The Rust programming language has garnered significant attention due to its robust safety features and memory management capabilities. Despite its guaranteed memory safety, Rust programs suffer from runtime errors that are unmanageable, i.e., panic errors. Notably, traditional memory issues such as null pointer dereferences, which are prevalent in other languages, are less likely to be triggered in Rust due to its strict ownership rules. However, the unique nature of Rust's panic bugs, which arise from the language's stringent safety and ownership paradigms, presents a distinct challenge. Over half of the bugs in rustc, Rust's own compiler, are attributable to crashes stemming from panic errors. However, addressing Rust panic bugs is challenging and requires significant effort, as existing fix patterns are not directly applicable due to the design and feature of Rust language. Therefore, developing foundational infrastructure, including datasets, fixing patterns, and automated repair tools, is both critical and urgent.This paper introduces a comprehensive infrastructure, namely PanicFI, aimed at providing support for understanding Rust panic bugs and developing automated techniques. In PanicFI, we construct a dataset, Panic4R, comprising 102 real panic bugs and their fixes from the top 500 most downloaded open-source crates. Then, through an analysis of the Rust compiler implementation, we identify Rust-specific patterns for fixing panic bugs, providing insights and guidance for generating patches. Moreover, based on these patterns, we develop an automated fixing tool, namely PanicKiller, as an artifact, which has already contributed to the resolution of 28 panic bugs in open-source projects. The practicality and efficiency of PanicKiller confirm the effectiveness of the patterns mined within PanicFI. Furthermore, Panic4R serves as a benchmark for evaluating APR tools focused on Rust panic bugs. We believe the construction and release of PanicFI could enable the expansion of automated repair research tailored specifically to Rust programs, addressing unique challenges and contributing significantly to advancements in this field.","2025-11","2025-11-25 22:30:06","2025-11-25 22:30:06","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","program repair; fault localization; Rust","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PEHKF5TB","book","2025","","DataEd '25: Proceedings of the 4th International Workshop on Data Systems Education: Bridging Education Practice with Education Research","","979-8-4007-1918-9","","","","","2025","2025-11-25 22:30:06","2025-11-25 22:30:06","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3D8TLPS9","conferencePaper","2025","Xin, Amy; Qi, Yunjia; Yao, Zijun; Zhu, Fangwei; Zeng, Kaisheng; Xu, Bin; Hou, Lei; Li, Juanzi","LLMAEL: Large Language Models are Good Context Augmenters for Entity Linking","Proceedings of the 34th ACM International Conference on Information and Knowledge Management","979-8-4007-2040-6","","10.1145/3746252.3761156","https://doi.org/10.1145/3746252.3761156","Specialized entity linking (EL) models are well-trained at mapping mentions to unique knowledge base (KB) entities according to a given context. However, specialized EL models struggle to disambiguate long-tail entities due to their limited training data. Meanwhile, extensively pre-trained large language models (LLMs) possess broader knowledge of uncommon entities. Yet, with a lack of specialized EL training, LLMs frequently fail to generate accurate KB entity names, limiting their standalone effectiveness in EL. With the observation that LLMs are more adept at context generation instead of EL execution, we introduce LLM-Augmented Entity Linking (LLMAEL), the first framework to enhance specialized EL models with LLM data augmentation. LLMAEL leverages off-the-shelf, tuning-free LLMs as context augmenters, generating entity descriptions to serve as additional input for specialized EL models. Experiments show that LLMAEL sets new state-of-the-art results across 6 widely adopted EL benchmarks: compared to prior methods that integrate tuning-free LLMs into EL, LLMAEL achieves an absolute 8.9% gain in EL accuracy. We release our code and datasets.","2025","2025-11-25 22:30:06","2025-11-25 22:30:06","","3550–3559","","","","","","","CIKM '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Seoul, Republic of Korea","","","","large language models; data augmentation; entity disambiguation; entity linking; knowledge graphs","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QVQLCPUT","conferencePaper","2025","Jin, Hyoungwook; Yoo, Minju; Park, Jeongeon; Lee, Yokyung; Wang, Xu; Kim, Juho","TeachTune: Reviewing Pedagogical Agents Against Diverse Student Profiles with Simulated Students","Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems","979-8-4007-1394-1","","10.1145/3706598.3714054","https://doi.org/10.1145/3706598.3714054","Large language models (LLMs) can empower teachers to build pedagogical conversational agents (PCAs) customized for their students. As students have different prior knowledge and motivation levels, teachers must review the adaptivity of their PCAs to diverse students. Existing chatbot reviewing methods (e.g., direct chat and benchmarks) are either manually intensive for multiple iterations or limited to testing only single-turn interactions. We present TeachTune, where teachers can create simulated students and review PCAs by observing automated chats between PCAs and simulated students. Our technical pipeline instructs an LLM-based student to simulate prescribed knowledge levels and traits, helping teachers explore diverse conversation patterns. Our pipeline could produce simulated students whose behaviors correlate highly to their input knowledge and motivation levels within 5% and 10% accuracy gaps. Thirty science teachers designed PCAs in a between-subjects study, and using TeachTune resulted in a lower task load and higher student profile coverage over a baseline.","2025","2025-11-25 22:30:07","2025-11-25 22:30:07","","","","","","","","","CHI '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","LLM-assisted evaluation; Pedagogical conversational agents; Simulated students","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RDDAP86B","conferencePaper","2024","Lahiri, Sumit; Kalita, Pankaj Kumar; Chittora, Akshay Kumar; Vankudre, Varun; Roy, Subhajit","Program Synthesis Meets Visual What-Comes-Next Puzzles","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695015","https://doi.org/10.1145/3691620.3695015","What-Comes-Next (WCN) puzzles challenge us to identify the next figure that ""logically follows"" a provided sequence of figures. WCN puzzles are a favorite of interviewers and examiners—there is hardly any aptitude test that misses WCN puzzles. In this work, we propose to automatically synthesize WCN puzzles. The key insight to our methodology is that generation of WCN problems can be posed as a program synthesis problem. We design a small yet expressive language, PuzzlerLang, to capture solutions to WCN puzzles. PuzzlerLang is expressive enough to explain almost all human generated WCN puzzles that we collected, and yet, small enough to allow synthesis in a reasonable time. To ensure that the generated puzzles are appealing to humans, we infer a machine learning model to approximate the appeal factor of given WCN puzzle to humans. We use this model within our puzzle synthesizer as an optimization function to generate highly appealing and correct-by-construction WCN puzzles. We implemented our ideas in a tool, PuzzleGen; we found that PuzzleGen is fast, clocking an average time of about 3.4s per puzzle. Further, statistical tests over the responses from a user-study supported that the PuzzleGen generated puzzles were indistinguishable from puzzles created by humans.","2024","2025-11-25 22:30:07","2025-11-25 22:30:07","","418–429","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9I7I4NRC","book","2024","","ISEC '24: Proceedings of the 17th Innovations in Software Engineering Conference","","979-8-4007-1767-3","","","","","2024","2025-11-25 22:30:07","2025-11-25 22:30:07","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GSXTE96W","journalArticle","2025","Muramatsu, Ryotaro; Brščić, Dražen; Kanda, Takayuki","How Can Robots Acquire Venue-Specific Social Strategies? The Case of Robot Clerks Learning Sales Negotiations","J. Hum.-Robot Interact.","","","10.1145/3772064","https://doi.org/10.1145/3772064","Social robots are gradually being integrated into diverse social spheres, assuming multifaceted roles such as store clerks. Many of the interactive tasks entrusted to these robots are intricate, requiring more than simply responding to requests, and thus are not easily preprogrammed. One such task is sales negotiation. Empowering robots with negotiation skills is challenging due to the nuanced, context-dependent nature of effective tactics, which typically vary across different stores. To tackle this, we explore the potential of reinforcement learning without demonstrations for robots to grasp complex social interactions and adapt to specific environments, using sales negotiation as a test case. Drawing insights from interviews with experienced sales clerks and observations of negotiation role-plays, we identified essential verbal and nonverbal features. Coupled with a fast-learning reinforcement method, we evaluated the system through interactions with human participants in two distinct store settings. Results show that the robot was able to acquire negotiation strategies tailored to each environment.","2025-10","2025-11-25 22:30:07","2025-11-25 22:30:07","","","","","","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","reinforcement learning; sales negotiation; shopkeeper robot","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9VRRBQSE","conferencePaper","2023","Gumina, Sharon; Dalton, Travis; Gerdes, John","Teaching IT Software Fundamentals: Strategies and Techniques for Inclusion of Large Language Models: Strategies and Techniques for Inclusion of Large Language Models","Proceedings of the 24th Annual Conference on Information Technology Education","979-8-4007-0130-6","","10.1145/3585059.3611409","https://doi.org/10.1145/3585059.3611409","This paper argues for the inclusion of tools that utilize Artificial Intelligence (AI) Large Language Models (LLMs) in information technology (IT) undergraduate courses that teach the fundamentals of software. LLM tools have become widely available and disrupt traditional methods for teaching software concepts. Learning objectives are compromised when students submit AI-generated code for a classroom assignment without comprehending or validating the code. Since LLM tools including OpenAI Codex, Copilot by GitHub, and ChatGPT are being used in industry for software development, students need to be familiar with their use without compromising student learning. Incorporating LLM tools into the curriculum prepares students for real-world software development. However, students still need to understand software fundamentals including how to write and debug code. There are many challenges associated with the inclusion of AI tools into the IT curriculum that need to be addressed and mitigated. This paper presents strategies and techniques to integrate student use of LLM tools, assist students’ interaction with the tools, and help prepare students for careers that increasingly use AI tools to design, develop, and maintain software.","2023","2025-11-25 22:30:07","2025-11-25 22:30:07","","60–65","","","","","","","SIGITE '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Marietta, GA, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AYAIZILE","conferencePaper","2025","Böhme, Lukas; Thiede, Christoph; Mattis, Toni; Beckmann, Tom; Lincke, Jens; Hirschfeld, Robert","Toward Bridging the Tool Gap: Equipping Large Language Models with Tools to Answer Programmers’ Questions","Proceedings of the 4th ACM SIGPLAN International Workshop on Programming Abstractions and Interactive Notations, Tools, and Environments","979-8-4007-2160-1","","10.1145/3759534.3762682","https://doi.org/10.1145/3759534.3762682","Programmers ask complex questions in their search for solutions during software development. Along with traditional tools such as debuggers and profilers, state-of-the-art approaches like Babylonian Programming can help programmers answer those questions through interactive and visual feedback. Large language models (LLMs) and programming agents are part of programmers' toolboxes and are well-integrated into their development workflows. However, they are not yet helpful in considering questions involving run-time behavior. In this paper, we first review the literature to identify concerns programmers face during development and highlight how humans usually address them. We then focus on questions about program behavior and propose integrating Babylonian-style programming techniques with LLMs to help answer related questions. Finally, we suggest four key properties that future LLM-based development tools should support: (1) LLM tool usage traceability for explainability, (2) resumability of development progress for handovers between human programmers and LLM-based programming agents, (3) context efficiency through selective data querying, and (4) multi-source synthesis for tool integration.","2025","2025-11-25 22:30:07","2025-11-25 22:30:07","","15–24","","","","","","","PAINT '25","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Singapore, Singapore","","","","large language models; developer questions; programming assistants; tool-augmented LLMs","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DG86KILF","book","2025","","ITiCSE 2025: Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 1","","979-8-4007-1567-9","","","","Welcome to the 30th annual conference on Innovation and Technology in Computer Science Education (ITiCSE 2025), hosted by Radboud University in Nijmegen, The Netherlands.ITiCSE 2025 will take place from Friday, 27 June to Wednesday, 2 July. The conference program includes plenary lectures, paper sessions, panels, tips, techniques &amp; courseware demonstrations, posters, a doctoral consortium, and working group presentations. Working groups meet from 27 to 29 June and will submit draft reports before the conference begins on 30 June.The program includes two keynote talks by scholars from neighbouring disciplines, to help the community explore new challenges and reflect on scientific progress in the field. Our opening speaker, Inge Molenaar, will present her views about 'Human-AI Collaboration in Education: The Hybrid Future'. At the end of the conference, Danny Beckers will address the conference in a plenary session with a talk entitled 'On the Connection between Blind Dates and Teaching Programming'.Reviewing of submissions for ITiCSE 2025 involved 323 researchers and practitioners from computing education and related fields, including 38 programme committee members. Thanks to their outstanding effort and commitment, every submission received a metareview and most received at least three reviews, providing authors of all submissions with constructive feedback. Although no review process is flawless, we are confident that this effort led to a vibrant conference program, capturing multiple voices and perspectives in the field.We received 354 full paper submissions for the conference. The programme committee worked very hard to select 99 papers to be presented at ITiCSE 2025 and published in the proceedings. The proceedings also include the opening keynote abstract, 23 tips, techniques, &amp; courseware submissions, and abstracts for 9 working groups, 3 panels, 45 posters, and 16 doctoral consortium participants.","2025","2025-11-25 22:30:07","2025-11-25 22:30:07","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NXDWKQIH","conferencePaper","2024","Liu, Fang; Liu, Zhenwei; Zhao, Qianhui; Jiang, Jing; Zhang, Li; Sun, Zian; Li, Ge; Li, Zhongqi; Ma, Yuchi","FastFixer: An Efficient and Effective Approach for Repairing Programming Assignments","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695062","https://doi.org/10.1145/3691620.3695062","Providing personalized and timely feedback for student's programming assignments is useful for programming education. Automated program repair (APR) techniques have been used to fix the bugs in programming assignments, where the Large Language Models (LLMs) based approaches have shown promising results. Given the growing complexity of identifying and fixing bugs in advanced programming assignments, current fine-tuning strategies for APR are inadequate in guiding the LLM to identify bugs and make accurate edits during the generative repair process. Furthermore, the autoregressive decoding approach employed by the LLM could potentially impede the efficiency of the repair, thereby hindering the ability to provide timely feedback. To tackle these challenges, we propose FastFixer, an efficient and effective approach for programming assignment repair. To assist the LLM in accurately identifying and repairing bugs, we first propose a novel repair-oriented fine-tuning strategy, aiming to enhance the LLM's attention towards learning how to generate the necessary patch and its associated context. Furthermore, to speed up the patch generation, we propose an inference acceleration approach that is specifically tailored for the program repair task. The evaluation results demonstrate that FastFixer obtains an overall improvement of 20.46% in assignment fixing when compared to the state-of-the-art baseline. Considering the repair efficiency, FastFixer achieves a remarkable inference speedup of 16.67× compared to the autoregressive decoding algorithm.","2024","2025-11-25 22:30:07","2025-11-25 22:30:07","","669–680","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","large language models; automated program repair; programming education; inference acceleration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5HX262KQ","conferencePaper","2024","Li, Xueyang; Meng, Guozhu; Liu, Shangqing; Xiang, Lu; Sun, Kun; Chen, Kai; Luo, Xiapu; Liu, Yang","Attribution-guided Adversarial Code Prompt Generation for Code Completion Models","Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering","979-8-4007-1248-7","","10.1145/3691620.3695517","https://doi.org/10.1145/3691620.3695517","Large language models have made significant progress in code completion, which may further remodel future software development. However, these code completion models are found to be highly risky as they may introduce vulnerabilities unintentionally or be induced by a special input, i.e., adversarial code prompt. Prior studies mainly focus on the robustness of these models, but their security has not been fully analyzed.In this paper, we propose a novel approach AdvPro that can automatically generate adversarial code prompts for these code completion models. AdvPro incorporates 14 code mutation strategies at the granularity of five levels. The mutation strategies are ensured to make no modifications to code semantics, which should be insensitive to the models. Moreover, we leverage gradient attribution to localize the important code as mutation points and speed up adversarial prompt generation. Extensive experiments are conducted on 13 state-of-the-art models belonging to 7 families. The results show that our approach can effectively generate adversarial prompts, with an increased rate of 69.6% beyond the baseline ALERT. By comparing the results of attribution-guided localization, we find that the recognition results of important tokens in input codes are almost identical among different models. This finding reduces the limitation of using open-source alternative models to guide adversarial attacks against closed-source models. The results of the ablation study on the components of AdvPro show that CCMs focus on variable names, but other structures are equally crucial.","2024","2025-11-25 22:30:07","2025-11-25 22:30:07","","1460–1471","","","","","","","ASE '24","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: Sacramento, CA, USA","","","","adversarial prompts; attribution-guided localization; code completion models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4BCR994U","journalArticle","2025","Liu, Zixi; Feng, Yang; Ni, Yunbo; Li, Shaohua; Yin, Xizhe; Shi, Qingkai; Xu, Baowen; Su, Zhendong","An Empirical Study of Bugs in the rustc Compiler","Proc. ACM Program. Lang.","","","10.1145/3763800","https://doi.org/10.1145/3763800","Rust is gaining popularity for its well-known memory safety guarantees and high performance, distinguishing it from C/C++ and JVM-based languages. Its compiler, rustc, enforces these guarantees through specialized mechanisms such as trait solving, borrow checking, and specific optimizations. However, Rust's unique language mechanisms introduce complexity to its compiler, resulting in bugs that are uncommon in traditional compilers. With Rust's increasing adoption in safety-critical domains, understanding these language mechanisms and their impact on compiler bugs is essential for improving the reliability of both rustc and Rust programs. Such understanding could provide the foundation for developing more effective testing strategies tailored to rustc. Improving the quality of rustc testing is essential for enhancing compiler reliability, which in turn strengthens the safety and correctness of all Rust programs, as compiler bugs can silently propagate into every compiled program. Yet, we still lack a large-scale, detailed, and in-depth study of rustc bugs. To bridge this gap, this work presents a comprehensive and systematic study of rustc bugs, specifically those originating in semantic analysis and intermediate representation (IR) processing, which are stages that implement essential Rust language features such as ownership and lifetimes. Our analysis examines issues and fixes reported between 2022 and 2024, with a manual review of 301 valid issues. We categorize these bugs based on their causes, symptoms, affected compilation stages, and test case characteristics. Additionally, we evaluate existing rustc testing tools to assess their effectiveness and limitations. Our key findings include: (1) rustc bugs primarily arise from Rust's type system and lifetime model, with frequent errors in the High-Level Intermediate Representation (HIR) and Mid-Level Intermediate Representation (MIR) modules due to complex checkers and optimizations; (2) bug-revealing test cases often involve unstable features, advanced trait usages, lifetime annotations, standard APIs, and specific optimization levels; (3) while both valid and invalid programs can trigger bugs, existing testing tools struggle to detect non-crash errors, underscoring the need for further advancements in rustc testing.","2025-10","2025-11-25 22:30:07","2025-11-25 22:30:07","","","","OOPSLA2","9","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","testing; empirical study; Rust; bug study; compiler","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XQLNRZQD","journalArticle","2024","Pezzè, Mauro; Ciniselli, Matteo; Di Grazia, Luca; Puccinelli, Niccolò; Qiu, Ketai","The Trailer of the ACM 2030 Roadmap for Software Engineering","SIGSOFT Softw. Eng. Notes","","0163-5948","10.1145/3696117.3696126","https://doi.org/10.1145/3696117.3696126","The landscape of software engineering has dramatically changed. The recent advances in AI, the new opportunities of quantum computing, and the new challenges of sustainability and cyber security upset the software engineering research prospective. The 2030 SE-Roadmap special issue of ACM TOSEM Transactions on Software Engineering and Methodology gives a 360° view of the research challenges of the 30ties with a thorough editorial, four roadmap papers from the ACM TOSEM editorial board, and over 30 peer-reviewed papers from the research community.This paper previews the main content of the 2030 roadmap special issue with a report from the 2030 Software Engineering Roadmap workshop, co-located with ACM SIGSOFT FSE Foundations of Software engineering on July 15th and 16th, 2024 in Porto de Galinhas, Brazil, that spotlighted the software engineering research horizon to feed ideas into the ACM TOSEM special issue. The paper discusses the new challenges to software engineering that emerged in the SE2030 workshop: AI for software engineering, software engineering by and for humans, sustainable software engineering, automatic programming, cyber security, validation and verification, and quantum software engineering.","2024-10","2025-11-25 22:30:07","2025-11-25 22:30:07","","31–40","","4","49","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WF3EB2JG","conferencePaper","2023","Yang, Jun; Wang, Yuehan; Lou, Yiling; Wen, Ming; Zhang, Lingming","A Large-Scale Empirical Review of Patch Correctness Checking Approaches","Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","979-8-4007-0327-0","","10.1145/3611643.3616331","https://doi.org/10.1145/3611643.3616331","Automated Program Repair (APR) techniques have drawn wide attention from both academia and industry. Meanwhile, one main limitation with the current state-of-the-art APR tools is that patches passing all the original tests are not necessarily the correct ones wanted by developers, i.e., the plausible patch problem. To date, various Patch-Correctness Checking (PCC) techniques have been proposed to address this important issue. However, they are only evaluated on very limited datasets as the APR tools used for generating such patches can only explore a small subset of the search space of possible patches, posing serious threats to external validity to existing PCC studies. In this paper, we construct an extensive PCC dataset, PraPatch (the largest manually labeled PCC dataset to our knowledge), to revisit all nine state-of-the-art PCC techniques. More specifically, our PCC dataset PraPatch includes 1,988 patches generated from the recent PraPR APR tool, which leverages highly-optimized bytecode-level patch executions and can exhaustively explore all possible plausible patches within its large predefined search space (including well-known fixing patterns from various prior APR tools). Our extensive study of representative PCC techniques on PraPatch has revealed various findings, including: 1) the assumption made by existing static PCC techniques that correct patches are more similar to buggy code than incorrect plausible patches no longer holds, 2) state-of-the-art learning-based techniques tend to suffer from the dataset overfitting problem, 3) while dynamic techniques overall retain their effectiveness on our new dataset, their performance drops substantially on patches with more complicated changes and 4) the very recent naturalness-based techniques can substantially outperform traditional static techniques and could be a promising direction for PCC. Based on our findings, we also provide various guidelines/suggestions for advancing PCC in the near future.","2023","2025-11-25 22:30:07","2025-11-25 22:30:07","","1203–1215","","","","","","","ESEC/FSE 2023","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: San Francisco, CA, USA","","","","Empirical assessment; Program repair; Patch correctness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BYEG97T9","bookSection","2025","Ma, Youpeng; Chen, Tao; Li, Ke","Faster Configuration Performance Bug Testing with Neural Dual-Level Prioritization","Proceedings of the IEEE/ACM 47th International Conference on Software Engineering","979-8-3315-0569-1","","","https://doi.org/10.1109/ICSE55347.2025.00201","As software systems become more complex and configurable, more performance problems tend to arise from the configuration designs. This has caused some configuration options to unexpectedly degrade performance which deviates from their original expectations designed by the developers. Such discrepancies, namely configuration performance bugs (CPBugs), are devastating and can be deeply hidden in the source code. Yet, efficiently testing CPBugs is difficult, not only due to the test oracle is hard to set, but also because the configuration measurement is expensive and there are simply too many possible configurations to test. As such, existing testing tools suffer from lengthy runtime or have been ineffective in detecting CPBugs when the budget is limited, compounded by inaccurate test oracle.In this paper, we seek to achieve significantly faster CP-Bug testing by neurally prioritizing the testing at both the configuration option and value range levels with automated oracle estimation. Our proposed tool, dubbed NDP, is a general framework that works with different heuristic generators. The idea is to leverage two neural language models: one to estimate the CPBug types that serve as the oracle while, more vitally, the other to infer the probabilities of an option being CPBug-related, based on which the options and the value ranges to be searched can be prioritized. Experiments on several widely-used systems of different versions reveal that NDP can, in general, better predict CPBug type in 87% cases and find more CPBugs with up to 88.88× testing efficiency speedup over the state-of-the-art tools.","2025","2025-11-25 22:30:07","2025-11-25 22:30:07","","988–1000","","","","","","","","","","","IEEE Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"87F7L5GW","journalArticle","2024","Gooch, Daniel; Waugh, Kevin; Richards, Mike; Slaymaker, Mark; Woodthorpe, John","Exploring the Profile of University Assessments Flagged as Containing AI-Generated Material","ACM Inroads","","2153-2184","10.1145/3656478","https://doi.org/10.1145/3656478","","2024-05","2025-11-25 22:30:07","2025-11-25 22:30:07","","39–47","","2","15","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"33945MQ4","conferencePaper","2023","Liu, Jiawei; Peng, Jinjun; Wang, Yuyao; Zhang, Lingming","NeuRI: Diversifying DNN Generation via Inductive Rule Inference","Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","979-8-4007-0327-0","","10.1145/3611643.3616337","https://doi.org/10.1145/3611643.3616337","Deep Learning (DL) is prevalently used in various industries to improve decision-making and automate processes, driven by the ever-evolving DL libraries and compilers. The correctness of DL systems is crucial for trust in DL applications. As such, the recent wave of research has been studying the automated synthesis of test-cases (i.e., DNN models and their inputs) for fuzzing DL systems. However, existing model generators only subsume a limited number of operators, lacking the ability to pervasively model operator constraints. To address this challenge, we propose NeuRI, a fully automated approach for generating valid and diverse DL models composed of hundreds of types of operators. NeuRI adopts a three-step process: (i) collecting valid and invalid API traces from various sources; (ii) applying inductive program synthesis over the traces to infer the constraints for constructing valid models; and (iii) using hybrid model generation which incorporates both symbolic and concrete operators. Our evaluation shows that NeuRI improves branch coverage of TensorFlow and PyTorch by 24% and 15% over the state-of-the-art model-level fuzzers. NeuRI finds 100 new bugs for PyTorch and TensorFlow in four months, with 81 already fixed or confirmed. Of these, 9 bugs are labelled as high priority or security vulnerability, constituting 10% of all high-priority bugs of the period. Open-source developers regard error-inducing tests reported by us as ""high-quality"" and ""common in practice"".","2023","2025-11-25 22:30:07","2025-11-25 22:30:07","","657–669","","","","","","","ESEC/FSE 2023","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: San Francisco, CA, USA","","","","Fuzzing; Compiler Testing; Deep Learning Compilers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D5K76ZQI","book","2024","","ICSIE '24: Proceedings of the 2024 13th International Conference on Software and Information Engineering","","979-8-4007-1776-5","","","","","2024","2025-11-25 22:30:07","2025-11-25 22:30:07","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8CTVLIP2","journalArticle","2025","Gao, Yi; Hu, Xing; Yang, Xiaohu; Xia, Xin","Automated Unit Test Refactoring","Proc. ACM Softw. Eng.","","","10.1145/3715750","https://doi.org/10.1145/3715750","Test smells arise from poor design practices and insufficient domain knowledge, which can lower the quality of test code and make it harder to maintain and update. Manually refactoring of test smells is time-consuming and error-prone, highlighting the necessity for automated approaches. Current rule-based refactoring methods often struggle in scenarios not covered by predefined rules and lack the flexibility needed to handle diverse cases effectively. In this paper, we propose a novel approach called UTRefactor, a context-enhanced, LLM-based framework for automatic test refactoring in Java projects. UTRefactor extracts relevant context from test code and leverages an external knowledge base that includes test smell definitions, descriptions, and DSL-based refactoring rules. By simulating the manual refactoring process through a chain-of-thought approach, UTRefactor guides the LLM to eliminate test smells in a step-by-step process, ensuring both accuracy and consistency throughout the refactoring. Additionally, we implement a checkpoint mechanism to facilitate comprehensive refactoring, particularly when multiple smells are present. We evaluate UTRefactor on 879 tests from six open-source Java projects, reducing the number of test smells from 2,375 to 265, achieving an 89% reduction. UTRefactor outperforms direct LLM-based refactoring methods by 61.82% in smell elimination and significantly surpasses the performance of a rule-based test smell refactoring tool. Our results demonstrate the effectiveness of UTRefactor in enhancing test code quality while minimizing manual involvement.","2025-06","2025-11-25 22:30:07","2025-11-25 22:30:07","","","","FSE","2","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; Test Smells; Test Refactoring","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6IY3V4EG","journalArticle","2024","Neumann, Peter G.; Lindqvist, Ulf","The Future of Misuse Detection","Commun. ACM","","0001-0782","10.1145/3689596","https://doi.org/10.1145/3689596","From lessons learned to new directions.","2024-10","2025-11-25 22:30:07","2025-11-25 22:30:07","","27–28","","11","67","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C6JPTDZS","conferencePaper","2023","Jain, Kush; Alon, Uri; Groce, Alex; Le Goues, Claire","Contextual Predictive Mutation Testing","Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","979-8-4007-0327-0","","10.1145/3611643.3616289","https://doi.org/10.1145/3611643.3616289","Mutation testing is a powerful technique for assessing and improving test suite quality that artificially introduces bugs and checks whether the test suites catch them. However, it is also computationally expensive and thus does not scale to large systems and projects. One promising recent approach to tackling this scalability problem uses machine learning to predict whether the tests will detect the synthetic bugs, without actually running those tests. However, existing predictive mutation testing approaches still misclassify 33% of detection outcomes on a randomly sampled set of mutant-test suite pairs. We introduce MutationBERT, an approach for predictive mutation testing that simultaneously encodes the source method mutation and test method, capturing key context in the input representation. Thanks to its higher precision, MutationBERT saves 33% of the time spent by a prior approach on checking/verifying live mutants. MutationBERT, also outperforms the state-of-the-art in both same project and cross project settings, with meaningful improvements in precision, recall, and F1 score. We validate our input representation, and aggregation approaches for lifting predictions from the test matrix level to the test suite level, finding similar improvements in performance. MutationBERT not only enhances the state-of-the-art in predictive mutation testing, but also presents practical benefits for real-world applications, both in saving developer time and finding hard to detect mutants that prior approaches do not.","2023","2025-11-25 22:30:08","2025-11-25 22:30:08","","250–261","","","","","","","ESEC/FSE 2023","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","event-place: San Francisco, CA, USA","","","","test oracles; mutation analysis; code coverage","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F8D7NQ8T","journalArticle","2025","Papicchio, Simone; Papotti, Paolo; Cagliero, Luca","QATCH: Automatic Evaluation of SQL-Centric Tasks on Proprietary Data","ACM Trans. Intell. Syst. Technol.","","2157-6904","10.1145/3712704","https://doi.org/10.1145/3712704","Tabular Representation Learning (TRL) and Large Language Models (LLMs) have become established for tackling Question Answering (QA) and Semantic Parsing (SP) tasks on tabular data. State-of-the-art models are pre-trained and evaluated on large open-domain datasets. However, the performance on existing QA and SP benchmarks is not necessarily representative of that achieved on proprietary data as the characteristics of the input and the complexity of the posed queries show high variability. To tackle this challenge, our goal is to allow end-users to evaluate TRL and LLM performance on their own proprietary data. We present Query-Aided TRL CHecklist (QATCH), a toolbox to automatically generate a testing checklist tailored to QA and SP. QATCH provides a testing suite highlighting models’ strengths and weaknesses on relational tables unseen at training time. The proposed toolbox relies on a SQL query generator that crafts tests of varying types and complexity including, amongst others, tests on null values, projection, selections, joins, group by, and having clauses. QATCH also supports a set of general cross-task performance metrics providing more insights into SQL-related model capabilities than currently used metrics. The empirical results, achieved by state-of-the-art TRL models and LLMs, show substantial performance differences (1) between existing benchmarks and proprietary data, (2) across queries of different complexity.","2025-04","2025-11-25 22:30:08","2025-11-25 22:30:08","","","","2","16","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Large Language Models; Query Generation; Semantic Parsing; Table Question Answering; Tabular Representation Learning; Text2SQL","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9DBXRJ5H","journalArticle","2024","Chen, Zhenpeng; Zhang, Jie M.; Hort, Max; Harman, Mark; Sarro, Federica","Fairness Testing: A Comprehensive Survey and Analysis of Trends","ACM Trans. Softw. Eng. Methodol.","","1049-331X","10.1145/3652155","https://doi.org/10.1145/3652155","Unfair behaviors of Machine Learning (ML) software have garnered increasing attention and concern among software engineers. To tackle this issue, extensive research has been dedicated to conducting fairness testing of ML software, and this article offers a comprehensive survey of existing studies in this field. We collect 100 papers and organize them based on the testing workflow (i.e., how to test) and testing components (i.e., what to test). Furthermore, we analyze the research focus, trends, and promising directions in the realm of fairness testing. We also identify widely adopted datasets and open-source tools for fairness testing.","2024-06","2025-11-25 22:30:08","2025-11-25 22:30:08","","","","5","33","","","","","","","","","","","","","","","","","Place: New York, NY, USA Publisher: Association for Computing Machinery","","","","Machine learning; survey; analysis; fairness testing; trends","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7SEHXPDH","book","2025","","Koli Calling '25: Proceedings of the 25th Koli Calling International Conference on Computing Education Research","","979-8-4007-1599-0","","","","","2025","2025-11-25 22:30:08","2025-11-25 22:30:08","","","","","","","","","","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B2VA3KBE","conferencePaper","2025","Zhu, Haining; Zhang, Haibo","Framework and Performance Evaluation of Test Case Generation for Large Language Models in Software Testing","2025 4th International Conference on Electronic Information Technology (EIT)","","","10.1109/EIT67313.2025.11232178","","Aiming at the unique properties of LLM generated test cases, this paper introduces a multi-dimensional quality evaluation framework, involving three levels: basic quality, model-specific performance, and structural coverage. The experiment is carried out with reference to the public dataset TestEval released in 2024 , and six mainstream LLM models are selected for comprehensive testing. Models such as GPT-4o show strong capabilities in test execution ratio and structural coverage, but generally have high hallucination rates, poor consistency, and lack of context understanding. The quantitative parameter values such as TE, BD, HR, and CU introduced in this paper can accurately reveal the behavioral divisions and capability boundaries of LLM generated test cases. The combination of multi-dimensional evaluation and behavioral modeling is a key way to improve the credibility of LLM test case generation.","2025-08","2025-11-25 22:33:23","2025-11-25 22:33:23","","642-647","","","","","","","","","","","","","","","","","","","","","","","","Analytical models; coverage; hallucination rate; large language model; Large language models; Optimization; Performance evaluation; quality assessment; Quality assessment; Semantics; software testing; Software testing; Solid modeling; Stability analysis; test case generation; Test pattern generators; TestEval dataset","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ASAM4PXE","conferencePaper","2025","Wei, Wei","Static Analysis and LLM for Comprehensive Java Unit Test Generation","2025 8th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)","","","10.1109/AEMCSE65292.2025.11042526","","Software testing is crucial in ensuring the reliability and correctness of software applications. However, generating comprehensive test cases manually can be time-consuming and error-prone. This paper introduces SAGEN, a tool designed to automate Java unit test generation by leveraging static analysis of Syntax Trees (AST) and large language models (LLMs). SAGEN identifies literal values and their ranges, generating test cases that improve coverage and quality. In our experiments, SAGEN outperforms traditional test case generation tools such as EvoSuite and Randoop. It demonstrates a 10 % improvement in code coverage and a 13 % enhancement in test case quality. Furthermore, SAGEN achieves a compile pass rate of 89.7 %, proving its effectiveness in producing both high-quality and reliable test cases.","2025-05","2025-11-25 22:33:23","2025-11-25 22:33:23","","87-92","","","","","","","","","","","","","","","","","","","","","","","","Large language models; software testing; Software testing; test case generation; Test pattern generators; Java; Java unit testing; LLM; Manuals; Software; Software engineering; Software reliability; static analysis; Static analysis; Syntactics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V9IN53PH","conferencePaper","2025","Li, Na; Xu, Yimei; Luo, Jiafan; Fang, Jiaxin","AI-Driven Test Case Generation Based on DeepSeek-Chat Large-Language Model","2025 5th International Symposium on Computer Technology and Information Science (ISCTIS)","","","10.1109/ISCTIS65944.2025.11065082","","With the increasing complexity of software systems, traditional test case generation methods have become inadequate to meet the demands for efficient and high-quality software testing[1]. This paper proposes an AI-driven test case generation technology based on the DeepSeek-Chat large model, aiming to streamline the process from requirements to automated test case generation and enhance the automation level and efficiency of software testing in the industry. Leveraging natural language processing (NLP) and machine learning (ML) techniques[2], DeepSeek-Chat is capable of automatically generating high-quality test cases, significantly improving test coverage and defect detection rates. Experimental results demonstrate that, compared to traditional methods, this technology achieves a 30% improvement in test coverage and a 25% increase in defect detection rate. This study provides a novel and efficient test case generation approach for the software testing industry, offering significant practical application value.","2025-05","2025-11-25 22:33:23","2025-11-25 22:33:23","","1-4","","","","","","","","","","","","","","","","","","","","","","","","Software testing; Syntactics; AI-Driven Test Case Generation; Automation; DeepSeek-Chat; Defect detection; Defect Detection Rate; Industries; Machine learning; Machine Learning; Natural language processing; Natural Language Processing; Software quality; Software systems; Software Test Automation; Test Coverage; Writing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2ZMP8EIX","conferencePaper","2025","Ardoğan, Berre; Kilinc, H. Hakan; Tarak, Eren; Aşkın, Barış","Understanding LLM Behavior in Test Case Prioritization: A Comparative Study","2025 10th International Conference on Computer Science and Engineering (UBMK)","","","10.1109/UBMK67458.2025.11206767","","Test case prioritization plays an important role in accelerating software testing and optimizing resource allocation by ensuring that the most critical test cases are executed first. In this work, we investigate the ability of large language models (LLMs) to autonomously prioritize test cases in the absence of labeled guidance. We use a dataset containing test cases from real-world HVAC web applications, labeled by an expert test engineer. We evaluate three advanced LLM programs (Chat-GPT, DeepSeek and Gemini) under three different scenarios: with all metadata, without execution time and without severity. The results show that Gemini achieves the highest accuracy (54.34%) in the all-metadata scenario, but its performance drops significantly and feature dependency emerges when severity is eliminated. In contrast, DeepSeek demonstrated the most stable and balanced behavior in all scenarios. ChatGPT, although consistent, performed the poorest in terms of alignment with the priorities set by the expert. The study reveals the success and deviations of LLMs in test case prioritization and provides insights into the consistency and explainability of their decision rationales.","2025-09","2025-11-25 22:33:23","2025-11-25 22:33:23","","1258-1263","","","","","","","","","","","","","","","","","","","","ISSN: 2521-1641","","","","Large language models; Optimization; Software testing; LLM; Accuracy; Chatbots; ChatGPT; DeepSeek; Gemini; HVAC; Life estimation; Market research; Metadata; Prioritization; Resource management; Test cases","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HJ8649EM","journalArticle","2025","Haldar, Susmita; Pierce, Mary; Fernando Capretz, Luiz","Exploring the Integration of Generative AI Tools in Software Testing Education: A Case Study on ChatGPT and Copilot for Preparatory Testing Artifacts in Postgraduate Learning","IEEE Access","","2169-3536","10.1109/ACCESS.2025.3545882","","Software testing education is important for building qualified testing professionals. To ensure that software testing graduates are ready for real-world challenges, it is necessary to integrate modern tools and technologies into the curriculum. With the emergence of Large Language Models (LLMs), their potential use in software engineering has become a focus, but their application in software testing education remains largely unexplored. This study, conducted in the Capstone Project course of a postgraduate software testing program, was carried out over two semesters with two distinct groups of students. A custom-built Travel Application limited to a web platform was used in the first semester. In the second semester, a new set of students worked with an open-source application, offering a larger-scale, multi-platform experience across web, desktop, and mobile platforms. Students initially created preparatory testing artifacts manually as a group deliverable. Following this, they were assigned an individual assignment to generate the same artifacts using LLM tools such as ChatGPT 3.5 in the first semester and Microsoft Copilot in the second. This process directly compared manually created artifacts and those generated using LLMs, leveraging AI for faster outputs. After completion, they responded to a set of assigned questions. The students’ responses were assessed using an integrated methodology, including quantitative and qualitative assessments, sentiment analysis to understand emotions, and a thematic approach to extract deeper insights. The findings revealed that while LLMs can assist and augment manual testing efforts, they cannot entirely replace the need for manual testing. By incorporating innovative technology into the curriculum, this study highlights how Generative AI can support active learning, connect theoretical concepts with practical applications, and align educational practices with industry needs.","2025","2025-11-25 22:33:23","2025-11-25 22:33:23","","46070-46090","","","13","","","","","","","","","","","","","","","","","","","","","Large language models; Software testing; Software engineering; Industries; Accuracy; Chatbots; ChatGPT; Capstone project; Education; generative AI; Generative AI; Microsoft Copilot; sentiment analysis; Sentiment analysis; software testing education; Systematic literature review","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LBYX6NQN","conferencePaper","2025","Verkholomova, Alina; Nazaruka, Erika","Overview of the Application of Generative AI in Software Performance Testing","2025 66th International Scientific Conference on Information Technology and Management Science of Riga Technical University (ITMS)","","","10.1109/ITMS67030.2025.11236704","","Generative Artificial Intelligence (AI) tools and Large Language Models (LLMs) were integrated into all stages of the Software Development Life Cycle (SDLC), influencing requirements gathering and analysis, design and planning, development, testing, deployment, maintenance and support, and documentation. Meanwhile, software performance testing is an essential type of software testing that ensures stable software behavior under various scenarios. This paper aims to point out the most common approaches in integrating Generative AI across the stages of software performance testing and in related software testing activities, as well as the main challenges and limitations faced by researchers. This research followed the systematic literature review approach to analyze and synthesize the existing studies on the application of Generative AI in software performance testing. Within the review, eight papers published between 2024 and 2025 in research literature databases, including ScienceDirect, SpringerLink, IEEE Xplore, and Google Scholar, were selected and analyzed. The review results reveal that the application of Generative AI is mainly concentrated in functional testing, specifically in test case generation, with a limited adoption in test scenario generation and capturing non-functional requirements. Key challenges identified include the inconsistency of generated output and hallucinations of LLMs. The findings indicate a significant research gap in applying Generative AI in the process of software performance testing.","2025-10","2025-11-25 22:33:23","2025-11-25 22:33:23","","1-6","","","","","","","","","","","","","","","","","","","","ISSN: 2771-6937","","","","Large language models; software testing; Software testing; Generative AI; Systematic literature review; Internet; Maintenance; performance testing; Planning; Scenario generation; Software development management; Software performance; testing process improvement","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DHG7HE52","conferencePaper","2025","Azzam, Adham; Hany, Omar; Mansour, Hesham","Comparative Study on Test Case generation using Generative AI","2025 Intelligent Methods, Systems, and Applications​ (IMSA)","","","10.1109/IMSA65733.2025.11166964","","Testing is an important part of Development life cycle, resource intensive and prone to human error. Over the years, various tools have been developed, with a recent surge in those based on Large Language Models (LLMs). Examples include prompting LLMs in chat applications to generate test cases or using AI agents with project context to generate tests. This paper comparatively studies the most prominent open and closed-source models available at the time of writing. This paper covers the most famous open and closed source models as of the date of writing this paper in a comparative study. This study analyzes each model’s characteristics and performance against well known metrics like average code coverage and mutation score. While closed source models Like GPT-4o are demonstrating better performance than all other models at 35.2% coverage, some open source models such as Llama 3.1 70B is demonstrating significantly p romising p erformance with 30.6% coverage, or models like DeepSeekCoderV2 16B that have a better chance of running locally in a normal home setting with 28.2% coverage. This study highlights LLM capabilities in automated test generation.","2025-07","2025-11-25 22:33:23","2025-11-25 22:33:23","","366-369","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Test pattern generators; Software; Writing; Generative AI; Codes; Comparative Study; Faces; Large Language Model; Measurement; Surges; Testing; Unit Test Generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XKLX8TA5","conferencePaper","2025","Wang, Xu; Li, Shijie; Wang, Dandan; Ge, Yuhao; Yang, Yang","From Prototype Interface to Test Case Generation with Large Language Model","2025 25th International Conference on Software Quality, Reliability, and Security Companion (QRS-C)","","","10.1109/QRS-C65679.2025.00037","","As software systems continue to grow in complexity, the efficient generation of test cases has become a significant research challenge. Recently, the Large Language Model (LLM) has shown promise in various domains of software engineering, offering innovative solutions for software testing. This paper presents a novel approach to test case generation utilizing LLM from prototype interface. Prompts are dynamically constructed from interface images and used to guide the generation process. Experimental results indicate that the proposed method produces accurate and executable test cases, even in the absence of source code or in cross platform settings.","2025-07","2025-11-25 22:33:23","2025-11-25 22:33:23","","232-239","","","","","","","","","","","","","","","","","","","","ISSN: 2693-9371","","","","large language model; Large language models; Semantics; software testing; Software testing; test case generation; Software engineering; Software systems; Accuracy; Adaptation models; prompt engineering; Prototypes; Source coding; Visualization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7DE99B5N","conferencePaper","2025","Abdullin, Azat; Derakhshanfar, Pouria; Panichella, Annibale","Test Wars: A Comparative Study of SBST, Symbolic Execution, and LLM-Based Approaches to Unit Test Generation","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST62969.2025.10989033","","Generating tests automatically is a key and ongoing area of focus in software engineering research. The emergence of Large Language Models (LLMs) has opened up new op-portunities, given their ability to perform a wide spectrum of tasks. However, the effectiveness of LLM -based approaches compared to traditional techniques such as search-based software testing (SBST) and symbolic execution remains uncertain. In this paper, we perform an extensive study of automatic test generation approaches based on three tools: EvoSuite for SBST, Kex for symbolic execution, and TestSpark for LLM-based test generation. We evaluate tools' performance on the GitBug Java dataset and compare them using various execution-based and feature-based metrics. Our results show that while LLM-based test generation is promising, it falls behind traditional methods w.r.t. coverage. However, it significantly outperforms them in mutation scores, suggesting that LLMs provide a deeper semantic understanding of code. LLM-based approach performed worse than SBST and symbolic execution-based approaches w.r.t. fault detection capabilities. Additionally, our feature-based analysis shows that all tools are affected by the complexity and internal dependencies of the class under test (CUT), with LLM-based approaches being especially sensitive to the CUT size.","2025-03","2025-11-25 22:33:23","2025-11-25 22:33:23","","221-232","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Semantics; Software testing; Test pattern generators; Java; Software engineering; Codes; Measurement; automatic test generation; Computer bugs; concolic testing; Fault detection; large language models; search-based software testing; symbolic execution","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VW2T6NGB","conferencePaper","2023","Bayrı, Vahit; Demirel, Ece","AI-Powered Software Testing: The Impact of Large Language Models on Testing Methodologies","2023 4th International Informatics and Software Engineering Conference (IISEC)","","","10.1109/IISEC59749.2023.10391027","","Software testing is a crucial aspect of the software development lifecycle, ensuring the delivery of high-quality, reliable, and secure software systems. With the advancements in Artificial Intelligence (AI) and Natural Language Processing (NLP), Large Language Models (LLMs) have emerged as powerful tools capable of understanding and processing natural language texts easly. This article investigates the application of AI-based software testing, with a specific focus on the impact of LLMs in traditional testing methodologies. Through a comprehensive review of relevant literature and SeturDigital’s 25 year testing experience, this article explores the potential benefits, challenges, and prospects of integrating LLMs into software testing.","2023-12","2025-11-25 22:33:23","2025-11-25 22:33:23","","1-4","","","","","","","","","","","","","","","","","","","","","","","","large language model; software testing; Software testing; Software reliability; Industries; Software systems; Chatbots; artificial intelligence; Artificial intelligence; Location awareness; software testing life cycle","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BRVAIPTJ","conferencePaper","2024","Ma, Ruiyang; Yang, Yuxin; Liu, Ziqian; Zhang, Jiaxi; Li, Min; Huang, Junhua; Luo, Guojie","VerilogReader: LLM-Aided Hardware Test Generation","2024 IEEE LLM Aided Design Workshop (LAD)","","","10.1109/LAD62341.2024.10691801","","Test generation has been a critical and labor-intensive process in hardware design verification. Recently, the emergence of Large Language Model (LLM) with their advanced understanding and inference capabilities, has introduced a novel approach. In this work, we investigate the integration of LLM into the Coverage Directed Test Generation (CDG) process, where the LLM functions as a Verilog Reader. It accurately grasps the code logic, thereby generating stimuli that can reach unexplored code branches. We compare our framework with random testing, using our self-designed Verilog benchmark suite. Experiments demonstrate that our framework outperforms random testing on designs within the LLM’s comprehension scope. Our work also proposes prompt engineering optimizations to augment LLM’s understanding scope and accuracy.","2024-06","2025-11-25 22:33:24","2025-11-25 22:33:24","","1-5","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Optimization; Test pattern generators; LLM; Codes; Automatic Test Generation; Benchmark testing; Conferences; Hardware; Hardware design languages; Logic; Prompt engineering; Verilog","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PQ28FV95","conferencePaper","2025","Yilmazer, Merve; Karakose, Mehmet","LLM-Based Video Analytics Test Scenario Generation in Smart Cities","2025 29th International Conference on Information Technology (IT)","","","10.1109/IT64745.2025.10930297","","Rapid advances in the field of artificial intelligence have made significant contributions to the automation of software development and testing stages. Software created for use in various fields is tested with test scenarios created manually by software test experts or using test automation. Testing large-scale software with these methods complicates the testing phases because it requires increased human intervention and includes complex applications. In this study, an LLM-based scenario generation framework enhanced with prompt engineering is proposed for testing software to be used for video analysis in smart cities and smart campus areas. Thus, software test scenarios are created by strengthening large language models that are fast, flexible and have high learning ability using prompt engineering techniques. Test scenarios produced through LLM reinforced with prompt engineering techniques were evaluated with rarity and reality metrics and it was determined that more robust scenarios were produced compared to randomly generated test scenarios in the relevant field.","2025-02","2025-11-25 22:33:24","2025-11-25 22:33:24","","1-4","","","","","","","","","","","","","","","","","","","","ISSN: 2836-3744","","","","large language model; Large language models; software testing; Software testing; Software; Automation; Scenario generation; Software development management; Measurement; prompt engineering; Prompt engineering; generative artificial intelligence; smart cities; Smart cities; Visual analytics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2X6B3BX6","conferencePaper","2025","Genç, Sevdanur; Ceylan, Mustafa Furkan; İstanbullu, Ayhan","Software Unit Test Automation with LLM-Based Generative AI: Evaluating Test Quality through Code Coverage and Edge-Case Analysis","2025 10th International Conference on Computer Science and Engineering (UBMK)","","","10.1109/UBMK67458.2025.11206953","","Software unit testing is a critical verification step to ensure the correctness and reliability of software. However, manual writing of test cases is a time-consuming and error-prone process. This paper examines the integration of generative artificial intelligence models (LLM) into software test engineering and addresses automatic unit test generation. In the proposed method, test functions in pytest format are generated from the GPT-4 model using sample functions written in Python and these outputs are systematically evaluated. The validity of the tests is tested in terms of functional correctness by running them with pytest, and code coverage rates are measured with the coverage.py tool. Furthermore, the automatic generation of edge-case scenarios was analysed. The results show that the proposed method can produce tests with high accuracy and an average code coverage rate as high as 100%. At least one edge-case for each function was successfully tested. The results obtained show that generative artificial intelligence-based test generation can be integrated into software development processes and can significantly speed up and improve the accuracy of manual test writing. In this context, the study demonstrates that LLM-based test automation can be used as a productivity enhancing tool in software engineering and makes an original contribution to the literature.","2025-09","2025-11-25 22:33:24","2025-11-25 22:33:24","","242-247","","","","","","","","","","","","","","","","","","","","ISSN: 2521-1641","","","","Test pattern generators; Manuals; Software; Software reliability; Automation; Writing; Accuracy; Generative AI; Codes; Testing; Automated Unit Test Automation; Code Coverage; Edge-Case Analysis; Generative Artificial Intelligence; LLM (Large Language Models)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UA965LGT","conferencePaper","2023","Yi, Gaolei; Chen, Zizhao; Chen, Zhenyu; Wong, W. Eric; Chau, Nicholas","Exploring the Capability of ChatGPT in Test Generation","2023 IEEE 23rd International Conference on Software Quality, Reliability, and Security Companion (QRS-C)","","","10.1109/QRS-C60940.2023.00013","","The design of test is a crucial step in the field of software testing. The quality of test significantly impacts the effectiveness of software testing, with well-designed test cases improving the efficiency of bug detection. However, manual test case design and writing are often considered time-consuming and labor-intensive. With the emergence of large language models (LLMs), especially ChatGPT, the potential of LLMs in the field of test generation has become evident. Pretrained LLMs can learn and understand code in various programming languages and design test cases using multiple testing frameworks. In this paper, we used ChatGPT to generate tests for some tested projects. Through experiments, we found that ChatGPT has some gaps compared to traditional test generation tools, but its performance is closer to manual testing. However, the tests generated by ChatGPT exhibit higher readability. We believe that ChatGPT is better suited to serve as a manual testing assistant, helping understand the tested code and providing testing ideas.","2023-10","2025-11-25 22:33:24","2025-11-25 22:33:24","","72-80","","","","","","","","","","","","","","","","","","","","ISSN: 2693-9371","","","","Software testing; Test pattern generators; LLM; Manuals; Software reliability; Writing; Chatbots; ChatGPT; Codes; Test Generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KMX2VSZH","conferencePaper","2024","Haldar, Susmita; Pierce, Mary; Capretz, Luiz Fernando","WIP: Assessing the Effectiveness of ChatGPT in Preparatory Testing Activities","2024 IEEE Frontiers in Education Conference (FIE)","","","10.1109/FIE61694.2024.10893214","","This innovative practice WIP paper describes a research study that explores the integration of ChatGPT into the software testing curriculum and evaluates its effectiveness compared to human-generated testing artifacts. In a Capstone Project course, students were tasked with generating preparatory testing artifacts using ChatGPT prompts, which they had previously created manually. Their understanding and the effectiveness of the Artificial Intelligence generated artifacts were assessed through targeted questions. The results, drawn from this in-class assignment at a North American community college indicate that while ChatGPT can automate many testing preparation tasks, it cannot fully replace human expertise. However, students already familiar with Information Technology at the postgraduate level, found the integration of ChatGPT into their workflow to be straightforward. The study suggests that AI can be gradually introduced into software testing education to keep pace with technological advancements.","2024-10","2025-11-25 22:33:24","2025-11-25 22:33:24","","1-5","","","","","","","","","","","","","","","","","","","","ISSN: 2377-634X","","","","Software testing; Chatbots; ChatGPT; Artificial intelligence; Black-box Testing; Higher Education; Information technology; North America; Product Testing; Software Testing Education","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PP5LIVK5","conferencePaper","2025","Chenail-Larcher, Zacharie; Minani, Jean Baptiste; Moha, Naouel","Test Generation from Use Case Specifications for IoT Systems: Custom, LLM-Based, and Hybrid Approaches","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST62969.2025.10988996","","IoT systems are increasingly developed and deployed across various domains, where End-to-End (E2E) testing is critical to ensure reliability and expected behavior. However, generating comprehensive tests remains challenging due to the heterogeneity, distributed nature, and unique characteristics of IoT systems, which limit the effectiveness of generic test generation approaches. Recent studies demonstrated the effectiveness of Large Language Models (LLM) for test generation in traditional software systems. Building on this foundation, this study explores and evaluates four distinct approaches for generating E2E tests from use case specifications (UCSs) tailored to IoT systems. These include (1) a custom, (2) a single-stage LLM, (3) a multi-stage LLM, and (4) a hybrid approach combining custom and LLM capabilities. We evaluated these approaches on an IoT system, focusing on correctness and scenario coverage criteria. Experimental results indicate that all approaches perform well, with notable variations in specific aspects of test generation. The custom and hybrid approaches are more reliable in producing correctly structured and complete tests, with the hybrid approach slightly outperforming others. This study is a work in progress, requiring further investigation to fully realize its potential.","2025-03","2025-11-25 22:33:24","2025-11-25 22:33:24","","597-602","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Software testing; Test pattern generators; Software reliability; Software systems; Test Generation; Automated Testing; Buildings; End-to-End Testing; Focusing; Hybrid power systems; IoT System Testing; System testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TMLIIP9E","conferencePaper","2025","Lops, Andrea; Narducci, Fedelucio; Ragone, Azzurra; Trizio, Michelantonio; Bartolini, Claudio","A System for Automated Unit Test Generation using Large Language Models and Assessment of Generated Test Suites","2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","","","10.1109/ICSTW64639.2025.10962454","","Unit tests are fundamental for ensuring software correctness but are costly and time-intensive to design and create. Recent advances in Large Language Models (LLMs) have shown potential for automating test generation, though existing evaluations often focus on simple scenarios and lack scalability for real-world applications. To address these limitations, we present AgoneTest, an automated system for generating and assessing complex, class-level test suites for Java projects. Leveraging the Methods2Test dataset, we developed Classes2Test, a new dataset enabling the evaluation of LLM-generated tests against human-written tests. Our key contributions include a scalable automated software system, a new dataset, and a detailed methodology for evaluating test quality.","2025-03","2025-11-25 22:33:24","2025-11-25 22:33:24","","29-36","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Software testing; Test pattern generators; Java; Software systems; Large Language Model; Scalability; Conferences; Automatic Assessment; Software Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QABJY89S","conferencePaper","2025","Barboni, Morena; Lampa, Filippo; Morichetta, Andrea; Polini, Andrea; Zulkoski, Edward","Alchemist: LLM-Driven Test Generation using Solidity Mutants and the Scientific Method","2025 IEEE International Conference on Blockchain and Cryptocurrency (ICBC)","","","10.1109/ICBC64466.2025.11114643","","Bugs in Solidity smart contracts have led to significant financial losses, highlighting the importance of rigorous testing. Mutation testing is a powerful technique for evaluating test suite adequacy by identifying undetected faults introduced through small code changes. However, writing test cases for detecting live mutants is a labor-intensive task. This is especially true in the context of smart contracts, which involve complex interactions, access control considerations, and blockchain-specific behavior. To address this challenge, we propose Alchemist, a framework for generating Solidity test cases using Large Language Models (LLMs). Alchemist embeds the principles of the scientific method into the code generation process. This workflow can support the creation of more focused and interpretable mutant-killing tests, ultimately reducing developer effort.","2025-06","2025-11-25 22:33:24","2025-11-25 22:33:24","","1-5","","","","","","","","","","","","","","","","","","","","ISSN: 2832-8906","","","","Large language models; Test pattern generators; Writing; Codes; Large Language Model; Testing; Computer bugs; Test Generation; Blockchains; Cryptocurrency; Ethereum; Fault diagnosis; Mutation Testing; Smart Contract; Smart contracts; Solidity","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6JRWVSTG","conferencePaper","2024","Liu, Chang; Jiang, Yan; Zheng, Dongxia","Exploration of Course Practice on the Integration of AI Language Model and Ideological and Political Education : The Course of ""Software Testing"" as an Example","2024 5th International Conference on Information Science and Education (ICISE-IE)","","","10.1109/ICISE-IE64355.2024.11025195","","With the rapid development of information technology, artificial intelligence technology, and digital technology, cutting-edge technologies such as big data, intelligent algorithms, cloud computing, the Internet of Things, and blockchain are gradually penetrating into the field of education, promoting the modernization and digitization of education. In this context, software testing courses, as important courses in computer science and technology and software engineering majors, are facing new challenges and opportunities. In response to the mismatch between the curriculum teaching system and the new demands of the artificial intelligence era, it is proposed to focus on artificial intelligence technology, optimize the curriculum system from multiple dimensions, introduce AI language models to drive teaching, and pay attention to the deep integration of ideological and political education in the curriculum. This article demonstrates how to integrate ideological and political elements into software testing courses through specific cases. Practice has proven that AI driven courses can better stimulate students' interest in learning, help improve their ideological level, technical ability, and industry competitiveness.","2024-12","2025-11-25 22:33:24","2025-11-25 22:33:24","","392-396","","","","","","","","","","","","","","","","","","","","","","","","Software testing; Software; Software engineering; Industries; Education; Large Language Model; Artificial intelligence; Software algorithms; AI; Course ideology and politics; Roads; Software test; Technological innovation; Water","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UK37BSWD","conferencePaper","2024","Naimi, Lahbib; Bouziane, El Mahi; Manaouch, Mohamed; Jakimi, Abdeslam","A new approach for automatic test case generation from use case diagram using LLMs and prompt engineering","2024 International Conference on Circuit, Systems and Communication (ICCSC)","","","10.1109/ICCSC62074.2024.10616548","","The automation of test case generation from UML diagrams is a growing field that aims to make the software development process smoother. This paper suggests a new framework that uses generative artificial intelligence (AI) to turn use case diagrams into test cases that can be executed. By getting information from the XML representation of use case diagrams, we can create detailed instructions that guide a generative AI model to make test cases for each use case scenario. This method not only makes test case creation easier but also ensures we cover everything well and accurately, which could make software products get to market faster. this approach shows how traditional software engineering methods and new AI techniques can work well together, giving us an idea of what automated software testing might look like in the future.","2024-06","2025-11-25 22:33:24","2025-11-25 22:33:24","","1-5","","","","","","","","","","","","","","","","","","","","","","","","software testing; Software testing; Software; Software engineering; Automation; Software development management; Prompt engineering; LLMs prompt engineering; test cases; Unified modeling language; XML","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IQA63NJW","conferencePaper","2024","Keller, Tim","Thinktank: Leveraging LLM Reasoning for Advanced Task Execution in CI/CD","2024 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","","","10.1109/ICSTW60967.2024.00032","","Thinktank is a task execution framework that harnesses the reasoning capabilities of large language models (LLMs). These models demonstrate an ability to provide intermediate steps essential for approaching problem solutions. Thinktank capitalizes on this by employing an LLM, such as GPT-4, to iteratively solve given objectives. Initially, Thinktank processes each objective into actionable tasks that can be executed immediately using the available information. We employ a technique called ReAct prompting (https://arxiv.org/abs/2210.03629) to leverage the LLM’s reasoning abilities, guiding it to select appropriate Agents. Our method deviates from the original paper’s proposal; recognizing that the vast number of available functions could overwhelm the decision-making of statistical language models, we restrict the model’s function choices by automatically clustering Agents into no more than ten capabilities. These capabilities are dynamically recalculated whenever new Agents are added to the system.","2024-05","2025-11-25 22:33:24","2025-11-25 22:33:24","","108-108","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","large language model; Large language models; Software testing; Conferences; Cognition; Decision making; llm; Proposals; python; task execution","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P74KHSIK","conferencePaper","2025","Hasan, Navid Bin; Islam, Md. Ashraful; Khan, Junaed Younus; Senjik, Sanjida; Iqbal, Anindya","Automatic High-Level Test Case Generation using Large Language Models","2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR)","","","10.1109/MSR66628.2025.00105","","We explored the challenges practitioners face in software testing and proposed automated solutions to address these obstacles. We began with a survey of local software companies and 26 practitioners, revealing that the primary challenge is not writing test scripts but aligning testing efforts with business requirements. Based on these insights, we constructed a usecase \rightarrow (high-level) test-cases dataset to train/fine-tune models for generating high-level test cases. High-level test cases specify what aspects of the software’s functionality need to be tested, along with the expected outcomes. We evaluated large language models, such as GPT-4o, Gemini, LLaMA 3.1 8B, and Mistral 7B, where fine-tuning (the latter two) yields improved performance. A final (human evaluation) survey confirmed the effectiveness of these generated test cases. Our proactive approach strengthens requirement-testing alignment and facilitates early test case generation to streamline development.","2025-04","2025-11-25 22:33:24","2025-11-25 22:33:24","","674-685","","","","","","","","","","","","","","","","","","","","ISSN: 2574-3864","","","","Large language models; Software testing; Software; Writing; Faces; Large Language Model; Test Case Generation; Business; Companies; Data mining; Dataset; Surveys; Test Case; Use Case","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"28TQTWI9","conferencePaper","2024","Yoon, Juyeon; Feldt, Robert; Yoo, Shin","Intent-Driven Mobile GUI Testing with Autonomous Large Language Model Agents","2024 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST60714.2024.00020","","GUI testing checks if a software system behaves as expected when users interact with its graphical interface, e.g., testing specific functionality or validating relevant use case scenarios. Currently, deciding what to test at this high level is a manual task since automated GUI testing tools target lower level adequacy metrics such as structural code coverage or activity coverage. We propose DroidAgent, an autonomous GUI testing agent for Android, for semantic, intent-driven automation of GUI testing. It is based on Large Language Models and support mechanisms such as long- and short-term memory. Given an Android app, DroidAgent sets relevant task goals and subsequently tries to achieve them by interacting with the app. Our empirical evaluation of DroidAgent using 15 apps from the Themis benchmark shows that it can set up and perform realistic tasks, with a higher level of autonomy. For example, when testing a messaging app, DroidAgent created a second account and added a first account as a friend, testing a realistic use case, without human intervention. On average, DroidAgent achieved 61% activity coverage, compared to 51 % for current state-of-the-art GUI testing techniques. Further, manual analysis shows that 317 out of the 547 autonomously created tasks are realistic and relevant to app functionalities, and also that DroidAgent interacts deeply with the apps and covers more features.","2024-05","2025-11-25 22:36:44","2025-11-25 22:36:44","","129-139","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","large language model; Large language models; Semantics; software testing; Software testing; Manuals; Automation; Software systems; Measurement; artificial intelligence; test automation; GUI testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M24Y7LVF","conferencePaper","2024","Santos, Robson; Santos, Italo; Magalhaes, Cleyton; de Souza Santos, Ronnie","Are We Testing or Being Tested? Exploring the Practical Applications of Large Language Models in Software Testing","2024 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST60714.2024.00039","","A Large Language Model (LLM) represents a cutting-edge artificial intelligence model that generates content, including grammatical sentences, human-like paragraphs, and syntactically code snippets. LLMs can play a pivotal role in soft-ware development, including software testing. LLMs go beyond traditional roles such as requirement analysis and documentation and can support test case generation, making them valuable tools that significantly enhance testing practices within the field. Hence, we explore the practical application of LLMs in software testing within an industrial setting, focusing on their current use by professional testers. In this context, rather than relying on existing data, we conducted a cross-sectional survey and collected data within real working contexts-specifically, engaging with practitioners in industrial settings. We applied quantitative and qualitative techniques to analyze and synthesize our collected data. Our findings demonstrate that LLMs effectively enhance testing documents and significantly assist testing professionals in programming tasks like debugging and test case automation. LLMs can support individuals engaged in manual testing who need to code. However, it is crucial to emphasize that, at this early stage, software testing professionals should use LLMs with caution while well-defined methods and guidelines are being built for the secure adoption of these tools.","2024-05","2025-11-25 22:36:44","2025-11-25 22:36:44","","353-360","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; software testing; Software testing; Manuals; Software; Automation; Codes; large language models; Debugging; survey; LLMs; test engineers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BZV2IM9G","conferencePaper","2024","Edirisinghe, Hasali; Wickramaarachchi, Dilani","Quality Assurance for LLM-Generated Test Cases: A Systematic Literature Review","2024 8th SLAAI International Conference on Artificial Intelligence (SLAAI-ICAI)","","","10.1109/SLAAI-ICAI63667.2024.10844968","","The rapid advancements in artificial intelligence have transformed software testing, with Large Language Models (LLMs) emerging as powerful tools for automating test case generation. This paper explores Quality Assurance (QA) for LLM-generated test cases in black-box testing through a systematic literature review. Though LLMs are increasingly used for test case generation, challenges in ensuring their quality remain. Following PRISMA guidelines, relevant studies were selected from databases focusing on critical quality attributes, QA frameworks, metrics, and challenges. LLMs demonstrate high efficiency but face numerous issues. A recommendation for future research is given on addressing standardized metrics and improving human-AI collaboration for enhanced testing outcomes.","2024-12","2025-11-25 22:36:44","2025-11-25 22:36:44","","1-6","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Software testing; test case generation; Systematic literature review; Faces; Measurement; Focusing; large language models (LLMs); evaluation metrics; black-box testing; Collaboration; Databases; Guidelines; Quality assurance; Quality Assurance (QA)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LEV2K6KC","conferencePaper","2024","Kang, Long; Ai, Jun; Lu, Minyan","Automated Structural Test Case Generation for Human-Computer Interaction Software Based on Large Language Model","2024 11th International Conference on Dependable Systems and Their Applications (DSA)","","","10.1109/DSA63982.2024.00027","","As software systems expand in complexity, managing the vast and varied collection of test cases becomes increasingly difficult with traditional manual testing methods. This paper presents a new approach for automating the generation of structured test cases, named Test Element Extraction and Restructuring (TEER), which leverages the advanced natural language processing capabilities of large language models (LLMs). Specifically targeting human-computer interaction (HCI) software, TEER employs prompt tuning techniques to extract critical elements from natural language test cases and systematically reassemble them into structured formats. The study evaluates the effectiveness of TEER by applying it to common test cases from desktop HCI applications. The experimental results demonstrate that this method successfully produces structured test cases that meet predefined requirements.","2024-11","2025-11-25 22:36:45","2025-11-25 22:36:45","","132-140","","","","","","","","","","","","","","","","","","","","ISSN: 2767-6684","","","","Large language models; Software testing; LLM; Manuals; Software systems; Accuracy; Measurement; Test Case; Data models; Human computer interaction; Software Test; Training; Tuning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XWUHBQ4G","conferencePaper","2025","Ramler, Rudolf; Straubinger, Philipp; Plösch, Reinhold; Winkler, Dietmar","Poster: Unit Testing Past vs. Present: Examining LLMs' Impact on Defect Detection and Efficiency","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST62969.2025.10988960","","The integration of Large Language Models (LLMs), such as ChatGPT and GitHub Copilot, into software engineering workflows has shown potential to enhance productivity, particularly in software testing. This paper investigates whether LLM support improves defect detection effectiveness during unit testing. Building on prior studies comparing manual and tool-supported testing, we replicated and extended an experiment where participants wrote unit tests for a Java-based system with seeded defects within a time-boxed session, supported by LLMs. Comparing LLM supported and manual testing, results show that LLM support significantly increases the number of unit tests generated, defect detection rates, and overall testing efficiency. These findings highlight the potential of LLMs to improve testing and defect detection outcomes, providing empirical insights into their practical application in software testing.","2025-03","2025-11-25 22:36:45","2025-11-25 22:36:45","","733-736","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Software testing; Manuals; Software engineering; Defect detection; Chatbots; Software development management; Large Language Models; Buildings; Software Testing; Productivity; Defect Detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L38YTZI9","conferencePaper","2025","Bagmar, Vivek; Abhichandani, Sumit","Accelerating Software Release Cycles with Generative AI and Predictive Analytics: A Review","2025 International Conference on Engineering Innovations and Technologies (ICoEIT)","","","10.1109/ICoEIT63558.2025.11211611","","In the fast-changing world of software development, quality assurance is a core component in guaranteeing functionality, security, and reliability. With decreasing development cycles in Agile and DevOps development pipelines, conventional testing approaches need to evolve to stay effective, scalable, and agile in the quality assurance process. The advent of Artificial Intelligence (AI), specifically through the central pillars of Generative AI and Predictive Analytics, is transforming software testing by enabling intelligent automation, early detection of defects, and sophisticated testing techniques. This research explains AI-based solutions for testing and explores how generative models, including Generative Adversarial Networks (GANs), and predictive analytics platforms solve deep-seated issues in the testing pipeline. This paper, based on academic research, industry case studies, and expert opinions from consulting experts, develops a holistic framework that incorporates AI technologies in the Continuous Integration/Continuous Deployment (CI/CD) pipeline for realizing better test coverage, defect leakage reduction, and time-to-market. It discusses implementation issues, ethical implications, and potential developments in the use of AI in automating the testing process.","2025-07","2025-11-25 22:36:45","2025-11-25 22:36:45","","1304-1309","","","","","","","","","","","","","","","","","","","","","","","","Software testing; Software; Software reliability; Generative AI; Technological innovation; Security; Quality assurance; CI/CD integration; Pipelines; Predictive analytics; Reviews; Self-healing testing frameworks; Test automation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6DMB8WHL","conferencePaper","2023","Zhang, Yifan; Towey, Dave; Pike, Matthew","Automated Metamorphic-Relation Generation with ChatGPT: An Experience Report","2023 IEEE 47th Annual Computers, Software, and Applications Conference (COMPSAC)","","","10.1109/COMPSAC57700.2023.00275","","This paper reports on a pilot study of using ChatGPT, a language model based on GPT-3.5 architecture, for automatic generation of metamorphic relations (MRs), in the context of testing of autonomous driving systems (ADSs). The oracle problem is a major challenge in testing such systems, where it is difficult to determine whether or not the output of a system is correct. Metamorphic testing (MT) can alleviate this problem by checking the consistency of the system’s outputs under various transformations. However, manual generation of MRs is often a time-consuming and error-prone process. Automated MR generation can yield several benefits, including enhanced efficiency, quality, coverage, scalability, and reusability in software testing, thereby facilitating a more comprehensive and effective testing process. In this paper, we investigate the effectiveness of using ChatGPT for automatic generation of MRs for ADSs. We provide a detailed methodology for generating MRs using ChatGPT and evaluate the generated MRs using our domain knowledge and existing MRs. The results of our study indicate that our proposed approach is effective at generating high-quality MRs, and can significantly reduce the manual effort required for MR generation. Furthermore, we discuss the practical implications and limitations of using ChatGPT for MR generation and provide recommendations for future research. Our study contributes to the advancement of automated testing of ADSs, which is crucial for ensuring their safety and reliability in real-world scenarios.","2023-06","2025-11-25 22:36:45","2025-11-25 22:36:45","","1780-1785","","","","","","","","","","","","","","","","","","","","ISSN: 0730-3157","","","","Software testing; Manuals; Software; Chatbots; ChatGPT; Scalability; large language model (LLM); Safety; Autonomous driving system (ADS); Computer architecture; metamorphic relation (MR); metamorphic testing (MT); natural language processing (NLP); oracle problem","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TTNFFIZG","conferencePaper","2024","Troussas, Christos; Papakostas, Christos; Krouska, Akrivi; Mylonas, Phivos; Sgouropoulou, Cleo","Evaluating ChatGPT-driven Automated Test Generation for Personalized Programming Education","2024 2nd International Conference on Foundation and Large Language Models (FLLM)","","","10.1109/FLLM63129.2024.10852510","","Large Language Models (LLMs), such as ChatGPT, hold immense potential to irrevocably influence the dimension of educational technology by providing empowerment for personalized learning experiences. This paper covers the integration of ChatGPT into existing eLearning platforms toward supporting Java programming education. Using artificial intelligence-based capabilities of ChatGPT in natural language processing, our software enables instructors to generate individual module assessments tailored to a student's profile with unprecedented ease. Evaluation of the effectiveness and efficiency of the system was performed by a comprehensive review of the system, obtaining feedback from instructors and students, and analyzing test performance metrics. The evaluation showed that most teachers found the system very user friendly, with significant savings in time for test creation. Satisfaction related to personalized tests designed by ChatGPT was also adequate, and the average scores achieved on test cases set by ChatGPT were relatively high compared to those manually curated. Results underline the potential of ChatGPT-driven automated test generation for enhancing personalized programming education on eLearning platforms, making available tailored assessment, by consideration of individual needs of students, and increased learning outcomes and efficiency.","2024-11","2025-11-25 22:36:45","2025-11-25 22:36:45","","194-200","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Test pattern generators; Java; Software; Natural Language Processing; Accuracy; Chatbots; ChatGPT; Testing; Large Language Models; Automated Test Generation; Programming Education; Assessment; Educational Technology; eLearning; Electronic learning; Java Programming; Personalized Learning; Programming profession; Question generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"89CEEQSD","conferencePaper","2025","Ravi, Ravin; Bradshaw, Dylan; Ruberto, Stefano; Jahangirova, Gunel; Terragni, Valerio","LLMLOOP: Improving LLM-Generated Code and Tests Through Automated Iterative Feedback Loops","2025 IEEE International Conference on Software Maintenance and Evolution (ICSME)","","","10.1109/ICSME64153.2025.00109","","Large Language Models (LLMs) are showing remarkable performance in generating source code, yet the generated code often has issues like compilation errors or incorrect code. Researchers and developers often face wasted effort in implementing checks and refining LLM-generated code, frequently duplicating their efforts. This paper presents LLMLOOP, a framework that automates the refinement of both source code and test cases produced by LLMs. LLMLOOP employs five iterative loops: resolving compilation errors, addressing static analysis issues, fixing test case failures, and improving test quality through mutation analysis. These loops ensure the generation of high-quality test cases that serve as both a validation mechanism and a regression test suite for the generated code. We evaluated llmloop on HumanEval-X, a recent benchmark of programming tasks. Results demonstrate the tool effectiveness in refining LLM-generated outputs. A demonstration video of the tool is available at https://youtu.be/2CLG9x1fsNI.","2025-09","2025-11-25 22:36:45","2025-11-25 22:36:45","","930-934","","","","","","","","","","","","","","","","","","","","ISSN: 2576-3148","","","","Large language models; software testing; Software testing; Test pattern generators; Static analysis; Codes; Source coding; Large Language Models; automated test generation; program synthesis; AI4SE; Feedback loop; Iterative methods; Refining; Videos","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZIXW2GMU","conferencePaper","2025","Gorla, Daniele; Kumar, Shivam; Roselli Lorenzini, Pietro Nicolaus; Alipourfaz, Alireza","CubeTesterAI: Automated JUnit Test Generation Using the LLaMA Model","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST62969.2025.10988978","","This paper presents an approach to automating JUnit test generation for Java applications using the Spring Boot framework, leveraging the LLaMA (Large Language Model Architecture) model to enhance the efficiency and accuracy of the testing process. The resulting tool, called CubeTesterAI, includes a user-friendly web interface and the integration of a CI/CD pipeline using GitLab and Docker. These components streamline the automated test generation process, allowing developers to generate JUnit tests directly from their code snippets with minimal manual intervention. The final implementation executes the LLaMA models through RunPod, an online GPU service, which also enhances the privacy of our tool. Using the advanced natural language processing capabilities of the LLaMA model, CubeTesterAI is able to generate test cases that provide high code coverage and accurate validation of software functionalities in Java-based Spring Boot applications. Furthermore, it efficiently manages resource-intensive operations and refines the generated tests to address common issues like missing imports and handling of private methods. By comparing CubeTesterAI with some state-of-the-art tools, we show that our proposal consistently demonstrates competitive and, in many cases, better performance in terms of code coverage in different real-life Java programs.","2025-03","2025-11-25 22:36:45","2025-11-25 22:36:45","","565-576","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Software testing; Test pattern generators; Java; Software; Accuracy; Codes; Large Language Models; Proposals; Automated test generation; Pipelines; AI-assisted software testing; JUnit tests; LLaMA; Privacy","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4JVI4SVR","conferencePaper","2024","Bhat, Hemachandra Ramanath; P, Manesha; Muduli, Ankur Kumar; N, Suresh; N, Sudhakar K; Pullela, Phani Kumar","Multi-Objective Test Case Generation and Prioritization using Generative AI for ASIC Verification","2024 8th International Conference on Computational System and Information Technology for Sustainable Solutions (CSITSS)","","","10.1109/CSITSS64042.2024.10817057","","Great advancements in ASIC technology have cre-ated a need to develop reliable and efficient verification methods. Generative AI can be utilized for multi-objective prioritization and test case generation while verifying ASICs. Approaches that are guided by AI have had huge improvements in the last few years making them viable for dealing with the complexities of ASIC designs. This is really important in handling a large amount of test cases while verifying an ASIC. Although there are several challenges that need to be overcome before this is applied on a large scale. These challenges include dealing with complex structures of ASIC designs and several constraints. AI has the potential to revolutionize the traditional ASIC verification for Multi-objective test cases.","2024-11","2025-11-25 22:36:45","2025-11-25 22:36:45","","1-5","","","","","","","","","","","","","","","","","","","","ISSN: 2767-1097","","","","coverage; Generative AI; Benchmark testing; Information technology; testing; GenAI; Reliability; ASIC verification; Complexity theory; Research and development","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8SSE8KBL","conferencePaper","2024","Garlapati, Anusha; Satya Sai Muni Parmesh, M N V; Savitha; S, Jaisri","AI-Powered Multi-Agent Framework for Automated Unit Test Case Generation: Enhancing Software Quality through LLM’s","2024 5th IEEE Global Conference for Advancement in Technology (GCAT)","","","10.1109/GCAT62922.2024.10923987","","Recent years have witnessed an enormous rise in the design, repair and the enhancement of software automation tests. The reliability of program’s unit testing has major impact on its overall performance. The anticipated influence of Artificial Intelligence advancements on test automation methodologies are significant. Many studies on automated testing implicitly assume that the test results are deterministic, means that similar tests faults remain same. The precision of software is largely ensured by unit testing. But writing unit tests manually is a time-consuming process, which leads us to drive into ""Automation Analysis"". Recent years comprised the application of Large Language Models (LLM’s) in numerous fields related to software development, especially the automated creation of unit testing.However, these frameworks require more instructions, or few shot learnings on sample tests that already exist. This research provides a comprehensive empirical assessment of the efficiency of LLM’s for automating unit testing production, with no need for further manual analysis. The method we employ is put into practice for test cases, an adaptable Agents and LLM-based testing framework that evaluates test cases generated, by reviewing and re-writing them in different phases. Evaluation of this test cases was done by using mistral-large LLM Model. The analysis results that developed acquired an overall coverage of 100% for code given. Finally, to enhance the typical evaluation, this research suggests and concludes that LLMs, can be successfully incorporated into present practices, through adaptative instructions and improvements.","2024-10","2025-11-25 22:36:45","2025-11-25 22:36:45","","1-5","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Java; Manuals; Software reliability; Automation; Writing; Codes; Testing; Artificial Intelligence; Unit Tests; Standards; Agents; Large Language Models - LLM’s; Manual Testing; Switches","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VLNF26CT","conferencePaper","2025","Pan, Rangeet; Kim, Myeongsoo; Krishna, Rahul; Pavuluri, Raju; Sinha, Saurabh","ASTER: Natural and Multi-Language Unit Test Generation with LLMs","2025 IEEE/ACM 47th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)","","","10.1109/ICSE-SEIP66354.2025.00042","","Implementing automated unit tests is an important but time-consuming activity in software development. To assist developers in this task, many techniques for automating unit test generation have been developed. However, despite this effort, usable tools exist for very few programming languages. Moreover, studies have found that automatically generated tests suffer poor readability and do not resemble developer-written tests. In this work, we present a rigorous investigation of how large language models (LLMs) can help bridge the gap. We describe a generic pipeline that incorporates static analysis to guide LLMs in generating compilable and high-coverage test cases. We illustrate how the pipeline can be applied to different programming languages, specifically Java and Python, and to complex software requiring environment mocking. We conducted an empirical study to assess the quality of the generated tests in terms of code coverage and test naturalness-evaluating them on standard as well as enterprise Java applications and a large Python benchmark. Our results demonstrate that LLM-based test generation, when guided by static analysis, can be competitive with, and even outperform, state-of-the-art test-generation techniques in coverage achieved while also producing considerably more natural test cases that developers find easy to understand. We also present the results of a user study, conducted with 161 professional developers, that highlights the naturalness characteristics of the tests generated by our approach.","2025-04","2025-11-25 22:36:45","2025-11-25 22:36:45","","413-424","","","","","","","","","","","","","","","","","","","","ISSN: 2832-7659","","","","large language model; Large language models; Test pattern generators; Java; LLM; Software; Software engineering; Static analysis; Software development management; unit test generation; Standards; Python; Pipelines","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BHRKSTCF","conferencePaper","2023","Jalil, Sajed; Rafi, Suzzana; LaToza, Thomas D.; Moran, Kevin; Lam, Wing","ChatGPT and Software Testing Education: Promises & Perils","2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","","","10.1109/ICSTW58534.2023.00078","","Over the past decade, predictive language modeling for code has proven to be a valuable tool for enabling new forms of automation for developers. More recently, we have seen the ad-vent of general purpose ""large language models"", based on neural transformer architectures, that have been trained on massive datasets of human written text, which includes code and natural language. However, despite the demonstrated representational power of such models, interacting with them has historically been constrained to specific task settings, limiting their general applicability. Many of these limitations were recently overcome with the introduction of ChatGPT, a language model created by OpenAI and trained to operate as a conversational agent, enabling it to answer questions and respond to a wide variety of commands from end users.The introduction of models, such as ChatGPT, has already spurred fervent discussion from educators, ranging from fear that students could use these AI tools to circumvent learning, to excitement about the new types of learning opportunities that they might unlock. However, given the nascent nature of these tools, we currently lack fundamental knowledge related to how well they perform in different educational settings, and the potential promise (or danger) that they might pose to traditional forms of instruction. As such, in this paper, we examine how well ChatGPT performs when tasked with answering common questions in a popular software testing curriculum. We found that given its current capabilities, ChatGPT is able to respond to 77.5% of the questions we examined and that, of these questions, it is able to provide correct or partially correct answers in 55.6% of cases, provide correct or partially correct explanations of answers in 53.0% of cases, and that prompting the tool in a shared question context leads to a marginally higher rate of correct answers and explanations. Based on these findings, we discuss the potential promises and perils related to the use of ChatGPT by students and instructors.","2023-04","2025-11-25 22:36:45","2025-11-25 22:36:45","","4130-4137","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Software testing; Chatbots; ChatGPT; Codes; Conferences; testing; education; case study; Limiting; Natural languages; Predictive models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HPMCA9SU","conferencePaper","2023","Yu, Shengcheng; Fang, Chunrong; Ling, Yuchen; Wu, Chentian; Chen, Zhenyu","LLM for Test Script Generation and Migration: Challenges, Capabilities, and Opportunities","2023 IEEE 23rd International Conference on Software Quality, Reliability, and Security (QRS)","","","10.1109/QRS60937.2023.00029","","This paper investigates the application of large language models (LLM) in the domain of mobile application test script generation. Test script generation is a vital component of software testing, enabling efficient and reliable automation of repetitive test tasks. However, existing generation approaches often encounter limitations, such as difficulties in accurately capturing and reproducing test scripts across diverse devices, platforms, and applications. These challenges arise due to differences in screen sizes, input modalities, platform behaviors, API inconsistencies, and application architectures. Overcoming these limitations is crucial for achieving robust and comprehensive test automation.By leveraging the capabilities of LLMs, we aim to address these challenges and explore its potential as a versatile tool for test automation. We investigate how well LLMs can adapt to diverse devices and systems while accurately capturing and generating test scripts. Additionally, we evaluate its cross-platform generation capabilities by assessing its ability to handle operating system variations and platform-specific behaviors. Furthermore, we explore the application of LLMs in cross-app migration, where it generates test scripts across different applications and software environments based on existing scripts.Throughout the investigation, we analyze its adaptability to various user interfaces, app architectures, and interaction patterns, ensuring accurate script generation and compatibility. The findings of this research contribute to the understanding of LLMs’ capabilities in test automation. Ultimately, this research aims to enhance software testing practices, empowering app developers to achieve higher levels of software quality and development efficiency.","2023-10","2025-11-25 22:36:45","2025-11-25 22:36:45","","206-217","","","","","","","","","","","","","","","","","","","","ISSN: 2693-9177","","","","Software testing; Software reliability; Automation; Software quality; ChatGPT; Large Language Model; Test Generation; Mobile App Testing; Mobile applications; Behavioral sciences; Test Migration; User interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UBQXIVKQ","conferencePaper","2025","Steenhoek, Benjamin; Tufano, Michele; Sundaresan, Neel; Svyatkovskiy, Alexey","Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation","2025 IEEE/ACM International Workshop on Deep Learning for Testing and Testing for Deep Learning (DeepTest)","","","10.1109/DeepTest66595.2025.00011","","Software testing is a crucial but time-consuming aspect of software development, and recently, Large Language Models (LLMs) have gained popularity for automated test case generation. However, because LLMs are trained on vast amounts of open-source code, they often generate test cases that do not adhere to best practices and may even contain test smells (anti-patterns). To address this issue, we propose Reinforcement Learning from Static Quality Metrics (RLSQM), wherein we utilize Reinforcement Learning to generate high-quality unit tests based on static analysis-based quality metrics. First, we analyzed LLM-generated tests and show that LLMs frequently do generate undesirable test smells — up to 37% of the time. Then, we implemented lightweight static analysis-based reward model and trained LLMs using this reward model to optimize for five code quality metrics. Our experimental results demonstrate that the RL-optimized Codex model consistently generated higher-quality test cases than the base LLM, improving quality metrics by up to 23%, and generated nearly 100% syntactically-correct code. RLSQM also outperformed GPT-4 on all code quality metrics, in spite of training a substantially cheaper Codex model. We provide insights into how reliably utilize RL to improve test generation quality and show that RLSQM is a significant step towards enhancing the overall efficiency and reliability of automated software testing. Our data are available at this link: https://doi.org/10.6084/m9.figshare.25983166.","2025-05","2025-11-25 22:36:45","2025-11-25 22:36:45","","37-44","","","","","","","","","","","","","","","","","","","","","","","","Analytical models; Large language models; Software testing; Test pattern generators; Software reliability; Codes; Measurement; test generation; codex; gpt-4; reinforcement learning; test smells; deep learning; code smells; Deep learning; Training; Reinforcement learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PATTRP3T","conferencePaper","2024","Lohiya, Darshan; Golla, Monika Rani; Godboley, Sangharatna; Krishna, P. Radha","Poster: gptCombFuzz: Combinatorial Oriented LLM Seed Generation for effective Fuzzing","2024 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST60714.2024.00048","","The important contribution that large language models (LLMs) have made to the development of a new software testing era is the main objective of this proposed approach. It emphasizes the role that LLMs play in producing complex and diverse input seeds, which opens the way for efficient bug discovery. In the study we also introduce a systematic approach for combining various input values, employing the principles of Combinatorial testing using the PICT (Pairwise independent Combinatorial testing). By promoting a more varied set of inputs for thorough testing, PICT enhances the seed production process. Then we show how these different seeds may be easily included in the American Fuzzy Lop (AFL) tool, demonstrating how AFL can effectively use them to find and detect software flaws. This integrated technique offers a powerful yet straightforward approach to software Quality.","2024-05","2025-11-25 22:36:45","2025-11-25 22:36:45","","438-441","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Software quality; Codes; Large Language Model; Computer bugs; AFL; Combinatorial testing; Pairwise independent combinatorial testing; Production; Systematics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5URTDYX5","conferencePaper","2023","Aleti, Aldeida","Software Testing of Generative AI Systems: Challenges and Opportunities","2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE)","","","10.1109/ICSE-FoSE59343.2023.00009","","Software Testing is a well-established area in software engineering, encompassing various techniques and methodologies to ensure the quality of software systems. However, with the arrival of generative artificial intelligence (GenAI) systems, new challenges arise in the testing domain. These systems, capable of generating novel and creative outputs, introduce unique complexities that require novel testing approaches. In this paper, I aim to explore the challenges posed by GenAI systems and discuss potential opportunities for future research in the area of testing. I will touch on the specific characteristics of GenAI systems that make traditional testing techniques inadequate or insufficient. By addressing these challenges and pursuing further research, we can enhance our understanding of how to safeguard GenAI and pave the way for improved quality assurance in this rapidly evolving area.","2023-05","2025-11-25 22:36:45","2025-11-25 22:36:45","","4-14","","","","","","","","","","","","","","","","","","","","","","","","Software testing; Software engineering; Generative AI; Training; Complexity theory; Systematics; oracle; test suite adequacy; Uncertainty","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N4DFGFPU","journalArticle","2024","Wang, Junjie; Huang, Yuchao; Chen, Chunyang; Liu, Zhe; Wang, Song; Wang, Qing","Software Testing With Large Language Models: Survey, Landscape, and Vision","IEEE Transactions on Software Engineering","","1939-3520","10.1109/TSE.2024.3368208","","Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs. This paper provides a comprehensive review of the utilization of LLMs in software testing. It analyzes 102 relevant studies that have used LLMs for software testing, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative. It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs. It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in software testing.","2024-04","2025-11-25 22:36:45","2025-11-25 22:36:45","","911-936","","4","50","","","","","","","","","","","","","","","","","","","","","software testing; Software testing; LLM; Natural language processing; Software systems; Codes; GPT; Reviews; Computational modeling; Pre-trained large language model; Task analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NRE5XAK6","conferencePaper","2024","Deng, Yifei; Chen, Renzhi; Xiao, Chao; Yang, Zhijie; Luo, Yuanfeng; Zhao, Jingyue; Li, Na; Wan, Zhong; Ai, Yongbao; Dai, Huadong; Wang, Lei","LLM - TG: Towards Automated Test Case Generation for Processors Using Large Language Models","2024 IEEE 42nd International Conference on Computer Design (ICCD)","","","10.1109/ICCD63220.2024.00066","","Design verification (DV) has existed for decades and is crucial for identifying potential bugs before chip tape- out. Hand-crafting test cases is time-consuming and error-prone, even for experienced verification engineers. Prior work has attempted to lighten this burden by rule-guided random test case generation. However, this approach does not eliminate the manual effort required to write rules that describe detailed hardware behavior. Motivated by advances in large language models (LLMs), we explore their potential to capture register transfer level (RTL) behavior and construct prompts for test case generation based on RTL behavior. First, we introduce a prompt framework, LLM - Driven Test Generation (LLM - TG), to generate test cases, thereby enhancing LLMs' test generation capabilities. Additionally, we provide an open-source prompt library that offers a set of standardized prompts for processor verification, aiming to improve test generation efficiency. Lastly, we use an LLM to verify a 12-stage, multi-issue, out-of-order RV64GC processor, achieving at least an 8.34 % increase in block coverage and at least a 5.8 % increase in expression coverage compared to the state-of-the-art (SOTA) methods, LLM4DV and RISCV- DV. The prompt library is available at https://github.com/LLM-TGIPrompt_Library.","2024-11","2025-11-25 22:36:45","2025-11-25 22:36:45","","389-396","","","","","","","","","","","","","","","","","","","","ISSN: 2576-6996","","","","Large language models; Test pattern generators; Manuals; Natural language processing; Computer bugs; Hardware; Large Language Model (LLM); Libraries; Computational modeling; Design Verification; Out of order; Prompt Library; Register transfer level","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3P3SWFNC","conferencePaper","2025","Otto, Björn; Aidarkhan, Assanali; Ristin, Marko; Braunisch, Nico; Diedrich, Christian; van de Venn, Hans Wernher; Wollschlaeger, Martin","Code and Test Generation for I4.0 State Machines with LLM-based Diagram Recognition","2025 IEEE 21st International Conference on Factory Communication Systems (WFCS)","","","10.1109/WFCS63373.2025.11077624","","In the context of Industry 4.0, the automatic code and test generation from state diagrams embedded in specifications is a critical challenge for software correctness. In this paper we present an approach that leverages Large Language Models (LLMs) for the recognition of state diagrams to generate code and unit tests automatically. We compare the performance of LLMs with traditional computer vision models, highlighting the advantages of LLMs in terms of generalization and simplicity of setup. The results on two prominent industrial communication protocols, PROFINET and OPC UA, demonstrate the applicability of the approach, achieving significant reductions in manual effort and improving the accuracy of code and test generation.","2025-06","2025-11-25 22:36:45","2025-11-25 22:36:45","","1-8","","","","","","","","","","","","","","","","","","","","ISSN: 2835-8414","","","","Large language models; Test pattern generators; LLM; Manuals; Software; Codes; Large Language Model; Code generation; Computer vision; Diagram recognition; Fourth Industrial Revolution; Industrial communication; Industry 4.0; Production facilities; Protocols; Test case generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZNJAW8A4","conferencePaper","2025","Nan, Zifan; Guo, Zhaoqiang; Liu, Kui; Xia, Xin","Test Intention Guided LLM-Based Unit Test Generation","2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)","","","10.1109/ICSE55347.2025.00243","","The emergence of Large Language Models (LLMs) has accelerated the progress of intelligent software engineering technologies, which brings promising possibilities for unit test generation. However, existing approaches for unit tests directly generated from Large Language Models (LLMs) often prove impractical due to their low coverage and insufficient mocking capabilities. This paper proposes IntUT, a novel approach that utilizes explicit test intentions (e.g., test inputs, mock behaviors, and expected results) to effectively guide the LLM to generate high-quality test cases. Our experimental results on three industry Java projects and live study demonstrate that prompting LLM with test intention can generate high-quality test cases for developers. Specifically, it achieves the improvements on branch coverage by 94 % and line coverage by 49 %. Finally, we obtain developers' feedback on using IntUT to generate cases for three new Java projects, achieving over 80 % line coverage and 30 % efficiency improvement on writing unit test cases.","2025-04","2025-11-25 22:37:02","2025-11-25 22:37:02","","1026-1038","","","","","","","","","","","","","","","","","","","","ISSN: 1558-1225","","","","Large language models; Test pattern generators; Java; LLM; Software engineering; Industries; Life estimation; unit test generation; program analysis; mocking; test intention","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F8NNYF9K","conferencePaper","2025","Abideen, Zain Ul; Junxia, Guo","Intent Based E2E Automated Test Case Generation for Web Applications Using LLM","2025 IEEE 49th Annual Computers, Software, and Applications Conference (COMPSAC)","","","10.1109/COMPSAC65507.2025.00161","","Ensuring reliability in dynamic web applications is like hitting a moving target, where content constantly changes in response to user interactions and real-time updates. Therefore, testing these applications automatically remains challenging. Additionally, <canvas> elements, which render graphical content outside the DOM and lack semantic metadata, makes it difficult to automatically extract and validate their content, leading to incomplete testing in existing methods. This paper proposes a novel approach named E2E-TestGen, which based on intent-driven testing, leverages Large Language Models (LLMs). E2ETestGen interacts dynamically with the DOM by triggering JavaScript-driven updates and tracking real-time state transitions, ensuring comprehensive validation across all dynamic elements and interactions to improve coverage. For canvas-rendered graphical components, we integrate an image-based object detection and text extraction module to extract meaningful data from visual content, enabling the generation of realistic test case scenarios guided by user intent. The experiments on multiple open-source web applications are conducted, which show E2E-TestGen achieving 88.5% and 80.1% code and branch coverage, respectively, along with a 27.91% improvement in fault detection over baseline approaches.","2025-07","2025-11-25 22:37:02","2025-11-25 22:37:02","","1281-1290","","","","","","","","","","","","","","","","","","","","ISSN: 2836-3795","","","","Large language models; Semantics; Test pattern generators; Software; Large Language Model; Testing; Visualization; Fault detection; Test Generation; Automated Testing; Data mining; Robustness; Real-time systems; Web Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EUW9NYK5","conferencePaper","2025","Chandrasekaran, Jaganmohan; Patel, Ankita Ramjibhai; Lanus, Erin; Freeman, Laura J.","Evaluating Large Language Model Robustness using Combinatorial Testing","2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","","","10.1109/ICSTW64639.2025.10962520","","Recent advancements in large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating human-like text, leading to widespread adoption across domains. Given LLM’s versatile capabilities, current evaluation practices assess LLMs across a wide variety of tasks, including answer generation, sentiment analysis, text completion, and question and answers, to name a few. Multiple choice questions (MCQ) have emerged as a widely used evaluation task to assess LLM’s understanding and reasoning across various subject areas. However, studies from the literature have revealed that LLMs exhibit sensitivity to the ordering of options in MCQ tasks, with performance variations based on option sequence, thus underscoring the robustness concerns in LLM performance.This work presents a combinatorial testing-based framework for systematic and comprehensive robustness assessment of pre-trained LLMs. By leveraging the sequence covering array, the framework constructs test sets by systematically swapping the order of options, which are then used in ascertaining the robustness of LLMs. We performed an experimental evaluation using the Measuring Massive Multitask Language Understanding (MMLU) dataset, a widely used MCQ dataset and evaluated the robustness of GPT 3.5 Turbo, a pre-trained LLM. Results suggest the framework can effectively identify numerous robustness issues with a relatively minimal number of tests.","2025-03","2025-11-25 22:37:02","2025-11-25 22:37:02","","300-309","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Sentiment analysis; Conferences; Cognition; Robustness; Combinatorial testing; Systematics; Combinatorial Testing; LLM Evaluation; LLM Robustness; Option Order Swapping; Sensitivity; Testing AI; Testing LLM","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TTM24UBK","conferencePaper","2025","Maryam, Maryam; Biagiola, Matteo; Stocco, Andrea; Riccio, Vincenzo","Benchmarking Generative AI Models for Deep Learning Test Input Generation","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST62969.2025.10989043","","Test Input Generators (TIGs) are crucial to assess the ability of Deep Learning (DL) image classifiers to provide correct predictions for inputs beyond their training and test sets. Recent advancements in Generative AI(GenAI) models have made them a powerful tool for creating and manipulating synthetic images, although these advancements also imply increased complexity and resource demands for training. In this work, we benchmark and combine different GenAI models with TIGs, assessing their effectiveness, efficiency, and quality of the generated test images, in terms of domain validity and label preservation. We conduct an empirical study involving three different GenAI architectures (VAEs, GANs, Diffusion Models), five classification tasks of increasing complexity, and 364 human evaluations. Our results show that simpler architectures, such as VAEs, are sufficient for less complex datasets like MNIST. However, when dealing with feature-rich datasets, such as ImageNet, more sophisticated architectures like Diffusion Models achieve superior performance by generating a higher number of valid, misclassification-inducing inputs.","2025-03","2025-11-25 22:37:02","2025-11-25 22:37:02","","174-185","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Software testing; Generative AI; Benchmark testing; Software Testing; Deep Learning; Deep learning; Training; Computer architecture; Generators; Complexity theory; Diffusion models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CMD9SFVF","conferencePaper","2024","RAMABU, Tlou; MALEBANE, Tumelo","Guidelines for Effective Use of ChatGPT in Introductory Programming Education","2024 IST-Africa Conference (IST-Africa)","","","10.23919/IST-Africa63983.2024.10569684","","In the wake of highly capable Large Language Models (LLM) like ChatGPT, educational institutions have been navigating how to position themselves within this Artificial Intelligence (AI) era. There have been various suggestions and attempts to exclude ChatGPT in the education sector due to its AI abilities to give accurate responses to students, plagiarism and over reliance on the tool. However, there are also attempts to formally incorporate ChatGPT in education, such as in the field of economics, computer sciences or Mathematics. Without proper guidelines on the uses of ChatGPT in education, these AI technologies can be disruptive, uncontrollable and pose a risk to academic integrity. Based on the synthesis of ideas in the literature and ChatGPT experimental tests, this paper presents relevant guidelines for effective use of ChatGPT in the introductory programming education.","2024-05","2025-11-25 22:37:02","2025-11-25 22:37:02","","1-8","","","","","","","","","","","","","","","","","","","","ISSN: 2576-8581","","","","LLM; Accuracy; Chatbots; ChatGPT; Education; Artificial intelligence; education; Economics; Introductory programming; Mathematics; Navigation; Plagiarism","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9P9J7UXQ","conferencePaper","2025","Djajadi, Natanael; Deljouyi, Amirhossein; Zaidman, Andy","Using Large Language Models to Generate Concise and Understandable Test Case Summaries","2025 IEEE/ACM 33rd International Conference on Program Comprehension (ICPC)","","","10.1109/ICPC66645.2025.00040","","Software testing is essential, and automatic test case generation can be an important aid to software engineers. However, generated tests are sometimes difficult to understand. Test summarization approaches that provide an overview of what exactly is tested can provide help, but existing summarization approaches generate documentation that is lengthy and redundant. In this paper, we investigate whether large language models (LLMs) can be used to generate more concise, yet understandable summaries. In a small-scale user study with 11 participants, we obtained positive feedback on the LLM-generated summaries.","2025-04","2025-11-25 22:37:02","2025-11-25 22:37:02","","322-326","","","","","","","","","","","","","","","","","","","","ISSN: 2643-7171","","","","Large language models; Software testing; Test pattern generators; Software; Large Language Models; Automated Test Generation; Unit Testing; Readability; Documentation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CPEQREV8","conferencePaper","2025","Straubinger, Philipp; Kreis, Marvin; Lukasczyk, Stephan; Fraser, Gordon","Mutation Testing via Iterative Large Language Model-Driven Scientific Debugging","2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","","","10.1109/ICSTW64639.2025.10962485","","Large Language Models (LLMs) can generate plausible test code. Intuitively they generate this by imitating tests seen in their training data, rather than reasoning about execution semantics. However, such reasoning is important when applying mutation testing, where individual tests need to demonstrate differences in program behavior between a program and specific artificial defects (mutants). In this paper, we evaluate whether Scientific Debugging, which has been shown to help LLMs when debugging, can also help them to generate tests for mutants. In the resulting approach, LLMs form hypotheses about how to kill specific mutants, and then iteratively generate and refine tests until they succeed, all with detailed explanations for each step. We compare this method to three baselines: (1) directly asking the LLM to generate tests, (2) repeatedly querying the LLM when tests fail, and (3) search-based test generation with Pynguin. Our experiments evaluate these methods based on several factors, including mutation score, code coverage, success rate, and the ability to identify equivalent mutants. The results demonstrate that LLMs, although requiring higher computational cost, consistently outperform Pynguin in generating tests with better fault detection and coverage. Importantly, we observe that the iterative refinement of test cases is important for achieving high-quality test suites.","2025-03","2025-11-25 22:37:02","2025-11-25 22:37:02","","358-367","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Test pattern generators; Software; Codes; Fault detection; Large Language Models; Debugging; Test Generation; Mutation Testing; Cognition; Iterative methods; Computational modeling; Costs; Scientific Debugging","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JRAHWUNT","conferencePaper","2023","Li, Shun-Hang; Zhou, Gang; Li, Zhi-Bo; Lu, Ji-Cang; Huang, Ning-Bo","The Causal Reasoning Ability of Open Large Language Model: A Comprehensive and Exemplary Functional Testing","2023 IEEE 23rd International Conference on Software Quality, Reliability, and Security (QRS)","","","10.1109/QRS60937.2023.00032","","As the intelligent software, the development and application of large language models are extremely hot topics recently, bringing tremendous changes to general AI and software industry. Nonetheless, large language models, especially open source ones, incontrollably suffer from some potential software quality issues such as instability, inaccuracy, and insecurity, making software testing necessary. In this paper, we propose the first solution for functional testing of open large language models to check full-scene availability and conclude empirical principles for better steering large language models, particularly considering their black box and intelligence properties. Specifically, we focus on the model’s causal reasoning ability, which is the core of artificial intelligence but almost ignored by most previous work. First, for comprehensive evaluation, we deconstruct the causal reasoning capability into five dimensions and summary the forms of causal reasoning task as causality identification and causality matching. Then, rich datasets are introduced and further modified to generate test cases along with different ability dimensions and task forms to improve the testing integrity. Moreover, we explore the ability boundary of open large language models in two usage modes: prompting and lightweight fine-tuning. Our work conducts comprehensive functional testing on the causal reasoning ability of open large language models, establishes benchmarks, and derives empirical insights for practical usage. The proposed testing solution can be transferred to other similar evaluation tasks as a general framework for large language models or their derivations.","2023-10","2025-11-25 22:37:02","2025-11-25 22:37:02","","240-249","","","","","","","","","","","","","","","","","","","","ISSN: 2693-9177","","","","Software testing; Software reliability; Software quality; Cognition; black-box testing; Data models; Training data; Sensitivity; causal reasoning; lightweight fine-tuning; open large language model; prompt design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J2YB5AZ2","conferencePaper","2025","Rincon, André Mesquita; Vincenzi, Auri Marcelo Rizzo; Faria, João Pascoal","LLM Prompt Engineering for Automated White-Box Integration Test Generation in REST APIs","2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","","","10.1109/ICSTW64639.2025.10962507","","This study explores prompt engineering for automated white-box integration testing of RESTful APIs using Large Language Models (LLMs). Four versions of prompts were designed and tested across three OpenAI models (GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o) to assess their impact on code coverage, token consumption, execution time, and financial cost. The results indicate that different prompt versions, especially with more advanced models, achieved up to 90% coverage, although at higher costs. Additionally, combining test sets from different models increased coverage, reaching 96% in some cases. We also compared the results with EvoMaster, a specialized tool for generating tests for REST APIs, where LLM-generated tests achieved comparable or higher coverage in the benchmark projects. Despite higher execution costs, LLMs demonstrated superior adaptability and flexibility in test generation.","2025-03","2025-11-25 22:37:02","2025-11-25 22:37:02","","21-28","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Software testing; Test pattern generators; Codes; Large Language Models; Benchmark testing; Conferences; Prompt engineering; Prompt Engineering; REST APIs; Costs; Automated Integration Testing; Glass box; Restful API; White-Box Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q7VYGLES","conferencePaper","2024","Liu, Pan; Luo, Ruyi; Jiang, Chengwu; Gao, Tong; Li, Yihao","AI-Assisted Bug Detection in Open-Source Software","2024 11th International Conference on Dependable Systems and Their Applications (DSA)","","","10.1109/DSA63982.2024.00065","","With the rapid development of internet technology, open-source software mirror sites have become indispensable tools for developers and tech enthusiasts, serving as crucial platforms for resource acquisition. We selected and tested two open-source applications by downloading them from the Tsinghua University Open Source Software Mirror Site and the Nanjing University Open Source Software Mirror Site. By implementing black-box testing strategies, we identified several bugs and design flaws in the software. Furthermore, we utilized large language models such as ChatGPT-3.5, Gemini, Kimi, Llama, ERNIE Bot, and Tongyi Qianwen to analyze the potential causes of these issues, exploring new approaches to AI-assisted software quality assurance. Through comparative analysis of feedback from multiple interactions with these large language models, we systematically evaluated their effectiveness in the field of software testing. This study provides empirical evidence for optimizing model applications and enhancing testing efficiency.","2024-11","2025-11-25 22:37:02","2025-11-25 22:37:02","","428-429","","","","","","","","","","","","","","","","","","","","ISSN: 2767-6684","","","","Analytical models; Large language models; Software testing; Software quality; Chatbots; Computer bugs; Fault diagnosis; open-source software; AI-assisted testing; Closed box; Mirrors; Open source software; software bug analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B6JPVH5T","conferencePaper","2025","Uddin, Md Arfan","Graph-Based LLM Prompting for Scalable Microservice API Testing","2025 IEEE International Conference on Service-Oriented System Engineering (SOSE)","","","10.1109/SOSE67019.2025.00034","","Microservices offer flexibility and scalability, but their decentralized and fast-changing nature makes it difficult to maintain consistent and meaningful test coverage, particularly at the level of service endpoints. Services are developed independently, logic is spread across multiple layers, and execution paths vary widely based on input and control flow. As a result, automated testing is hard to scale, and manual testing is time-consuming and error-prone. Recent advances in large language models present a promising opportunity for generating tests automatically. However, existing approaches often rely on providing the entire source code as input, which can exceed model limitations and include unrelated logic that reduces test quality. This paper proposes a structured approach that uses interprocedural control flow graphs to guide language models in generating accurate, maintainable endpoint tests that better reflect the complexity of modern microservice systems.","2025-07","2025-11-25 22:37:02","2025-11-25 22:37:02","","243-244","","","","","","","","","","","","","","","","","","","","ISSN: 2642-6587","","","","Large language models; Manuals; Static analysis; Testing; Source coding; Scalability; Logic; Static Analysis; Test Automation; Large Language Model (LLM); Microservice; API Testing; Control Flow Graph (CFG); Flow graphs; Microservice architectures; Service-oriented systems engineering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2HGQRQXH","conferencePaper","2025","Rawson, Jessica; Reddivari, Sandeep","A ChatGPT-Powered Tool for Automating Context-Aware Acceptance Criteria Generation for User Stories","2025 IEEE International Conference on Information Reuse and Integration and Data Science (IRI)","","","10.1109/IRI66576.2025.00067","","There has been a growing interest in using Natural Language Processing (NLP), such as OpenAI's ChatGPT for software engineering tasks, including requirements engineering (RE), software design, and software testing. This paper covers a practical implementation of a ChatGPT-powered prompt engineering framework designed to generate context-specific acceptance criteria for user stories. The tool automates each stage of the framework—preprocessing contextual information and generating the final tailored acceptance criteria. We outline the design and implementation of the tool in this paper and its effectiveness through two repositories. This work demonstrates the potential of large language models (LLMs) to reduce the manual effort involved in RE, streamline development workflows, and minimize rework-related costs in agile software projects.","2025-08","2025-11-25 22:37:03","2025-11-25 22:37:03","","325-330","","","","","","","","","","","","","","","","","","","","ISSN: 2835-5776","","","","Large language models; Software testing; Manuals; Software engineering; Chatbots; ChatGPT; Prompt engineering; Requirements Engineering; Artificial Intelligence; Prompt Engineering; Acceptance Criteria; Data science; Requirements engineering; Rough surfaces; Software design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NUHEG8BS","conferencePaper","2025","Chintala, Hemanth G.; Alawneh, Luay; Al-Sharif, Ziad A.; Omari, Safwan","Enhancing Software Testing Using AI and Graph Similarity","2025 16th International Conference on Information and Communication Systems (ICICS)","","","10.1109/ICICS65354.2025.11073112","","Software testing plays a vital role in the development lifecycle, ensuring the prevention of failures and the enhancement of software quality. Despite its importance, the testing phase is often resource-intensive, involving numerous test cases that can become redundant or overlapping over time-leading to increased complexity and prolonged testing durations. To address these inefficiencies, this paper proposes a novel approach that integrates graph similarity analysis with generative AI and deep learning to optimize test suites. By leveraging call graphs derived from test cases, the method identifies redundant and closely related test scenarios. A machine learning model is used to predict similarity scores between these call graphs, facilitating the classification and prioritization of test cases. Lower similarity scores correspond to test cases with more unique code coverage and are thus assigned higher priority. This prioritization enables test engineers to focus on a more diverse and effective subset of test cases, ensuring thorough code coverage while improving efficiency. The proposed framework ultimately reduces redundancy, lowers testing costs, and upholds high standards of software quality, offering a systematic solution for determining the optimal level of testing required to meet study objectives. While the current study experimentally validates the use of graph similarity metrics for test case prioritization, the application of generative AI models is proposed as part of future extensions.","2025-07","2025-11-25 22:37:03","2025-11-25 22:37:03","","1-6","","","","","","","","","","","","","","","","","","","","ISSN: 2573-3346","","","","Software testing; Software quality; Resource management; Generative AI; Codes; Measurement; Software Testing; Standards; Systematics; Call Graph; Graph Similarity; Prevention and mitigation; Redundancy; Redundant Test Cases; Regression Testing; Similarity Scores; Test Case Prioritization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7XMIGBMT","journalArticle","2025","Cao, Daipeng; Hong, Yang; Pan, Qianqian; Wu, Jun","Program Interoperable Large Language Model Software Testing Scheme: A Case Study on JavaScript Engine Fuzzing","IEEE Transactions on Dependable and Secure Computing","","1941-0018","10.1109/TDSC.2025.3581213","","Large language models (LLMs) have cultivated impressive semantics capabilities and expert knowledge from their vast pre-training corpora, especially showing prospects in automated software testing. However, LLMs are designed for human interaction, which poses the following challenges when interacting with programs for testing: 1) LLMs cannot communicate directly with programs, and there is no existing paradigm to establish interaction between them. 2) Existing evaluation methods are unable to assess the quality of LLM-generated tests during software testing. 3) Current LLM-guided testing generation cannot be optimized in real time, resulting in low testing efficiency. To address these challenges, we present PILLM, a program interoperable LLM scheme. First, we designed a prompt mechanism for interactive program testing based on the source code semantics and expert knowledge from the LLM. Second, we proposed an evaluation mechanism for the PILLM's test code generation, thus obtaining test seeds that are semantically related to the corresponding source code. Third, PILLM optimizes the next generated tests based on the coverage and source code execution information obtained in the program execution. In the 24-hour running experiment, PILLM improved the coverage by 45.7% and 14.9% compared to Fuzz4all and Fuzzilli respectively. PILLM proves its effectiveness by finding four new real-world bugs in the JavaScript engine. We have released the source code of PILLM as open source on Github.","2025-11","2025-11-25 22:37:03","2025-11-25 22:37:03","","6179-6195","","6","22","","","","","","","","","","","","","","","","","","","","","Optimization; Semantics; Syntactics; Chatbots; Codes; Source coding; Computer bugs; Fuzzing; LLMs; Training; code semantic; Engines; JS engine; program interoperability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BMLG38XP","conferencePaper","2023","Lee, Eric; Gong, Jiayu; Cao, Qinghong","Object Oriented BDD and Executable Human-Language Module Specification","2023 26th ACIS International Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD-Winter)","","","10.1109/SNPD-Winter57765.2023.10223873","","This paper presents an approach to software development which uses a generative AI Model as compiler to translate human language requirements into high-level programming language. We propose an executable human-language module specification and a tool to support it, which has been used successfully for human-language UI test automation. We anticipate further development of this approach to enable complex software to be programmed in human language, allowing for more intuitive and efficient software development.","2023-07","2025-11-25 22:37:03","2025-11-25 22:37:03","","127-133","","","","","","","","","","","","","","","","","","","","","","","","Software; Automation; Generative AI; GPT; Behavioral sciences; Computational modeling; BDD; Computer languages; HLP (Human Language Programming); LLM (Large Language Model); Object oriented modeling; OOBDD (Object-oriented Behavior Driven Development); Program processors; SDD (Specification Driven Development)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6NQ467EB","conferencePaper","2025","Ni, Chao; Wang, Xiaoya; Yin, Xin; Chen, Liushan; Ma, Guojun","A Cascaded Pipeline for Self-Directed, Model-Agnostic Unit Test Generation via LLMs","2025 IEEE 36th International Symposium on Software Reliability Engineering (ISSRE)","","","10.1109/ISSRE66568.2025.00037","","While existing ML-based unit test generation methods show promising results, they face three key limitations: (1) incomplete test case generation with excessive focus on test oracles, (2) semantic inconsistencies between test components, and (3) dependency on closed-source models compromising data security. In this paper, we propose a novel approach named CasModaTest, a cascaded, model-agnostic, and end-to-end unit test generation framework, to alleviate the above limitations. Specifically, CasModaTest first splits the unit test generation task as two cascaded steps: test prefix generation and test oracle generation. Then, to better stimulate models’ learning ability, we manually build large-scale demo pools to provide CasModaTest with high-quality test prefixes and test oracles examples. Finally, CasModaTest assembles test components and validates their functionality through execution, with error correction during compilation/runtime. Our evaluation on the Defects4J benchmark demonstrates CasModaTest’s superiority over five state-of-the-art approaches, showing significant improvements in both accuracy and focal method coverage. Further validation across \mathbf1, 6 2 5 methods from six real-world projects reveals that CasModaTest achieves substantially higher code coverage metrics (method/line/branch coverage) compared to the dedicated coverage tool EvoSuite.","2025-10","2025-11-25 22:37:03","2025-11-25 22:37:03","","276-287","","","","","","","","","","","","","","","","","","","","ISSN: 2332-6549","","","","Large language models; Semantics; Test pattern generators; Software reliability; Faces; Large Language Model; Measurement; Unit Test Generation; Data security; Data models; Pipelines; Cascaded Pipeline; Error correction; Model-agnostic","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TSHBHN8M","conferencePaper","2024","Rachh, Rashmi; Kavatagi, Sanjana","Study on Students' Perceptions of Generative AI in Learning Programming Courses","2024 3rd International Conference for Advancement in Technology (ICONAT)","","","10.1109/ICONAT61936.2024.10774681","","The proliferation of generative AI has taken the world by storm, and the education sector is no exception. It is imperative to leverage this emerging technology to bring about positive transformation in teaching and learning. However, the adoption of generative AI in education must be approached with caution, necessitating several pedagogical interventions. This paper discusses the various challenges and issues involved in adopting generative AI for teaching programming language courses and proposes plausible solutions.","2024-09","2025-11-25 22:37:03","2025-11-25 22:37:03","","1-5","","","","","","","","","","","","","","","","","","","","","","","","Education; Generative AI; Programming; Programming profession; Computer languages; Pedagogy; Storms","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GQEUBCFS","conferencePaper","2024","Nabeel, Mohamad; Nimara, Doumitrou Daniil; Zanouda, Tahar","Test Code Generation for Telecom Software Systems Using Two-Stage Generative Model","2024 IEEE International Conference on Communications Workshops (ICC Workshops)","","","10.1109/ICCWorkshops59551.2024.10615269","","In recent years, the evolution of Telecom towards achieving intelligent, autonomous, and open networks has led to an increasingly complex Telecom Software system, supporting various heterogeneous deployment scenarios, with multi-standard and multi-vendor support. As a result, it becomes a challenge for large-scale Telecom software companies to develop and test software for all deployment scenarios. To address these challenges, we propose a framework for Automated Test Generation for large-scale Telecom Software systems. We begin by generating Test Case Input data for test scenarios observed using a time-series Generative model trained on historical Telecom Network data during field trials. Additionally, the time-series Generative model helps in preserving the privacy of Telecom data. The generated time-series software performance data are then utilized with test descriptions written in natural language to generate Test Script using the Generative Large Language Model. Our comprehensive experiments on public datasets and Telecom datasets obtained from operational Telecom Networks demonstrate that the framework can effectively generate comprehensive test case data input and useful test code.","2024-06","2025-11-25 22:37:03","2025-11-25 22:37:03","","1231-1236","","","","","","","","","","","","","","","","","","","","ISSN: 2694-2941","","","","Software systems; Codes; Adaptation models; Conferences; Data models; Natural languages; and Code Generation; Generative AI for Test automation; Large Language Models for Software Testing; TelcoAI; Telecommunications","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H2HB9CN7","conferencePaper","2024","Rajbhoj, Asha; Sant, Tanay; Somase, Akanksha; Kulkarni, Vinay","Leveraging Generative AI for Accelerating Enterprise Application Development: Insights from ChatGPT","2024 31st Asia-Pacific Software Engineering Conference (APSEC)","","","10.1109/APSEC65559.2024.00052","","Enterprise application development faces significant challenges, with each phase of the software development life cycle (SDLC) requiring experts with specific skills. The expertise of the individuals involved, greatly affects the quality and speed of work in each phase. The large size and complexity of modern software systems further exacerbates these problems. Recently, there has been a growing interest in using Generative AI (GenAI) techniques for software engineering tasks. GenAI can help Subject Matter Experts (SMEs) work more efficiently and can help in overcoming skill barriers. By leveraging GenAI, SMEs can save significant time and effort. This paper introduces meta-model based prompting approach to generate enterprise application code leveraging large language models (LLMs). Prompts help in the refinement of input requirements into refined requirements and design specifications using LLMs, ultimately generating code from these specifications. We share our approach and results of applying approach to generate small yet complex applications.","2024-12","2025-11-25 22:37:03","2025-11-25 22:37:03","","412-421","","","","","","","","","","","","","","","","","","","","ISSN: 2640-0715","","","","Large language models; Software engineering; Software systems; Chatbots; ChatGPT; Generative AI; Software development management; Codes; Faces; Large Language Models; AI in SDLC; Automated Software Development; SDLC automation; Complexity theory; Subject matter experts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SLNW3G6J","journalArticle","2025","Fraser, Gordon; Arcuri, Andrea","A Retrospective on Whole Test Suite Generation: On the Role of SBST in the Age of LLMs","IEEE Transactions on Software Engineering","","1939-3520","10.1109/TSE.2025.3539458","","This paper presents a retrospective of the article “Whole Test Suite Generation”, published in the IEEE Transactions on Software Engineering, in 2012. We summarize its main contributions, and discuss how this work impacted the research field of Search-Based Software Testing (SBST) in the last 12 years. The novel techniques presented in the paper were implemented in the tool EvoSuite, which has been so far the state-of-the-art in unit test generation for Java programs using SBST. SBST has shown practical and impactful applications, creating the foundations to open the doors to tackle several other software testing problems besides unit testing, like for example system testing of Web APIs with EvoMaster. We conclude our retrospective with our reflections on what lies ahead, especially considering the important role that SBST still plays even in the age of Large Language Models (LLMs).","2025-03","2025-11-25 22:37:03","2025-11-25 22:37:03","","874-878","","3","51","","","","","","","","","","","","","","","","","","","","","Optimization; Software testing; Test pattern generators; Java; LLM; Software; Software engineering; System testing; Software algorithms; Python; EvoMaster; EvoSuite; Pynguin; SBST; Search problems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GIZZFFQA","journalArticle","2024","Mehmood, Abid; Ilyas, Qazi Mudassar; Ahmad, Muneer; Shi, Zhongliang","Test Suite Optimization Using Machine Learning Techniques: A Comprehensive Study","IEEE Access","","2169-3536","10.1109/ACCESS.2024.3490453","","Software testing is an essential yet costly phase of the software development lifecycle. While machine learning-based test suite optimization techniques have shown promise in reducing testing costs and improving fault detection, a comprehensive evaluation of their effectiveness across different environments is still lacking. This paper reviews 43 studies published between 2018 and 2023, covering various test case selection, prioritization, and reduction techniques using machine learning. The findings reveal that conventional machine learning techniques, particularly supervised learning methods, have been widely adopted for test case prioritization and selection. Recent advancements, such as deep learning and hybrid models, show potential in improving fault detection rates and scalability, though challenges remain in adapting these techniques to large-scale and dynamic environments. Additionally, Generative AI and large language models (LLMs) are emerging as promising tools for automating aspects of test case generation and prioritization, offering new avenues for future research in enhancing test suite optimization. The study identifies recent trends, challenges, and opportunities for further research, with a focus on both conventional and emerging methods, including deep learning, hybrid approaches, and Generative AI models. By systematically analyzing these techniques, this work contributes to the understanding of how machine learning and Generative AI can enhance test suite optimization and highlights future directions for improving the scalability and real-world applicability of these methods.","2024","2025-11-25 22:37:03","2025-11-25 22:37:03","","168645-168671","","","12","","","","","","","","","","","","","","","","","","","","","Optimization; software testing; Software testing; Machine learning; Software quality; Market research; Generative AI; Testing; Deep learning; Reinforcement learning; Systematics; Costs; evaluation metrics for test suite optimization; machine learning in software testing; test case selection; test suite optimization (TSO)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V26M5HIQ","conferencePaper","2025","Sezgin, Anıl; Özkan, Gürkan; Coşgun, Esra","Leveraging Large Language Models in Software Testing: A Review of Applications and Challenges","2025 13th International Symposium on Digital Forensics and Security (ISDFS)","","","10.1109/ISDFS65363.2025.11011986","","The integration of Large Language Models (LLMs), such as GPT and BERT, into software testing has introduced a transformative approach to ensuring software reliability, functionality, and security. This review explores the diverse applications of LLMs in automating and enhancing software testing processes, addressing challenges in traditional methodologies. LLMs excel in automated test case generation by leveraging vast datasets, user stories, and bug reports to produce comprehensive and context-aware test scenarios. These capabilities reduce manual effort, improve coverage, and identify critical edge cases. Moreover, their application in bug detection and code analysis enhances early issue identification, improving software quality and reducing costs. LLMs also streamline the creation of high-quality documentation, enabling better collaboration and scalability in software projects. Despite their potential, LLM adoption in software testing faces challenges, including model interpretability, scalability, and the need for high-quality, diverse training datasets. Security and ethical considerations, such as data privacy and the risk of misuse, also demand attention. The paper evaluates state-of-the-art LLM applications, showcasing advancements in test automation and vulnerability detection while identifying areas for improvement. Future directions include refining hybrid approaches, enhancing domain-specific adaptability, and addressing ethical and governance concerns. This study provides a comprehensive analysis of LLM-driven testing methodologies, emphasizing their transformative potential in modern software development workflows. By fostering interdisciplinary collaboration, the field can harness LLMs to revolutionize software testing, paving the way for efficient, scalable, and secure software solutions.","2025-04","2025-11-25 22:37:03","2025-11-25 22:37:03","","1-7","","","","","","","","","","","","","","","","","","","","ISSN: 2768-1831","","","","Large language models; software testing; Software testing; Software reliability; Computer bugs; large language models; Scalability; bug detection; Security; Ethics; automation; Collaboration; Training; Reviews","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JINV3E73","conferencePaper","2024","Dolata, Mateusz; Lange, Norbert; Schwabe, Gerhard","Development in Times of Hype: How Freelancers Explore Generative AI?","2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE)","","","10.1145/3597503.3639111","","The rise of generative AI has led many companies to hire freelanc-ers to harness its potential. However, this technology presents unique challenges to developers who have not previously engaged with it. Freelancers may find these challenges daunting due to the absence of organizational support and their reliance on positive client feedback. In a study involving 52 freelance developers, we identified multiple challenges associated with developing solutions based on generative AI. Freelancers often struggle with as-pects they perceive as unique to generative AI such as unpredict-ability of its output, the occurrence of hallucinations, and the in-consistent effort required due to trial-and-error prompting cycles. Further, the limitations of specific frameworks, such as token lim-its and long response times, add to the complexity. Hype-related issues, such as inflated client expectations and a rapidly evolving technological ecosystem, further exacerbate the difficulties. To address these issues, we propose Software Engineering for Generative AI (SE4GenAI) and Hype-Induced Software Engineering (HypeSE) as areas where the software engineering community can provide effective guidance. This support is essential for freelancers working with generative AI and other emerging technologies.","2024-04","2025-11-25 22:37:03","2025-11-25 22:37:03","","2257-2269","","","","","","","","","","","","","","","","","","","","ISSN: 1558-1225","","","","Software engineering; Generative AI; Companies; Challenges; AI-based Systems; Complexity theory; Ecosystems; Fashion; Freelancers; Hype; Hype-Induced SE; Hype-SE; Novelty; Paradigm; Product; Qualitative Research; SE for Generative AI; SE4GenAI; Time factors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XQM8U2KB","journalArticle","2024","Tang, Yutian; Liu, Zhijie; Zhou, Zhichao; Luo, Xiapu","ChatGPT vs SBST: A Comparative Assessment of Unit Test Suite Generation","IEEE Transactions on Software Engineering","","1939-3520","10.1109/TSE.2024.3382365","","Recent advancements in large language models (LLMs) have demonstrated exceptional success in a wide range of general domain tasks, such as question answering and following instructions. Moreover, LLMs have shown potential in various software engineering applications. In this study, we present a systematic comparison of test suites generated by the ChatGPT LLM and the state-of-the-art SBST tool EvoSuite. Our comparison is based on several critical factors, including correctness, readability, code coverage, and bug detection capability. By highlighting the strengths and weaknesses of LLMs (specifically ChatGPT) in generating unit test cases compared to EvoSuite, this work provides valuable insights into the performance of LLMs in solving software engineering problems. Overall, our findings underscore the potential of LLMs in software engineering and pave the way for further research in this area.","2024-06","2025-11-25 22:37:12","2025-11-25 22:37:12","","1340-1359","","6","50","","","","","","","","","","","","","","","","","","","","","Software; Chatbots; ChatGPT; Codes; Computer bugs; large language models; search-based software testing; Benchmark testing; Task analysis; Question answering (information retrieval)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PW3SIN36","conferencePaper","2025","Khandaker, Shaker Mahmud; Kifetew, Fitsum; Prandi, Davide; Susi, Angelo","AugmenTest: Enhancing Tests with LLM-Driven Oracles","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST62969.2025.10988926","","Automated test generation is crucial for ensuring the reliability and robustness of software applications while at the same time reducing the effort needed. While significant progress has been made in test generation research, generating valid test oracles still remains an open problem. To address this challenge, we present AugmenTest, an approach leveraging Large Language Models (LLMs) to infer correct test oracles based on available documentation of the software under test. Unlike most existing methods that rely on code, AugmenTest utilizes the semantic capabilities of LLMs to infer the intended behavior of a method from documentation and developer comments, without looking at the code. AugmenTest includes four variants: Simple Prompt, Extended Prompt, RAG with a generic prompt (without the context of class or method under test), and RAG with Simple Prompt, each offering different levels of contextual information to the LLMs. To evaluate our work, we selected 142 Java classes and generated multiple mutants for each. We then generated tests from these mutants, focusing only on tests that passed on the mutant but failed on the original class, to ensure that the tests effectively captured bugs. This resulted in 203 unique tests with distinct bugs, which were then used to evaluate AugmenTest. Results show that in the most conservative scenario, AugmenTest's Extended Prompt consistently outperformed the Simple Prompt, achieving a success rate of 30% for generating correct assertions. In comparison, the state-of-the-art TOGA approach achieved 8.2%. Contrary to our expectations, the RAG-based approaches did not lead to improvements, with performance of 18.2% success rate for the most conservative scenario. Our study demonstrates the potential of LLMs in improving the reliability of automated test generation tools, while also highlighting areas for future enhancement.","2025-03","2025-11-25 22:37:12","2025-11-25 22:37:12","","279-289","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Semantics; Software testing; Test pattern generators; Software; Software reliability; Codes; Computer bugs; Large Language Models; Software Testing; Retrieval-Augmented Generation; Assertion Generation; Robustness; Documentation; Context-Aware Testing; Test Oracles","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UZ3QYERX","conferencePaper","2025","Jagielski, Jakub; Rojas, Consuelo; Abel, Markus","Exploratory Study on Private GPTs for LLM-Driven Test Generation in Software and Machine Learning Development","2025 2nd International Generative AI and Computational Language Modelling Conference (GACLM)","","","10.1109/GACLM67198.2025.11232354","","In this contribution, we examine the capability of private GPTs to automatically generate executable test code based on requirements. More specifically, we use acceptance criteria as input, formulated as part of epics, or stories, which are typically used in modern development processes. This gives product owners, or business intelligence, respectively, a way to directly produce testable criteria through the use of LLMs. We explore the quality of the so-produced tests in two ways: i) directly by letting the LLM generate code from requirements, ii) through an intermediate step using Gherkin syntax. As a result, it turns out that the two-step procedure yields better results - where we define better in terms of human readability and best coding practices, i.e. lines of code and use of additional libraries typically used in testing. Concretely, we evaluate prompt effectiveness across two scenarios—a simple ""Hello World"" program and a digit classification model—showing that structured prompts lead to higher-quality test outputs.","2025-08","2025-11-25 22:37:12","2025-11-25 22:37:12","","245-249","","","","","","","","","","","","","","","","","","","","","","","","Test pattern generators; Software; Syntactics; Machine learning; Codes; Testing; Reliability; Standards; component; Encoding; formatting; insert; style; styling; Translation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6RY8ZNJ8","conferencePaper","2025","Ruberto, Stefano; Perera, Judith; Jahangirova, Gunel; Terragni, Valerio","From Implemented to Expected Behaviors: Leveraging Regression Oracles for Non-regression Fault Detection using LLMs","2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","","","10.1109/ICSTW64639.2025.10962503","","Automated test generation tools often produce assertions that reflect implemented behavior, limiting their usage to regression testing. In this paper, we propose LLMProphet, a black-box approach that applies Few-Shot Learning with LLMs, using automatically generated regression tests as context to identify non-regression faults without relying on source code. By employing iterative cross-validation and a leave-one-out strategy, LLMProphet identifies regression assertions that are misaligned with expected behaviors. We outline LLMProphet’s workflow, feasibility, and preliminary findings, demonstrating its potential for LLM-driven fault detection.","2025-03","2025-11-25 22:37:12","2025-11-25 22:37:12","","37-40","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Software testing; Test pattern generators; Source coding; Fault detection; Large Language Models; Conferences; Fault diagnosis; Iterative methods; Limiting; Fault Detection; Few shot learning; Few-Shot Learning; Non-Regression Faults; Regression Oracles","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X8JIE5BE","conferencePaper","2023","Garg, Anshumaan; Sharma, Dolly","Generative AI for Software Test Modelling with a focus on ERP Software","2023 International Conference on Advances in Computation, Communication and Information Technology (ICAICCIT)","","","10.1109/ICAICCIT60255.2023.10466102","","Generative AI in this context refers to the type of artificial intelligence that can generate content or give new information based on patterns it has learned. In the case of software testing, it refers to the use of generative AI to model or for the creation of test scenarios, test cases or objects for ERP (Enterprise Resource Planning) software. The special focus on ERP software means that generative AI-based techniques have been particularly designed and optimized for the purpose of software testing. It does take into account the unique features and complexity of the ERP systems which allows for more effective and accurate testing. The problem with the existing chatbots is that they are not integrated with generative AI and the training is either not properly done or the data used for training is biased. The objective of this work is to develop a chatbot integrated with the generative AI-based framework and develop training data to cater to user needs. Methods and tools used in this approach are the OpenAI's API used for integrating chatbot with the generative AI-based software, Postman API has also been used to send and receive API requests and prompts and completions to be generated using Python code. The result of this approach is that a chatbot has been developed which develops test cases and scenarios, requests sent and received successfully and prompts and completions have been successfully generated using Python code. To state it simply, generative AI for software test modelling with a focus on ERP software means creating test cases and scenarios using AI and generating them automatically which helps testers ensure that the software is working correctly and meets the needs of the business operations.","2023-11","2025-11-25 22:37:12","2025-11-25 22:37:12","","187-193","","","","","","","","","","","","","","","","","","","","","","","","Software testing; Software quality; Chatbots; Generative AI; Codes; Training; Training data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DGGBDYBU","conferencePaper","2025","Godage, Tharindu; Nimishan, Sivaraj; Vasanthapriyan, Shanmuganathan; Palanisamy, Vigneshwaran; Joseph, Charles; Thuseethan, Selvarajah","Evaluating the Effectiveness of Large Language Models in Automated Unit Test Generation","2025 5th International Conference on Advanced Research in Computing (ICARC)","","","10.1109/ICARC64760.2025.10962997","","The increasing use of Artificial Intelligence (AI) in software development underscores the need to select suitable Large Language Models (LLMs) for automating software unit test generation. No prior work has been conducted to evaluate the performance of LLM in this domain. To address this gap, this study evaluates the effectiveness of four prominent LLMs—GPT-4, Claude 3.5, Command-R-08-2024 and Llama 3.1—in generating unit test cases. This study particularly aims to evaluate the performance of these models in real-world testing scenarios. Hence, 106 test cases from 23 test suites based on interviews with software experts and QA engineers are used to ensure relevance and comprehensiveness. These test cases are analyzed using JavaScript Engines Specification Tester (JEST) for code coverage and Stryker for mutation testing while adopting both quantitative and qualitative analysis. The findings reveal that Claude 3.5 consistently outperforms the other models against test success rate, statement coverage, and mutation score with the achieved accuracy of 93.33%, 98.01%, and 89.23% respectively. The results also provide insights into the capabilities of LLMs for automated unit test generation and their integration into the continuous software integration pipeline. Further, the findings authenticated the importance of systematically comparing LLMs for test case generation.","2025-02","2025-11-25 22:37:12","2025-11-25 22:37:12","","1-6","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Optimization; Test pattern generators; Software; Accuracy; Software development management; Measurement; Testing; large language models; mutation testing; javascript; test automation; Standards; Pipelines; software unit testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QHMYTY7W","conferencePaper","2025","Tomic, Stevan; Alégroth, Emil; Isaac, Maycel","Evaluation of the Choice of LLM in a Multi-Agent Solution for GUI-Test Generation","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST62969.2025.10989038","","Automated testing, particularly for GUI-based systems, remains a costly and labor-intensive process and prone to errors. Despite advancements in automation, manual testing still dominates in industrial practice, resulting in delays, higher costs, and increased error rates. Large Language Models (LLMs) have shown great potential to automate tasks traditionally requiring human intervention, leveraging their cognitive-like abilities for test generation and evaluation. In this study, we present PathFinder, a Multi-Agent LLM (MALLM) framework that incorporates four agents responsible for (a) perception and summarization, (b) decision-making, (c) input handling and extraction, and (d) validation, which work collaboratively to automate exploratory web-based GUI testing. The goal of this study is to assess how different LLMs, applied to different agents, affect the efficacy of automated exploratory GUI testing. We evaluate PathFinder with three models, Mistral-Nemo, Gemma2, and Llama3.1, on four e-commerce websites. Thus, 27 permutations of the LLMs, across three agents (excluding the validation agent), to test the hypothesis that a solution with multiple agents, each using different LLMs, is more efficacious (efficient and effective) than a multi-agent solution where all agents use the same LLM. The results indicate that the choice of LLM constellation (combination of LLMs) significantly impacts efficacy, suggesting that a single LLM across agents may yield the best balance of efficacy (measured by F1-score). Hypothesis to explain this result include, but are not limited to: improved decision-making consistency and reduced task coordination discrepancies. The contributions of this study are an architecture for MALLM-based GUI testing, empirical results on its performance, and novel insights into how LLM selection impacts the efficacy of automated testing.","2025-03","2025-11-25 22:37:12","2025-11-25 22:37:12","","487-497","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Software testing; Test pattern generators; Testing; Adaptation models; Automated Testing; Decision making; Large Language Models (LLMs); Multi-Agent Systems; AI-Assisted Software Testing; Electronic commerce; Graphical user interfaces; MALLM; Multi-agent systems; Periodic structures","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"37AG63CW","conferencePaper","2025","Wang, Zhiyao; Guo, Xiujing; Tsuchiya, Tatsuhiro","Retrieval-Augmented Generation for Software Requirement-Based Test Case Generation","2025 25th International Conference on Software Quality, Reliability and Security (QRS)","","","10.1109/QRS65678.2025.00022","","Testers often need to manually write black-box test cases based on software artifacts such as requirement documents. In agile development, this process is often time-consuming and is further complicated by frequent requirement changes, leading to continuous maintenance overhead. Automating this process is therefore essential. Given the strong natural language understanding and generation capabilities of large language models (LLMs), combined with Retrieval-Augmented Generation (RAG), we propose a RAG-based framework for automated test case generation. Before generation, we embed software artifacts to construct a vectorbased knowledge database. At runtime, software requirements are used as queries to retrieve relevant context, which is integrated into a prompt and passed to the LLM for test case generation. This approach addresses several shortcomings of LLMs, including limited context length, attention dilution over large inputs, and the tendency to hallucinate or over-look key domain-specific constraints. By providing query-specific external knowledge, RAG enhances both accuracy and efficiency. We deploy the framework with different models locally and conduct experiments on two open-source datasets. Compared with the manually written benchmark test cases, our method achieves full requirement coverage with fewer test cases, improved efficiency, reduced error potential, and realized better readability.","2025-07","2025-11-25 22:37:12","2025-11-25 22:37:12","","108-119","","","","","","","","","","","","","","","","","","","","ISSN: 2693-9177","","","","Large language models; Software reliability; Natural language processing; Software quality; Maintenance; Large Language Model; Test Case Generation; Retrieval-Augmented Generation; Security; Databases; Closed box; Retrieval augmented generation; Runtime; Software Requirement","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"59V4NEED","conferencePaper","2025","Nyein Chan, Samad Alias; Alalfi, Manar H.","SmartTinkerer: Bridging Smart Home Testing Gaps with LLM, Digital Twin & Reinforcement Learning","2025 IEEE 49th Annual Computers, Software, and Applications Conference (COMPSAC)","","","10.1109/COMPSAC65507.2025.00182","","There has been a significant rise in the use of Internet of Things (IoT) devices in recent years, but along with the increase in usage, there’s also a rise in security vulnerabilities in the IoT software. The security vulnerabilities are difficult to find, and even more difficult to exploit dynamically. The software technologies, particularly used in the Samsung SmartThings environment, have a huge gap in terms of deprecated language usage, having a good testing environment and a security vulnerability scanning. This paper introduces a new tool, SmartTinkerer , filling these gaps by employing LLMs to translate deprecated language to new APIs, creating a digital twin simulating a virtual smart home device system for a testing environment and using reinforcement learning on the digital simulation to better scan for vulnerabilities.","2025-07","2025-11-25 22:37:12","2025-11-25 22:37:12","","1452-1461","","","","","","","","","","","","","","","","","","","","ISSN: 2836-3795","","","","Large language models; Software testing; Software; Software Testing; Reinforcement Learning; Large Language Model (LLM); Security; Reinforcement learning; Translation; Digital twins; Digital Twins; Filling; Internet of Things; Internet of Things (IoT); Smart homes","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XTISSPTV","conferencePaper","2025","Almohsen, Khadija; Albalooshi, Fawzi; Ammari, Jafla Al","Exploring the Use of GenAI in Automating Software Testing","2025 IEEE 4th International Conference on Computing and Machine Intelligence (ICMI)","","","10.1109/ICMI65310.2025.11139839","","Software testing is an important step in software development as it ensures the reliability, functionality, and performance of developed software. However, manual testing approaches are often labor-intensive, time-consuming, and prone to human error. Automating this phase offers significant benefits, making it a key area of interest for researchers. This study aims to explore the adoption of generative AI (GenAI) technologies in automating software testing and provides a thorough review of recent advancements. This research work includes a comparative analysis of existing studies, revealing a predominant focus on Large Language Model (LLM)-driven solutions for generating the test cases, with notable applications in unit testing and integration testing. Evaluation metrics such as test coverage, efficiency, and relevance are frequently used to assess the effectiveness of these approaches. Furthermore, the review identifies critical limitations in current GenAI-based solutions and suggests potential directions for future work. This work offers a timely contribution to the software engineering community, laying the groundwork for more advanced research work in the domain.","2025-04","2025-11-25 22:37:12","2025-11-25 22:37:12","","1-6","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Software testing; Manuals; Software; Software engineering; Software reliability; Automation; Software development management; Measurement; Software Testing; LLMs; GenAI; RAG; Reviews; Machine intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5E6PV8QW","conferencePaper","2024","Halaweh, Mohanad; Refae, Ghaleb El","Examining the Accuracy of AI Detection Software Tools in Education","2024 Fifth International Conference on Intelligent Data Science Technologies and Applications (IDSTA)","","","10.1109/IDSTA62194.2024.10747004","","Educators have raised concerns about the utilization of ChatGPT in generating unoriginal text and the possibility of plagiarism. To address these concerns, various AI text detection software tools have been developed to evaluate whether a text is AI generated or human generated. The aim of this research is to empirically examine the accuracy of AI detection tools in identifying AI-generated texts. An experiment was conducted using textual data generated by ChatGPT, which was assessed using Turnitin and four other AI detection tools. Through multiple iterations and interventions, the text was paraphrased by ChatGPT until it appeared original and could not be detected as AI-generated by Turnitin’s AI detection tool. The findings revealed that all the AI detection software tools that were examined failed to detect the AI-generated text by ChatGPT in the final iteration. The findings provide valuable insights that have implications for various stakeholders, including educators, researchers, and AI text detection software developers. Based on the tools examined, educators and researchers do not need to set a specific threshold or percentage, including 0%, to determine what qualifies as acceptable AI-generated text. This is because establishing such a threshold can be misleading, considering the current limitations in the algorithms of these tools. Furthermore, the data generated in this paper can provide a solid basis for replicated research or software testing and assessment. It can be utilized to evaluate the accuracy of alternative AI detection tools and any future advancements in the tools mentioned in this investigation.","2024-09","2025-11-25 22:37:12","2025-11-25 22:37:12","","186-190","","","","","","","","","","","","","","","","","","","","","","","","Software testing; Accuracy; Chatbots; ChatGPT; Education; Generative AI; Artificial intelligence; Software algorithms; Artificial Intelligence; Plagiarism; AI Detection Tools; Software tools; Solids; Stakeholders; Text detection; Turnitin","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"48CG7D87","conferencePaper","2025","Liang, Chendan; Ma, Zeyu; Wang, Wanying; Ding, Minjie; Cao, Zhiyuan; Chen, Mingang","MCM: A Multi-Agent Collaborative Multimodal Framework For Traditional Chinese Medicine Diagnosis","2025 IEEE International Conference on Image Processing (ICIP)","","","10.1109/ICIP55913.2025.11084334","","The advancement of information technology and the rise of generative AI have paved the way for the development of Large Language Models (LLMs) tailored for TCM diagnostics. However, existing LLMs in the field of TCM face challenges in interpretability, limited modality in interaction, and robustness. To address these limitations, we propose MCM, a Multi-Agent Collaborative Multimodal Framework for TCM Diagnosis. This framework enables robust and interpretable multimodal diagnosis through multi-agent collaboration, offering novel methodologies for applying LLMs in the TCM domain. Experimental results demonstrate that the model within the MCM framework improved performance after fine-tuning, with additional capability gains under the MCM framework’s support, effectively addressing the challenges faced by LLMs in TCM, including interpretability, limited data modality, and lack of robustness. The code is open-sourced at: https://github.com/JerryMazeyu/MCM.","2025-09","2025-11-25 22:37:12","2025-11-25 22:37:12","","1438-1443","","","","","","","","","","","","","","","","","","","","ISSN: 2381-8549","","","","Analytical models; Large language models; Large Language Model; Multi-Agent Collaboration; Robustness; Collaboration; Medical services; Image analysis; Intelligent Diagnosis; Interpretability; Knowledge graphs; Medical diagnostic imaging; Multi-modality; Tongue; Traditional Chinese Medicine; Vectors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GMZ6X5JH","journalArticle","2025","Marian Pasca, Emil; Delinschi, Daniela; Erdei, Rudolf; Matei, Oliviu","LLM-Driven, Self-Improving Framework for Security Test Automation: Leveraging Karate DSL for Augmented API Resilience","IEEE Access","","2169-3536","10.1109/ACCESS.2025.3554960","","Modern software architectures heavily rely on APIs, yet face significant security challenges, particularly with Broken Object Level Authorization (BOLA) vulnerabilities, which remain the most critical API security risk according to OWASP. This paper introduces Karate-BOLA-Guard, an innovative framework leveraging Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) techniques to automate security-focused test case generation for APIs. Our approach integrates vector databases for context retrieval, multiple LLM models for test generation, and observability tools for process monitoring. Initial experiments were carried out on three deliberately vulnerable APIs (VAmPI, Crapi, and OWASP Juice Shop), with subsequent validation on fifteen additional production APIs spanning diverse domains including social media, version control systems, financial services, and transportation services. Our evaluation metrics show Llama 3 8B achieving consistent performance (Accuracy: 3.1-3.4, Interoperability: 3.7-4.3) with an average processing time of 143.76 seconds on GPU. Performance analysis revealed significant GPU acceleration benefits, with 20-25x improvement over CPU processing times. Smaller models demonstrated efficient processing, with Phi-3 Mini averaging 69.58 seconds and Mistral 72.14 seconds, while maintaining acceptable accuracy scores. Token utilization patterns showed Llama 3 8B using an average of 36,591 tokens per session, compared to Mistral’s 25,225 and Phi-3 Mini’s 31,007. Our framework’s effectiveness varied across APIs, with notably strong performance in complex platforms (Instagram: A = 4.3, I = 4.4) while maintaining consistent functionality in simpler implementations (VAmPI: A = 3.6, I = 4.3). The iterative refinement process, evaluated through comprehensive metrics including Accuracy (A), Complexity (C), and Interoperability (I), represents a significant advancement in automated API security testing, offering an efficient, accurate, and adaptable approach to detecting BOLA vulnerabilities across diverse API architectures.","2025","2025-11-25 22:37:12","2025-11-25 22:37:12","","56861-56886","","","13","","","","","","","","","","","","","","","","","","","","","software testing; Software testing; Test pattern generators; Automation; Accuracy; Testing; Security; cybersecurity; Computer architecture; Systematics; Retrieval augmented generation; API security; Application programming interfaces; automation testing tools; restful API","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SKHZ75XA","conferencePaper","2024","Yeh, Hang-Wei; Ma, Shang-Pin; Chen, Yi","Test Case Migration from Monolith to Microservices Using Large Language Models","2024 IEEE International Conference on e-Business Engineering (ICEBE)","","","10.1109/ICEBE62490.2024.00014","","Due to microservices' modularity, high scalability, and good fault tolerance, more and more software systems are transitioning from monolithic architectures to microservice architectures. In this migration process, the migration of test cases is a crucial step to ensure functional consistency and completeness before and after the migration, thereby maintaining the stability and reliability of the microservice system post-migration. However, despite numerous studies on architecture migration, there is still a lack of methods to efficiently convert test cases of monolithic systems into ones for the migrated microservices. Therefore, this study proposes a test migration approach based on Large Language Models (LLM), called LTM^3 (LLM-based Test Migration from Monolith to Microservices). During migration, LTM^3 identifies the correspondence between existing test cases and monolithic functions, the connections between monolithic functions and migrated microservices, and the dependencies between microservices. Subsequently, by utilizing LLM and appropriate prompting, the integration test cases of the monolithic system are transformed into contract test cases for each microservice. The experimental results showed that LTM^3 can effectively migrate most test cases, with only a tiny portion requiring manual adjustment.","2024-10","2025-11-25 22:37:13","2025-11-25 22:37:13","","29-35","","","","","","","","","","","","","","","","","","","","ISSN: 2472-8527","","","","large language model; Large language models; Stability analysis; Software reliability; Software systems; Testing; Microservice; Computer architecture; Microservice architectures; consumer-driven contract testing; Contracts; test case migration; Thermal stability; Transforms","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4HFHXJVH","conferencePaper","2025","Peccia, Federico Nicolás; Hald, Tobias; Bringmann, Oliver","LLM-aided Test Generation for Custom Neural Network Hardware Accelerators","2025 IEEE European Test Symposium (ETS)","","","10.1109/ETS63895.2025.11049634","","Large Language Models (LLMs) are increasingly being utilized to automate tasks in software development, including test generation. This paper introduces a novel workflow for the automated generation of tests for custom hardware accelerators for Neural Networks (NN) using LLMs. The workflow is designed to handle the unique challenges posed by custom or proprietary accelerators, where the LLM has no prior knowledge about the hardware, and there are not enough code examples to fine-tune the LLM for it. A five-part prompt is proposed to encompass all the necessary information for the LLM to generate accurate tests. The Gemmini accelerator is selected as a use case, and tests are defined and generated using six different LLMs, each one evaluated at 4, 16 and 32-bit floating point precision. For each LLM and precision combination, latency and energy consumption are recorded for each generated test. We evaluate the correctness of the generated tests and the impact of adding the human-in-the-loop to apply minimal corrections to them if necessary. We define an efficiency metric to evaluate the tradeoff between consumed energy and correctly generated tests. The workflow evaluation reveals that LLMs are able to generate tests for custom hardware accelerators, with Codestral and Gemma 2 being the most effective LLMs for these tasks, capable of generating all the proposed tests with minimal human corrections. This workflow offers developers a new and efficient method to accelerate the test generation of custom hardware accelerators for NN, with potential applications in verifying new accelerators or new features of existing ones.","2025-05","2025-11-25 22:37:13","2025-11-25 22:37:13","","1-6","","","","","","","","","","","","","","","","","","","","ISSN: 1558-1780","","","","Large language models; Test pattern generators; Life estimation; Software development management; Large Language Model; Measurement; Test generation; Artificial neural networks; Energy consumption; Hardware acceleration; Hardware accelerator; Human in the loop; Neural network hardware","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XE5LCJWT","conferencePaper","2024","S, Shakthi; Srivastava, Pratibha; L, Ravi Kumar; Prasad, SG","Automated Test Case Generation for Satellite FRD Using NLP and Large Language Model","2024 4th International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)","","","10.1109/ICECCME62383.2024.10796866","","In recent times, the research on the use of Large Language Models (LLMs) for developing software applications has grown exponentially. Generating test cases for satellite Functional Requirement Documents (FRDs) pose a significant challenge due to their complex nature, requiring intricate analysis. Manual methods are time-consuming and error-prone, prompting the need for automated solutions or semi-automated solutions. This work proposes a novel approach to automate test case generation from FRDs using LLMs and Natural Language Processing (NLP). By harnessing the capabilities of LLMs, our system extracts and interprets complex variables and equations, facilitating the automated creation of comprehensive test cases. This approach aims to streamline the satellite testing process, improving efficiency and accuracy while reducing the burden on human analysts. We generate a custom dataset of 10 samples and then benchmark 4 LLMs on the dataset. We open-source the complete codebase for implementation and for further research.","2024-11","2025-11-25 22:37:13","2025-11-25 22:37:13","","1-8","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Manuals; Natural language processing; Accuracy; large language models; Reliability; Security; few-shot prompting; Refining; Computational modeling; chain of thought prompt; Heuristic algorithms; Satellites","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UBZ4LZSD","journalArticle","2025","Yang, Liqun; Wei, Chaoren; Yang, Jian; Xia, Wanxu; Yang, Yuze; Luo, Yang; Niyato, Dusit; Sun, Liang; Liu, Zhiquan","FuzzCoder: Code Large Language Model-Based Fuzz Testing for Industrial IoT Programs","IEEE Internet of Things Journal","","2327-4662","10.1109/JIOT.2025.3577602","","Fuzz testing is an dynamic program analysis technique designed for discovering vulnerabilities in IoT systems. The core goal is to deliberately feed maliciously crafted inputs into an IoT device or service, triggering vulnerabilities such as system crashes, buffer overflow exploits, and memory corruption, etc. Efficiently generating malicious inputs remains challenging, with leading methods often relying on randomly mutating existing valid inputs. In this work, we propose to adopt fine-tuned large language models (FuzzCoder) to learn patterns in the input files from successful attacks to guide future fuzzing explorations. Specifically, we develop a framework that leverages code large language models (LLMs) to guide the mutation process to perform meaningful input mutations. We formulate the mutation process as the sequence-to-sequence modeling, where LLM receives a sequence of bytes and outputs the mutated byte sequence. FuzzCoder is fine-tuned on our created instruction dataset (FuzzInstruct), where the successful fuzzing history is collected from the heuristic fuzzing tool. FuzzCoder can predict mutation positions and strategies for input files to trigger abnormal behaviors of the program. Most importantly, the experiment reveals results that FuzzCoder achieves better fuzzing performance compared to traditional and other American fuzzy lop (AFL)-based fuzzers, such as AFL, AFL++, AFLSmart, etc. On average, FuzzCoder achieves an improvement in code coverage of more than 20%, along with a significant increase in the number of crashes.","2025-09","2025-11-25 22:37:13","2025-11-25 22:37:13","","36842-36851","","18","12","","","","","","","","","","","","","","","","","","","","","Large language models; Codes; Benchmark testing; XML; Fuzzing; fuzzing; large language model (LLM); Security; Training; Internet of Things; American fuzzing Lop; Computer crashes; IoT; Microprogramming; vulnerability discovery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"86X7PKHZ","journalArticle","2025","Forgács, István; Kovács, Attila","Action-State Testing—A Model for Test Design Automation","IEEE Access","","2169-3536","10.1109/ACCESS.2025.3563337","","Model-based testing (MBT) is essential in software testing, offering automation, comprehensive coverage, and defect prevention. It uses abstract models to automatically design and generate test cases, representing the expected system behaviour, including states, transitions, inputs, and outputs. This paper explores the action-state testing modelling technique, originally introduced by the authors in Forgács and Kovács (2000). In this approach, a model step comprises an action (input), one or more responses (outputs), and an optional state. The steps can be arranged sequentially, or they may be forked and joined. Sequential steps appear within the same test case. Forked steps are distributed across different test cases. The joined steps also belong to separate test cases. In addition, the graphical model can be constructed using a text editor. This paper builds upon the concept by establishing its theoretical foundation. We demonstrate how the action-state model eliminates the need for guard conditions and coding, maintains a concise and manageable structure, and seamlessly incorporates outputs, ultimately enhancing testing efficiency. Additionally, we provide guidelines for adding new states and empirically validate the benefits of action-state testing over alternative techniques, achieving a 100% defect detection percentage (DDP). This paper marks the first instalment of the author’s Test Design Trilogy, dedicated to refining and unifying various test design techniques.","2025","2025-11-25 22:37:13","2025-11-25 22:37:13","","75535-75545","","","13","","","","","","","","","","","","","","","","","","","","","Software testing; LLM; Automation; Defect detection; Codes; Testing; Unified modeling language; test generation; test automation; modeling; Guidelines; Computational modeling; Prevention and mitigation; Action-state testing; EFSM; Electronic mail; FSM; guard condition; stateful and stateless models; test design; test selection criteria","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QYMC272P","conferencePaper","2025","Gao, Yi; Hu, Xing; Chen, Zirui; Xu, Tongtong; Yang, Xiaohu","Vulnerability-Triggering Test Case Generation from Third-Party Libraries","2025 IEEE/ACM Second International Conference on AI Foundation Models and Software Engineering (Forge)","","","10.1109/Forge66646.2025.00021","","Open-source third-party libraries are widely used in software development. These libraries offer substantial advantages in terms of time and resource savings. However, a significant concern arises due to the publicly disclosed vulnerabilities within these libraries. Existing automated vulnerability detection tools often suffer from false positives and fail to accurately assess the propagation of inputs capable of triggering vulnerabilities from client projects to vulnerable code in libraries. In this paper, we propose a novel approach called VulEUT (Vulnerability Exploit Unit Test Generation), which combines vulnerability exploitation reachability analysis and LLM-based unit test generation. VulEUT is designed to automatically verify the exploitability of vulnerabilities in third-party libraries commonly used in Java client software projects. VulEUT first analyzes the client projects to determine the reachability of vulnerability conditions. And then, it leverages the Large Language Model (LLM) to generate unit tests for vulnerability confirmation. To evaluate the effectiveness of VulEUT, we collect 32 vulnerabilities from various third-party libraries and conduct experiments on 70 real client projects. Besides, we also compare our approach with two representative tools, i.e., TRANSFER and VESTA. Our results demonstrate the effectiveness of VulEUT, with 229 out of 292 generated unit tests successfully confirming vulnerability exploitation across 70 client projects, which outperforms baselines by 24%.","2025-04","2025-11-25 22:37:13","2025-11-25 22:37:13","","125-135","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Test pattern generators; Java; Software; Software engineering; Software development management; Codes; Large Language Model; Unit Test Generation; Libraries; Foundation models; Reachability analysis; Third-party Library; Vulnerability Exploitation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VAMYWUJI","conferencePaper","2025","Zhou, Han; Luo, Yu; Zhang, Mengtao; Xu, Dianxiang","C2RustTV: An LLM-based Framework for C to Rust Translation and Validation","2025 IEEE 49th Annual Computers, Software, and Applications Conference (COMPSAC)","","","10.1109/COMPSAC65507.2025.00158","","Transitioning legacy C codebases to Rust has emerged as a promising approach to addressing memory safety issues inherent in C. However, existing methods often produce non-idiomatic or semantically inconsistent Rust code. This paper introduces C2RustTV, a framework that leverages large language models (LLMs) to translate C programs to idiomatic Rust code while assuring translation quality through automated conformance testing. C2RustTV integrates test case generation, automated translation of both production and test code, and conformance validation via test execution. Experiments across multiple datasets demonstrate the effectiveness of C2RustTV, achieving higher rates of compilation success and functional conformance compared to state-of-the-art techniques.","2025-07","2025-11-25 22:37:13","2025-11-25 22:37:13","","1254-1259","","","","","","","","","","","","","","","","","","","","ISSN: 2836-3795","","","","Large language models; Software; Codes; Large Language Model; Scalability; Robustness; Safety; Rust; Iterative methods; Refining; Production; Translation; C; Program translation; Validation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R8IQGB2A","conferencePaper","2025","Liu, Wentao; Pan, Ya; Bai, Hanli","Improving Android GUI Automated Testing via Knowledge-Guided Reinforcement Learning and LLM-Generated Inputs","2025 7th International Conference on Natural Language Processing (ICNLP)","","","10.1109/ICNLP65360.2025.11108452","","Recent studies have shown that, despite the increasing application of model-based and machine learning-driven approaches in Android GUI testing, these methods face significant limitations. Model-based techniques, while systematic, are costly to construct and maintain, and deep learning-based methods rely heavily on large labeled datasets, lacking generalization and interpretability. Reinforcement learning offers a promising alternative by balancing exploration and exploitation without extensive labeled datasets but often struggles with inefficient early-stage learning in complex state spaces. Additionally, all these methods face challenges in generating effective text inputs, a critical yet underexplored aspect of GUI testing. To address these issues, this paper proposes GuideBot, a model-enhanced reinforcement learning framework that integrates human prior knowledge to accelerate learning and reduce initial exploration time. GuideBot also incorporates a dynamic adaptive strategy, a fuzzy reward mechanism, and a context-aware, LLM-based text generation module to handle text input challenges effectively.","2025-03","2025-11-25 22:37:23","2025-11-25 22:37:23","","128-132","","","","","","","","","","","","","","","","","","","","","","","","large language model; Automation; Natural language processing; Faces; Testing; Adaptation models; reinforcement learning; text input generation; Mobile applications; Reinforcement learning; Systematics; Graphical user interfaces; GUI test automation; Learning systems; mobile application testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A8EDFGKJ","conferencePaper","2025","Baldonado, Juan Manuel; Bonomo-Braberman, Flavia; Braberman, Víctor A.","Towards a Probabilistic Framework for Analyzing and Improving LLM-Enabled Software","2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","","","10.1109/ICSTW64639.2025.10962470","","Ensuring the reliability and verifiability of large language model (LLM)-enabled systems remains a significant challenge in software engineering. We propose a probabilistic framework for systematically analyzing and improving these systems by modeling and refining distributions over clusters of semantically equivalent outputs. This framework facilitates the evaluation and iterative improvement of Transference Models– key software components that utilize LLMs to transform inputs into outputs for downstream tasks. To illustrate its utility, we apply the framework to the autoformalization problem, where natural language documentation is transformed into formal program specifications. Our case illustrates how distribution-aware analysis enables the identification of weaknesses and guides focused alignment improvements, resulting in more reliable and interpretable outputs. This principled approach offers a foundation for addressing critical challenges in the development of robust LLM-enabled systems.","2025-03","2025-11-25 22:37:23","2025-11-25 22:37:23","","418-422","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Software testing; Software; Software engineering; Software reliability; prompt engineering; Prompt engineering; LLMs; Refining; Natural languages; Transforms; autoformalization; Probabilistic logic; Reliability engineering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V6EEGFBI","conferencePaper","2024","Li, Yishu; Keung, Jacky; Ma, Xiaoxue","Integrating Generative AI in Software Engineering Education: Practical Strategies","2024 International Symposium on Educational Technology (ISET)","","","10.1109/ISET61814.2024.00019","","The transformative influence of generative artificial intelligence (AI), notably large language models (LLMs), has significantly reshaped the software engineering (SE) landscape, impacting various aspects of software development within industry and academia. The imperative to integrate generative AI into educational programs arises from the necessity to furnish graduates with contemporary methodologies that enhance software quality and streamline development processes. Nevertheless, a research gap exists concerning the systematic integration of established SE education guidelines with specific course contexts to strengthen SE education through incorporating generative AI. In response to this gap, our study presents a vision for integrating generative AI into SE education, with a particular emphasis on practical integration strategies aimed at endowing students with essential competencies tailored for contemporary software development. Aligning our vision with the knowledge domains within SE education, we delineate its application across specific areas such as code generation, auto test case completion, and others. The overall objective of these proposed initiatives is to furnish students in SE with an updated and immersive learning experience, thereby addressing the evolving demands of the field.","2024-07","2025-11-25 22:37:23","2025-11-25 22:37:23","","49-53","","","","","","","","","","","","","","","","","","","","ISSN: 2766-2144","","","","Large language models; Software engineering; Industries; Software quality; generative AI; Generative AI; large language models; software engineering; code generation; education; Systematics; auto test case completion; Educational technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3HDUG6ZP","journalArticle","2025","Jang, Woosung; Young Chul Kim, R.","Automatic Test Case Generation Mechanism With Natural Language-Based Korean Requirement Specifications","IEEE Access","","2169-3536","10.1109/ACCESS.2025.3620431","","In the software industry, software testers are forced to make test cases with informal or formal requirements. From a software perspective, to conduct user acceptance testing, we should manually create numerous test cases to achieve the desired test coverage based on a precise analysis of clear and unstructured requirements. Until now, defining and analyzing the meaning of natural language-based requirements has been challenging, and automating this process has also been complex. In software engineering, rare research generates test cases with natural language requirement sentences, especially in Korea. To solve this problem, we propose a mechanism for automatically generating test cases from natural language requirements, as used by software engineers. This approach involves 1) simplifying unstructured Korean requirements, 2) normalizing requirement sentences, 3) modeling the requirement simplification process (C3Tree modeling), 4) automatically generating cause-effect graph model, and then test cases via decision table model with the graph models through metamodel transformation methods, 5) semi-automatic generation of test scripts, 6) generating test result reports, and 7) finally automatic generation of requirement traceability matrices. Ultimately, simplifying unstructured Korean requirements sentences facilitates understanding of requirements among stakeholders and reduces testing costs. At this moment, we will compare the exact way in which the test case generation approach and generative AI-based test case generation are created.","2025","2025-11-25 22:37:23","2025-11-25 22:37:23","","177305-177317","","","13","","","","","","","","","","","","","","","","","","","","","Semantics; Software; Accuracy; Artificial intelligence; Unified modeling language; Natural languages; Redundancy; Transforms; Automatic test case generation; C3Tree model; cause-effect graph; Computer aided software engineering; decision table; Korean natural language requirements specifications; model-driven architecture (MDA); Text mining","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KG4H66JQ","conferencePaper","2025","Treviğno-Villalobos, Marlen; Quesada-López, Christian; Jiménez-Delgado, Efrén; Quirós-Oviedo, Rocío; Díaz-Oreiro, Ignacio","A Comparative Evaluation of ChatGPT, DeepSeek and Gemini in Automatic Unit Test Generation: a Success Rate Analysis","2025 IEEE VIII Congreso Internacional en Inteligencia Ambiental, Ingenieria de Software y Salud Electronica y Movil (AmITIC)","","","10.1109/AmITIC68284.2025.11214621","","The advancement of large-scale language models (LLMs) has opened up new possibilities for automating unit test generation, a traditionally manual and expensive task. This quantitative study evaluates the performance of three LLMs-ChatGPT 4o mini, DeepSeek v3, and Gemini 2.5 Flash Pro-in generating test cases for methods in C# developed in Unity. The execution success rate of the generated tests was measured using real and synthetic data. The synthetic data was intentionally created to represent common structures, while the real data came from existing project functions. The experimental design was controlled and included the factors LLM and data type and the blocks cyclomatic complexity and contextual memory with four replicates per combination, for a total of 96 experimental treatments. The results show that LLMs have a high potential to support the automatic generation of unit tests. Furthermore, it was evidenced that the choice of model has a significant effect on the success rate of the generated tests. These findings provide useful initial evidence to guide the selection and use of LLMs in test automation processes within software development environments","2025-09","2025-11-25 22:37:23","2025-11-25 22:37:23","","1-8","","","","","","","","","","","","","","","","","","","","","","","","Test pattern generators; LLM; Manuals; Software; Chatbots; Software development management; unit testing; automatic testing; Computational modeling; C# languages; Indexes; Monitoring; prompt; Synthetic data; Unity","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T3JJB6YD","journalArticle","2024","Schäfer, Max; Nadi, Sarah; Eghbali, Aryaz; Tip, Frank","An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation","IEEE Transactions on Software Engineering","","1939-3520","10.1109/TSE.2023.3334955","","Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. Large Language Models (LLMs) have recently been applied to various aspects of software development, including their suggested use for automated generation of unit tests, but while requiring additional training or few-shot learning on examples of existing tests. This paper presents a large-scale empirical evaluation on the effectiveness of LLMs for automated unit test generation without requiring additional training or manual effort. Concretely, we consider an approach where the LLM is provided with prompts that include the signature and implementation of a function under test, along with usage examples extracted from documentation. Furthermore, if a generated test fails, our approach attempts to generate a new test that fixes the problem by re-prompting the model with the failing test and error message. We implement our approach in TestPilot, an adaptive LLM-based test generation tool for JavaScript that automatically generates unit tests for the methods in a given project's API. We evaluate TestPilot using OpenAI's gpt3.5-turbo LLM on 25 npm packages with a total of 1,684 API functions. The generated tests achieve a median statement coverage of 70.2% and branch coverage of 52.8%. In contrast, the state-of-the feedback-directed JavaScript test generation technique, Nessie, achieves only 51.3% statement coverage and 25.6% branch coverage. Furthermore, experiments with excluding parts of the information included in the prompts show that all components contribute towards the generation of effective test suites. We also find that 92.8% of TestPilot's generated tests have łeq≤ 50% similarity with existing tests (as measured by normalized edit distance), with none of them being exact copies. Finally, we run TestPilot with two additional LLMs, OpenAI's older code-cushman-002 LLM and StarCoder, an LLM for which the training process is publicly documented. Overall, we observed similar results with the former (68.2% median statement coverage), and somewhat worse results with the latter (54.0% median statement coverage), suggesting that the effectiveness of the approach is influenced by the size and training set of the LLM, but does not fundamentally depend on the specific model.","2024-01","2025-11-25 22:37:23","2025-11-25 22:37:23","","85-105","","1","50","","","","","","","","","","","","","","","","","","","","","Test pattern generators; Software; Codes; Source coding; Test generation; language models; JavaScript; Training; Documentation; Electronic mail","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"49G56ZIP","conferencePaper","2024","Wang, Taiyan; Wang, Ruipeng; Chen, Yu; Yu, Lu; Pan, Zulie; Zhang, Min; Ma, Huimin; Zheng, Jinghua","Enhancing Black-box Compiler Option Fuzzing with LLM through Command Feedback","2024 IEEE 35th International Symposium on Software Reliability Engineering (ISSRE)","","","10.1109/ISSRE62328.2024.00039","","Since the compiler acts as a core component in software building, it is essential to ensure its availability and reliability through software testing and security analysis. Most research has focused on compiler robustness when compiling various test cases, while the reliability of compiler options lacks attention, especially since each option can activate a specific compiler function. Although some researchers have made efforts in testing it, the insufficient utilization of compiler command feedback messages leads to the poor efficiency, which hinders more diverse and in-depth testing.In this paper, we propose a novel solution to enhance black-box compiler option fuzzing by utilizing command feedback, such as error messages, standard output and compiled files, to guide the error fixing and option pruning via prompting large language models for suggestions. We have implemented the prototype and evaluated it on 4 versions of LLVM. Experiments show that our method significantly improves the detection of crashes, reduces false negatives, and even increase the success rate of compilation when compared to the baseline. To date, our method has identified hundreds of unique bugs, and 9 of them are previously unknown. Among these, 8 have been assigned CVE numbers, and 1 has been fixed following our report.","2024-10","2025-11-25 22:37:23","2025-11-25 22:37:23","","319-330","","","","","","","","","","","","","","","","","","","","ISSN: 2332-6549","","","","large language model; Large language models; software testing; Software reliability; Prototypes; Computer bugs; Buildings; Fuzzing; fuzzing; Security; Standards; Robustness; Closed box; compiler options","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G2RPAE9E","journalArticle","2025","Li, Siyuan; Xie, Kaiyu; Li, Yuekang; Li, Hong; Ren, Yimo; Sun, Limin; Zhu, Hongsong","TransferFuzz-Pro: Large Language Model Driven Code Debugging Technology for Verifying Propagated Vulnerability","IEEE Transactions on Software Engineering","","1939-3520","10.1109/TSE.2025.3584774","","Code reuse in software development frequently facilitates the spread of vulnerabilities, leading to imprecise scopes of affected software in CVE reports. Traditional methods focus primarily on detecting reused vulnerability code in target software but lack the ability to confirm whether these vulnerabilities can be triggered in new software contexts. In previous work, we introduced the TransferFuzz framework to address this gap by using historical trace-based fuzzing. However, its effectiveness is constrained by the need for manual intervention and reliance on source code instrumentation. To overcome these limitations, we propose TransferFuzz-Pro, a novel framework that integrates Large Language Model (LLM)-driven code debugging technology. By leveraging LLM for automated, human-like debugging and Proof-of-Concept (PoC) generation, combined with binary-level instrumentation, TransferFuzz-Pro extends verification capabilities to a wider range of targets. Our evaluation shows that TransferFuzz-Pro is significantly faster and can automatically validate vulnerabilities that were previously unverifiable using conventional methods. Notably, it expands the number of affected software instances for 15 CVE-listed vulnerabilities from 15 to 53 and successfully generates PoCs for various Linux distributions. These results demonstrate that TransferFuzz-Pro effectively verifies vulnerabilities introduced by code reuse in target software and automatically generation PoCs.","2025-08","2025-11-25 22:37:23","2025-11-25 22:37:23","","2396-2411","","8","51","","","","","","","","","","","","","","","","","","","","","large language model; Large language models; Manuals; Software; Software development management; Codes; Source coding; Debugging; Fuzzing; fuzzing; code reuse; Instruments; Linux; Vulnerability verification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QME4CMR7","conferencePaper","2025","Demir, Kadir Alpaslan; Muppalla, Tarun Narasimha Varma; Liu, Bozhen","Generative AI Efficiency and Effectiveness in Software Project Documentation Review Process","2025 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)","","","10.1109/ACDSA65407.2025.11165833","","In this research, we report the capabilities of Large Language Model (LLM)-based Generative Artificial Intelligence (GenAI) tools, such as ChatGPT and Gemini, in the software project documentation review process. In software projects, the document artifact review process is an essential part of the quality assurance process. This review process is a human effort-intensive process. Introducing automation in this process will help reduce human effort, speed up the development, and reduce the potential of errors in the documents. With its enhanced natural language processing capabilities, LLM-based GenAI tools provide significant potential in the above process. In this research, we investigate the efficiency and effectiveness of LLM-based GenAI tools (such as ChatGPT and Gemini) for software documentation review process. Our experimental design includes comparing the manual document review process with the automated document review process conducted by LLM-based GenAI tools. The research results indicate that utilizing LLM-based GenAI tools provides significant efficiency in the document review process. However, LLM-based GenAI tools can only reach half the effectiveness and review quality that can be achieved by a human. To the best of our knowledge, this study is one of the first studies investigating the effectiveness and efficiency of utilizing GenAI for the software documentation review process. This research provides significant insights into improving the software document artifact review process utilizing LLM-based GenAI techniques.","2025-08","2025-11-25 22:37:23","2025-11-25 22:37:23","","1-7","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Manuals; Software; Software engineering; Chatbots; Generative AI; Software development management; software engineering; software development; Quality assurance; Reviews; Documentation; document review; software documentation; software process improvement","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X2FFC6KJ","conferencePaper","2024","Lee, Jae Yong; Kang, Sungmin; Yoon, Juyeon; Yoo, Shin","The GitHub Recent Bugs Dataset for Evaluating LLM-Based Debugging Applications","2024 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST60714.2024.00049","","While Large Language Models (LLMs) have demon-strated strong natural language and code processing capabilities, concern has been raised as to whether existing bug benchmarks are included in their training data. We examine the training data of the open-source LLM StarCoder, and find it likely that data from the widely used Defects4J benchmark was included, raising the possibility of its inclusion in the training data of the GPT model as well. This makes it difficult to tell how well LLM-based results on Defects4J would generalize, as for any results it would be unclear whether a technique's performance is due to LLM generalization or memorization. To remedy this issue and facilitate continued research on LLM-based SE, we present the GitHub Recent Bugs (GHRB) framework, which continuously gathers real-world Java bugs for use in evaluation of LLM-based techniques. To date, we have gathered 89 bugs reported after the GPT-3.5 training data cutoff point of September 2021.","2024-05","2025-11-25 22:37:24","2025-11-25 22:37:24","","442-444","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Software testing; Java; Machine Learning; Computer bugs; Debugging; Benchmark; Training data; Natural languages","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YLZ8L3L8","journalArticle","2025","Yang, Zhuoying; Wang, Lei","IntelliUnitGen: A Unit Test Case Generation Framework Based on the Integration of Static Analysis and Prompt Learning","IEEE Access","","2169-3536","10.1109/ACCESS.2025.3615990","","Test case generation is a critical research topic in the field of automated software engineering. In recent years, with the rapid advancement of large language models (LLMs) such as the GPT series, LLaMA series, and DeepSeek series, both domestic and international researchers have attempted to leverage prompt learning capabilities of LLMs for automated test case generation and have achieved promising results. However, most existing approaches rely solely on the language model’s ability to respond to natural language prompts, while neglecting systematic analysis of static features within source code. These methods often fail to apply traditional static analysis techniques to extract structural and semantic information from the source code, resulting in incomplete generation, omission of key elements, or semantic misinterpretation. To address these limitations, this paper proposes IntelliUnitGen — an intelligent unit test case generation framework that integrates static analysis with prompt learning. This framework aims to combine the advantages of traditional static analysis techniques with the cutting-edge capabilities of LLMs to enhance the automation level and quality of unit test case generation. IntelliUnitGen synthesizes static code features extracted via static analysis into structured, task-relevant prompts for the language model. It adopts a chain-of-thought prompting strategy to guide the LLM in generating rigorous and comprehensive test cases. The overall workflow of the proposed framework is presented, followed by a detailed introduction to its key techniques and implementation details. An illustrative example is provided to intuitively demonstrate the test case generation process of the framework, and experimental validation is conducted on three open-source projects. Experimental results show that IntelliUnitGen significantly outperforms traditional (non-LLM-based) tools and LLM-only prompt learning approaches in metrics such as coverage, generalizability, and compilability & executability, achieving state-of-the-art (SOTA) performance. This study demonstrates the potential of integrating static code analysis with prompt learning to improve not only unit test case generation but also other source code processing tasks, potentially overcoming existing limitations of LLMs in software engineering applications. This framework demonstrates a practical step forward in addressing long-standing challenges in automated software testing. To facilitate reproducibility and further research, the complete implementation of the IntelliUnitGen framework, alongside the generated test cases and JaCoCo coverage reports for all experimental projects, has been made publicly available at: https://github.com/yangzy0115/IntelliUnitGen","2025","2025-11-25 22:37:24","2025-11-25 22:37:24","","177760-177784","","","13","","","","","","","","","","","","","","","","","","","","","Analytical models; Semantics; Test pattern generators; Software engineering; static analysis; Static analysis; Codes; Testing; Source coding; large language models; Logic; Cognition; automated software engineering; chain-of-thought prompting; prompt learning; source code processing; unit test case generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JSCFZSSH","conferencePaper","2023","Li, Yihao; Xu, Jialong; Zhu, Yinghua; Liu, Huashuo; Liu, Pan","The Impact of ChatGPT on Software Engineering Education: A Quick Peek","2023 10th International Conference on Dependable Systems and Their Applications (DSA)","","","10.1109/DSA59317.2023.00087","","Since its public launch at the end of 2022, ChatGPT has garnered global attention, showcasing the diverse capabilities of AI in tackling human tasks. Its rapid growth and widespread adoption have permeated every corner of our daily routine. This paper provides a quick peek at the impact of ChatGPT from the perspective of software engineering education. Specifically, to make our case study creative and interesting, we compare the impact answered by ChatGPT with the real feedback from existing literature. In this way, we explore the potential of ChatGPT as a teaching and learning tool in software engineering.","2023-08","2025-11-25 22:37:24","2025-11-25 22:37:24","","595-596","","","","","","","","","","","","","","","","","","","","ISSN: 2767-6684","","","","Software engineering; Chatbots; ChatGPT; Education; Artificial intelligence; software engineering; education; Task analysis; teaching and learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FZUBTJ6E","conferencePaper","2025","Chen, Boqi; Chene, Kua; Hernández López, José Antonio; Mussbacher, Gunter; Varró, Dániel; Feizpour, Amir","SHERPA: A Model-Driven Framework for Large Language Model Execution","2025 ACM/IEEE 28th International Conference on Model Driven Engineering Languages and Systems (MODELS)","","","10.1109/MODELS67397.2025.00024","","Recently, large language models (LLMs) have achieved widespread application across various fields. Despite their impressive capabilities, LLMs suffer from a lack of structured reasoning ability, particularly for complex tasks requiring domain-specific best practices, which are often unavailable in the training data. Although multi-step prompting methods incorporating human best practices, such as chain-of-thought and tree-of-thought, have gained popularity, they lack a general mechanism to control LLM behavior. In this paper, we propose SHERPA, a model-driven framework to improve the LLM performance on complex tasks by explicitly incorporating domain-specific best practices into hierarchical state machines. By structuring the LLM execution processes using state machines, SHERPA enables more fine-grained control over their behavior via rules or decisions driven by machine learning-based approaches, including LLMs. We show that SHERPA is applicable to a wide variety of tasks-specifically, code generation, class name generation, and question answering-replicating previously proposed approaches while further improving the performance. We demonstrate the effectiveness of SHERPA for the aforementioned tasks using various LLMs. Our systematic evaluation compares different state machine configurations against baseline approaches without state machines. Results show that integrating well-designed state machines significantly improves the quality of LLM outputs, and is particularly beneficial for complex tasks with well-established human best practices but lacking data used for training LLMs.","2025-10","2025-11-25 22:37:24","2025-11-25 22:37:24","","197-208","","","","","","","","","","","","","","","","","","","","","","","","large language model; Large language models; Cognition; Training; Training data; Systematics; Costs; Question answering (information retrieval); best practice integration; Best practices; Model driven engineering; Process control; state machine; structured reasoning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JJHDXXSD","conferencePaper","2024","Xiao, Chao; Deng, Yifei; Yang, Zhijie; Chen, Renzhi; Wang, Hong; Zhao, Jingyue; Dai, Huadong; Wang, Lei; Tang, Yuhua; Xu, Weixia","LLM-based Processor Verification: A Case Study for Neuromorphic Processor","2024 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","","10.23919/DATE58400.2024.10546707","","With the increasing complexity of the hardware design, conducting verification before the tapeout is of utmost importance. Simulation-based verification remains the primary method owing to its scalability and flexibility. A comprehensive verification of modern processors usually requires numerous effective tests to cover all possible conditions and use cases, leading to significant time, resource, and manual effort even with the EDA. Moreover, novel domain specific architecture (DSA), such as neuromorphic processors, will exacerbate the challenge of verification. Fortunately, emerging large language models (LLMs) have been demonstrating a powerful ability to complete specific tasks assigned by human instructions. In this paper, we explore the challenges and opportunities encountered when using the LLMs to accelerate the DSA verification using the proposed LLM-based workflow consisting of test generation, compilation&simulation, and result collection&processing. By verifying a RISC-V core and a neuromorphic processor, we examine the capabilities and limitations of the LLMs when using them for the function verification of traditional processors and emerging DSA. In the experiment, 36 C programs and 128 assembly snippets for the RISC-V core and the neuromorphic processor are generated using an advanced LLM to demonstrate our claim. The experimental results show that the code coverage based on the LLM test generation can reach 89% and 91% for the above two architectures respectively, showing a promising research direction for the future processor verification in the new golden age for computer architecture.","2024-03","2025-11-25 22:37:24","2025-11-25 22:37:24","","1-6","","","","","","","","","","","","","","","","","","","","ISSN: 1558-1101","","","","Large language models; Test pattern generators; Manuals; Life estimation; Scalability; test generation; large language model (LLM); Computer architecture; Task analysis; Program processors; Instruction sets; neuromorphic processor; Neuromorphics; processor function verification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T7KRMS9N","conferencePaper","2024","Wanigasekara, Sashini; Asanka, Dinesh; Rajapakse, Chathura; Wickramaarachchi, Dilani; Wijesinghe, Abhiru","Comparing the Adaptability of a Genetic Algorithm and an LLM-Based Framework for Automated Software Test Data Generation: In the Context of Web Applications","2024 IEEE 3rd International Conference on Data, Decision and Systems (ICDDS)","","","10.1109/ICDDS62937.2024.10910287","","In the fast-paced world of software development, ensuring software quality is paramount. Software Quality Assurance (SQA) plays a vital role, primarily through testing, which can be carried out manually or automatically. Yet, creating comprehensive test data (TD) for web applications can be a formidable task. Manual test data generation (TDG) is time-consuming and error prone. Automation of TDG has become increasingly important in the realm of software quality assurance as it enables efficient and effective testing of software systems. The need for an appropriate framework for automated TDG is critical to achieve comprehensive and reliable test coverage. Automated TDG offers significant advantages, including time and resource savings, improved test coverage, and seamless integration into the software development process. The core aim of this research is to bridge the gap between manual and existing automated methods, resulting in time and cost savings, heightened testing efficiency, and elevated software quality. Research objectives encompass comparing the adaptability of an AGA based automated TDG model and a LLM based automated TDG model to a web application. The results from the LLM model for triangle classification program was found to be potentially acceptable and accurate than the AGA model's results. This research discusses the challenges encountered when implementing and using the AGA-based framework in the web application context and how an LLM model could overcome the challenges. The study highlights the benefits of using the LLM approach, demonstrating its relevance and accuracy in generating test data compared to the Genetic Algorithm-based model. The practical implications for software quality assurance practices are discussed, emphasizing the enhanced efficiency and effectiveness of the LLM model in improving software quality.","2024-12","2025-11-25 22:37:24","2025-11-25 22:37:24","","1-6","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Manuals; Automation; Software quality; Software systems; Software development management; Large Language Model; Testing; Adaptation models; Retrieval Augmented Generation; Test automation; Adaptive Genetic Algorithm; Automated test data generation; Data collection; Genetic algorithms","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HSHSLCHC","journalArticle","2025","Nettur, Suresh Babu; Karpurapu, Shanthi; Nettur, Unnati; Gajja, Likhit Sagar","Cypress Copilot: Development of an AI Assistant for Boosting Productivity and Transforming Web Application Testing","IEEE Access","","2169-3536","10.1109/ACCESS.2024.3521407","","In today’s fast-paced software development environment, Agile methodologies demand rapid delivery and continuous improvement, making automated testing essential for maintaining quality and accelerating feedback loops. Our study addresses the challenges of developing and maintaining automation code for web-based application testing. In this paper, we propose a novel approach that leverages large language models (LLMs) and a novel prompt technique, few-shot chain, to automate code generation for web application testing. We chose the Behavior-Driven Development (BDD) methodology owing to its advantages and selected the Cypress tool for automating web application testing, as it is one of the most popular and rapidly growing frameworks in this domain. We comprehensively evaluated various OpenAI models, including GPT-4-Turbo, GPT-4o, and GitHub Copilot, using zero-shot and few-shot chain prompt techniques. Furthermore, we extensively validated with a vast set of test cases to identify the optimal approach. Our results indicate that the Cypress automation code generated by GPT-4o using a few-shot chained prompt approach excels in generating complete code for each test case, with fewer empty methods and improved syntactical accuracy and maintainability. Based on these findings, we developed a novel open-source Visual Studio Code (IDE) extension, “Cypress Copilot” utilizing GPT-4o and a few-shot chain prompt technique, which has shown promising results. Finally, we validate the Cypress Copilot tool by generating automation code for end-to-end web tests, demonstrating its effectiveness in testing various web applications and its ability to streamline development processes. More importantly, we are releasing this tool to the open-source community, as it has the potential to be a promising partner in enhancing productivity in web application automation testing.","2025","2025-11-25 22:37:24","2025-11-25 22:37:24","","3215-3229","","","13","","","","","","","","","","","","","","","","","","","","","large language model; software testing; test case generation; Java; Automation; Accuracy; Codes; Testing; prompt engineering; Visualization; Business; Productivity; machine learning; GPT-4; OpenAI; code generation; test automation; GPT-4o; GitHub Copilot; zero-shot; GPT-3; Graphical user interfaces; Stakeholders; Agile software development; AI assistant tools; behavior driven development; cypress; few-shot; GPT3.5; selenium; web application","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S6PJE6P3","conferencePaper","2024","Raghi, K R; Sudha, K; M, Sreeram A; Joshua S, Steve","Software Development Automation Using Generative AI","2024 International Conference on Emerging Research in Computational Science (ICERCS)","","","10.1109/ICERCS63125.2024.10894980","","The Software Development Lifecycle (SDLC) is a structured process that guides the development of software projects, encompassing phases from planning to deployment. Traditionally, the SDLC has relied on manual input, making it prone to delays, errors, and inefficiencies. With the recent advancements in Generative AI (GenAI) and Large Language Models (LLMs) such as GPT, it is now feasible to automate substantial portions of the SDLC. This paper presents a novel approach to automating the SDLC using LLMs and the Langchain framework, aiming to streamline the entire software development process. By au-tomating key phases, including project planning, requirements gathering, code generation, testing, and deployment, this research explores how AI can minimize human intervention and accelerate software development timelines. The paper also discusses the potential advantages of AI-driven SDLC automation, such as improved efficiency, consistency, and scalability, while addressing challenges related to its integration. The proposed approach offers a glimpse into the future of software engineering, where AI plays a central role in transforming how software is developed and delivered.","2024-12","2025-11-25 22:37:24","2025-11-25 22:37:24","","1-6","","","","","","","","","","","","","","","","","","","","","","","","Manuals; Software; Software engineering; Automation; Generative AI; Planning; Software development management; Testing; Scalability; SDLC automation; GPT; Large Language Models (LLMs); code generation; AI-driven software development; Generative AI (GenAI); Langchain; Scientific computing; Software Development Lifecycle (SDLC)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4URENID5","conferencePaper","2024","Almutawa, Mustafa; Ghabrah, Qusai; Canini, Marco","Towards LLM-Assisted System Testing for Microservices","2024 IEEE 44th International Conference on Distributed Computing Systems Workshops (ICDCSW)","","","10.1109/ICDCSW63686.2024.00011","","As modern applications are being designed in a distributed, Microservices Architecture (MSA), it becomes increasingly difficult to debug and test those systems. Typically, it is the role of software testing engineers or Quality Assurance (QA) engineers to write software tests to ensure the reliability of applications, but such a task can be labor-intensive and time-consuming. In this paper, we explore the potential of Large Language Models (LLMs) in assisting software engineers in generating test cases for software systems, with a particular focus on performing end-to-end (black-box) system testing on web-based MSA applications. We present our experience building Kashef, a software testing tool that utilizes the advanced capabilities of current LLMs in code generation and reasoning, and builds on top of the concept of communicative agents.","2024-07","2025-11-25 22:37:24","2025-11-25 22:37:24","","29-34","","","","","","","","","","","","","","","","","","","","ISSN: 2332-5666","","","","Large language models; Software testing; Software systems; System testing; Software Testing; Large Language Models (LLMs); Quality assurance; Microservice architectures; Reliability engineering; Communicative Agents; Testing Automation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MHCAVL9K","conferencePaper","2024","Helmy, Mona; Sobhy, Omar; ElHusseiny, Farida","AI-Driven Testing: Unleashing Autonomous Systems for Superior Software Quality Using Generative AI","2024 International Telecommunications Conference (ITC-Egypt)","","","10.1109/ITC-Egypt61547.2024.10620598","","In software development, achieving comprehensive code coverage is imperative for ensuring the reliability and robustness of programs. This paper presents a novel approach to attaining between 80% and 100% code coverage in C programs by leveraging Large Language Models (LLMS) for test case generation and subsequently measuring the coverage using the GCOV tool. The methodology outlined in this research harnesses the capabilities of LLMS, such as GPT-3.5-turbo, Code llama, and llama-2 to autonomously generate diverse and complex test cases tailored to the intricacies of C code. Through empirical experimentation and case studies, we demonstrate the feasibility and efficacy of our approach in achieving from 80%-100% code coverage in a variety of C programs across different domains. Furthermore, we discuss the implications of our methodology in enhancing software quality, reducing the likelihood of undiscovered defects, and ultimately improving the reliability and maintainability of C codebases.","2024-07","2025-11-25 22:37:24","2025-11-25 22:37:24","","1-6","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Software reliability; Software quality; Generative AI; Codes; Robustness; Autonomous systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TTK8D8F4","conferencePaper","2023","Virvou, Maria; Tsihrintzis, George A.","Is ChatGPT Beneficial to Education? A Holistic Evaluation Framework Based on Intelligent Tutoring Systems","2023 14th International Conference on Information, Intelligence, Systems & Applications (IISA)","","","10.1109/IISA59645.2023.10345949","","The recent launch of ChatGPT by OpenAI has created a profound global impact, initiating deep questions among educators about how it might affect education, syllabi and teaching methods. Currently, the full scope of potential benefits and risks associated with ChatGPT in education remains unclear, given that its impact surpasses the level of preparation educators and institutions may have had for such a pre-trained generative AI tool. While Artificial Intelligence in Education has long been a subject of research, with a particular focus on developing Intelligent Tutoring Systems, the emergence of ChatGPT marks a distinctive advancement in this field. Unlike dedicated Intelligent Tutoring Systems, ChatGPT is readily available to a diverse spectrum of educational stakeholders, including teachers, students, schools, universities, and educational institutions. Scholars have initiated assessments of ChatGPT's effectiveness across various educational disciplines, even though ChatGPT was not explicitly designed for educational purposes. However, the widespread accessibility of ChatGPT, coupled with its extensive knowledge base, necessitates the development of comprehensive evaluation frameworks. In this paper, we introduce a holistic evaluation framework tailored for ChatGPT. This framework takes into account both soft and hard skills, and it is designed to seamlessly incorporate ChatGPT into Intelligent Tutoring Systems, making it suitable for a wide range of educational fields. By establishing a connection between ITS and ChatGPT, as they are both AI tools, we can benefit from the substantial background work achieved by previous research in ITSs to evaluate the educational influence of ChatGPT.","2023-07","2025-11-25 22:37:24","2025-11-25 22:37:24","","1-8","","","","","","","","","","","","","","","","","","","","","","","","Chatbots; ChatGPT; Education; generative AI; large language models; Cognition; Ethics; Stakeholders; AI in Education; e-learning; Educational Evaluation Frameworks; educational software; Intelligent Tutoring Systems; Knowledge based systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PTL99YH2","conferencePaper","2024","Sudheerbabu, Gaadha; Ahmad, Tanwir; Truscan, Dragos; Vain, Jüri; Porres, Ivan","Iterative Optimization of Hyperparameter-based Metamorphic Transformations","2024 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","","","10.1109/ICSTW60967.2024.00016","","Verification and validation of a software system to ensure compliance with the specification and intended functional behaviour often pose a challenge when it lacks an explicit test oracle. We present an efficient black-box metamorphic testing approach in which test cases are automatically generated based on metamorphic transformations. The hyperparameters of several metamorphic transformations are optimized on the fly using a generative AI with a feedback loop for optimal test generation and test suite minimization. The proposed method uses several combined metamorphic relations to define test inputs and to determine the test verdict. The feedback on test quality is evaluated based on the metamorphic relation’s fitness function and used to optimize the next iterations of test generation. The effectiveness of the proposed approach is evaluated on an industrial case study of a crane’s load position system which lacks an explicit test oracle. The experimental results confirm that optimizing the morphing transformations using the feedback loop improves the effectiveness of metamorphic test input generation. The outcome of the study shows that the approach can be potentially applied for functional safety verification in software systems with a test oracle problem.","2024-05","2025-11-25 22:37:24","2025-11-25 22:37:24","","13-20","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Software testing; Software systems; Generative AI; Software algorithms; Artificial Intelligence; Test Automation; Metamorphic testing; Safety; Feedback loop; Minimization; Optimization methods; Verification and Validation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UT86JM3D","conferencePaper","2025","Olianas, Dario; Leotta, Maurizio; Ricca, Filippo","Leveraging Large Language Models for Explicit Wait Management in End-to-End Web Testing","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST62969.2025.10988981","","End-to-end (E2E) testing is an approach in which an application is automatically tested through scripts that simulate the actions a user would perform. Properly managing asynchronous interactions is crucial in this approach to avoid test failures and flakiness. In the Selenium WebDriver framework, this is typically addressed by using thread sleeps (which pause the test for a fixed time) or explicit waits (function calls that pause the test execution until a specified condition is met). Explicit waits require the selection of both a condition to wait for (e.g., element visibility, element clickability) and an element on which that condition applies. Since thread sleeps are unreliable and replacing them with appropriate explicit waits is a time consuming task, in this work, we leverage a Large Language Model (LLM) to assist testers in selecting the most appropriate explicit waits. We defined a structured procedure (a series of prompts) for engaging with the LLM and validated this approach empirically on three test suites affected by asynchronous waiting issues, as well as on 12 synthetic examples. Additionally, we compared our approach with SleepReplacer, the current state-of-the-art tool for replacing thread sleeps with explicit waits in E2E web test suites. The results show that the LLM-based approach can automatically replace the majority of thread sleeps in a test suite on the first attempt, outperforming SleepReplacer.","2025-03","2025-11-25 22:37:39","2025-11-25 22:37:39","","577-581","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Software testing; LLM; Testing; Artificial Intelligence; Selenium WebDriver; End-to-end; Flak-iness; Selenium; Waiting Strategies","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RBL44SZV","conferencePaper","2025","Xie, Wei; Qian, Jian; Li, Yangdi; Huang, Jianye; Zheng, Yanrong; Liu, Yanan","A Method for Intelligent Optimization Algorithms to Automatically Generate LLM Test Data","2025 5th Asia-Pacific Conference on Communications Technology and Computer Science (ACCTCS)","","","10.1109/ACCTCS66275.2025.00047","","As AI products become more and more complex, traditional methods of manual test data generation are facing problems such as high cost, low efficiency, and insufficient test coverage, which affect the quality and stability of products. In order to solve the above problems, this paper proposes a method for automatic generation of AI (LLM) test data by intelligent optimization algorithm, and applies it to practical cases to carry out more detailed research. Experiments show that after a company introduces an intelligent optimization algorithm to automatically generate AI test data, based on this method, the test data generation time of its software has been reduced from 80.16 hours to 20.25 hours. And at the same time, the test coverage has increased from 60.32 % to 95.10 %, and the defect discovery rate has also increased from 65.51 % to 90.17 %. It can be seen that this method can significantly improve the test efficiency, optimize the test cost, and enhance the stability and quality of the software. It has been proved that intelligent optimization algorithms can be reasonably applied to automatically generate software test data, and then form a reliable method to provide enterprises with more accurate and efficient software testing solutions, and provide other companies with valuable optimization experience.","2025-04","2025-11-25 22:37:39","2025-11-25 22:37:39","","230-234","","","","","","","","","","","","","","","","","","","","","","","","Optimization; Software testing; Software; Software reliability; Software quality; Artificial intelligence; Software algorithms; Companies; Costs; Data collection; Automatically generated; intelligent optimization algorithm; Methods for automatic generation of AI test data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GFSXV3HK","conferencePaper","2025","Biagiola, Matteo; Ghislotti, Gianluca; Tonella, Paolo","Improving the Readability of Automatically Generated Tests Using Large Language Models","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST62969.2025.10989020","","Search-based test generators are effective at producing unit tests with high coverage. However, such automatically generated tests have no meaningful test and variable names, making them hard to understand and interpret by developers. On the other hand, large language models (LLMs) can generate highly readable test cases, but they are not able to match the effectiveness of search-based generators, in terms of achieved code coverage. In this paper, we propose to combine the effectiveness of search-based generators with the readability of LLM generated tests. Our approach focuses on improving test and variable names produced by search-based tools, while keeping their semantics (i.e., their coverage) unchanged. Our evaluation on nine industrial and open source LLMs show that our readability improvement transformations are overall semantically-preserving and stable across multiple repetitions. Moreover, a human study with ten professional developers, show that our LLM-improved tests are as readable as developer-written tests, regardless of the LLM employed.","2025-03","2025-11-25 22:37:39","2025-11-25 22:37:39","","162-173","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Semantics; Software testing; Codes; Large Language Models; Software Testing; Generators; Readability; Hands","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S9WQB9HZ","journalArticle","2025","Deng, Yao; Tu, Zhi; Yao, Jiaohong; Zhang, Mengshi; Zhang, Tianyi; Zheng, Xi","TARGET: Traffic Rule-Based Test Generation for Autonomous Driving via Validated LLM-Guided Knowledge Extraction","IEEE Transactions on Software Engineering","","1939-3520","10.1109/TSE.2025.3569086","","Recent incidents with autonomous vehicles highlight the need for rigorous testing to ensure safety and robustness. Constructing test scenarios for autonomous driving systems (ADSs), however, is labor-intensive. We propose TARGET, an end-to-end framework that automatically generates test scenarios from traffic rules. To address complexity, we leverage a Large Language Model (LLM) to extract knowledge from traffic rules. To mitigate hallucinations caused by large context during input processing, we introduce a domain-specific language (DSL) designed to be syntactically simple and compositional. This design allows the LLM to learn and generate test scenarios in a modular manner while enabling syntactic and semantic validation for each component. Based on these validated representations, TARGET synthesizes executable scripts to render scenarios in simulation. Evaluated seven ADSs with 284 scenarios derived from 54 traffic rules, TARGET uncovered 610 rule violations, collisions, and other issues. For each violation, TARGET generates scenario recordings and detailed logs, aiding root cause analysis. Two identified issues were confirmed by ADS developers: one linked to an existing bug report and the other to limited ADS functionality.","2025-07","2025-11-25 22:37:39","2025-11-25 22:37:39","","1950-1968","","7","51","","","","","","","","","","","","","","","","","","","","","Semantics; Test pattern generators; LLM; Syntactics; Scenario generation; Testing; Roads; Data mining; Test generation; autonomous driving system testing; Autonomous vehicles; DSL; Meteorology; scenario-based testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WVQA7J8H","conferencePaper","2025","Maybury, Mark; Forrest, Greg; O'Donnell, Donna","Lockheed Martin AI Factory: Generative AI and MLOps for Engineering, Enterprise and Edge","2025 IEEE International Conference on AI and Data Analytics (ICAD)","","","10.1109/ICAD65464.2025.11114065","","This article reports on the rapid creation and deployment at scale of hundreds of Large Language Model (LLM) applications, highlighting several across enterprise, engineering and edge use cases. This outcome was accelerated by the creation of an open architecture, secure and scalable generative AI Factory. An extensible platform, AI Factory empowers thousands of developers and over 50,000 end users across a diverse set of data types and use cases throughout our global enterprise. This evolvable approach reveals how to affordably deploy generative AI to create value securely at scale.","2025-06","2025-11-25 22:37:39","2025-11-25 22:37:39","","1-7","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Generative AI; LLMs; security; Security; Production facilities; AI Factory; Data analysis; scale","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TJG567VW","conferencePaper","2025","Isaku, Erblin; Laaber, Christoph; Sartaj, Hassan; Ali, Shaukat; Schwitalla, Thomas; Nygård, Jan F.","LLMs in the Heart of Differential Testing: A Case Study on a Medical Rule Engine","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST62969.2025.10989025","","The Cancer Registry of Norway (CRN) uses an automated cancer registration support system (CaReSS) to support core cancer registry activities, i.e., data capture, data curation, and producing data products and statistics for various stakeholders. GURI is a core component of CaReSS, which is responsible for validating incoming data with medical rules. Such medical rules are manually implemented by medical experts based on medical standards, regulations, and research. Since large language models (LLMs) have been trained on a large amount of public information, including these documents, they can be employed to generate tests for GURI. Thus, we propose an LLM-based test generation and differential testing approach (LLMeDiff) to test GURI. We experimented with four different LLMs, two medical rule engine implementations, and 58 real medical rules to investigate the hallucination, success, time efficiency, and robustness of the LLMs to generate tests, and these tests' ability to find potential issues in GURI. Our results showed that GPT-3.5 hallucinates the least, is the most successful, and is generally the most robust; however, it has the worst time efficiency. Our differential testing revealed 22 medical rules where implementation inconsistencies were discovered (e.g., regarding handling rule versions). Finally, we provide insights for practitioners and researchers based on the results.","2025-03","2025-11-25 22:37:40","2025-11-25 22:37:40","","429-440","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Software testing; Test pattern generators; Large Language Models; Test Generation; Differential Testing; Standards; Robustness; Engines; Stakeholders; Automated Software Testing; Cancer; Electronic Health Records; Medical Rules; Regulation; Web services","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QESEKL63","conferencePaper","2024","Couder, Juan Ortiz; Gomez, Dawson; Ochoa, Omar","Requirements Verification Through the Analysis of Source Code by Large Language Models","SoutheastCon 2024","","","10.1109/SoutheastCon52093.2024.10500073","","In the most recent years, Large Language Models (LLMs) have gained popularity and have been accepted and used in different domains due to their ability to understand and generate written language. LLMs allow us to analyze large amounts of data in a few moments, yet they are also extremely simple to use, making them a very powerful assistive tool that can aid in a wide range of tasks; from planning a family trip, to aid during the development process of a huge system. For software developers, LLMs have been mostly used for code generation, explanation, or optimization. Software verification is a crucial part of software development as it is the process of ensuring that a system meets specific requirements. Requirements specifications play a pivotal role in software verification as they define what a system should do. In this paper we propose the use of LLMs for code verification through the analysis of requirements specifications. We prove that LLMs, such as GPT-3.5, can verify a list of requirements through a given code and evaluate why the requirements have or have not been met.","2024-03","2025-11-25 22:37:40","2025-11-25 22:37:40","","75-80","","","","","","","","","","","","","","","","","","","","ISSN: 1558-058X","","","","Software; ChatGPT; Planning; Codes; Large Language Model; Source coding; Software Requirements; Software Engineering; GPT-3.5; Verification; Robustness; Software design; Mathematical models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7TRZUINF","conferencePaper","2025","Yoon, Juyeon; Kim, Seah; Kim, Somin; Jung, Sukchul; Yoo, Shin","Integrating LLM-Based Text Generation with Dynamic Context Retrieval for GUI Testing","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST62969.2025.10989041","","Automated GUI testing plays a crucial role for smartphone vendors who have to ensure that the widely used mobile apps-that are not essentially developed by the vendors-are compatible with new devices and system updates. While existing testing techniques can automatically generate event sequences to reach different GUI views, inputs such as strings and numbers remain difficult to generate, as their generation often involves semantic understanding of the app functionality. Recently, Large Language Models (LLMs) have been successfully adopted to generate string inputs that are semantically relevant to the test case. This paper evaluates the LLM-based input generation in the industrial context of vendor testing of both in-house and 3rd party mobile apps. We present DROIDFILLER, an LLM based input generation technique that builds upon existing work with more sophisticated prompt engineering and customisable context retrieval. DROIDFILLER is empirically evaluated using a total of 120 textfields collected from a total of 45 apps, including both in-house and 3rd party ones. The results show that DROIDFILLER can outperform both vanilla LLM based input generation as well as the existing resource pool approach. We integrate DROIDFILLER into the existing GUI testing framework used at Samsung, evaluate its performance, and discuss the challenges and considerations for practical adoption of LLM-based input generation in the industry.","2025-03","2025-11-25 22:37:40","2025-11-25 22:37:40","","394-405","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Semantics; Software testing; Automation; Industries; Accuracy; Testing; Large Language Models; Prompt engineering; GUI Testing; Test Automation; Mobile applications; Graphical user interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"26IICWM4","conferencePaper","2025","Barboni, Morena; Lampa, Filippo; Morichetta, Andrea; Polini, Andrea; Zulkoski, Edward","Mutant-Driven Test Generation for Ethereum Smart Contracts via LLMs","2025 IEEE International Conference on Artificial Intelligence Testing (AITest)","","","10.1109/AITest66680.2025.00033","","Bugs in Solidity smart contracts caused millions of dollars in losses in the past year alone. Mutation testing can expose weaknesses in test suites that simpler coverage metrics often miss, but the effort required to generate test cases for live mutants remains a major barrier to adoption. To address this, we present Alchemist, a framework that generates test cases from Solidity mutants via LLMs. Alchemist incorporates the scientific method into its test generation process, enabling the systematic refinement of test cases through hypotheses. Evaluation on Solidity projects demonstrates that this method improves test quality over direct prompting while reducing developer effort.","2025-07","2025-11-25 22:37:40","2025-11-25 22:37:40","","209-216","","","","","","","","","","","","","","","","","","","","ISSN: 2835-3560","","","","Large language models; Test pattern generators; Large Language Model; Measurement; Testing; Computer bugs; Test Generation; Blockchains; Ethereum; Mutation Testing; Smart Contract; Smart contracts; Solidity; Systematics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6682HFJG","conferencePaper","2024","Molina, Facundo; Copia, Juan Manuel; Gorla, Alessandra","Improving Patch Correctness Analysis via Random Testing and Large Language Models","2024 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST60714.2024.00036","","Patch correctness assessment represents a crucial step in the patch validation process, with the potential to enhance the practical adoption of automated program repair (APR) techniques and substantially reduce validation costs. While some automated techniques have been proposed for assessing patch correctness, they primarily focus on either ranking patches based on their likelihood of being correct or classifying them as correct or incorrect without offering any further explanatory information. In this paper, we introduce FIXCHECK, a novel approach that combines random testing and large language models to automatically generate fault-revealing tests for potentially incorrect patches. To achieve this, FIXCHECK employs a two-fold process: Firstly, a random testing procedure generates a comprehensive set of test cases. Secondly, a large language model is utilized to derive meaningful assertions for each test case. Additionally, FIXCHECK incorporates a selection and prioritization mechanism, which evaluates the generated tests executed on the patched program and discards or ranks them based on their likelihood of revealing faults in the patch. To assess the effectiveness of our approach, we conducted evaluations on a benchmark comprising 160 patches, encompassing both patches created by developers and patches generated by APR tools. The results demonstrate that FIXCHECK effectively generates fault-revealing tests for 62 % of incorrect patches written by developers, with a high level of confidence. Furthermore, it complements existing patch correctness assessment techniques by providing fault-revealing tests for up to 50% of the incorrect patches identified by state-of-the-art techniques.","2024-05","2025-11-25 22:37:40","2025-11-25 22:37:40","","317-328","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Analytical models; Large language models; Software testing; Large Language Models; Benchmark testing; Fault diagnosis; Patch Correctness Assessment; Costs; Maintenance engineering; Random Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E5QAPYIW","conferencePaper","2024","Hyun, Sangwon; Guo, Mingyu; Babar, M. Ali","METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities","2024 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST60714.2024.00019","","Large-Language Models (LLMs) have shifted the paradigm of natural language data processing. However, their black-boxed and probabilistic characteristics can lead to potential risks in the quality of outputs in diverse LLM applications. Recent studies have tested Quality Attributes (QAs), such as robustness or fairness, of LLMs by generating adversarial input texts. However, existing studies have limited their coverage of QAs and tasks in LLMs and are difficult to extend. Additionally, these studies have only used one evaluation metric, Attack Success Rate (ASR), to assess the effectiveness of their approaches. We propose a MEtamorphic Testing for Analyzing LLMs (METAL) framework to address these issues by applying Metamorphic Testing (MT) techniques. This approach facilitates the systematic testing of LLM qualities by defining Metamorphic Relations (MRs), which serve as modularized evaluation metrics. The METAL framework can automatically generate hundreds of MRs from templates that cover various QAs and tasks. In addition, we introduced novel metrics to assess the effectiveness of MRs accurately by integrating the ASR method into the semantic qualities of text. Through the experiments conducted with three prominent LLMs, we have confirmed that the METAL framework effectively evaluates essential QAs on primary LLM tasks and reveals the quality risks in LLMs. Moreover, the newly proposed metrics can guide the optimal MRs for testing each task and suggest the most effective method for generating MRs.","2024-05","2025-11-25 22:37:40","2025-11-25 22:37:40","","117-128","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Semantics; Software testing; Measurement; Metamorphic testing; Systematics; Probabilistic logic; Large-language models; Metals; Perturbation methods; Quality attributes; Text perturbations","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8TFNBH8U","conferencePaper","2025","Kaplan, Hasan","Systematic Testing of Security-Related Vulnerabilities in LLM-Based Applications","2025 IEEE/ACM 4th International Conference on AI Engineering – Software Engineering for AI (CAIN)","","","10.1109/CAIN66642.2025.00043","","Large Language Models (LLMs) have emerged as transformative tools in natural language understanding and generation. They possess billions of parameters, which enable them to generate coherent and contextually rich text [1]. These capabilities have made LLMs vital in domains such as customer service, content creation, and programming assistance [2], [3]. However, these advances come with significant risks. For example, a recent study showed that LLM responses contain private or sensitive information accidentally exposed during training [4], [5]. Furthermore, adversarial attacks have been shown to reduce system accuracy by as much as under controlled conditions [6]. A high-profile example is the misuse of LLMs to generate biased or harmful text when manipulated through adversarial prompts [7].","2025-04","2025-11-25 22:37:40","2025-11-25 22:37:40","","264-266","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Natural language processing; Accuracy; Testing; Programming; Training; Systematics; adversarial attacks; Application software; Customer services; data leakage; devsecops; large language models (llms); prompts; security risks","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9B5AP2BM","conferencePaper","2025","Yoon, Juyeon; Feldt, Robert; Yoo, Shin","Adaptive Testing for LLM-Based Applications: A Diversity-Based Approach","2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","","","10.1109/ICSTW64639.2025.10962467","","The recent surge of building software systems powered by Large Language Models (LLMs) has led to the development of various testing frameworks, primarily focused on treating prompt templates as the unit of testing. Despite the significant costs associated with test input execution and output assessment, the curation of optimized test suites is yet overlooked in these tools, which calls for tailored test selection or prioritization strategies. In this paper, we show that diversity-based testing techniques, such as Adaptive Random Testing (ART) with appropriate string distance metrics, can be effectively applied to the testing of prompt templates. Our proposed adaptive testing approach adjusts the conventional ART process to this context by selecting new test inputs based on scores derived from existing test suite and their labelling results. Our results, obtained using various implementations that explore several string-based distances, confirm that our approach enables the discovery of failures with reduced testing budgets and promotes the generation of more varied outputs.","2025-03","2025-11-25 22:37:40","2025-11-25 22:37:40","","375-382","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Software testing; Manuals; Software systems; Life estimation; Measurement; Surges; Testing; Test Prioritization; Reviews; Adaptive Random Testing; LLM Applications; LLM Testing; Subspace constraints; Test Selection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CZ5458K2","conferencePaper","2025","Xin, Zhidong; Huo, Haitao; Yang, Ka; Xue, Zishan","Application of Large Language Model Text Analysis in the Study of Urban Image","2025 5th International Conference on Artificial Intelligence, Big Data and Algorithms (CAIBDA)","","","10.1109/CAIBDA65784.2025.11183480","","To improve the accuracy of emotion discrimination in massive text analysis and analyze urban international image in detail, so as to provide a reference for urban construction and capital image building. In this paper, more than 80,000 social media comments were collected with the key words “Beijing” and “Peking”. Large language models (LLMs) was used to mine multi-level information of social media comments related to the evaluation of the city of Beijing, introduce in detail the relevant technical methods for large-scale text analysis using the LLMs, and comprehensively evaluate its performance. Our comparison results with the manual review results showed that the LLMs could accurately identify themes and discriminate emotions, which is expected to significantly improve the efficiency of text analysis and help researchers quickly grasp the themes and intentions of massive text information. Large Language Models (LLMs) significantly enhance text analysis efficiency. When combined with modular Python programming and iterative testing, they optimize performance and streamline debugging. Compared to traditional software, LLMs excel in interpreting complex semantics like irony and puns, and they adeptly handle multilingual social media comments. Their profound understanding of internet language and popular expressions further underscores their superiority in modern text analysis. Our text analysis on Beijing's image as an international city showed that Beijing's overall urban image on social platforms was positive and favorable, with higher attention paid to its economic, scientific, political and cultural dimensions, and higher subjective ratings in traditional culture, urban landscape, economic and technological fields, municipal facilities and public services. The international city image of Beijing in social media is dynamic, with multi-level and multi-theme relative independence and evolutionary stability.","2025-06","2025-11-25 22:37:40","2025-11-25 22:37:40","","371-378","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Semantics; Stability analysis; Software; Testing; Large language model; social media; Economics; Beijing city image; Social networking (online); Streaming media; text analysis; Text analysis; Urban areas","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BPLU7HMT","conferencePaper","2024","Rahman, Tajmilur; Zhu, Yuecai; Maha, Lamyea; Roy, Chanchal; Roy, Banani; Schneider, Kevin","Take Loads Off Your Developers: Automated User Story Generation using Large Language Model","2024 IEEE International Conference on Software Maintenance and Evolution (ICSME)","","","10.1109/ICSME58944.2024.00082","","Software Maintenance and Evolution (SME) is moving fast with the assistance of artificial intelligence (AI), especially Large Language Models (LLM). Researchers have already started automating various activities of the SME workflow. Un-derstanding the requirements for maintenance and development work i.e. Requirements Engineering (RE) is a crucial phase that kicks off the SME workflow through multiple discussions on a proposed scope of work documented in different forms. The RE phase ends with a list of user stories for each unit task and usually created and tracked on a project management tool such as GitHub, Jira, AzurDev, etc. In this research, we collaborated with Bell Mobility to develop a tool “Geneus” (Generate UserSory) using GPT-4-turbo to automatically create user stories from software requirements documents. Requirements documents are usually long and contain complex information. Since LLMs typically suffer from hallucination when the input is too complex, this paper proposes a new prompting strategy, “Refine and Thought” (RaT), to mitigate that issue and improve the performance of the LLM in prompts with large and noisy contexts. Along with manual evaluation using RUST (Readability, Understandability, Specificity, Technical-aspects) survey questionnaire, automatic evaluation with BERTScore, and AlignScore evaluation metrics are used to evaluate the results of the “Geneus” tool. Results show that our method with RaT performs consistently better in most of the cases of interactions compared to the single-shot baseline method. However, the BERTScore and AlignScore test results are not consistent. In the median case, Geneus performs significantly better in all three interactions (requirements specifi-cation, user story details, and test case specifications) according to AlignScorebut it shows slightly low performance in requirements specifications according to BERTScore. Distilling RE documents requires significant time & effort from the senior members of the team through multiple meetings with stakeholders. We believe automating this process will certainly reduce additional loads off the software engineers and increase the ultimate productivity allowing them to utilize their time on other prioritized tasks.","2024-10","2025-11-25 22:37:40","2025-11-25 22:37:40","","791-801","","","","","","","","","","","","","","","","","","","","ISSN: 2576-3148","","","","Large language models; LLM; Maintenance; Planning; Testing; Surveys; Productivity; Prompt Engineering; Requirements engineering; Stakeholders; Auto Generate; Project management; Refine and Thought; Software maintenance; Software Maintenance Tasks; User Story","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T3SDJHRW","journalArticle","2024","Karpurapu, Shanthi; Myneni, Sravanthy; Nettur, Unnati; Gajja, Likhit Sagar; Burke, Dave; Stiehm, Tom; Payne, Jeffery","Comprehensive Evaluation and Insights Into the Use of Large Language Models in the Automation of Behavior-Driven Development Acceptance Test Formulation","IEEE Access","","2169-3536","10.1109/ACCESS.2024.3391815","","Behavior-driven development (BDD) is an Agile testing methodology fostering collaboration among developers, QA analysts, and stakeholders. In this manuscript, we propose a novel approach to enhance BDD practices using large language models (LLMs) to automate acceptance test generation. Our study uses zero and few-shot prompts to evaluate LLMs such as GPT-3.5, GPT-4, Llama-2-13B, and PaLM-2. The paper presents a detailed methodology that includes the dataset, prompt techniques, LLMs, and the evaluation process. The results demonstrate that GPT-3.5 and GPT-4 generate error-free BDD acceptance tests with better performance. The few-shot prompt technique highlights its ability to provide higher accuracy by incorporating examples for in-context learning. Furthermore, the study examines syntax errors, validation accuracy, and comparative analysis of LLMs, revealing their effectiveness in enhancing BDD practices. However, our study acknowledges that there are limitations to the proposed approach. We emphasize that this approach can support collaborative BDD processes and create opportunities for future research into automated BDD acceptance test generation using LLMs.","2024","2025-11-25 22:37:40","2025-11-25 22:37:40","","58715-58721","","","12","","","","","","","","","","","","","","","","","","","","","Large language models; software testing; Software testing; Test pattern generators; Syntactics; Automation; generative AI; Testing; prompt engineering; natural language processing; Behavioral sciences; Task analysis; Best practices; Agile software development; automated acceptance testing; Calculators; cucumber; GPT-35 and GPT-4; Llama-13B; PaLM-2; test case automation; zero-shot and few-shot","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9E2XZN2T","conferencePaper","2025","Mitani, Shohei; Moona, Salonee; Matsuo, Shin'ichiro; Burger, Eric","LLM-AQuA-DiVeR: LLM-Assisted Quality Assurance Through Dialogues on Verifiable Specification with Requirement Owners","2025 IEEE/ACM International Workshop on Responsible AI Engineering (RAIE)","","","10.1109/RAIE66699.2025.00008","","Quality Assurance (QA) is important for verifying software compliance with stakeholder requirements. QA faces a fundamental challenge of requirement interpretation ambiguity, which can result in insufficient software verification and failure in achieving the stakeholders' intended quality. The interpre-tation challenge intensifies in software development driven by Large Language Models (LLMs), where over-reliance can lead to missed quality-critical alternatives. However, existing works have paid limited attention to stakeholder involvement. We propose an LLM-assisted QA framework extending conventional LLM-driven development to enable stakeholder engagement in software verification. Our framework employs formal methods and rigorous testing to meet diverse quality demands, though this comprehensive verification introduces technical complexity affecting stakeholder engagement and verification costs. Our framework addresses these challenges through two key LLM roles: 1) an explanation assistant for stakeholder understanding, 2) a refinement assistant for incorporating stakeholder feedback while maintaining feasible verification costs. Our initial evaluation empirically demonstrates the framework's effectiveness through participant assessment scores, showing improved quality risk comprehension and efficient feedback incorporation in the verification process.","2025-04","2025-11-25 22:37:40","2025-11-25 22:37:40","","21-28","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Software quality; Software development management; Faces; Measurement; Testing; Large Language Models; Standards; Quality assurance; Costs; Stakeholders; Formal Method; LLM-Driven Development; Software Quality Assurance","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TJN5EVMC","conferencePaper","2025","Emektar, Miraç; Harmancı, Fatih M.; Güran, Aysun; Karadavut, Berat; Kirmizi, Günay; Öztürk, Berkay; Karamuk, Ata Emir; Güven, Mert","AI-Driven Synthetic Test Data and Scenario Generation via GAN-LLM Integration: A Modular Approach for Web Application Testing","2025 10th International Conference on Computer Science and Engineering (UBMK)","","","10.1109/UBMK67458.2025.11206764","","The growing complexity of web applications has heightened the need for automated and adaptive testing solutions. Manual test case authoring and static data generation often fall short in handling dynamic user interfaces and form-driven workflows. In this paper, we present a modular test automation architecture that integrates Generative Adversarial Networks (GANs) and Large Language Models (LLMs) to enable end-to-end generation and execution of test scenarios. The system utilizes a LLaMA4-based LLM to semantically extract input fields from web forms and guide both data and scenario generation. CTGAN (Conditional Tabular Generative Adversarial Network) is used to synthesize realistic, context-aware tabular test data, while SeqGAN (Sequence Generative Adversarial Network) produces logically ordered user interaction steps. These outputs are executed in a browser using Selenium WebDriver to simulate real-time user behavior. The architecture supports both contextual (URL-based) and non-contextual (prompt-based) modes and allows individual modules to be used independently or as a unified pipeline. The system also includes a graphical user interface to facilitate test configuration and execution. Experimental results across five public web applications show high format compliance in synthetic data (95.05 percent) and strong scenario validity, with an average automation success rate of 84.6 percent. The proposed approach enhances test coverage while significantly reducing manual effort, offering a scalable and privacy-aware solution for modern QA workflows.","2025-09","2025-11-25 22:37:40","2025-11-25 22:37:40","","1217-1222","","","","","","","","","","","","","","","","","","","","ISSN: 2521-1641","","","","Large language models; Manuals; Automation; Scenario generation; Testing; Large Language Models; Test Automation; Scenario-based Testing; Computer architecture; Real-time systems; Synthetic data; Selenium; Generative adversarial networks; Synthetic Data Generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KKILYQCW","conferencePaper","2025","Oliveira, Flávia; Tiago, Leonardo; Nascimento, Alay; Castro, Renata; Chaves, Lennon","Employing Prompt Engineering for Generation of Bug Report: An Experience Report in Industry","2025 International Conference on Computer Technology Applications (ICCTA)","","","10.1109/ICCTA65425.2025.11166286","","The anticipated identification and correction of bugs during software testing processes reduce costs and ensure product quality, preventing these bugs from reaching end users. Consequently, communication between testers and developers regarding existing problems in software occurs through bug reports, which necessitates clear and precise documentation to enable developers to implement corrections effectively. In this context, Large Language Models (LLMs) have been utilized to facilitate bug report composition. This paper presents the results of an empirical study on the application of LLMs to assist in writing bug reports within a real software test team from the industry. To conduct this study, prompts were developed with the objective of employing them to report the most common defects according to the test scope managed by the test team. To evaluate the created prompts, a questionnaire was designed to measure the perception of testers regarding the use of LLMs in generating bug reports. Of the 7 participants from the test team, selected through non-probability convenience sampling, who participated in the study, 71% strongly agreed that utilizing an LLM to report bugs is straightforward, and 86% strongly agreed that it is feasible to use the LLM’s output to create bug reports. Moreover, the participants provided observations regarding the study, such as the occurrence of hallucinations and the necessity to modify the prompt when LLM did not produce the desired response. Thus, this experience report evaluates the implementation of prompts for reporting issues in a test team from the software industry.","2025-05","2025-11-25 22:37:40","2025-11-25 22:37:40","","40-46","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Quality assessment; Software testing; LLM; Software; Industries; Computer bugs; Prompt engineering; Prompt Engineering; Software Test; Browsers; Bug Report; Product design; Servers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HR2QH7XW","journalArticle","2025","Li, Ying; Zhong, Ye; Yang, Lijuan; Wang, Yanbo; Zhu, Penghua","LLM-Guided Crowdsourced Test Report Clustering","IEEE Access","","2169-3536","10.1109/ACCESS.2025.3530960","","This paper proposes a clustering method for crowdsourced test reports based on a large language model to solve the limitations of existing methods in processing repeated reports and utilizing multi-modal information. Existing crowdsourced test report clustering methods have significant shortcomings in handling duplicate reports, ignoring the semantic information of screenshots, and underutilizing the relationship between text and images. The emergence of LLM provides a new way to solve these problems. By integrating the semantic understanding ability of LLM, key information can be extracted from the test report more accurately, and the semantic relationship between screenshots and text descriptions can be used to guide the clustering process, thus improving the accuracy and effectiveness of clustering. The method in this paper uses a pre-trained LLM (such as GPT-4) to encode the text in the test report, and uses a visual model such as CLIP to encode the application screenshots, converting the text descriptions and images into high-dimensional semantic vectors. The cosine similarity is then used to calculate the similarity between the vectors, and semantic binding rules are constructed to guide the clustering process, ensuring that semantically related reports are assigned to the same cluster and semantically different reports are assigned to different clusters. Through experimental verification, this method is significantly superior to traditional methods in several evaluation indicators, demonstrating its great potential in improving the efficiency and quality of crowdsourced test report processing. In the future, this method is expected to be widely used in the process of software testing and maintenance, and further promote technological progress.","2025","2025-11-25 22:37:40","2025-11-25 22:37:40","","24894-24904","","","13","","","","","","","","","","","","","","","","","","","","","Large language models; Semantics; Software; Natural language processing; Testing; Data mining; Large language model; Security; Vectors; Feature extraction; Clustering methods; crowdsourced testing; test report clustering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PUWFSS3H","conferencePaper","2024","Li, Youwei; Li, Yangyang; Yang, Yangzhao","Test-Agent: A Multimodal App Automation Testing Framework Based on the Large Language Model","2024 IEEE 4th International Conference on Digital Twins and Parallel Intelligence (DTPI)","","","10.1109/DTPI61353.2024.10778901","","This paper introduces a multimodal agent-based app automation testing framework, named Test-Agent, built on the Large Language Model (LLM), designed to address the growing challenges in mobile application automation testing. As mobile applications become more prevalent and emerging systems like Harmony OS Next and mini-programs emerge, traditional automated testing methods, which depend on manually crafting test cases and scripts, are no longer sufficient for cross-platform compatibility and complex interaction logic. The Test-Agent framework employs artificial intelligence technologies to analyze application interface screenshots and user natural language instructions. Combined with deep learning models, it automatically generates and executes test actions on mobile devices. This innovative approach eliminates the need for pre-written test scripts or backend system access, relying solely on screenshots and UI structure information. It achieves cross-platform and cross-application universality, significantly reducing the workload of test case writing, enhancing test execution efficiency, and strengthening cross-platform adaptability. Test-Agent offers an innovative and efficient solution for automated testing of mobile applications.","2024-10","2025-11-25 22:37:40","2025-11-25 22:37:40","","609-614","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Automation; Large Language Model; Testing; Adaptation models; Logic; Agent; Deep learning; Mobile applications; Natural languages; Digital twins; App Automation Testing; Mobile handsets","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UGDH5QTN","conferencePaper","2025","Tao, Yingjie; Wang, Weiwei; Guo, Junxia","Intent-driven Web UI Tests Repair with LLM","2025 8th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)","","","10.1109/AEMCSE65292.2025.11042761","","As web applications are frequently updated, changes may introduce to web elements in new versions, causing test cases to fail. Consequently, automatic web test repair techniques are proposed to reduce the cost of regression testing. Most existing methods focus on finding the correct candidate elements or related attributes to fix the broken test case. However, when test case failures are caused by test flow changes or propagated breakages, those methods that focus solely on matching the failing element in the new version cannot work well. Through empirical analysis, we found that the test intent and the reasons that caused the test failure are useful in test case reparation. This paper proposes a novel intent-driven web test repair approach named LetTe, which first parses the test intent and failure reasons of failed web UI tests, and then guides the Large Language Model (LLM) to fix them via prompt design and fine-tuning. LetTe’s repair logic is to simulate that of a human expert. According to the test intent and reason for failure, “think about” the possible repair plan and then generate repair candidates through the corresponding chain-of-thought. We evaluate LetTe on 7 web applications collected from open-source websites as well as publicly available datasets. The experimental results show that our approach has a 75% correct repair rate, which is higher than all baseline methods.","2025-05","2025-11-25 22:37:40","2025-11-25 22:37:40","","306-314","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Manuals; Software engineering; Large Language Model; Testing; Logic; Prompt engineering; Data mining; Costs; Maintenance engineering; Computers; Test Case Repair; Test Intent; Web Application Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C8LWJ9LI","conferencePaper","2024","Yabaku, Mounika; Pombo, Nuno; Ouhbi, Sofia","Exploring the Potential Use of Generative AI in Software Engineering Education","2024 IEEE 18th International Conference on Application of Information and Communication Technologies (AICT)","","","10.1109/AICT61888.2024.10740416","","The integration of Generative AI into software engineering education marks a transformative shift in teaching methodologies. This paper explores its potential, highlighting the benefits of enhancing student engagement, creativity, and efficiency while preparing them for industry challenges. Through a comprehensive analysis of 13 popular generative AI tools, we examine their roles in various software engineering tasks such as requirements analysis, design, coding, debugging, and testing. This paper contributes to the broader discourse on the future of software engineering education by offering evidence-based recommendations for leveraging generative AI to create adaptive and forward-thinking instructional strategies.","2024-09","2025-11-25 22:37:40","2025-11-25 22:37:40","","1-7","","","","","","","","","","","","","","","","","","","","ISSN: 2472-8586","","","","Software; Software engineering; Education; Generative AI; Software development management; Testing; Debugging; Software Engineering Education; Large Language Models (LLMs); Requirements engineering; AIDriven Educational Tools; Pedagogical Innovation; Software measurement; Usability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WJD5WCT2","conferencePaper","2024","Yao, Yi; Wang, Jun; Hu, Yabai; Wang, Lifeng; Zhou, Yi; Chen, Jack; Gai, Xuming; Wang, Zhenming; Liu, Wenjun","BugBlitz-AI: An Intelligent QA Assistant","2024 IEEE 15th International Conference on Software Engineering and Service Science (ICSESS)","","","10.1109/ICSESS62520.2024.10719045","","The evolution of software testing from manual to automated methods has significantly influenced quality assurance (QA) practices. However, challenges persist in post-execution phases, particularly in result analysis and reporting. Traditional post-execution validation phases require manual intervention for result analysis and report generation, leading to inefficiencies and potential development cycle delays. This paper introduces BugBlitz-AI, an AI-powered validation toolkit designed to enhance end-to-end test automation by automating result analysis and bug reporting processes. BugBlitz-AI leverages recent advancements in artificial intelligence to reduce the time-intensive tasks of manual result analysis and report generation, allowing QA teams to focus more on crucial aspects of product quality. By adopting BugBlitz-AI, organizations can advance automated testing practices and integrate AI into QA processes, ensuring higher product quality and faster time-to-market. The paper outlines BugBlitz-AI's architecture, discusses related work, details its quality enhancement strategies, and presents results demonstrating its effectiveness in real-world scenarios.","2024-09","2025-11-25 22:37:50","2025-11-25 22:37:50","","57-63","","","","","","","","","","","","","","","","","","","","ISSN: 2327-0594","","","","Quality assessment; Software testing; Manuals; Software engineering; Software quality; Artificial intelligence; log analysis; large language model (LLM); test automation; software quality assurance; Quality assurance; Product design; Delays; Organizations","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CPWHXTDV","conferencePaper","2025","Even-Mendoza, Karine; Menéndez, Héctor D.; Langdon, W.B; Dakhama, Aidan; Petke, Justyna; Bruce, Bobby R.","Search+LLM-Based Testing for ARM Simulators","2025 IEEE/ACM 47th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)","","","10.1109/ICSE-SEIP66354.2025.00047","","In order to aid quality assurance of large complex hardware architectures, system simulators have been developed. However, such system simulators do not always accurately mirror what would happen on a real device. A significant challenge in testing these simulators arises from the complexity of having to model both the simulation and the infinite number of software that could be run on such a device. Our previous work introduced SearchSYS, a testing frame-work for software simulators. SearchSYS leverages a large language model for initial seed C code generation, which is then compiled, and the resultant binary is fed to a fuzzer. We then use differential testing by running the outputs of fuzzing on real hardware and a system simulator to identify mismatches. We present and discuss our solution to the problem of testing software simulators, using SearchSYS to test the gem5 VLSI digital circuit simulator, employed by ARM to test their systems. In particular, we focus on the simulation of the ARM silicon chip Instruction Set Architecture (ISA). SearchSYS can create test cases that activate bugs by combining LLMs, fuzzing, and differential testing. Using only LLM, SearchSYS identified 74 test cases that activated bugs. By incorporating fuzzing, this number increased by 93 additional bug-activating cases within 24 hours. Through differential testing, we identified 624 bugs with LLM -generated test cases and 126 with fuzzed test inputs. Out of the total number of bug-activating test cases, 4 unique bugs have been reported and acknowledged by developers. Additionally, we provided developers with a test suite and fuzzing statistics, and open-sourced SearchSYS 11See https:iizenodo.orgirecordsI13450472 for initial seeds and SearchSYS code, and https:iizenodo.orgirecordsI14721385 for adjustment to ARMv8 and fuzzed seeds..","2025-04","2025-11-25 22:37:50","2025-11-25 22:37:50","","469-480","","","","","","","","","","","","","","","","","","","","ISSN: 2832-7659","","","","LLM; Software engineering; Codes; Testing; prompt engineering; Computer bugs; Hardware; Fuzz Testing; Fuzzing; Differential Testing; CodeLlama; Language Models; Computer architecture; Search problems; AFL++; CodeBooga; gem5; GPT-3.5-turbo; Llama2; Magicoder; Ollama; Phi2; Reduced instruction set computing; Search-Based Software Testing; SearchSYS; Silicon; Software System Simulation; TinyLlama","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RLELZC9R","conferencePaper","2025","Joshi, Vijay; Band, Iver","Disrupting Test Development with AI Assistants","2025 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)","","","10.1109/IAICT65714.2025.11101520","","Recent advancements in large language models, including GPT-4 and its variants, and Generative AI-assisted coding tools like GitHub Copilot, ChatGPT, and Tabnine, have significantly transformed software development. This paper analyzes how these innovations impact productivity and software test development metrics. These tools enable developers to generate complete software programs with minimal human intervention before deployment. However, thorough review and testing by developers are still crucial. Utilizing the Test Pyramid concept, which categorizes tests into unit, integration, and end-to-end tests, we evaluate three popular AI coding assistants by generating and comparing unit tests for open-source modules. Our findings show that AIgenerated tests are of equivalent quality to original tests, highlighting differences in usage and results among the tools. This research enhances the understanding and capabilities of AI-assistant tools in automated testing.","2025-07","2025-11-25 22:37:50","2025-11-25 22:37:50","","421-425","","","","","","","","","","","","","","","","","","","","ISSN: 2834-8249","","","","Test pattern generators; Software; Chatbots; ChatGPT; Generative AI; Software development management; Testing; Technological innovation; LLMs; Unit Testing; GitHub Copilot; Documentation; Thermal stability; Testing Automation; Organizations; AI-assistant Tools; Standards organizations; Tabnine; Testing Pyramid","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VRKJGUPS","journalArticle","2025","Sarferaz, Siar","Implementing Generative AI Into ERP Software","IEEE Access","","2169-3536","10.1109/ACCESS.2025.3564133","","Enterprise Resource Planning (ERP) systems digitize all business processes within companies in order to enhance automation and optimize efficiency. These solutions integrate data and processes across multiple functions such as sales, marketing, finance, supply chain, manufacturing, services, procurement, and human resources, serving as a central repository of information for numerous organizations. ERP systems typically encompass tens of thousands of business processes and manage data across thousands of tables, creating significant opportunities for the integration of Generative Artificial Intelligence (AI) for increasing process automatization and optimization. Nonetheless, embedding Generative AI into ERP solutions is a complex task due to the intricate nature of these systems, which consist of hundreds of millions of lines of code and cater to a wide array of industry-specific and regional requirements. Consequently, the key research question addressed in this paper is: How to systematically develop and operate Generative AI business applications in ERP systems? This article aims to answer this question by conducting a use case analysis, deriving business requirements, designing and implementing a solution framework, and evaluating its effectiveness through real-world ERP use cases.","2025","2025-11-25 22:37:50","2025-11-25 22:37:50","","73342-73354","","","13","","","","","","","","","","","","","","","","","","","","","Software; generative AI; Generative AI; artificial intelligence; Artificial intelligence; AI; Business; Libraries; Taxonomy; Prevention and mitigation; AI development; AI operations; business AI; business applications; enterprise AI; Enterprise resource planning; ERP; Fraud; Manufacturing; software integration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UGFNVCLH","conferencePaper","2025","Franzosi, Diogo Buarque; Alégroth, Emil; Isaac, Maycel","LLM-Based Labelling of Recorded Automated GUI-Based Test Cases","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST62969.2025.10988984","","Graphical User Interface (GUI) based testing is a commonly used practice in industry. Although valuable and, in many cases, necessary, it is associated with challenges such as high cost and requirements on both technical and domain expertise. Augmented testing, a novel approach to GUI test automation, aims to mitigate these challenges by allowing users to record and render test cases and test data directly on the GUI of the system under test (SUT). In this context, Scout is an augmented testing tool that captures system states and transitions during manual interaction with the SUT, storing them in a test model that is visually represented in the form of state trees and reports. While this representation provides basic overview of a test suite, e.g. about its size and number of scenarios, it is limited in terms of analysis depth, interpretability, and reproducibility. In particular, without human state labeling, it is challenging to produce meaningful and easily understandable test reports. To address this limitation, we present a novel solution and a demonstrator, integrated into Scout, which leverages large language models (LLMs) to enrich the model-based test case representation by automatically labeling and describing states and describing transitions. We conducted two experiments to evaluate the impact of the solution. First, we compared LLM-enhanced reports with expert-generated reports using embedding distance evaluation metrics. Second, we assessed the usability and perceived value of the enhanced reports through an industrial survey. The results of the study indicate that the plugin can improve readability, actionability, and interpretability of test reports. This work contributes to the automation of GUI testing by reducing the need for manual intervention, e.g. labeling, and technical expertise, e.g. to understand test case models. Although the solution is studied in the context of augmented testing, we argue for the solution's generalizability to related test automation techniques. In addition, we argue that this approach enables actionable insights and lays the groundwork for further research into autonomous testing based on Generative AI.","2025-03","2025-11-25 22:37:50","2025-11-25 22:37:50","","453-463","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Software testing; Manuals; Automation; Software development management; Testing; Surveys; Graphical user interfaces; Transforms; Usability; Labeling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QQFN2BAM","conferencePaper","2025","Mani, Nariman; Attaranasl, Salma","Adaptive Test Healing using LLM/GPT and Reinforcement Learning","2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","","","10.1109/ICSTW64639.2025.10962516","","Flaky tests disrupt software development pipelines by producing inconsistent results, undermining reliability and efficiency. This paper introduces a hybrid framework for adaptive test healing, combining Large Language Models (LLMs) like GPT with Reinforcement Learning (RL) to address test flakiness dynamically. LLMs analyze test logs to classify failures and extract contextual insights, while the RL agent learns optimal strategies for test retries, parameter tuning, and environment resets. Experimental results demonstrate the framework's effectiveness in reducing flakiness and improving CI/CD pipeline stability, outperforming traditional approaches. This work paves the way for scalable, intelligent test automation in dynamic development environments.","2025-03","2025-11-25 22:37:50","2025-11-25 22:37:50","","9-16","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Software reliability; Automation; Debugging; Technological innovation; GPT; Large Language Models (LLMs); Tuning; Pipelines; Reinforcement learning; Costs; Translation; Adaptive Test Healing; Continuous Integration (CI); Flaky Tests; Reinforcement Learning (RL); Self-Healing Test Automation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ALAE72G3","conferencePaper","2024","Schwachhofer, Denis; Domanski, Peter; Becker, Steffen; Wagner, Stefan; Sauer, Matthias; Pflüger, Dirk; Polian, Ilia","Large Language Model-Based Optimization for System-Level Test Program Generation","2024 IEEE International Symposium on Defect and Fault Tolerance in VLSI and Nanotechnology Systems (DFT)","","","10.1109/DFT63277.2024.10753556","","System-Level Test (SLT) is essential for testing integrated circuits, focusing on functional and non-functional properties of the Device under Test (DUT). Traditionally, test engineers manually create tests with commercial software to simulate the DUT's end-user environment. This process is both time-consuming and offers limited control over non-functional properties. This paper proposes Large Language Models (LLMs) enhanced by Structural Chain of Thought (SCoT) prompting, a temperature schedule, and a pool of previously generated snippets to generate high-quality code snippets for SLT. We repeatedly query the LLM for a better snippet using previously generated snippets as examples, thus creating an iterative optimization loop. This approach can automatically generate snippets for SLT that target specific non-functional properties, reducing time and effort. Our findings show that this approach improves the quality of the generated snippets compared to unstructured prompts containing only a task description.","2024-10","2025-11-25 22:37:50","2025-11-25 22:37:50","","1-6","","","","","","","","","","","","","","","","","","","","ISSN: 2765-933X","","","","Optimization; Software; Codes; Measurement; Testing; Large Language Models; Test Generation; Process control; Functional Test; Nanotechnology; Schedules; System-Level Test; Temperature; Very large scale integration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"45VIB4HS","conferencePaper","2025","Su, Zemin; Bai, Cuiqin; Wei, Shaowen; Liu, Kangyong; Liu, Yang; Huan, Zhenxuan; Wang, Ke; Wang, Xiaoyu","Self-Healing UI Test Automation via Multi-Modal Fusion","2025 5th International Conference on Artificial Intelligence and Industrial Technology Applications (AIITA)","","","10.1109/AIITA65135.2025.11047603","","Traditional UI automated testing relies heavily on application implementation details (such as XPath and element ID), resulting in high script maintenance costs and poor robustness. This paper proposes a multimodal self-healing framework, which combines the semantic reasoning ability of Large Language Model (LLM), computer vision positioning technology to achieve dynamic repair of test scripts. Experiments show that the success rate of this method in GUI change scenarios is 89%, and the maintenance cost is reduced by 67%. The industrial practicability of this method is verified through the business case of China Mobile.","2025-03","2025-11-25 22:37:50","2025-11-25 22:37:50","","1421-1424","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Semantics; LLM; Automation; Maintenance; Testing; Business; UI automation test; Robustness; Costs; Graphical user interfaces; Maintenance engineering; Multimodal semantics; Self-healing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QY53B5UZ","conferencePaper","2024","Cao, Zhiyuan; Ma, Zeyu; Chen, Mingang","An Evaluation System for Large Language Models based on Open-Ended Questions","2024 IEEE 11th International Conference on Cyber Security and Cloud Computing (CSCloud)","","","10.1109/CSCloud62866.2024.00019","","We designed a large language model evaluation system based on open-ended questions. The system accomplished multidimensional evaluation of LLMs using open-ended questions, and it presented evaluation results with evaluation reports. Currently, the evaluation of large-scale language models often exists with two prominent limitations: (1) The evaluation methods are often single-minded, resulting in less credible results. (2) Most evaluations are based on datasets with closed-ended questions, treating generative large language models as discriminative models, which fails to adequately reflect the high output flexibility characteristic of these models. For these two limitations, we proposed an evaluation system for LLMs based on open-ended questions. Our experiments on the adapted open-source datasets demonstrated the effectiveness of this system. The code of the system was released on https://github.com/JerryMazeyu/GreatLibrarian.","2024-06","2025-11-25 22:37:50","2025-11-25 22:37:50","","65-72","","","","","","","","","","","","","","","","","","","","ISSN: 2693-8928","","","","Large language models; LLM; Natural Language Processing; Accuracy; Codes; Adaptation models; Artificial Intelligence; LLM Evaluation; Cloud computing; Open-Ended Questions; Reflection; System performance; Text Generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HRVIDIN9","conferencePaper","2024","Zilberman, Sol; Betty Cheng, H. C.","“No Free Lunch” when using Large Language Models to Verify Self-Generated Programs","2024 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","","","10.1109/ICSTW60967.2024.00018","","Large Language Models (LLMs) have shown great success in a wide range of text-generation tasks including the synthesis of code from natural language descriptions. As LLMbased techniques continue to grow in popularity, especially amongst entry-level developers, LLM-generated code has the potential to be deployed in a diverse set of application domains. While LLMs can generate syntactically correct code output, recent work has shown the presence of nonsensical and faulty reasoning in LLM-generated text. As such, overreliance on LLMs for software generation may potentially result in the deployment of faulty software leading to critical system failures. This study explores the capabilities of a single LLM to generate both software and corresponding test suites from the same initial program descriptions, which can be considered analogous to an individual developer coding and unit testing for a given piece of software. We present an empirical framework and evaluation methodology to assess the usefulness of LLM-generated test cases for verifying programs generated by the same LLM. Our findings indicate that LLMs frequently generate irrelevant tests that suffer from numerous quality concerns.","2024-05","2025-11-25 22:37:50","2025-11-25 22:37:50","","29-36","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Software testing; Automation; Codes; Conferences; software engineering; automated software testing; deep learning; Reviews; Natural languages","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JMXJGEGE","conferencePaper","2025","More, Riddhi; Bradbury, Jeremy S.","An Analysis of LLM Fine-Tuning and Few-Shot Learning for Flaky Test Detection and Classification","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST62969.2025.10988995","","Flaky tests exhibit non-deterministic behavior during execution and they may pass or fail without any changes to the program under test. Detecting and classifying these flaky tests is crucial for maintaining the robustness of automated test suites and ensuring the overall reliability and confidence in the testing. However, flaky test detection and classification is challenging due to the variability in test behavior, which can depend on environmental conditions and subtle code interactions. Large Language Models (LLMs) offer promising approaches to address this challenge, with fine-tuning and few-shot learning (FSL) emerging as viable techniques. With enough data fine-tuning a pre-trained LLM can achieve high accuracy, making it suitable for organizations with more resources. Alternatively, we introduce FlakyXbert, an FSL approach that employs a Siamese network architecture to train efficiently with limited data. To understand the performance and cost differences between these two methods, we compare fine-tuning on larger datasets with FSL in scenarios restricted by smaller datasets. Our evaluation involves two existing flaky test datasets, FlakyCat and IDoFT. Our results suggest that while fine-tuning can achieve high accuracy, FSL provides a cost-effective approach with competitive accuracy, which is especially beneficial for organizations or projects with limited historical data available for training. These findings underscore the viability of both fine-tuning and FSL in flaky test detection and classification with each suited to different organizational needs and resource availability.","2025-03","2025-11-25 22:37:50","2025-11-25 22:37:50","","349-359","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Software testing; Accuracy; Codes; large language models; fine-tuning; Robustness; flaky tests; few-shot learning; Training; Costs; Few shot learning; Organizations; Network architecture; test classification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YCAJD7TU","conferencePaper","2024","Simaremare, Mario; Edison, Henry","The State of Generative AI Adoption from Software Practitioners' Perspective: An Empirical Study","2024 50th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)","","","10.1109/SEAA64295.2024.00024","","Context: Generative AI (GenAI) brings new op-portunities to the software industry and the digital economy in a broader context. Objective: This study aimed to explore and capture the practitioners' perception of GenAI adoption in the fast-paced software industry in the context of developing countries. Method: We conducted online focus group discussions with 18 practitioners from various roles to collect qualitative data. The practitioners have an average of 7.8 years of working experience and have used GenAI for over a year. We employed thematic analysis and the Human-AI Collaboration and Adaptation Framework (HACAF) to identify the influencing factors of GenAI adoption, such as awareness, use cases, and challenges. Results: The adoption of GenAI technology is evident from practitioners. We identified 22 practical use cases, three of which were novel, i.e., contextualizing solutions, assisting the internal audit process, and benchmarking the internal software development process. We also discovered seven key challenges associated with the GenAI adoption, two of which were novel, namely, no matching use cases and unforeseen benefits. These challenges slow GenAI adoption and potentially hinder developing countries from entering a high-skill industry. Conclusion: While the adoption of GenAI technology is promising, industry-academia collaboration is needed to find solutions and strategies to address the challenges and maximize its potential benefits.","2024-08","2025-11-25 22:37:50","2025-11-25 22:37:50","","106-113","","","","","","","","","","","","","","","","","","","","ISSN: 2376-9521","","","","Software; Software engineering; Industries; Generative AI; Software development management; Benchmark testing; Business; Collaboration; Developing countries; developing country; software industry; software practitioner; Trajectory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XD4ZG55S","conferencePaper","2025","Bhatt, Nigam","Automated Unit Test Generation via Metaprogramming: A Deterministic Alternative to AI-Based Test Synthesis","2025 International Conference on Information, Implementation, and Innovation in Technology (I2ITCON)","","","10.1109/I2ITCON65200.2025.11208854","","Automated test generation plays a crucial role in modern software development, with AI-driven methods and search-based heuristics being widely explored. However, these approaches often suffer from incomplete test coverage, nondeterministic behavior, and high computational costs. This paper introduces a novel metaprogramming-based technique for automatically generating unit tests from integration tests in Python. By leveraging function metadata at runtime, the approach extracts meaningful test cases without requiring manual test design or AI-driven synthesis. Unlike generative AI, which relies on probabilistic models, the method ensures deterministic and reproducible test generation. Experimental results demonstrate that the approach maintains high code coverage while significantly reducing the overhead associated with traditional test generation techniques. This work presents a practical and efficient alternative for developers seeking reliable automated testing without the uncertainty of AI-based solutions.","2025-07","2025-11-25 22:37:51","2025-11-25 22:37:51","","1-7","","","","","","","","","","","","","","","","","","","","","","","","Test pattern generators; Metadata; Software development management; Codes; Testing; Technological innovation; Automated Test Generation; Reliability; Python; Uncertainty; Runtime; Metaprogramming","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"93QUVNFN","conferencePaper","2024","Pangavhane, Shreyas; Raktate, Gokul; Pariane, Prasad; Shelar, Krishna; Wakchaure, Rohit; Kale, J.N.","AI-Augmented Software Development: Boosting Efficiency and Quality","2024 International Conference on Decision Aid Sciences and Applications (DASA)","","","10.1109/DASA63652.2024.10836523","","Software developer's approaches to coding, testing, and deployment are changing due to the incorporation of Artificial Intelligence (AI) into software development. AI-Augmented Software Development Tools simplify repetitive activities, boost productivity, and lessen the cognitive load on developers by utilizing machine learning, natural language processing (NLP), and other AI techniques. These tools support several development phases, including DevOps optimization, testing automation, problem discovery, and code generation. For example, AI-driven systems find flaws and inefficiencies in the code, while AI-powered code generation tools like GitHub Copilot help with code suggestion and completion. Additionally, AI automates debugging and test case creation, ensuring the scalability and dependability of software systems. Artificial intelligence (AI) improves quality assurance in complex contexts such as microservices and APIs by maximizing test coverage and identifying abnormalities. Additionally, AI-powered assistants help guide engineers through code inspections, security checks, and performance optimization. Notwithstanding these advantages, there are drawbacks to using AI tools, such as privacy issues and an excessive reliance on automation. With continued development, these tools have the potential to not only expedite development but also redirect the developer's attention towards more complex problem resolution, thereby transforming software engineering methodologies. [7]","2024-12","2025-11-25 22:37:51","2025-11-25 22:37:51","","1-5","","","","","","","","","","","","","","","","","","","","","","","","Optimization; Software engineering; Automation; Machine Learning; Natural language processing; Software systems; Generative AI; Software development management; Codes; Artificial intelligence; Software Testing; Productivity; Software Development; Artificial Intelligence (AI); APIs; Quality Assurance; Encoding","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MVNAXJZE","conferencePaper","2024","Xu, Jiahe; Xu, Jingwei; Chen, Taolue; Ma, Xiaoxing","Symbolic Execution with Test Cases Generated by Large Language Models","2024 IEEE 24th International Conference on Software Quality, Reliability and Security (QRS)","","","10.1109/QRS62785.2024.00031","","Symbolic execution is a powerful program analysis technique. External environment construction and internal path explosion are two long-standing problems which may affect the effectiveness and performance of symbolic execution on complex programs. The intrinsic challenge is to achieve a sufficient understanding of the program context to construct a set of execution environments which can guide the selection of symbolic states. In this paper, we propose a novel program-context-guided symbolic execution framework LangSym based on program’s instruction/user manual. Leveraging the capabilities of natural language understanding and code generation in large language models (LLMs), LangSym can automatically extract the knowledge related to the functionality of the program, and generate adequate test cases and the corresponding environments as the prior knowledge for symbolic execution. We instantiate LangSym in KLEE, a widely adopted symbolic execution engine, to build a pipeline that could automatically leverage LLMs to boost the symbolic execution. We evaluate LangSym on almost all GNU Coreutils programs and considerable large-scale programs, showing that LangSym outperforms the existing strategies in KLEE with at least a 10% increase for line coverage.","2024-07","2025-11-25 22:37:51","2025-11-25 22:37:51","","228-237","","","","","","","","","","","","","","","","","","","","ISSN: 2693-9177","","","","large language model; Large language models; software testing; Manuals; Software reliability; Software quality; Codes; symbolic execution; Pipelines; Reliability engineering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X53HTSYR","conferencePaper","2025","Bellur, Abhiram; Batole, Fraol","IDE Native, Foundation Model Based Agents for Software Refactoring","2025 IEEE/ACM Second IDE Workshop (IDE)","","","10.1109/IDE66625.2025.00013","","Foundation Models (FMs) have been trained on a massive amount of coding data and, at their best, are capable of looking at code like an expert software developer. This has led researchers to explore the possibility of FM-based agents for various software engineering tasks. These agents are capable of planning and executing complex tasks, such as bug repair, in the manner that an expert developer who is familiar with the codebase would. However, FMs often produce puzzling responses that are capable of resulting in buggy or vulnerable code. Inspired by recent work in agents for software engineering tasks, we discuss the idea of a FM-based refactoring agent, which is capable of scanning the entire codebase to suggest changes that improve the quality of the software system. Additionally, we posit that the IDEs (equipped with a massive number of static-analysis based checks), are the ideal place for these agents to live. In this paper, we discuss the challenges and issues related to building FM-based refactoring agents that live within the IDE.","2025-05","2025-11-25 22:37:51","2025-11-25 22:37:51","","42-45","","","","","","","","","","","","","","","","","","","","","","","","Software engineering; Software systems; Planning; Codes; Productivity; Encoding; Foundation models; Maintenance engineering; artificial-intelligence ide ai-agent refactoring software-engineering; Frequency modulation; Shape","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ASN2J8DW","conferencePaper","2023","Hu, Rui; Zhong, Junhao; Ding, Minjie; Ma, Zeyu; Chen, Mingang","Evaluation of Hallucination and Robustness for Large Language Models","2023 IEEE 23rd International Conference on Software Quality, Reliability, and Security Companion (QRS-C)","","","10.1109/QRS-C60940.2023.00089","","As large language models (LLMs) rapidly advance, rigorous testing and evaluation of these models grows increasingly crucial. To address this need, we have developed three types of questions: Chinese contextual, English contextual, and language context-independent. Testing in both Chinese and English probes the LLMs' hallucination tendencies. We investigate the impact of language on hallucinations from two perspectives: the type of language used in the input prompt and the cultural context underlying the prompt's content. Additionally, 52 multi-domain single-choice questions from C-EVAL are presented in original and randomized order to assess robustness to perturbations. Among the five LLMs, the tests demonstrate GPT -4 has the strongest anti-hallucination and robustness capabilities, answering with greater accuracy, consistency, and reliability. ChatGLM ranks second and outperforms GPT -4 on Chinese context-dependent questions. Emergent testing phenomena are analyzed from the user's perspective. Hallucinated responses are categorized and potential causal factors leading to hallucination and fragility are examined. Based on these findings, viable avenues for improvement are proposed.","2023-10","2025-11-25 22:37:51","2025-11-25 22:37:51","","374-382","","","","","","","","","","","","","","","","","","","","ISSN: 2693-9371","","","","large language model; Software reliability; Software quality; Testing; hallucination; Security; evaluation; Robustness; Perturbation methods; Probes; robustness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G3MZCEWB","conferencePaper","2025","Ahmed, Bestoun S.; Baader, Ludwig Otto; Bayram, Firas; Jagstedt, Siri; Magnusson, Peter","Quality Assurance for LLM-RAG Systems: Empirical Insights from Tourism Application Testing","2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","","","10.1109/ICSTW64639.2025.10962487","","This paper presents a comprehensive framework for testing and evaluating quality characteristics of Large Language Model (LLM) systems enhanced with Retrieval-Augmented Generation (RAG) in tourism applications. Through systematic empirical evaluation of three different LLM variants across multiple parameter configurations, we demonstrate the effectiveness of our testing methodology in assessing both functional correctness and extra-functional properties. Our framework implements 17 distinct metrics that encompass syntactic analysis, semantic evaluation, and behavioral evaluation through LLM judges. The study reveals significant information about how different architectural choices and parameter configurations affect system performance, particularly highlighting the impact of temperature and top-p parameters on response quality. The tests were carried out on a tourism recommendation system for the Varmland region in Sweden, utilizing standard and RAG-enhanced configurations. The results indicate that the newer LLM versions show modest improvements in performance metrics, though the differences are more pronounced in response length and complexity rather than in semantic quality. The research contributes practical insights for implementing robust testing practices in LLM-RAG systems, providing valuable guidance to organizations deploying these architectures in production environments.","2025-03","2025-11-25 22:37:51","2025-11-25 22:37:51","","200-207","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Semantics; Syntactics; System testing; Large Language Models (LLM); Quality assurance; Systematics; Retrieval augmented generation; Standards organizations; Temperature; System performance; AI Quality Assurance; Extra-Functional Properties; ML System Testing; Retrieval-Augmented Generation (RAG); Software Quality Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BPKTKH42","conferencePaper","2025","Lee, Hyeonseok; An, Gabin; Yoo, Shin","Metamon: Finding Inconsistencies between Program Documentation and Behavior using Metamorphic LLM Queries","2025 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code)","","","10.1109/LLM4Code66737.2025.00020","","Code documentation can, if written precisely, help developers better understand the code they accompany. However, unlike code, code documentation cannot be automatically verified via execution, potentially leading to inconsistencies between documentation and the actual behavior. While such inconsistencies can be harmful for the developer’s understanding of the code, checking and finding them remains a costly task due to the involvement of human engineers. This paper proposes Metamon, which uses an existing search-based test generation technique to capture the current program behavior in the form of test cases, and subsequently uses LLM-based code reasoning to identify the generated regression test oracles that are not consistent with the program specifications in the documentation. Metamon is supported in this task by metamorphic testing and self-consistency. An empirical evaluation against 9,482 pairs of code documentation and code snippets, generated using five open-source projects from Defects4J v2.0.1, shows that Metamon can classify the code-and-documentation inconsistencies with a precision of 0.72 and a recall of 0.48.","2025-05","2025-11-25 22:37:51","2025-11-25 22:37:51","","120-127","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Test pattern generators; Codes; Large Language Model; Testing; Location awareness; Conferences; Cognition; Metamorphic Testing; Documentation; Maintenance engineering; Code Documentation; Oracle Problem; Regression Test","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"99R46DRV","conferencePaper","2024","Koziolek, Heiko; Ashiwal, Virendra; Bandyopadhyay, Soumyadip; R, Chandrika K","Automated Control Logic Test Case Generation using Large Language Models","2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)","","","10.1109/ETFA61755.2024.10711016","","Testing PLC and DCS control logic in industrial automation is laborious and challenging since appropriate test cases are often complex and difficult to formulate. Researchers have previously proposed several automated test case generation approaches for PLC software applying symbolic execution and search-based techniques. Often requiring formal specifications and performing a mechanical analysis of programs, these approaches may uncover specific programming errors but some-times suffer from state space explosion and cannot process rather informal specifications. We proposed a novel approach for the automatic generation of PLC test cases that queries a Large Language Model (LLM) to synthesize test cases for code provided in a prompt. Experiments with ten open-source function blocks from the OSCAT automation library showed that the approach is fast, easy to use, and can yield test cases with high statement coverage for low-to-medium complex programs. However, we also found that LLM-generated test cases suffer from erroneous assertions in many cases, which still require manual adaption.","2024-09","2025-11-25 22:37:51","2025-11-25 22:37:51","","1-8","","","","","","","","","","","","","","","","","","","","ISSN: 1946-0759","","","","Large language models; Manuals; Software; Generative AI; Testing; Large Language Models; Logic; Programming; Benchmark; Libraries; Automation engineering; Control engineering; Control Logic Generation; Formal specifications; IEC 61131–3; Logic testing; Manufacturing automation; Structured Text","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UIS8PAG4","conferencePaper","2024","Traykov, Kiril","A Framework for Security Testing of Large Language Models","2024 IEEE 12th International Conference on Intelligent Systems (IS)","","","10.1109/IS61756.2024.10705238","","The purpose of this paper is to present a framework for testing of large language models (LLMs) for security vulnerabilities before their implementation to production environment. The paper discusses the latest developments in the Artificial Intelligence (AI) and Generative Artificial Intelligence (Generative AI) adoption in the industry, the expectations for further accelerated adoption and evolving regulatory landscape. An overview of the most significant risks and vulnerabilities of the LLMs such as prompt injection and denial of service have been presented with their mitigation strategies. A testing approach and testing framework have been developed and implemented with simple chatbot app. The test scenarios have been executed and results have been obtained for three open-source LLMs from which two pass the test and one failed and demonstrated the application of the proposed testing framework. Source code of the application and test script are published open source for reproducibility and reuse. In conclusion the with the confirmation of the results the limitation of the reliance on semantic similarity for the responses of the models was discussed together with three areas for further development: expanding the test scenarios to significant risks, integration with popular cloud continuous development platforms and integrating blockchain for transparent publication of the final test results.","2024-08","2025-11-25 22:38:05","2025-11-25 22:38:05","","1-7","","","","","","","","","","","","","","","","","","","","ISSN: 2767-9802","","","","Large language models; Semantics; Industries; Generative AI; Testing; Source coding; Software Testing; Blockchains; Cybersecurity; Large language models (LLMs); Prevention and mitigation; Computer crime; LLM Risks; LLM Security; LLM Vulnerabilities; Reproducibility of results; Secure Software Development","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6F9IMYAB","conferencePaper","2025","Xiang, Yiming; Yang, Zhenning; Peng, Jingjia; Bauer, Hermann; Kon, Patrick Tser Jern; Qiu, Yiming; Chen, Ang","Automated Bug Discovery in Cloud Infrastructure-as-Code Updates with LLM Agents","2025 IEEE/ACM International Workshop on Cloud Intelligence & AIOps (AIOps)","","","10.1109/AIOps66738.2025.00011","","Cloud environments are increasingly managed by Infrastructure-as-Code (IaC) platforms (e.g., Terraform), which allow developers to define their desired infrastructure as a configuration program that describes cloud resources and their dependencies. This shields developers from low-level operations for creating and maintaining resources, since they are automatically performed by IaC platforms when compiling and deploying the configuration. However, while IaC platforms are rigorously tested for initial deployments, they exhibit myriad errors for runtime updates, e.g., adding/removing resources and dependencies. IaC updates are common because cloud infrastructures are long-lived but user requirements fluctuate over time. Unfortunately, our experience shows that updates often introduce subtle yet impactful bugs. The update logic in IaC frameworks is hard to test due to the vast and evolving search space, which includes diverse infrastructure setups and a wide range of provided resources with new ones frequently added. We introduce TerraFault, an automated, efficient, LLM-guided system for discovering update bugs, and report our findings with an initial prototype. TerraFault incorporates various optimizations to navigate the large search space efficiently and employs techniques to accelerate the testing process. Our prototype has successfully identified bugs even in simple IaC updates, showing early promise in systematically identifying update bugs in today's IaC frameworks to increase their reliability.","2025-05","2025-11-25 22:38:05","2025-11-25 22:38:05","","20-25","","","","","","","","","","","","","","","","","","","","","","","","Optimization; Software testing; Software reliability; Life estimation; Prototypes; Computer bugs; Logic; Debugging; Software testing and debugging; Reliability; Navigation; Runtime; Infrastructure-as-Code; Program update; Using LLMs for Cloud Ops","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WP8IVDPT","conferencePaper","2024","Randolph, Claire; Michaleas, Adam; Ricke, Darrell O.","LLMs for Closed-Library Multi-Document Query, Test Generation, and Evaluation","2024 IEEE High Performance Extreme Computing Conference (HPEC)","","","10.1109/HPEC62836.2024.10938420","","Learning complex, detailed, and evolving knowledge is a challenge in multiple technical professions. Relevant source knowledge is contained within many large documents and information sources with frequent updates to these documents. Knowledge tests need to be generated on new material and existing tests revised, tracking knowledge base updates. Large Language Models (LLMs) provide a framework for artificial intelligence-assisted knowledge acquisition and continued learning. Retrieval-Augmented Generation (RAG) provides a framework to leverage available, trained LLMs combined with technical area-specific knowledge bases. Herein, two methods are introduced, which together enable effective implementation of LLM-RAG question-answering on large documents. Additionally, the AI tools for knowledge intensive tasks (AIKIT) solution is presented for working with numerous documents for training and continuing education. AIKIT is provided as a containerized open source solution that deploys on standalone, high performance, and cloud systems. AIKIT includes LLM, RAG, vector stores, relational database with a Ruby on Rails web interface.","2024-09","2025-11-25 22:38:05","2025-11-25 22:38:05","","1-10","","","","","","","","","","","","","","","","","","","","ISSN: 2643-1971","","","","Large language models; Test pattern generators; Large Language Model (LLM); Artificial Intelligence (AI); Training; Retrieval augmented generation; Vectors; Knowledge based systems; Retrieval-Augmented Generation (RAG); Dictionaries; Document as a Dictionary (DaaDy); Relational databases; Structured Question Answer Dictionary (SQuAD); System-on-chip; Workstations","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZNTFD44Q","conferencePaper","2025","Liu, Pan; Chen, Zizhao; Li, Yihao; Wong, W. Eric","Evaluating Large Language Models Via Multi-Modal User Knowledge Graphs: A Comprehensive Assessment Framework","2025 11th International Symposium on System Security, Safety, and Reliability (ISSSR)","","","10.1109/ISSSR65654.2025.00047","","Large language models (LLMs) have been widely adopted across various industries, but issues such as hallucinations, biases, and erroneous outputs frequently arise, compromising their reliability and safety. Objectively assessing how well an LLM interprets user queries is crucial for selecting the right model for practical problem-solving. This paper proposes an evaluation method that leverages user knowledge graphs to measure an LLM's comprehension of user input. Specifically, we construct both text-based and graphical test cases derived from a user knowledge graph, thereby enabling multi-modal assessment of the LLM's understanding. To implement our approach, a knowledge graph is first built from the user's input and then mutated to produce diverse test cases. A search-based testing method is then applied to evaluate the model's comprehension. We provide a case study demonstrating the framework. Our findings indicate that multi-modal test cases outperform purely text-based test cases in revealing the true understanding capability of LLMs. Among the eight models tested, DeepSeek and Doubao exhibit stronger comprehension than the remaining six.","2025-04","2025-11-25 22:38:05","2025-11-25 22:38:05","","278-285","","","","","","","","","","","","","","","","","","","","ISSN: 2835-2823","","","","Large language models; LLM; Industries; Testing; Visualization; Cognition; Security; Robustness; Safety; Complexity theory; Knowledge graphs; Knowledge graph mutation; Multi-modal test case; Understanding ability evaluation; User knowledge graph","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S68T5LV2","conferencePaper","2025","Oțelea, Ionuț-Gabriel; Pintea, Bogdan; Rughiniș, Răzvan Victor; Tîrșu, Valentina","Evaluating LLMs for Automated Requirement and Test Case Generation in Railway Signaling Systems","2025 24th RoEduNet Conference: Networking in Education and Research (RoEduNet)","","","10.1109/RoEduNet68395.2025.11208370","","Large Language Models (LLMs) have shown potential in supporting requirements engineering through automation, especially in regulated and safety-critical domains. This paper evaluates the capabilities of 3 well-known LLMs (GPT-4, Claude, Gemini) in transforming user requirements into structured product requirements and corresponding test cases within the context of railway signaling. A custom dataset of client requirements, inspired by realistic signaling scenarios, was developed to enable consistent evaluation across models. Each model's outputs were assessed using defined metrics, including completeness, correctness, consistency, and traceability. The comparative results highlight variations in quality and structure of the generated artifacts, with specific strengths observed for different tasks. While all three models demonstrate promise, their reliability and consistency vary, and human oversight remains essential. This study provides practical insights into the applicability of current LLMs for augmenting early-stage requirements and verification workflows in critical systems engineering.","2025-09","2025-11-25 22:38:05","2025-11-25 22:38:05","","1-6","","","","","","","","","","","","","","","","","","","","ISSN: 2247-5443","","","","Solid modeling; LLM; Prompt engineering; Requirements Engineering; Reliability; NLP; Security; Safety; Costs; Requirements engineering; Automated Test Case Generation; Communication system signaling; Modeling; Rail transportation; Railway Signaling Systems; Requirements Transformation; Safety-Critical Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IR9T4TCQ","conferencePaper","2024","Duan, Zhihua; Wang, Jialin","AutoTest: Evolutionary Code Solution Selection with Test Cases","2024 IEEE 11th International Conference on Cyber Security and Cloud Computing (CSCloud)","","","10.1109/CSCloud62866.2024.00030","","With the development of code generation techniques, selecting the correct code solution from multiple candidate solutions has become a crucial task. This study proposes AutoTest, a novel technique that combines automated test case generation with code solution execution to optimize the selection process using an evolutionary genetic algorithm. Firstly, AutoTest utilizes large pre-trained language models such as codegen-16B, code-davinci-002, and incoder-6B to provide code solutions and their corresponding test cases. Then, by executing the code solutions and evaluating their performance on the test cases, a consensus set is formed. Fine-grained ranking is achieved through the selection, mutation, and crossover mechanisms based on the evolutionary genetic algorithm, with the adjustment of alpha and beta parameters. Finally, the best code solution is chosen. AutoTest demonstrates significant performance improvements on the HumanEval benchmark test. The HumanEval dataset consists of 164 programming problems, and AutoTest achieves approximately a 10% improvement over the baseline method in terms of pass@1 score.","2024-06","2025-11-25 22:38:05","2025-11-25 22:38:05","","132-134","","","","","","","","","","","","","","","","","","","","ISSN: 2693-8928","","","","Large language models; Codes; Large Language Model; Benchmark testing; Codex; Programming; Computational modeling; Computer languages; Cloud computing; CodeGen; HumanEval; InCoder","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6QPMEJG7","conferencePaper","2025","Korn, Alexander","Smells Like Trouble: Investigating the Impact of Requirements Quality on LLM-Supported Software Engineering","2025 IEEE 33rd International Requirements Engineering Conference (RE)","","","10.1109/RE63999.2025.00075","","Large Language Models (LLMs) are increasingly integrated into software engineering (SE) workflows, supporting tasks such as code generation, test case derivation, and requirements-to-code traceability. These tasks heavily rely on natural language requirements, making the quality of those requirements a critical factor in LLM performance. Previous research suggests that requirements smells, i.e. indicators of potential quality issues, can negatively affect the accuracy and reproducibility of LLM-generated outputs. However, the extent and nature of this impact remain largely unexplored. The dissertation aims to investigate how requirements smells influence LLM effectiveness across various SE tasks and to explore automated techniques for detecting and mitigating issues indicated by such smells. Initial results show that increasing the number of smells in requirements significantly degrades LLM performance in traceability tasks. The dissertation aims to establish empirical foundations for improving prompt quality and to support more reliable use of LLMs in SE through automated quality assurance mechanisms.","2025-09","2025-11-25 22:38:06","2025-11-25 22:38:06","","603-606","","","","","","","","","","","","","","","","","","","","ISSN: 2332-6441","","","","Large language models; LLM; Software engineering; Software reliability; Accuracy; Codes; software engineering; requirements; Quality assurance; Natural languages; Requirements engineering; Reproducibility of results; requirements smells","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H5YDTPBS","conferencePaper","2025","Min, Ziran; Budnik, Christof J.","Verification and Validation of LLM-RAG for Industrial Automation","2025 IEEE International Conference on Artificial Intelligence Testing (AITest)","","","10.1109/AITest66680.2025.00012","","Large Language Models (LLMs) have emerged as transformative tools in industrial automation, supporting decision-making from device-level control to enterprise-level coordination. When combined with Retrieval-Augmented Generation (RAG), these systems promise context-aware, domain-specific reasoning. However, their integration into mission-critical pipelines introduces considerable challenges to reliability, robustness, and continuous validation, particularly within Development and Operations (DevOps) and Continuous Integration/Continuous Delivery (CI/CD) environments. This work presents a comprehensive Verification and Validation (V&V) framework designed to address these challenges holistically across the AI system lifecycle. At its core, the framework introduces a novel Continuum-Based Failure Classification (CBFC) model that redefines validation from a binary pass/fail paradigm to a graded assessment of correctness, consistency, and uncertainty. This assessment spans key dimensions such as retrieval relevance, factual accuracy, coherence, and alignment with user intent. The CBFC model integrates embedding-based similarity measures, logical entailment checks, and quality-oriented metrics (e.g., FactScore, ROUGE) to uncover failure modes often overlooked by conventional evaluation methods. The framework emphasizes reliability and adaptability through structured testing, latency optimization, proactive model retraining, and iterative feedback loops. It is demonstrated in an industrial Root-Cause Analyzer application, where it significantly improves system performance, interpretability, and trustworthiness. By advancing toward continuous, evidence-based evaluation, this approach enables the resilient deployment of retrieval-augmented large language model systems in dynamic, real-world industrial environments.","2025-07","2025-11-25 22:38:06","2025-11-25 22:38:06","","50-53","","","","","","","","","","","","","","","","","","","","ISSN: 2835-3560","","","","Large language models; Automation; Testing; Adaptation models; Large Language Models (LLMs); CI/CD; DevOps; Robustness; Pipelines; Uncertainty; Retrieval augmented generation; System performance; Retrieval-Augmented Generation (RAG); Industrial Automation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6RXDBRAA","conferencePaper","2025","Donvir, Anujkumarsinh; Sharma, Gaurav","Ethical Challenges and Frameworks in AI-Driven Software Development and Testing","2025 IEEE 15th Annual Computing and Communication Workshop and Conference (CCWC)","","","10.1109/CCWC62904.2025.10903892","","Artificial Intelligence (AI) has revolutionized and transformed the landscape of software development and testing by introducing new efficiencies and capabilities through advancements like Generative AI (GenAI) and Large Language Models (LLMs). While these technologies bring major benefits in terms of productivity, personalization, and innovation, they also raise critical ethical challenges, such as biases, lack of transparency, data privacy concerns, and potential negative societal impacts. This paper examines the ethical considerations involved in developing such advanced AI systems as well using AI systems within software development and testing. It explores existing ethical frameworks and principles provided by leading organizations, emphasizing core concepts like human-centered design, accountability, transparency, fairness, and privacy. Practical strategies for integrating ethical practices throughout the AI development lifecycle are discussed, with a strong emphasis on the need for continuous ethical evaluation. The paper explores the ethical landscape of AI in software development, addressing challenges like algorithmic bias, data security, and broader societal impacts. Real-world case studies presented in the paper demonstrate the consequences of neglecting ethical considerations. Looking forward, the paper suggests future directions, including the development of unified ethical standards, collaborative ethical auditing, regulatory advancements, and higher societal engagement.","2025-01","2025-11-25 22:38:06","2025-11-25 22:38:06","","00569-00576","","","","","","","","","","","","","","","","","","","","","","","","Software development management; Testing; Artificial intelligence; Software Testing; Software algorithms; Technological innovation; Large Language Models (LLMs); Software Development; Artificial Intelligence (AI); Ethics; Transparency; Collaboration; Stakeholders; Generative AI (GenAI); Standards organizations; Accountability; Bias Detection; Case Studies in AI Ethics; Data privacy; Data Privacy; Ethical AI; Ethical Auditing; Ethical Frameworks; Explainable AI (XAI); Fairness and Non-Discrimination; Human-Centered Design; Regulatory Frameworks; Responsible AI; Societal Engagement","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZCI6EL62","conferencePaper","2023","Napoli, Emanuele Antonio; Gatteschi, Valentina","Evaluating ChatGPT for Smart Contracts Vulnerability Correction","2023 IEEE 47th Annual Computers, Software, and Applications Conference (COMPSAC)","","","10.1109/COMPSAC57700.2023.00283","","The growing number of exploits and hacks on the Ethereum blockchain has led to the development of powerful smart contract vulnerability detection tools and the frequent patching of the smart contract’s programming languages (such as Solidity). At the same time, an ever-increasing number of people are interested in blockchain and smart contract-related topics and willing to build and deploy their own Decentralized Applications (dApp). However, learning a new programming language and its best practices as long as how to actually deploy a smart contract on the blockchain is a difficult task even for experienced developers. Recently, ChatGPT, a new user-friendly deep learning tool, has been released to improve the ability of non-skilled users to write high-quality code and in general, to boost the performances of developers in key tasks related to code writing (i.e., writing functions, explaining runtime errors, fixing bugs, etc.). This paper aims to measure the capabilities of ChatGPT in fixing vulnerable smart contracts and to assess the effectiveness of this tool, determining whether it can be a valuable aid for those who want to correct their own smart contract or want to reuse existing ones by first checking their status and eventually fix their vulnerability. In particular, we asked ChatGPT to fix 143 smart contracts with well-known labeled vulnerabilities. We considered a vulnerability as ""fixed"" if the code corrected by ChatGPT no longer contained the vulnerability (for this purpose, we exploited Slither, one of the state-of-the-art tools for smart contracts vulnerability detection to check the status of the original and the corrected smart contracts). As a result we obtained that ChatGPT was able to fix bugs and vulnerable smart contracts on average the 57.1% of the time with an increase of +1.4% when a description of the bug was provided in addition to the smart contract’s source code.","2023-06","2025-11-25 22:38:06","2025-11-25 22:38:06","","1828-1833","","","","","","","","","","","","","","","","","","","","ISSN: 0730-3157","","","","Writing; Chatbots; ChatGPT; Codes; Computer bugs; Smart contracts; Solidity; smart contracts; blockchain; Pipelines; Computer languages; bug fix; code correction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XDWNCLXV","conferencePaper","2023","Raza, Muhammad Raheel; Hussain, Walayat","Preserving Academic Integrity in Teaching with ChatGPT: Practical Strategies","2023 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)","","","10.1109/WI-IAT59888.2023.00027","","ChatGPT is an AI-powered natural language processing tool that enables human-like communications with the chatbot in various ways. This language model can aid with various tasks, including writing emails, articles, and code and answering inquiries. The pros and cons of its usage in the education sector have always been discussed but did not provide clear recommendations to minimize its negative consequences. The paper highlights the different effects of ChatGPT on teachers, academicians, and students during their learning processes for the correct usage of ChatGPT to lessen its negative influence and for a rich learning process. Since there always remains a question on how to preserve academic integrity while using ChatGPT, the paper suggests golden recommendations to be implemented to achieve maximum performance using ChatGPT.","2023-10","2025-11-25 22:38:06","2025-11-25 22:38:06","","158-162","","","","","","","","","","","","","","","","","","","","","","","","Writing; Chatbots; ChatGPT; Education; Codes; academic integrity; education; Task analysis; teaching and learning; examinations; Intelligent agents","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3GV246RM","conferencePaper","2023","Jamieson, Peter; Bhunia, Suman; Rao, Dhananjai M.","With ChatGPT, Do We have to Rewrite Our Learning Objectives - CASE Study in Cybersecurity","2023 IEEE Frontiers in Education Conference (FIE)","","","10.1109/FIE58773.2023.10343052","","With the emergence of Artificial Intelligent chatbot tools such as ChatGPT and code writing AI tools such as GitHub Copilot, educators need to question what and how we should teach our courses and curricula in the future. In reality, automated tools may result in certain academic fields being deeply reduced in the number of employable people. In this work, we make a case study of cybersecurity undergrad education by using the lens of “Understanding by Design” (UbD). First, we provide a broad understanding of learning objectives (LOs) in cybersecurity from a computer science perspective. Next, we dig a little deeper into a curriculum with an undergraduate emphasis on cybersecurity and examine the major courses and their LOs for our cybersecurity program at Miami University. With these details, we perform a thought experiment on how attainable the LOs are with the above-described tools, asking the key question “what needs to be enduring concepts?” learned in this process. If an LO becomes something that the existence of automation tools might be able to do, we then ask “what level is attainable for the LO that is not a simple query to the tools?”. With this exercise, we hope to establish an example of how to prompt ChatGPT to accelerate students in their achievements of LOs given the existence of these new AI tools, and our goal is to push all of us to leverage and teach these tools as powerful allies in our quest to improve human existence and knowledge.","2023-10","2025-11-25 22:38:06","2025-11-25 22:38:06","","1-5","","","","","","","","","","","","","","","","","","","","ISSN: 2377-634X","","","","Writing; Chatbots; Education; Taxonomy; Computer science; Computer security; Vocabulary","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2WDIWMR4","conferencePaper","2025","Zhang, Chong; Lai, Qiuxia; Li, Yu","FDTest: Prioritizing Test Inputs for Object Detection Models via Foundation Model Exploitation","2025 International Joint Conference on Neural Networks (IJCNN)","","","10.1109/IJCNN64981.2025.11229202","","Testing Deep Neural Networks (DNNs) often incurs high labeling costs due to the need for extensive labeled data. Hence, it is critical to strategically prioritize test inputs that can uncover more model errors for efficient labeling. Existing test input prioritization methods focus on image classification. However, these methods may not be directly applicable to object detection (OD) models, as testing of OD models confronts unique challenges involving complex model errors related to object localization and counting. To address the above challenges, in this paper, we introduce FDTest, a black-box test input prioritization framework for OD models that incorporates foundation models trained on diverse datasets to provide supplementary information. Guided by the foundation model, we categorize the target model’s predictions as reliable and suspicious objects and conduct confidence calibration on them to improve the estimation of wrongly detected objects, i.e., false positives (FPs). Additionally, we use the foundation model to supply missed objects and cross-verify them to improve the estimation of missed objects, i.e., false negatives (FNs). Then, we prioritize test images according to the total estimated number of FPs and FNs. Experiments on two standard OD benchmarks demonstrate the superiority of FDTest in test input prioritization for OD models.","2025-06","2025-11-25 22:38:06","2025-11-25 22:38:06","","1-8","","","","","","","","","","","","","","","","","","","","ISSN: 2161-4407","","","","Visualization; Standards; Deep Neural Network; Object Detection; Predictive models; Closed box; Artificial neural networks; Foundation models; Labeling; Calibration; Estimation; Object detection; Test Input Prioritization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YJNEEYMR","conferencePaper","2024","Maddala, Karthik; Mali, Bhabesh; Karfa, Chandan","LAAG-RV: LLM Assisted Assertion Generation for RTL Design Verification","2024 IEEE 8th International Test Conference India (ITC India)","","","10.1109/ITCIndia62949.2024.10651860","","Writing SystemVerilog Assertions (SVA) is an important but complex step in verifying Register Transfer Level (RTL) designs. Conventionally, experts need to understand the design specifications and write the SVA assertions, which is time-consuming and error-prone. However, with the recent advancement of transformer models, the Large Language Models (LLMs) assisted assertion generation for design verification is gaining interest in recent times. Motivated by this, we proposed a novel LLM-based framework, LAAG-RV, to generate SVA from natural language specifications of the design. Our framework provides a one-time Verilog loop for signal synchronization in the generated SVA to improve the generated assertion quality. For our experiments, we created a custom LLM based on OpenAI GPT-4. Furthermore, we developed test cases to validate the LLM -generated assertions. Initial observations shows that some generated assertions contain issues and did not pass all the test cases. However, iteratively prompting the LLMs using carefully crafted manual prompts derived from test case failures in a simulator, the framework can generate correct SVAs. Our results on OpenTitan designs demonstrate that LLMs significantly simplify the process of generating assertions, making it efficient and less error-prone.","2024-07","2025-11-25 22:38:06","2025-11-25 22:38:06","","1-6","","","","","","","","","","","","","","","","","","","","ISSN: 2833-8391","","","","Large language models; LLM; Manuals; Writing; Transformers; Natural languages; Assertion Based Verification; Registers; SVA Generation; Synchronization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZKNKJA8N","conferencePaper","2024","Le, Tri; Tran, Thien; Cao, Duy; Le, Vy; Nguyen, Tien N.; Nguyen, Vu","KAT: Dependency-Aware Automated API Testing with Large Language Models","2024 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST60714.2024.00017","","API testing has increasing demands for software companies. Prior API testing tools were aware of certain types of dependencies that needed to be concise between operations and parameters. However, their approaches, which are mostly done manually or using heuristic-based algorithms, have limitations due to the complexity of these dependencies. In this paper, we present KAT (Katalon API Testing), a novel AI -driven approach that leverages the large language model GPT in conjunction with advanced prompting techniques to autonomously generate test cases to validate RESTful APIs. Our comprehensive strategy encompasses various processes to construct an operation depen-dency graph from an OpenAPI specification and to generate test scripts, constraint validation scripts, test cases, and test data. Our evaluation of KAT using 12 real-world RESTful services shows that it can improve test coverage, detect more undocumented status codes, and reduce false positives in these services in comparison with a state-of-the-art automated test generation tool. These results indicate the effectiveness of using the large language model for generating test scripts and data for API testing.","2024-05","2025-11-25 22:38:06","2025-11-25 22:38:06","","82-92","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Software testing; Codes; Software algorithms; Companies; API testing; Restful API; Heuristic algorithms; Black-box testing; Large language models for testing; REST API","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A57ZI4E8","conferencePaper","2025","Ji, Suhwan; Lee, Sanghwa; Lee, Changsup; Han, Yo-Sub; Im, Hyeonseung","Impact of Large Language Models of Code on Fault Localization","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST62969.2025.10989036","","Identifying the point of error is imperative in software debugging. Traditional fault localization (FL) techniques rely on executing the program and using the code coverage matrix in tandem with test case results to calculate a suspiciousness score for each method or line. Recently, learning-based FL techniques have harnessed machine learning models to extract meaningful features from the code coverage matrix and improve FL performance. These techniques, however, require compilable source code, existing test cases, and specialized tools for generating the code coverage matrix for each programming language of interest. In this paper, we propose, for the first time, a simple but effective sequence generation approach for fine-tuning large language models of code (LLMCs) for FL tasks. LLMCs have recently received much attention for various software engineering problems. In line with these, we leverage the innate understanding of code that LLMCs have acquired through pre-training on large code corpora. Specifically, we fine-tune 13 representative encoder, encoder-decoder, and decoder-based LLMCs (across 7 different architectures) for FL tasks. Unlike previous approaches, LLM Cs can analyze code sequences that do not compile. Still, they have a limitation on the length of the input data. Therefore, for a fair comparison with existing FL techniques, we extract methods with errors from the project-level benchmark, Defects4J, and analyze them at the line level. Experimental results show that LLMCs fine-tuned with our approach successfully pinpoint error positions in 50.6%, 64.2%, and 72.3% of 1,291 methods in Defects4J for Top-1/3/5 prediction, outperforming the best learning-based state-of-the-art technique by up to 1.35, 1.12, and 1.08 times, respectively. We also conduct an in-depth investigation of key factors that may affect the FL performance of LLMCs. Our findings suggest promising research directions for FL and automated program repair tasks using LLMCs.","2025-03","2025-11-25 22:38:06","2025-11-25 22:38:06","","302-313","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Software testing; Software engineering; Codes; Source coding; Location awareness; Benchmark testing; Fault Localization; Deep Learning; Vulnerability Detection; Fine-Tuning; Computer architecture; Feature extraction; Large Language Model of Code; Software debugging","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"94YX8PBF","conferencePaper","2023","Fan, Angela; Gokkaya, Beliz; Harman, Mark; Lyubarskiy, Mitya; Sengupta, Shubho; Yoo, Shin; Zhang, Jie M.","Large Language Models for Software Engineering: Survey and Open Problems","2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE)","","","10.1109/ICSE-FoSE59343.2023.00008","","This paper provides a survey of the emerging area of Large Language Models (LLMs) for Software Engineering (SE). It also sets out open research challenges for the application of LLMs to technical problems faced by software engineers. LLMs' emergent properties bring novelty and creativity with applications right across the spectrum of Software Engineering activities including coding, design, requirements, repair, refactoring, performance improvement, documentation and analytics. However, these very same emergent properties also pose significant technical challenges; we need techniques that can reliably weed out incorrect solutions, such as hallucinations. Our survey reveals the pivotal role that hybrid techniques (traditional SE plus LLMs) have to play in the development and deployment of reliable, efficient and effective LLM-based SE.","2023-05","2025-11-25 22:38:06","2025-11-25 22:38:06","","31-53","","","","","","","","","","","","","","","","","","","","","","","","Software; Software engineering; Software reliability; Generative AI; Testing; Large Language Models; Software Testing; Surveys; Software Engineering Education; Refactoring; Automated Program Repair; Search Based Software Engineering (SBSE); Genetic Improvement; Requirements engineering; Reliability engineering; Maintenance engineering; Documentation generation; Human-Computer Interaction; Software Analytics; Software Maintenance and Evolution; Software Processes","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2JWEIIUR","conferencePaper","2025","Ferreira, Margarida; Viegas, Luís; Faria, João Pascoal; Lima, Bruno","Acceptance Test Generation with Large Language Models: An Industrial Case Study","2025 IEEE/ACM International Conference on Automation of Software Test (AST)","","","10.1109/AST66626.2025.00007","","Large language model (LLM)-powered assistants are increasingly used for generating program code and unit tests, but their application in acceptance testing remains underexplored. To help address this gap, this paper explores the use of LLMs for generating executable acceptance tests for web applications through a two-step process: (i) generating acceptance test scenarios in natural language (in Gherkin) from user stories, and (ii) converting these scenarios into executable test scripts (in Cypress), knowing the HTML code of the pages under test. This two-step approach supports acceptance test-driven development, enhances tester control, and improves test quality. The two steps were implemented in the AutoUAT and Test Flow tools, respectively, powered by GPT-4 Turbo, and integrated into a partner company’s workflow and evaluated on real-world projects. The users found the acceptance test scenarios generated by AutoUAT helpful 95% of the time, even revealing previously overlooked cases. Regarding Test Flow, 92% of the acceptance test cases generated by Test Flow were considered helpful: 60% were usable as generated, 8% required minor fixes, and 24% needed to be regenerated with additional inputs; the remaining 8% were discarded due to major issues. These results suggest that LLMs can, in fact, help improve the acceptance test process, with appropriate tooling and supervision.","2025-04","2025-11-25 22:38:06","2025-11-25 22:38:06","","1-11","","","","","","","","","","","","","","","","","","","","ISSN: 2833-9061","","","","Large language models; Test pattern generators; Software; Automation; Accuracy; Codes; Testing; Large Language Models; Automatic Test Generation; Natural languages; Costs; Web Application Testing; Usability; Acceptance Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8X9D2HU9","conferencePaper","2024","Ding, Minjie; Shen, Ying; Chen, Mingang","Automated Functionality and Security Evaluation of Large Language Models","2024 9th IEEE International Conference on Smart Cloud (SmartCloud)","","","10.1109/SmartCloud62736.2024.00014","","Natural language processing (NLP) is rapidly developing. A series of Large Language Models (LLMs) have emerged, represented by ChatGPT, which have made significant breakthroughs in natural language understanding and generation, enabling fluent dialogue with humans, understanding human intentions, and completing complex tasks. However, in addition to the fairness and toxicity of traditional language models, some new problems, including hallucination, have also emerged in LLMs, making them hard to use. Evaluating LLMs manually is challenging due to subjectivity and inefficiency. In this paper, we focused on the fuzzy matching, toxicity detection, and hallucination detection in the evaluation of LLMs automatically, and fine-tune the Mixtral-8x7B Model, which can be deployed in private cloud environment, and prove the effectiveness of our method through experiments.","2024-05","2025-11-25 22:38:06","2025-11-25 22:38:06","","37-41","","","","","","","","","","","","","","","","","","","","","","","","LLM; Accuracy; Chatbots; hallucination; Security; evaluation; Task analysis; Toxicology; Cloud computing; fuzzy matching; Graphics processing units; toxicity","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DCSHZJYX","conferencePaper","2025","Kardum, Benjamin; Car, Željka","Exploring Stakeholders' Perspectives on the Impact of Generative AI on IT Project Management Roles","2025 MIPRO 48th ICT and Electronics Convention","","","10.1109/MIPRO65660.2025.11132036","","Generative artificial intelligence (AI) has emerged as a transformative force in IT project management, influencing stakeholder perceptions and challenging traditional paradigms. This paper examines the multifaceted concerns surrounding the potential displacement of jobs within the IT sector, analysing them alongside broader perspectives that span from fears of professional obsolescence to optimistic outlooks driven by the potential of technological innovation and progress. Stakeholders' opinions on the evolving importance of project managers and software engineers over the next decade are examined, alongside the perceived impact of generative AI on the relevance of professional certifications and qualifications. Additionally, the paper assesses the need for specialized training in generative AI, highlighting the spectrum of readiness and adaptability among professionals. The findings contribute to a deeper understanding of project management's trajectory over the next decade, highlighting the transformative potential of generative AI to drive innovation and redefine professional roles, processes and strategies within the field.","2025-06","2025-11-25 22:38:06","2025-11-25 22:38:06","","1862-1867","","","","","","","","","","","","","","","","","","","","ISSN: 1847-3938","","","","Stability analysis; Software engineering; Automation; Generative AI; generative artificial intelligence; Technological innovation; Software tools; Stakeholders; Project management; Trajectory; automation impact; project management; Qualifications; stakeholders","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3RFAZ2DD","conferencePaper","2024","Petrovic, Nenad; Lebioda, Krzysztof; Zolfaghari, Vahid; Schamschurko, André; Kirchner, Sven; Purschke, Nils; Pan, Fengjunjie; Knoll, Alois","LLM-Driven Testing for Autonomous Driving Scenarios","2024 2nd International Conference on Foundation and Large Language Models (FLLM)","","","10.1109/FLLM63129.2024.10852505","","In this paper, we explore the potential of leveraging Large Language Models (LLMs) for automated test generation based on free-form textual descriptions in area of automotive. As outcome, we implement a prototype and evaluate the proposed approach on autonomous driving feature scenarios in CARLA open-source simulation environment. Two pre-trained LLMs are taken into account for comparative evaluation: GPT-4 and Llama3. According to the achieved results, GPT-4 outperforms Llama3, while the presented approach speeds-up the process of testing (more than 10 times) and reduces cognitive load thanks to automated code generation and adoption of flexible simulation environment for quick evaluation.","2024-11","2025-11-25 22:38:06","2025-11-25 22:38:06","","173-178","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Test pattern generators; Codes; Testing; Prototypes; Transformers; Large Language Model (LLM); autonomous driving; Automotive engineering; Autonomous vehicles; CARLA; Cognitive load; Generative Pre-Trained Transformer (GPT); Llama3; Load modeling; Model-Driven Engineering (MDE)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PT3AI7LH","conferencePaper","2025","Huang, Linghan; Zhao, Peizhou; Ma, Lei; Chen, Huaming","On the Challenges of Fuzzing Techniques via Large Language Models","2025 IEEE International Conference on Software Services Engineering (SSE)","","","10.1109/SSE67621.2025.00028","","In the modern era where software plays a pivotal role, software security and vulnerability analysis are essential for secure software development. Fuzzing test, as an efficient and tra-ditional software testing method, has been widely adopted across various domains. Meanwhile, the rapid development in Large Language Models (LLMs) has facilitated their application in the field of software testing, demonstrating remarkable performance. As existing fuzzing test techniques are not fully automated and software vulnerabilities continue to evolve, there is a growing interest in leveraging large language models to generate fuzzing test. In this paper, we present a systematic overview of the developments that utilize large language models for the fuzzing test. To our best knowledge, this is the first work that covers the intersection of three areas, including LLMs, fuzzing test, and fuzzing test generated based on LLMs. A statistical analysis and discussion of the literature are conducted by summarizing the state-of-the-art methods up to date of the submission. Our work also investigates the potential for widespread deployment and application of fuzzing test techniques generated by LLMs in the future, highlighting their promise for advancing automated software testing practices.","2025-07","2025-11-25 22:38:26","2025-11-25 22:38:26","","162-171","","","","","","","","","","","","","","","","","","","","","","","","large language model; Large language models; Software; Software reliability; Software development management; Fuzzing; automated software testing; Security; Reviews; Statistical analysis; Systematics; Reliability engineering; Fuzzing test","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6YQ6QD88","conferencePaper","2025","Lavrentiev, Vladimir; Levshun, Dmitry","LLMSecurityTester: A Tool for Detection of Vulnerabilities in LLM-based Chatbots","2025 33rd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing (PDP)","","","10.1109/PDP66500.2025.00091","","The development of generative algorithms in the last few years, including large language models, imposes increasingly high requirements in the field of data security and protection. Vulnerabilities of information systems related to the generation of inappropriate information are becoming an increasingly serious challenge. They can lead to negative consequences, such as misinformation, creation of fake news or disclosure of sensitive data. This paper presents the architecture of a system designed to identify vulnerabilities in large language models and proposes an approach for vulnerability detection based on prompt engineering. The idea of the approach lies in making certain requests to large language models, the execution of which can help to use the algorithm for illegal purposes. The authors provide a detailed description of the system prototype for automating vulnerability detection, named LLMSecurityTester. Additionally, preliminary results of experiments on various models and datasets are presented, demonstrating the applicability of the solution.","2025-03","2025-11-25 22:38:26","2025-11-25 22:38:26","","608-615","","","","","","","","","","","","","","","","","","","","ISSN: 2377-5750","","","","large language model; Large language models; Automation; Chatbots; prompt engineering; Prototypes; Prompt engineering; Data security; test automation; vulnerability detection; chatbot; Reflection; Fake news; Information systems; Protection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZX5ZEE49","conferencePaper","2024","Karmarkar, Hrishikesh; Agrawal, Supriya; Chauhan, Avriti; Shete, Pranav","Navigating Confidentiality in Test Automation: A Case Study in LLM Driven Test Data Generation","2024 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)","","","10.1109/SANER60148.2024.00041","","In out sourced industrial projects for testing of web applications, often neither the application to be tested, nor its source code are provided to the testing team, due to confidentiality reasons, making systematic testing of these applications very challenging. However, textual descriptions of such systems are often available. So, one can consider leveraging a Large Language Model (LLM) to parse these descriptions and synthesize test generators (programs that produce test data). In our experience, LLM synthesized test generators suffer from two problems:- (1) unsound: the generators might produce invalid data and (2) incomplete: the generators typically fail to generate all expected valid inputs. To mitigate these problems, we introduce TestRefineGen a method for autonomously generating test data from textual descriptions. TestRe-fineGen begins by invoking an LLM to parse a given corpus of documents and produce multiple test gener-ators. It then uses a novel ranking approach to identify generators that can produce invalid test data, and then automatically repairs them using a counterexample-guided refinement process. Lastly, TestRefineGen per-forms a generalization procedure that offsets synthesis or refinements that leads to incompleteness, to obtain generators that produce more comprehensive valid in-puts. We evaluated the effectiveness of TestRefineGen on a manually curated set of 256 textual descriptions of test data. TestRefineGen synthesized generators that produce valid test data for 66.01 % of the descriptions. Using a combination of post-processing sanitisation and refinement it was able to successfully repair synthesized generators, which improved the success rate to 76.95 %. Further, our statistical analysis on a small subset of synthesized generators shows that TestRefineGen is able to generate test data that is well distributed across the input space. Thus, TestRefineGen can be an effective technique for autonomous test data generation for web testing in projects with confidentiality concerns.","2024-03","2025-11-25 22:38:26","2025-11-25 22:38:26","","337-348","","","","","","","","","","","","","","","","","","","","ISSN: 2640-7574","","","","Source coding; Large Language Models; testing; Statistical analysis; Systematics; Navigation; Costs; Maintenance engineering; automated test input generation; Packaging; refinement","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DVYGSWN7","conferencePaper","2023","Panagoulias, Dimitrios P.; Palamidas, Filippos A.; Virvou, Maria; Tsihrintzis, George A.","Evaluating the Potential of LLMs and ChatGPT on Medical Diagnosis and Treatment","2023 14th International Conference on Information, Intelligence, Systems & Applications (IISA)","","","10.1109/IISA59645.2023.10345968","","We evaluate the validity, accuracy, and usefulness of ChatGPT-returned medical diagnosis of lung disease based on symptoms described by a human. Specifically, Tuberculosis and its symptoms are selected as the test case and our evaluation follows the directions of (i) medical validity and accuracy of the returned diagnosis in terms of both context and references, (ii) its usefulness to both doctors and patients and (iii) the economic value added to the healthcare system. It is shown that ChatGPT performs well in diagnosing Tuberculosis, but its performance improves when supervised by a human medical expert. In the interest of adding reproducibility and comparability, we propose a novel general evaluation procedure for the medical domain, to be followed when interacting with Large Language Models. This procedure integrates the various steps employed in our evaluation process and encompasses the review indices utilized for quantifying the outcome.","2023-07","2025-11-25 22:38:26","2025-11-25 22:38:26","","1-9","","","","","","","","","","","","","","","","","","","","","","","","LLM; Software; Chatbots; ChatGPT; NLP; Economics; Reproducibility of results; AI-empowered software engineering; Biological system modeling; explainability; prompt-engineering; Pulmonary diseases; Tuberculosis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9NVMNK9J","journalArticle","2025","Umama; Usman Danyaro, Kamaluddeen; Nasser, Maged; Zakari, Abubakar; Abdullahi, Shamsu; Khanzada, Atika; Muntasir Yakubu, Muhammad; Shoaib, Sara","LLM-Based Code Generation: A Systematic Literature Review With Technical and Demographic Insights","IEEE Access","","2169-3536","10.1109/ACCESS.2025.3631952","","The rapid emergence of Large Language Models (LLMs) has significantly advanced the field of code generation, sparking growing research interest across both academia and industry. While existing reviews provide foundational insights into LLM-based code generation, they do not provide a comprehensive analysis of recent developments or adequately address the dual perspective of technical and demographic insights. This Systematic Literature Review (SLR) bridges critical gap in existing work by systematically analysing both technical and demographic dimensions of LLM-based code generation. Following a PRISMA-guided methodology 58 studies published between 2020 and 2025 were selected from four major digital libraries. Key findings reveal a sharp rise in evaluation studies and framework-driven contributions. Notably, HumanEval, MBPP and APPS emerged as dominant benchmarks. While functional-correctness metrics particularly pass@k are the most frequently adopted metrics. Moreover, the review uncovers a previously unexamined demographic landscape, highlighting, geographic distribution, institutional collaboration, team size, publishing trends, venue types, and the top 10 highly cited studies from the selected studies. Significantly, a high proportion of selected studies were published in conferences. Several persistent challenges found in existing reviews that hinder the reliability and applicability of LLM-based code generation. These include hallucinations, limited generalizability, security vulnerabilities, and a lack of interpretability. These limitations constrain the robustness, trustworthiness, and scalability of current systems. To address these issues, this review offers actionable insights and strategic guidelines to guide future research, evaluation strategies, and practical implementations in LLM-based code generation.","2025","2025-11-25 22:38:26","2025-11-25 22:38:26","","194915-194939","","","13","","","","","","","","","","","","","","","","","","","","","Market research; generative AI; Systematic literature review; Planning; Codes; Measurement; Benchmark testing; code generation; Programming; LLMs; Security; Libraries; program synthesis; code completion; Training","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E59NP7T7","conferencePaper","2025","Furman, Deborah Ann; Farchi, Eitan; Gildein, Michael Edward; Hicks, Andrew C. M.; Rawlins, Ryan Thomas","Combinatorial Test Design Model Creation using Large Language Models","2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","","","10.1109/ICSTW64639.2025.10962528","","In this paper, we report on our initial experience in using Large Language Models (LLMs), which continue to impact a growing multitude of domains, further expanding machine learning applicability. One possible use case is to apply LLMs as a tool to help drive more optimized test coverage via assisting to generate Combinatorial Test Design (CTD) models. This can lower the entry barrier for new CTD practitioners by requiring less subject matter expertise to generate a basic CTD model. In this paper we report on our initial experience in using LLMs to generate a base CTD model and analyze the usefulness of the approach. In common testing scenarios, the LLMs easily provide the necessary attributes and values that are needed to define the CTD model. Prompting the LLM for additional use cases is useful in highlighting possible interactions and determining constraints of the attributes identified in the first stage. Combining the two stages together facilitates the creation of base CTD models.","2025-03","2025-11-25 22:38:26","2025-11-25 22:38:26","","314-323","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Analytical models; large language model; Large language models; LLM; Machine learning; Education; Conferences; test; Reviews; Combinatorial testing; Computational modeling; Documentation; Subject matter experts; combinatorial test design; combinatorial testing; computer defects; configurations; ctd; environments","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GDPJCMLP","conferencePaper","2025","Busany, Nimrod; Hadar, Ethan; Hadad, Hananel; Rosenblum, Gil; Maszlanka, Zofia; Akhigbe, Okhaide; Amyot, Daniel","Automating Business Intelligence Requirements with Generative AI and Semantic Search","2025 IEEE 49th Annual Computers, Software, and Applications Conference (COMPSAC)","","","10.1109/COMPSAC65507.2025.00260","","Eliciting Business Intelligence (BI) requirements is challenging, especially in dynamic business environments. This paper introduces AutoBIR, an AI-driven system that uses semantic search and Large Language Models (LLMs) to automate BI specification and prototyping. Through a conversational interface, it translates user inputs into analytic code, descriptions, and data dependencies while generating test-case reports with optional visuals. AutoBIR refines BI reporting via feedback, accelerating data-driven decision-making. We also explore the broader potential of generative AI in transforming BI development, illustrating its role in enhancing data engineering practice for large-scale, evolving systems.","2025-07","2025-11-25 22:38:26","2025-11-25 22:38:26","","1891-1898","","","","","","","","","","","","","","","","","","","","ISSN: 2836-3795","","","","Large language models; Software; Generative AI; Codes; Visualization; Large Language Models; Decision making; Requirements; Translation; AI-Driven Data Engineering; Business intelligence; Business Intelligence; Data engineering; Ontology-Based Query Generation; Prototyping; Semantic search; Semantic Search; Text-to-SQL","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MHPIMST2","journalArticle","2024","Shin, Jiho; Hemmati, Hadi; Wei, Moshi; Wang, Song","Assessing Evaluation Metrics for Neural Test Oracle Generation","IEEE Transactions on Software Engineering","","1939-3520","10.1109/TSE.2024.3433463","","Recently, deep learning models have shown promising results in test oracle generation. Neural Oracle Generation (NOG) models are commonly evaluated using static (automatic) metrics which are mainly based on textual similarity of the output, e.g. BLEU, ROUGE-L, METEOR, and Accuracy. However, these textual similarity metrics may not reflect the testing effectiveness of the generated oracle within a test suite, which is often measured by dynamic (execution-based) test adequacy metrics such as code coverage and mutation score. In this work, we revisit existing oracle generation studies plus gpt-3.5 to empirically investigate the current standing of their performance in textual similarity and test adequacy metrics. Specifically, we train and run four state-of-the-art test oracle generation models on seven textual similarity and two test adequacy metrics for our analysis. We apply two different correlation analyses between these two different sets of metrics. Surprisingly, we found no significant correlation between the textual similarity metrics and test adequacy metrics. For instance, gpt-3.5 on the jackrabbit-oak project had the highest performance on all seven textual similarity metrics among the studied NOGs. However, it had the lowest test adequacy metrics compared to all the studied NOGs. We further conducted a qualitative analysis to explore the reasons behind our observations. We found that oracles with high textual similarity metrics but low test adequacy metrics tend to have complex or multiple chained method invocations within the oracle's parameters, making them hard for the model to generate completely, affecting the test adequacy metrics. On the other hand, oracles with low textual similarity metrics but high test adequacy metrics tend to have to call different assertion types or a different method that functions similarly to the ones in the ground truth. Overall, this work complements prior studies on test oracle generation with an extensive performance evaluation on textual similarity and test adequacy metrics and provides guidelines for better assessment of deep learning applications in software test generation in the future.","2024-09","2025-11-25 22:38:26","2025-11-25 22:38:26","","2337-2349","","9","50","","","","","","","","","","","","","","","","","","","","","large language model; Software; Accuracy; Codes; Measurement; Testing; automated testing; AI4SE; Electronic mail; and test case generation; Correlation; neural oracle generation; test adequacy metrics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VNXK3YEC","conferencePaper","2024","Elvira, Timothy; Procko, Tyler Thomas; Vonderhaar, Lynn; Ochoa, Omar","Exploring Testing Methods for Large Language Models","2024 International Conference on Machine Learning and Applications (ICMLA)","","","10.1109/ICMLA61862.2024.00177","","Large Language Models (LLMs) are extensive aggregations of human language, designed to understand and generate sophisticated text. LLMs are becoming ubiquitous in a range of applications, from social media to code generation. With their immense size, LLMs face scalability challenges, making testing methods particularly difficult to implement effectively. Traditional machine learning and software testing methods, derived and adapted for LLMs, test these models to a point; however, they still struggle to accurately capture the full complexity of model behavior. This paper aims to capture the current efforts and techniques in testing LLMs, specifically focusing on stress testing, mutation testing, regression testing, metamorphic testing, and adversarial testing. This survey focuses on how traditional testing methods must be adapted to fit the needs of LLMs. Furthermore, while this area is fairly novel, there are still gaps in the literature that have been identified for future research.","2024-12","2025-11-25 22:38:26","2025-11-25 22:38:26","","1152-1157","","","","","","","","","","","","","","","","","","","","ISSN: 1946-0759","","","","Large language models; Software testing; Machine learning; Systematic literature review; Large Language Model; Testing; Adaptation models; Surveys; Regression testing; Mutation testing; Metamorphic testing; Complexity theory; Social networking (online); Penetration testing; Stress; Stress testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W56KFZMZ","conferencePaper","2025","Cao, Bocan; Tong, Weiyuan; Tang, Zhanyong; Wang, Zixu; Huang, Hao; Yan, Yuheng","Nüwa: Enhancing MLIR Fuzzing with LLM-Driven Generation and Adaptive Mutation","2025 IEEE International Conference on Software Maintenance and Evolution (ICSME)","","","10.1109/ICSME64153.2025.00055","","MLIR, a modular compiler framework, evolves quickly, with regular updates expanding its dialects and operations across LLVM versions and downstream projects. This fast development reduces the effectiveness of traditional fuzzing tools, which test only a small portion of dialects, require extensive manual work (e.g., nearly ten thousand lines of C++ code), and do not match the update speed of MLIR. To address these challenges, we propose NÜwa, the first LLM-based approach for MLIR fuzzing. Nüwa employs a two-phase strategy: first generating valid operations by encoding constraints into LLMs prompts, then synthesizing multi-operation test cases by learning inter-operation dependencies. To enhance operation coverage, it incorporates high-coverage cases from MLIR's test suite and uses LLM-driven mutations to boost diversity. A self-improvement mechanism enhances the prompts using feedback from highquality test cases, improving the LLMs' understanding of MLIR's complex semantics. Nüwa demonstrates that the generation and mutation process can be fully automated via the intrinsic capabilities of llMs (including in-context learning), while being applicable to MLIR's fast evolution. The experimental study shows that NÜwa outperforms the state-of-the-art tools MLIRSmith and MLIRod, detecting 2.9 \times more unique bugs and achieving 1.6 \times greater code coverage. To date, Nüwa has identified 55 bugs in the MLIR framework, with 18 confirmed or fixed.","2025-09","2025-11-25 22:38:26","2025-11-25 22:38:26","","541-552","","","","","","","","","","","","","","","","","","","","ISSN: 2576-3148","","","","Large language models; Semantics; Test pattern generators; Manuals; Codes; Computer bugs; Large Language Models; Test Generation; Fuzzing; Libraries; MLIR; Transforms; Software maintenance; Test Mutation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L2JZ449N","conferencePaper","2025","Olianas, Dario; Leotta, Maurizio; Ricca, Filippo","SleepReplacer-GPT: AI-Based Thread Sleep Replacement in Selenium WebDriver Tests","2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","","","10.1109/ICSTW64639.2025.10962456","","Ensuring the quality of modern web applications through end-to-end (E2E) testing is crucial, especially for dynamic systems like single-page applications. Managing asynchronous calls effectively is a key challenge, often addressed using thread sleeps or explicit waits. While thread sleeps are simple to use, they cause inefficiencies and flakiness, whereas explicit waits are more efficient but demand careful implementation.This work explores extending SleepReplacer, a tool that automatically replaces thread sleeps with explicit waits in Selenium WebDriver test suites. We aim to enhance its capabilities by integrating it with ChatGPT, enabling intelligent and automated replacement of thread sleeps with optimal explicit waits. This integration aims to improve code quality and reduce flakiness.We developed a structured procedure for interacting with ChatGPT and validated it on three test suites and synthetic examples covering diverse cases.Results show that the LLM-based approach correctly replaces thread sleeps with explicit waits on the first attempt, consistently outperforming SleepReplacer. These findings support integrating ChatGPT with SleepReplacer to create a smarter, more efficient tool for managing asynchronous behavior in test suites.","2025-03","2025-11-25 22:38:26","2025-11-25 22:38:26","","111-120","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Software testing; LLM; Chatbots; Codes; Testing; Artificial intelligence; Conferences; Artificial Intelligence; Selenium WebDriver; End-to-end; Selenium; Waiting Strategies; Dynamical systems; Flakiness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"793Q7WVX","conferencePaper","2024","Song, Ao; Lei, Zhou; Chen, Shengbo","SFPL: Improving CRS via Prompt Learning based Semantic Fusion Module","2024 IEEE 4th International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)","","","10.1109/ICIBA62489.2024.10868795","","With the popularity of the Internet and the massive increase of content production, users are faced with massive information and content, and it is often difficult to accurately and efficiently find the content that meets their needs. This information overload motivates the emergence of recommendation systems that help users filter and recommend relevant content. Dialogue recommendation systems (CRS) aim to dynamically recommend high-quality items to users while eliciting user preferences by means of dialogue. CRS mainly consists of two parts, namely, the dialogue module and the recommendation module. An efficient CRS is to solve the problems existing in the two sub-tasks of dialogue and recommendation, namely, generating appropriate responses in dialogue and predicting user favorite items in recommendation. In addition, it is necessary to adopt a suitable method to integrate these two modules to ensure the accuracy of CRS recommendation. This paper proposes an efficient way of semantic fusion module to solve the integration problem of two modules, and proves the effectiveness of our method through experiments","2024-12","2025-11-25 22:38:26","2025-11-25 22:38:26","","1634-1639","","","4","","","","","","","","","","","","","","","","","","","","","Semantics; LLM; Internet; Information technology; Recommender systems; Data models; Training data; Production; prompt learning; conversational recommendation systems; Information filters; Knowledge representation; Learning (artificial intelligence); Transformer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZX3YCS5J","conferencePaper","2024","Alshahwan, Nadia; Harman, Mark; Harper, Inna; Marginean, Alexandru; Sengupta, Shubho; Wang, Eddy","Assured LLM-Based Software Engineering","2024 IEEE/ACM 2nd International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering (InteNSE)","","","","","In this paper we address the following question: How can we use Large Language Models (LLMs) to improve code independently of a human, while ensuring that the improved code(1)does not regress the properties of the original code ?(2)improves the original in a verifiable and measurable way ?To address this question, we advocate Assured LLM-Based Software Engineering; a generate-and-test approach, inspired by Genetic Improvement. Assured LLMSE applies a series of semantic filters that discard code that fails to meet these twin guarantees. This overcomes the potential problem of LLM’s propensity to hallucinate. It allows us to generate code using LLMs, independently of any human. The human plays the role only of final code reviewer, as they would do with code generated by other human engineers.This paper is an outline of the content of the keynote by Mark Harman at the International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering, Monday 15th April 2024, Lisbon, Portugal.","2024-04","2025-11-25 22:38:26","2025-11-25 22:38:26","","7-12","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Semantics; Codes; Benchmark testing; Conferences; Large Language Models (LLMs); Automated Code Generation; CodeLlama; Genetic Improvement (GI); Llama; Search Based Software Engineering (SBSE); Filters; Genetics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I5K3JV56","journalArticle","2025","Omran Almagrabi, Alaa; Khan, Rafiq Ahmad","Optimizing Secure AI Lifecycle Model Management With Innovative Generative AI Strategies","IEEE Access","","2169-3536","10.1109/ACCESS.2024.3491373","","Generative AI (GAI) is one of the significant components that can efficiently improve and augment the AI cycle model’s robustness when it comes to different threats, weaknesses, and abnormalities detection. When applied in this field, GAI is very useful in emulating the various forms of security violations in actual adversarial settings. These scenarios are important when different aspects of an AI system are tested on how robust they are and thus permit the developers to amend any vulnerability that may be induced before the time it could be utilized in practice. Data and model manipulation, data theft, and adversarial attacks as well as model inference threats which we do a systematic analysis to disrupt the integrity, confidentiality as well as availability of AI models. Considering the current weaknesses and threats related to GAI we provide a systematic approach to how safety concerns that are currently relevant can be integrated with every stage of Artificial Intelligence (AI) lifecycle management: from continuous monitoring to the application of cybersecurity trends and practices, etc. In our approach, the emphasis is placed on the multi-level security management strategy that incorporates the improvement of coding practices, validation and testing, and the implementation of advanced intrusion detection systems. Before proceeding to further analysis and discussion of the given topic, it is also critical to mention the aspect of regulation and ethical concern as the major drivers of GAI usage. Additionally, organizations can involve GAI in the lifecycle to address security needs, during the development, acquisition, deployment, updating, maintenance, and decommissioning of the AI system, making them reliable, safe, and secure all through their lifecycle. Toward these ends, the goal of this work is to present a set of canonical recommendations for the many scientists, engineers, managers, technologists, and policymakers who will play a key role in constructing a sound and secure AI future.","2025","2025-11-25 22:38:27","2025-11-25 22:38:27","","12889-12920","","","13","","","","","","","","","","","","","","","","","","","","","Generative AI; Artificial intelligence; Security; Ethics; Data models; Training; Synthetic data; Generative adversarial networks; Organizations; AI lifecycle model; Generative artificial intelligence; Law; security threats and practices; systematic mapping study","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5LCRTBPZ","conferencePaper","2025","Magalhaes, Cleyton; Santos, Italo; Stuart-Verner, Brody; De Souza Santos, Ronnie","Testing the Untestable? An Empirical Study on the Testing Process of LLM-Powered Software Systems","2025 IEEE International Conference on Source Code Analysis & Manipulation (SCAM)","","","10.1109/SCAM67354.2025.00015","","Background: Software systems powered by large language models are becoming a routine part of everyday technologies, supporting applications across a wide range of domains. In software engineering, many studies have focused on how LLMs support tasks such as code generation, debugging, and documentation. However, there has been limited focus on how full systems that integrate LLMs are tested during development. Aims: This study explores how LLM-powered systems are tested in the context of real-world application development. Method: We conducted an exploratory case study using 99 individual reports written by students who built and deployed LLM-powered applications as part of a university course. Each report was independently analyzed using thematic analysis, supported by a structured coding process. Results: Testing strategies combined manual and automated methods to evaluate both system logic and model behavior. Common practices included exploratory testing, unit testing, and prompt iteration. Reported challenges included integration failures, unpredictable outputs, prompt sensitivity, hallucinations, and uncertainty about correctness. Conclusions: Testing LLM-powered systems required adaptations to traditional verification methods, blending source-level reasoning with behavior-aware evaluations. These findings provide evidence on the practical context of testing generative components in software systems.","2025-09","2025-11-25 22:38:27","2025-11-25 22:38:27","","78-88","","","","","","","","","","","","","","","","","","","","ISSN: 2470-6892","","","","Large language models; software testing; Manuals; Software systems; Codes; Testing; Logic; Cognition; LLMs; Uncertainty; Sensitivity; guidelines; System integration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FXHX64MV","conferencePaper","2025","Salari, Mikael Ebrahimi; Enoiu, Eduard Paul; Bucaioni, Alessio; Afzal, Wasif; Seceleanu, Cristina","PyLC+: A Scalable Python Framework for Automated Translation and Testing of Industrial PLC Programs","2025 IEEE 49th Annual Computers, Software, and Applications Conference (COMPSAC)","","","10.1109/COMPSAC65507.2025.00085","","As industrial PLC programs become more complex, automated testing and verification methods are needed to ensure their reliability and correctness. This paper presents PyLC+, a modular framework that translates PLC programs into Python, allowing for automated AI-driven test generation. PyLC+ builds upon our previous work, addressing limitations by adopting a class-based modular architecture that improves the tool’s scalability, maintainability, and extensibility. This structural refinement eliminates reliance on nested functions, facilitating the translation of large-scale, real-world PLC programs while maintaining precise use of cyclic execution. Furthermore, PyLC+ introduces automated handling of stateful FBs, ensuring compliance with IEC 61131-3 execution semantics.Additionally, the tool proposes integrating LLM-driven test generation with search-based test generation to improve the efficiency and effectiveness of testing PLC software. We tested PyLC+ in a large-scale company developing train control systems, demonstrating its efficiency and effectiveness in handling complex industrial PLC programs.","2025-07","2025-11-25 22:38:27","2025-11-25 22:38:27","","628-639","","","","","","","","","","","","","","","","","","","","ISSN: 2836-3795","","","","Test pattern generators; LLM; Software; Testing; Scalability; AI; Search-based Testing; Reliability; Python; PLC; Translation; Control systems; Extensibility; FBD; IEC Standards; PyLC+; Test Automation Framework","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TSNPGNX6","conferencePaper","2025","Lima, Carlos D. Q.; Alves, Everton L. G.; Andrade, Wilkerson L.; Torres, Felipe","Exploring the Use of LLMs to Reduce the Discarding of MBT Test Cases","2025 IEEE 49th Annual Computers, Software, and Applications Conference (COMPSAC)","","","10.1109/COMPSAC65507.2025.00164","","Model-Based Testing (MBT) enables the automated generation of test suites from requirement models. However, the frequent changes in agile development often lead teams to indiscriminately discard existing test cases, undermining the efficiency of Test Case Maintenance (TCM). This practice results in the loss of valuable test artifacts and escalates costs due to redundant test generation. Previous research has explored test case reuse through distance functions, but this strategy often suffers from low precision and misclassification. These issues lead to an excessive number of test cases being incorrectly considered reusable. In this paper, we investigate the use of Large Language Models (LLMs) to improve test case management. Through an empirical study on two industrial systems, we analyzed the performance of 13 well-known LLMs in classifying the impact of use case edits using CoT (Chain of Thought)/ToT (Tree of Thought) prompting and Naive-RAG strategies. Our findings indicate that seven of these models effectively reduced the unnecessary discarding of test cases by accurately identifying high-impact requirement changes, achieving a 7% improvement over distance functions. This resulted in a more precise, reliable, and efficient TCM solution within MBT. However, compared to distance-function-based strategies, LLMs exhibited slightly lower recall, performing 6% worse in test case reuse and reduction information loss.","2025-07","2025-11-25 22:38:27","2025-11-25 22:38:27","","1308-1317","","","","","","","","","","","","","","","","","","","","ISSN: 2836-3795","","","","Large language models; Semantics; Test pattern generators; LLM; Software; Software reliability; Natural language processing; Maintenance; Measurement; Testing; Requirements engineering; Agile Development; MBT; Test Case Maintenance; Test Suite Evolution","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K4MJFYXS","conferencePaper","2025","Deshmukh, Ritesh","Optimizing Firmware Validation Strategies for NVMe SSDs in Enterprise Applications","2025 2nd International Generative AI and Computational Language Modelling Conference (GACLM)","","","10.1109/GACLM67198.2025.11232245","","Current Solid-State Drive (SSD) firmware validation faces significant challenges in guaranteeing reliability and performance under various operational conditions. This paper presents the VITAL (Validation Intelligence through Test Analytics and Learning) framework to address the deficiencies in the existing firmware validation mechanisms. The evaluation is based on an extensive set of 5000 test cases in 5 heavyweight firmware modules and investigates fault detection patterns, execution time correlations, and coverage statistics. The framework exhibits large gains in the efficiency of fault detection, with a fault detection rate of 63.2% over several error types such as bit flips, timeout conditions and voltage spikes. There are strong relationships between test coverage metrics and fault detection accuracy, 0.03% to 99.97% test coverage values. The approach combines mathematical models for a predictive fault analysis approach and a mapping of test execution parameters and system reliability measures.","2025-08","2025-11-25 22:38:27","2025-11-25 22:38:27","","129-135","","","","","","","","","","","","","","","","","","","","","","","","Optimization; Solid modeling; Generative AI; Measurement; Fault detection; Reliability; Statistical analysis; Fault Detection; Microprogramming; Mathematical models; Firmware Validation; Solid state drives; Solid-State Drive; Statistical Analysis; Test Coverage Optimization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2UGWZQ95","conferencePaper","2025","Yi, Gaolei; Zhao, Yuan; Feng, Runkang; Zhang, Quanjun; Chen, Zhenyu","Chattss: Improving Test Suite Simplification Via Large Language Models","2025 25th International Conference on Software Quality, Reliability and Security (QRS)","","","10.1109/QRS65678.2025.00021","","As a critical component of software testing activities, regression testing plays an indispensable role in ensuring the correctness of software systems after changes. With the increasing scale and complexity of modern software, a pressing challenge arises: how to efficiently select the most effective test cases from existing test suites for regression testing, thereby reducing the associated cost. Although numerous methods have been proposed for test suite reduction, most of them rely on the assumption that test cases are independent of each other. In this paper, we present ChatTSS, a novel test case simplification approach powered by LLM. Unlike conventional test suite reduction that only shrinks the size of the test suite without altering individual test cases, ChatTSSleverages the program analysis capabilities of LLM to decompose test cases into fine-grained test atoms. It then applies appropriate reduction algorithms to perform more precise and effective test suite simplification. We conducted experiments on seven open-source projects, comprising over 10,000 test cases, to evaluate the effectiveness of ChatTSS. Experimental results demonstrate that ChatTSS exhibits strong simplification performance across multiple evaluation dimensions, confirming its potential as an efficient and scalable TSR solution.","2025-07","2025-11-25 22:38:27","2025-11-25 22:38:27","","96-107","","","","","","","","","","","","","","","","","","","","ISSN: 2693-9177","","","","Large language models; Software testing; LLM; Software reliability; Software quality; Software systems; Security; Complexity theory; Regression Test; Atoms; Pressing; Test Suite Reduction; Test Suite Simplification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SP33XHXS","conferencePaper","2025","Garaccione, Giacomo; Vega Carrazan, Pablo Federico; Coppola, Riccardo; Ardito, Luca","Evaluating Large Language Models in Exercises of UML Use Case Diagrams Modeling","2025 IEEE/ACM International Workshop on Natural Language-Based Software Engineering (NLBSE)","","","10.1109/NLBSE66842.2025.00015","","In recent years, Large Language Models (LLMs) have been extensively used in several Software Engineering tasks, from requirements analysis to coding and software testing. Research has proved that LLMs can effectively generate software models to assist in software documentation. The goal of this study is to assess the capability of LLM agents to generate UML Use Case Diagrams (UCD), starting from software requirements in natural language. We perform the assessment in an educational setting, i.e., we evaluate the capability to solve software modeling exercises tailored for master's students in SE curricula. Our results, based on the comparison of the results obtained by a human and an LLM solver on 17 UCD modeling exercises, show that LLMs have comparable results in terms of completeness and redundancy of the generated diagrams, with no significant difference if compared to human-proposed solutions.","2025-04","2025-11-25 22:38:27","2025-11-25 22:38:27","","41-44","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Software testing; Software; Software engineering; Large Language Models; Conferences; Unified modeling language; Natural languages; Documentation; Redundancy; Encoding; Software Modeling; Use Case Diagrams","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VMGWPZ45","conferencePaper","2025","Elhayany, Mohamed; Meinel, Christoph","Empowering Educators: Towards a GPT-Based Approach to Automate Unit Test Generation","2025 IEEE Global Engineering Education Conference (EDUCON)","","","10.1109/EDUCON62633.2025.11016616","","Assessing code automatically is a significant challenge in distance learning, especially in large online courses with limited teaching resources. Although auto-gradable programming exercises address scalability, creating enough high-quality exer-cises-particularly designing comprehensive unit tests-remains time-consuming and labor-intensive. To address this, we introduce a GPT-based feature that automates unit test generation for customized exercises. With a single button press, instructors can adapt existing exercises to meet specific teaching objectives while preserving auto-gradability. The AI-generated tests comprehensively cover potential edge cases that might otherwise be overlooked, thus reducing the need for manual oversight. An empirical evaluation with eight experienced educators showed these tests to be both thorough and time-efficient, achieving an average System Usability Scale (SUS) score of 81.79. Participants, who reported intermediate to advanced proficiency in designing manual unit tests and intermediate familiarity with AI tools like ChatGPT, praised the feature's ease of use and seamless workflow integration. Their combined expertise in teaching, coding, and AI-informed course development allowed them to provide insightful feedback on the practicality and reliability of our GPT-based solution. Our study includes a small participant pool (\mathrmn=8) and primarily focuses on Python, a language wellsupported by GPT. Future research will involve expanding the participant group, exploring additional programming languages, and assessing long-term tool performance and adaptability in diverse educational contexts. By harnessing GPT's language modeling capabilities, our approach addresses the gap between generic, limited-coverage test generation and the need for robust, domain-specific tests. Early reports from participants suggest that specialized exercises-such as those involving advanced data structures-can also benefit from automated unit test generation, though further evaluation is necessary. By leveraging artificial intelligence, this method streamlines exercise customization and enhances the overall usability and effectiveness of programming education tools. It has the potential to revolutionize auto-gradable exercise creation at scale, empowering educators to deliver high-quality instruction while tackling both the technical and pedagogical challenges in programming education.","2025-04","2025-11-25 22:38:38","2025-11-25 22:38:38","","1-9","","","","","","","","","","","","","","","","","","","","ISSN: 2165-9567","","","","Test pattern generators; Manuals; Education; Codes; Testing; Artificial intelligence; Technological innovation; Unit testing; Programming Education; Programming profession; Usability; GPT-4omini; System Usability; User centered design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GLWW8VZG","conferencePaper","2025","Asgari, Ali; Guerriero, Antonio; Pietrantuono, Roberto; Russo, Stefano","Adaptive Probabilistic Operational Testing for Large Language Models Evaluation","2025 IEEE/ACM International Conference on Automation of Software Test (AST)","","","10.1109/AST66626.2025.00017","","Large Language Models (LLM) empower many modern software systems, and are required to be highly accurate and reliable. Evaluating LLM poses challenges due to the high costs of manual labeling and of validation of labeled data.This study investigates the suitability of probabilistic operational testing for effective and efficient evaluation of LLM, focusing on a case study with DistilBERT. To this aim, we adopt an existing framework (DeepSample) for Deep Neural Network (DNN) testing and adapt it to the LLM domain by introducing auxiliary variables tailored to LLM and classification tasks.Through a comprehensive evaluation, we demonstrate how sampling-based operational testing can yield reliable LLM accuracy estimates and effectively expose failures, or, under testing budget constraints, it can find a trade off between accuracy estimation and failure exposure. The experimental results, using DistilBERT on three sentiment analysis datasets, show that sampling-based methods can provide cost effective and reliable operational accuracy assessment for LLM. These findings offer practical insights for testers and help address critical gaps in current LLM evaluation practices.","2025-04","2025-11-25 22:38:38","2025-11-25 22:38:38","","103-113","","","","","","","","","","","","","","","","","","","","ISSN: 2833-9061","","","","Large language models; Software testing; Software systems; Accuracy; Testing; Large Language Models; Costs; Artificial neural networks; Probabilistic logic; Minimization; Entropy; LLM evaluation; Sampling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RCNIGHSA","conferencePaper","2024","Liu, Yangtao; Liu, Hengyuan; Yang, Zezhong; Li, Zheng; Liu, Yong","Empirical Evaluation of Large Language Models for Novice Program Fault Localization","2024 IEEE 24th International Conference on Software Quality, Reliability and Security (QRS)","","","10.1109/QRS62785.2024.00027","","Integrating Large Language Models (LLMs) into software fault localization represents a significant advancement in improving debugging efficiency for programmers. However, novice program fault localization, which is essential for computer science education, has not been thoroughly investigated in previous studies. In contrast to industrial programs target practical functionality, novice programs primarily deal with individual algorithmic issues. The distinct logic structures between novice and industrial programs can impact how effectively LLM understand and process them. Moreover, this difference reveals the inapplicability of the Competent Programmer Hypothesis, a fundamental assumption in industrial fault localization, to novice program fault localization. Therefore, industrial methodologies are unsuitable for novice programming, emphasizing the need for our empirical studies. To fill this gap, we evaluate LLMs’ effectiveness in localizing faults for novice programs in statement level. Using the widely used novice programs dataset Codeflaws and Condefects, we compare the performance of two commercial LLMs (i.e., ChatGPT-3.5 and ChatGPT-4) and three open-source LLMs (i.e., ChatGLM3, Llama2, and Code Llama) against traditional fault localization methods, examining their accuracy and overlap. Additionally, we investigate how prompt engineering improves localization precision. Our findings show ChatGPT-4’s overall superior performance, with ChatGPT-3.5 exhibiting minor advantages in certain cases. ChatGPT-4 outperforms the traditional methods with best performance by 592% and 137% on Codeflaws and Condefects. Specifically, each method exhibits unique strengths in localizing novice programming faults. Moreover, carefully crafted prompts can improve LLMs’ precision. These insights underscore the promising potential of utilizing LLMs for fault localization in novice programming.","2024-07","2025-11-25 22:38:38","2025-11-25 22:38:38","","180-191","","","","","","","","","","","","","","","","","","","","ISSN: 2693-9177","","","","Large language models; Software quality; Accuracy; Codes; Large Language Model; Location awareness; Software algorithms; Empirical Study; Prompt Engineering; Novice Programming; Fault Localization; Reliability engineering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XD8Q9PXM","conferencePaper","2025","Honarvar, Shahin","Evaluating Correct-Consistency and Robustness in Code-Generating LLMs","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST62969.2025.10988971","","Ensuring the reliability of large language models (LLMs) for code generation is crucial for their safe integration into software engineering practices, especially in safety-critical domains. Despite advances in LLM tuning, they frequently generate incorrect code, raising concerns about robustness and trustworthiness. This PhD research introduces Turbulence, a novel benchmark and evaluation framework designed to assess both the correct-consistency and robustness of LLMs through a structured coding question neighbourhood approach. By evaluating model performance across sets of semantically related but non-equivalent coding tasks, Turbulence identifies discontinuities in LLM generalisation, revealing patterns of success and failure that standard correctness evaluations often overlook. Applied to 22 instruction-tuned LLMs across Python coding question neighbourhoods, the benchmark highlights significant variability in correctness, including error patterns persisting even under deterministic settings. Future work will extend the question neighbourhood concept to Capture The Flag (CTF) challenges, enabling a deeper analysis of model reasoning capabilities in progressively complex tasks. This extension has attracted interest from the UK AI Safety Institute, which recognises the frame-work's potential for advancing rigorous evaluation methodologies in the context of safe and trusted AI for software engineering.","2025-03","2025-11-25 22:38:38","2025-11-25 22:38:38","","797-800","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Software testing; Software engineering; Software reliability; Codes; Large Language Models; Benchmark testing; Code Generation; Standards; Robustness; Evaluation; Tuning; Encoding; Correct-consistency","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U8NRF5DB","conferencePaper","2023","Le Traon, Yves","AI is a game-changing technology: how to test and robustify Machine-Learning software?","2023 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST57152.2023.00010","","The recent release of ChatGPT conversational agent has been a surprise to me, and to many of my colleagues from the software engineering community. Progress goes extremely fast, while it appears to be a true ""game-changing"" technology that can even generate programs and fix bugs. Machine Learning (ML) provides engineers with the prospect of producing data-driven software, with little manual code writing. These ML-enabled software bring us to a new era where systems’ logic is automatically produced from data, with a small amount of human-written code. Would we trust such software mixing ML and regular code, would you rely on it and under which conditions? This is still too early to answer these questions, and a challenging direction to explore.This radical change questions the way software are engineered, validated, secured, deployed and maintained. The overall challenge is thus to automate these activities accounting for the statistical nature of ML-enabled software.Taking a software engineering perspective, and starting from a concrete case from the finance industry, the presentation will focus on testing and robustifying a ML model which is integrated in a larger software system that takes as input domain objects (e.g. financial transaction, malware, network traffic). One traditional way to robustify a ML model consists in generating adversarial inputs, e.g. leading to a misclassification, and retraining the model. Indeed, despite their impressive performance, ML models are sensitive to small perturbations in the input. The resulting adversarial inputs raise multiple questions about the robustness of such systems, especially in safety- and business-critical domains. However, the generation of feasible, exploitable adversarial test examples is challenging, as they must satisfy the business logic constraints over the feature space. We analyse the limitations of current adversarial approaches and explore new algorithms that combine multi-objective search with constraint-solving techniques. While the attack part is the offensive weapon, we also consider the challenge to efficiently shield (e.g. repair) the systems against such threats, and finally end the seminar by mentioning other research directions to deploy robust ML-enabled systems.","2023-04","2025-11-25 22:38:38","2025-11-25 22:38:38","","12-12","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Software testing; Machine learning; Writing; Codes; Seminars; Telecommunication traffic; Weapons","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R6JA9CQ9","conferencePaper","2024","Jiri, Medlen; Emese, Bari; Medlen, Patrick","Leveraging Large Language Models for Python Unit Test","2024 IEEE International Conference on Artificial Intelligence Testing (AITest)","","","10.1109/AITest62860.2024.00020","","Generative Artificial Intelligence is becoming an integral and enduring part of our lives, growing more powerful with each passing day. This paper explores Large Language Models and their application in text generation, specifically examining their potential to assist software quality assurance engineers in their daily tasks. Our focus is on the generation of unit tests as a critical component of software development. The research question is simple: Can Generative AI generate comprehensive unit tests? We started with Python and a very simple use case, and if Gen AI is successful, we will continue with complex tasks. Current literature focuses on success, but we are interested in failures as well. How many test cases are missing?","2024-07","2025-11-25 22:38:38","2025-11-25 22:38:38","","95-100","","","","","","","","","","","","","","","","","","","","ISSN: 2835-3560","","","","Analytical models; Large language models; Software quality; Chatbots; Generative AI; Generative Artificial Intelligence; Unit Test; Standards; Python; Software Quality Assurance","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WNIVXV5Q","conferencePaper","2024","Garg, Aayush; Degiovanni, Renzo; Papadakis, Mike; Traon, Yves Le","On the Coupling between Vulnerabilities and LLM-Generated Mutants: A Study on Vul4J Dataset","2024 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST60714.2024.00035","","With the release of powerful language models trained on large code corpus (e.g., CodeBERT, trained on 6.4 million programs), a new family of mutation testing tools has arisen that promises to generate more “natural” mutants, where the mutated code aims at following the implicit rules and coding conventions produced by programmers. In this paper, we empirically study the observable behavior of CodeBERT-generated mutants and to what extent are these coupled with software vulnerabilities. To do so, we carefully analyze 45 reproducible vulnerabilities from the Vul4J dataset to determine whether the mutants and vulnerabilities fail the same tests and whether the failures are for the same reasons or not. Hence, we define different degrees of vulnerability-coupling classes. Strongly coupled mutants fail the same tests for the same reasons as the vulnerabilities, while test coupled mutants fail the same tests but for some different reason as the vulnerabilities. Partial coupling classes are also considered. Overall, CodeBERT-generated mutants strongly coupled with 32 out of these 45 vulnerabilities (i.e. The mutants fail on the same tests for the same reasons), while another 7 vulnerabilities are test-coupled by CodeBERT mutants (i.e. The mutants fail on the same tests but not for the same reasons). Interestingly, CodeBERT mutants are diverse enough to couple vulnerabilities from 14 out of the 15 types of vulnerabilities explored, i.e., CWEs (Common Weakness Enumeration). Finally, we observe that strongly coupled mutants are scarce (1.17 % of the killable mutants), test coupled mutants represent 7.2 %, and 64.9 % of the killable mutants are not coupled with the vulnerabilities.","2024-05","2025-11-25 22:38:39","2025-11-25 22:38:39","","305-316","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Software testing; Software; Codes; large language models; vulnerabilities; Encoding; Couplings; mutants","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"95FHZ7EK","conferencePaper","2024","Dahiya, Mahima; Li, Mark; Horton, Glen; Scherz, Thomas; Niu, Nan","Tracing Feature Tests to Textual Requirements","2024 IEEE International Conference on Information Reuse and Integration for Data Science (IRI)","","","10.1109/IRI62200.2024.00035","","Software features deliver values to the end users, and thus their qualities shall be assured. While the mainstream quality assurance technique is software testing, its adequacy can only be assessed by tracing the testing artifacts to system requirements. In this paper, we report an in-depth case study aiming to trace all the feature tests of a Web application to textual requirements. To that end, we experiment two automated trace recovery methods: vector space model and transformer-based semantic embedding. The tracing results, unfortunately, are not satisfactory. We then explore the use of large language models, OpenAI’s ChatGPT in particular. We engage the original developers into evaluating the tracing and ChatGPT-prompting results. Our study reveals the promises of exploiting large language models to assist in software tracing activities.","2024-08","2025-11-25 22:38:39","2025-11-25 22:38:39","","120-125","","","","","","","","","","","","","","","","","","","","ISSN: 2835-5776","","","","Large language models; Semantics; software testing; Software testing; Software; Chatbots; large language models; Transformers; Quality assurance; Data science; Rough surfaces; Vectors; requirements traceability; trace recovery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BFQIECZE","conferencePaper","2025","Baresi, Luciano; De Lucia, Andrea; Di Marco, Antinisca; Di Penta, Massimiliano; Di Ruscio, Davide; Mariani, Leonardo; Micucci, Daniela; Palomba, Fabio; Rossi, Maria Teresa; Zampetti, Fiorella","Students' Perception of ChatGPT in Software Engineering: Lessons Learned from Five Courses","2025 IEEE/ACM 37th International Conference on Software Engineering Education and Training (CSEE&T)","","","10.1109/CSEET66350.2025.00023","","A few years after their release, Large Language Models (LLMs)-based tools are becoming an essential component of software education, as calculators are used in math courses. When learning software engineering (SE), the challenge is the extent to which LLMs are suitable and easy to use for different software development tasks. In this paper, we report the findings and lessons learned from using LLM-based tools-ChatGPT in particular-in five SE courses from four universities. After instructing students on the LLM potentials in SE and about prompting strategies, we ask participants to complete a survey and be involved in semi-structured interviews. The collected results report (i) indications about the usefulness of the LLM for different tasks, (ii) challenges to prompt the LLM, i.e., interact with it, (iii) challenges to adapt the generated artifacts to their own needs, and (iv) wishes about some valuable features students would like to see in LLM-based tools. Although results vary among different courses, also because of students' seniority and course goals, the perceived usefulness is greater for lowlevel phases (e.g., coding or debugging/fault localization) than for analysis and design phases. Interaction and code adaptation challenges vary among tasks and are mostly related to the need for task-specific prompts, as well as better specification of the development context.","2025-04","2025-11-25 22:38:39","2025-11-25 22:38:39","","158-169","","","","","","","","","","","","","","","","","","","","ISSN: 2832-7578","","","","Large language models; Software; Software engineering; Education; Software development management; Codes; Location awareness; Surveys; Empirical Study; Software Engineering Education; Large Language Models for Software Engineering; Problem-solving; System analysis and design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"94A5X5TE","conferencePaper","2025","Margabandu, Logan; Chen, Zizhao; Wong, W. Eric; Hsu, Chih-Wei","A Design of Experiments Oracle LLM for Research-Grounded Combinatorial Testing","2025 25th International Conference on Software Quality, Reliability, and Security Companion (QRS-C)","","","10.1109/QRS-C65679.2025.00021","","As artificial intelligence moves across all scientific and engineering disciplines, a big challenge remains: expert domain knowledge, especially in specialized areas like Design-of-Experiments (DoE), is hard to interpret or inaccessible to non-experts and thus hinders broader application and innovation. As large language models (LLMS) continue to improve, they offer a practical way to help close the knowledge gap and make expert-level information more accessible to a wider audience. To address this need, we designed and developed a new solution: a Design-of-Experiments Oracle (DOE), a conversational LLM. This paper presents this AI-powered system that changes how complex DoE principles are understood and used. DOE uses state-of-the-art techniques including a Llama 3.2 model inside a Retrieval-Augmented Fine-Tuned Transformer (RAFT) architecture. Its knowledge base is informed by and enables semantic retrieval from combinatorial testing research. This is a conversational expert system. With this system, we want to enable engineers and researchers to apply complex DoE principles, give them interactive, research-based guidance. Ultimately, this is to contribute to the efforts to speed up innovation, improve system reliability and make advanced engineering methods more accessible in the digital age.","2025-07","2025-11-25 22:38:39","2025-11-25 22:38:39","","82-90","","","","","","","","","","","","","","","","","","","","ISSN: 2693-9371","","","","Large language models; Semantics; Software reliability; Software quality; Large Language Models; Technological innovation; Transformers; Security; Combinatorial testing; Combinatorial Testing; Retrieval augmented generation; Reliability engineering; Conversational AI; Design-of-Experiments (DoE); Oracle LLM; Retrieval Augmented Generation (RAG)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XLNKEB7J","conferencePaper","2024","Wang, Siyi; Wang, Sinan; Fan, Yujia; Li, Xiaolei; Liu, Yepang","Leveraging Large Vision-Language Model for Better Automatic Web GUI Testing","2024 IEEE International Conference on Software Maintenance and Evolution (ICSME)","","","10.1109/ICSME58944.2024.00022","","With the rapid development of web technology, more and more software applications have become web-based in the past decades. To ensure software quality and user experience, various techniques have been proposed to automatically test web applications by interacting with their GUIs. To achieve high functional coverage, web GUI testing tools often need to generate high-quality text inputs and interact with the associated GUI elements (e.g., click submit buttons). However, developing a holistic approach that solves both subtasks is challenging because the web GUI context can be complicated and highly dynamic, which is hard to process programmatically. The recent development of large vision-language models (LVLM) provides new opportunities to handle these longstanding problems. We in this paper propose VETL, the first LVLM-driven end-to-end web testing technique. With LVLM's scene understanding capabilities, VETL can generate valid and meaningful text inputs focusing on the local context, while avoiding the need to extract precise textual attributes. The selection of associated GUI elements is formulated as a visual question answering problem, allowing LVLM to capture the logical connection between the input box and the relevant element based on visual instructions. Further, the GUI exploration is guided by a multi-armed bandit module employing a curiosity-oriented strategy. Experiments show that VETL is effective in exploring web state/action spaces and detecting bugs. Compared with WebExplor, the state-of-the-art web testing technique, VETL can discover 25% more unique web actions on benchmark websites. Moreover, it can expose functional bugs in top-ranking commercial websites, which have been confirmed by the website maintainers. Our work makes the first attempt of leveraging LVLM in end-to-end GUI testing, demonstrating promising results of this research direction.","2024-10","2025-11-25 22:38:39","2025-11-25 22:38:39","","125-137","","","","","","","","","","","","","","","","","","","","ISSN: 2576-3148","","","","Software quality; Large Language Model; Testing; Visualization; Computer bugs; Focusing; Question answering (information retrieval); Graphical user interfaces; Space exploration; Software maintenance; Automatic Web GUI Testing; Large Vision-Language Model; Text Input Generation; User experience","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XUTRFBY9","conferencePaper","2025","Levy, Oz; Dikman, Ilya; Levy, Natan; Winokur, Michael","Work in Progress: AI-Powered Engineering-Bridging Theory and Practice","2025 IEEE Engineering Education World Conference (EDUNINE)","","","10.1109/EDUNINE62377.2025.10981330","","This paper explores how generative AI can help automate and improve key steps in systems engineering. It examines AI's ability to analyze system requirements based on INCOSE’s ""good requirement"" criteria, identifying well-formed and poorly written requirements. The AI does not just classify requirements but also explains why some do not meet the standards. By comparing AI assessments with those of experienced engineers, the study evaluates the accuracy and reliability of AI in identifying quality issues. Additionally, it explores AI’s ability to classify functional and non-functional requirements and generate test specifications based on these classifications. Through both quantitative and qualitative analysis, the research aims to assess AI’s potential to streamline engineering processes and improve learning outcomes. It also highlights the challenges and limitations of AI, ensuring its safe and ethical use in professional and academic settings.","2025-03","2025-11-25 22:38:39","2025-11-25 22:38:39","","1-4","","","","","","","","","","","","","","","","","","","","","","","","Test pattern generators; Generative AI; Artificial intelligence; Requirements Engineering; Standards; Ethics; Safety; Safe AI; Requirements engineering; Reliability engineering; AI enhanced test generation; AI technology in Systems Engineering; Educational programs; Engineering education; Quality Criteria for Requirements","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TJIIUXDY","journalArticle","2024","Fakhoury, Sarah; Naik, Aaditya; Sakkas, Georgios; Chakraborty, Saikat; Lahiri, Shuvendu K.","LLM-Based Test-Driven Interactive Code Generation: User Study and Empirical Evaluation","IEEE Transactions on Software Engineering","","1939-3520","10.1109/TSE.2024.3428972","","Large language models (LLMs) have shown great potential in automating significant aspects of coding by producing natural code from informal natural language (NL) intent. However, given NL is informal, it does not lend easily to checking that the generated code correctly satisfies the user intent. In this paper, we propose a novel interactive workflow TiCoder for guided intent clarification (i.e., partial formalization) through tests to support the generation of more accurate code suggestions. Through a mixed methods user study with 15 programmers, we present an empirical evaluation of the effectiveness of the workflow to improve code generation accuracy. We find that participants using the proposed workflow are significantly more likely to correctly evaluate AI generated code, and report significantly less task-induced cognitive load. Furthermore, we test the potential of the workflow at scale with four different state-of-the-art LLMs on two python datasets, using an idealized proxy for a user feedback. We observe an average absolute improvement of 45.97% in the pass@1 code generation accuracy for both datasets and across all LLMs within 5 user interactions, in addition to the automatic generation of accompanying unit tests.","2024-09","2025-11-25 22:38:39","2025-11-25 22:38:39","","2254-2268","","9","50","","","","","","","","","","","","","","","","","","","","","Accuracy; Codes; Artificial intelligence; Benchmark testing; test generation; code generation; LLMs; Python; Natural languages; Task analysis; cognitive load; human factors; Intent disambiguation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V5JW55F2","conferencePaper","2025","Ma, Lezhi; Liu, Shangqing; Li, Yi; Xie, Xiaofei; Bu, Lei","SpecGen: Automated Generation of Formal Program Specifications via Large Language Models","2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)","","","10.1109/ICSE55347.2025.00129","","In the software development process, formal program specifications play a crucial role in various stages, including requirement analysis, software testing, and verification. However, manually crafting formal program specifications is rather difficult, making the job time-consuming and labor-intensive. Moreover, it is even more challenging to write specifications that correctly and comprehensively describe the semantics of complex programs. To reduce the burden on software developers, automated specification generation methods have emerged. However, existing methods usually rely on predefined templates or grammar, making them struggle to accurately describe the behavior and functionality of complex real-world programs. To tackle this challenge, we introduce SpecGen, a novel technique for formal program specification generation based on Large Language Models (LLMs). Our key insight is to overcome the limitations of existing methods by leveraging the code comprehension capability of LLMs. The process of SpecGen consists of two phases. The first phase employs a conversational approach that guides the LLM in generating appropriate specifications for a given program, aiming to utilize the ability of LLM to generate high-quality specifications. The second phase, designed for where the LLM fails to generate correct specifications, applies four mutation operators to the model-generated specifications and selects verifiable specifications from the mutated ones through a novel heuristic selection strategy by assigning different weights of variants in an efficient manner. We evaluate SpecGen on two datasets, including the SV-COMP Java category benchmark and a manually constructed dataset containing 120 programs. Experimental results demonstrate that SpecGen succeeds in generating verifiable specifications for 279 out of 385 programs, outperforming the existing LLM-based approaches and conventional specification generation tools like Houdini and Daikon. Further investigations on the quality of generated specifications indicate that SpecGen can comprehensively articulate the behaviors of the input program.","2025-04","2025-11-25 22:38:39","2025-11-25 22:38:39","","16-28","","","","","","","","","","","","","","","","","","","","ISSN: 1558-1225","","","","large language model; Large language models; Semantics; Software testing; Java; Software; Software engineering; Software development management; Codes; Benchmark testing; Grammar; program verification; specification inference","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I5Y69JEP","conferencePaper","2024","Arora, Chetan; Sayeed, Ahnaf Ibn; Licorish, Sherlock; Wang, Fanyu; Treude, Christoph","Optimizing LLMs for Code Generation: Which Hyperparameter Settings Yield the Best Results?","2024 31st Asia-Pacific Software Engineering Conference (APSEC)","","","10.1109/APSEC65559.2024.00039","","Large Language Models (LLMs), such as GPT models, are increasingly used in software engineering for various tasks, such as code generation, requirements management, and debugging. While automating these tasks has garnered significant attention, a systematic study on the impact of varying hyperparameters on code generation outcomes remains unexplored. This study aims to assess LLMs' code generation performance by exhaustively exploring the impact of various hyperparameters. Hyperparameters for LLMs are adjustable settings that affect the model's behaviour and performance. Specifically, we investigated how changes to the hyperparameters-temperature, top probability (top_p), frequency penalty, and presence penalty-affect code generation outcomes. We systematically adjusted all hyperparameters together, exploring every possible combination by making small increments to each hyperparameter at a time. This exhaustive approach was applied to 13 Python code generation tasks, yielding one of four outcomes for each hyperparameter combination: no output from the LLM, non-executable code, code that fails unit tests, or correct and functional code. We analysed these outcomes for a total of 14,742 generated Python code segments, focusing on correctness, to determine how the hyperparameters influence the LLM to arrive at each outcome. Using correlation coefficient and regression tree analyses, we ascertained which hyperparameters influence which aspect of the LLM. Our results indicate that optimal performance is achieved with a temperature below 0.5, top probability below 0.75, frequency penalty above -1 and below 1.5, and presence penalty above -1. We make our dataset and results available to facilitate replication.","2024-12","2025-11-25 22:38:39","2025-11-25 22:38:39","","281-290","","","","","","","","","","","","","","","","","","","","ISSN: 2640-0715","","","","Large language models; Software engineering; Generative AI; Software development management; Codes; Large Language Models; Empirical Study; Software Engineering; Code Generation; LLMs; Python; Tuning; Systematics; Hyperparameter Analysis; LLM Temperature; Regression tree analysis; Requirements management; Temperature distribution","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6YJE4V4P","conferencePaper","2025","Peixoto, Myron; Baía, Davy; Nascimento, Nathalia; Alencar, Paulo; Fonseca, Baldoino; Ribeiro, Márcio","On the Effectiveness of LLMs for Manual Test Verifications","2025 IEEE/ACM International Workshop on Deep Learning for Testing and Testing for Deep Learning (DeepTest)","","","10.1109/DeepTest66595.2025.00012","","Background: Manual testing is vital for detecting issues missed by automated tests, but specifying accurate verifications is challenging. Aims: This study aims to explore the use of Large Language Models (LLMs) to produce verifications for manual tests. Method: We conducted two independent and complementary exploratory studies. The first study involved using 2 closed-source and 6 open-source LLMs to generate verifications for manual test steps and evaluate their similarity to original verifications. The second study involved recruiting software testing professionals to assess their perception and agreement with the generated verifications compared to the original ones. Results: The open-source models Mistral-7B and Phi-3-mini-4k demonstrated effectiveness and consistency comparable to closed-source models like Gemini-1.5-flash and GPT-3.5-turbo in generating manual test verifications. However, the agreement level among professional testers was slightly above 40%, indicating both promise and room for improvement. While some LLM-generated verifications were considered better than the originals, there were also concerns about AI hallucinations, where verifications significantly deviated from expectations. Conclusion: We contributed by evaluating the effectiveness of 8 LLMs through similarity and human acceptance studies, identifying top-performing models like Mistral-7B and GPT-3.5-turbo. Although the models show potential, the relatively modest 40% agreement level highlights the need for further refinement. Enhancing the accuracy, relevance, and clarity of the generated verifications is crucial to ensure greater reliability in real-world testing scenarios.","2025-05","2025-11-25 22:38:39","2025-11-25 22:38:39","","45-52","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Performance evaluation; Software testing; LLM; Manuals; Accuracy; Testing; Conferences; Reliability; Deep learning; AI-generated testing; manual testing; model performance evaluation; test verifications","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RENSGNRA","conferencePaper","2025","Zhang, Junna; Sun, Yulin; Duan, Li; Liu, Hui; Liu, Chunhong; Zhang, Aili; Yuan, Peiyan","A Practical Teaching Model of Software Engineering Courses for Artificial Intelligence Literacy Cultivation in Pre-Service Teachers","2025 IEEE International Conference on Software Services Engineering (SSE)","","","10.1109/SSE67621.2025.00034","","This study aims to guide pre-service teachers in effectively utilizing Generative Artificial Intelligence (GAI) tools to enhance their Artificial Intelligence (AI) literacy. By ana-lyzing the impact of GAI on the AI literacy of pre-service teachers, this study proposes a “teacher-student-machine” triadic interactive teaching model based on self-organized learning and deep empowerment through GAI. Using an experimental class in a software engineering course as a case study, we construct an end-to-end experimental environment for the project-driven teaching lifecycle, covering requirement analysis, system design, development and implementation, as well as software testing and maintenance phases. GAI is used to support the completion of experimental tasks while fostering pre-service teachers' AI literacy in five core competencies: data literacy, digital communication and collaboration, critical thinking, computational thinking, and ethical literacy. The study demonstrates that this model effectively improves the AI literacy of pre-service teachers, provides a reference for innovating software engineering practice teaching, and offers a useful framework for the application of GAI in education.","2025-07","2025-11-25 22:38:39","2025-11-25 22:38:39","","211-218","","","","","","","","","","","","","","","","","","","","","","","","Software testing; Software; Software engineering; Education; Generative AI; Maintenance; Artificial intelligence; Computational modeling; Learning (artificial intelligence); System analysis and design; artificial intelligence literacy; generative artificial in-telligence; pre-service teachers; Self-Organization Learning Theory; software engineering course","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2D3FD7GU","conferencePaper","2025","Wang, Xiaoyin; Xiao, Yuting","An Effective Framework for Cross-Site Scripting Payload Generation","2025 IEEE 5th International Conference on Software Engineering and Artificial Intelligence (SEAI)","","","10.1109/SEAI65851.2025.11108844","","In the field of cybersecurity, machine learning techniques are being increasingly utilized for cross-site scripting (XSS) vulnerability detection. These techniques playa crucial role in facilitating the identification process and reducing the heavy reliance on human analytics. This paper puts forward a novel approach for generating the payload of cross-site attack scripts. Initially, datasets are gathered for model training, and positional coding is incorporated into the input data to enhance the model's understanding of the structure. The generative AI model conducts an in-depth analysis of the dataset and then further combines the syntactic features of cross-site scripting with common bypass methods to formulate adaptive mutation strategies. Based on the attack response results, the model parameters are updated, and the payload generation process is iteratively optimized. The experimental results demonstrate that this method can generate highly deceptive malicious scripts with a notably high success rate.","2025-06","2025-11-25 22:38:39","2025-11-25 22:38:39","","253-258","","","","","","","","","","","","","","","","","","","","","","","","Software engineering; Syntactics; Machine learning; Machine Learning; Generative AI; Adaptation models; cross-site scripting; Data models; Training; Vectors; Cross-site scripting; generative AI Model; network security; Payloads; XSS Payload Generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JAIPAFIC","conferencePaper","2024","Saha, Barun Kumar","Generative Artificial Intelligence for Industry: Opportunities, Challenges, and Impact","2024 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)","","","10.1109/ICAIIC60209.2024.10463245","","The recent advances in Generative Artificial In-telligence (GenAI) and Large Language Models (LLMs) have generated significant interest across the world. For a successful adoption of GenAI and LLMs by industry, it is critical to identify their potential benefits, impact, and challenges. Accordingly, in this work, we investigate a few use cases of LLMs, which are relevant across most industry segments. In order to empirically evaluate the impact of GenAI on the code generation use case, we build CodePrompt, a handcrafted dataset of sequential prompts used by a human user to generate code. We approximate efficiency by considering the ratio of the number of tokens of code generated by an LLM to the number of tokens in the user's prompt. Experimental results reveal that a sequential trial of prompts for code generation may lead to an efficiency factor of about 6.33, on average, which means that a user's effort is reduced to about one-sixth.","2024-02","2025-11-25 22:38:39","2025-11-25 22:38:39","","081-086","","","","","","","","","","","","","","","","","","","","ISSN: 2831-6983","","","","Industries; Generative AI; Codes; Large Language Models; Artificial Intelligence; Code Generation; Bid Engineering; Data Pipeline; Tokens","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LZMC25IQ","conferencePaper","2024","Dasgupta, Dipankar; Roy, Arunava","Pitfalls of Generic Large Language Models (GLLMs) from reliability and security perspectives","2024 IEEE 6th International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications (TPS-ISA)","","","10.1109/TPS-ISA62245.2024.00054","","Generic Large Language Models (GLLMs) have grown popularity in many professions with limited or no technical knowledge. Larger and larger GLLMs are continuously being released with enhanced capabilities, promoting the abilities of these Generative AI at the grassroots level in businesses. These tools excel in text, image, and video generation (assembling, summarizing, translating) when proper queries and prompts are given; moreover, various augmentation of up-to-date knowledge bases, making these more efficient in providing current events. Practitioners and marketers showcase the benefits of GLLMs by demonstrating various use cases. However, the reliability of GLLMs' responses is yet questionable in certain scenarios, particularly due to issues like hallucinations, factual inaccuracies, and inappropriate or unrelated responses. Also there remain many open questions on data collection, privacy and ethical issues that need to be addressed. This study emphasizes the reliability and security aspects of GLLMs while recognizing significant benefits in a wide variety of applications. We also provide some insides of social impacts and future directions of AI/ML applications.","2024-10","2025-11-25 22:38:39","2025-11-25 22:38:39","","412-419","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Generative AI; Large Language Models (LLMs); Reliability; Security; Ethics; Data models; Translation; Knowledge based systems; Data privacy; Generative Pre-Trained Models (GPTs); Intelligent systems; Small Parameterized Data Models (SPDM)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QJ3DQCUG","conferencePaper","2025","Li, Yuanyuan; Sarna, Neeraj; Lin, Yang","Quantifying Correlations of Machine Learning Models","2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","","","10.1109/ICSTW64639.2025.10962482","","Machine Learning models are being extensively used in safety critical applications where errors from these models could cause harm to the user. Such risks are amplified when multiple machine learning models, which are deployed concurrently, interact and make errors simultaneously. This paper explores three scenarios where error correlations between multiple models arise, resulting in such aggregated risks. Using real-world data, we simulate these scenarios and quantify the correlations in errors of different models. Our findings indicate that aggregated risks are substantial, particularly when models share similar algorithms, training datasets, or foundational models. Overall, we observe that correlations across models are pervasive and likely to intensify with increased reliance on foundational models and widely used public datasets, highlighting the need for effective mitigation strategies to address these challenges.","2025-03","2025-11-25 22:38:56","2025-11-25 22:38:56","","410-417","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Analytical models; Software testing; Machine learning; AI safety; Safety; Data models; Training; Predictive models; Prevention and mitigation; Foundation models; Correlation; Error analysis; Machine learning algorithms; Model vulnerabilities","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XKPXN6MC","conferencePaper","2025","Mahmud, Tarek; Duan, Bin; Che, Meiru; Ngu, Anne; Yang, Guowei","Testing Android Third Party Libraries with LLMs to Detect Incompatible APIs","2025 IEEE/ACM Second International Conference on AI Foundation Models and Software Engineering (Forge)","","","10.1109/Forge66646.2025.00039","","Third-party libraries (TPLs) are an integral part of Android app development, offering app developers essential tools for enhancing app functionality, design, and integration capabilities. However, the fast-paced evolution of Android APIs introduces compatibility issues not only in Android apps but also in TPLs as they rely heavily on these Android APIs too. These challenges necessitate continuous updates and compatibility checks to maintain apps as well as TPL’s compatibility across Android’s diverse ecosystem. Prior research primarily focused on detecting compatibility issues induced by native Android APIs in Android apps falling short in detecting the incompatible APIs associated with TPLs due to the additional layer of abstraction they introduce as well as the obfuscation used by the TPL developers.In this paper, we propose LibCT that leverages a pre-trained Large Language Model (LLM), GPT-4, for detecting incompatible APIs in Android TPLs. It leverages GPT-4 to generate tests on TPL API usages and executes them across a wide range of Android devices available in the Amazon Device Farm. In our experimental evaluation, we tested 312 libraries with 12,831 APIs on 86 devices in the Amazon device farm, which revealed 274 incompatible APIs, highlighting the LibCT’s capability in identifying both evolution-induced and device-specific compatibility issues.","2025-04","2025-11-25 22:38:56","2025-11-25 22:38:56","","280-291","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Test pattern generators; Software engineering; Large Language Model; Testing; Test Generation; Android; Libraries; Ecosystems; Foundation models; Incompatible APIs; Object recognition; Third-party libraries","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MDY2CR8N","conferencePaper","2025","Li, Hui; Dong, Zhen; Wang, Siao; Zhang, Hui; Shen, Liwei; Peng, Xin; She, Dongdong","Extracting Formal Specifications From Documents Using LLMS for Test Automation","2025 IEEE/ACM 33rd International Conference on Program Comprehension (ICPC)","","","10.1109/ICPC66645.2025.00039","","Automated test generation plays a crucial role in ensuring software security. It heavily relies on formal specifications to validate the correctness of the system behavior. However, the main approach to defining these formal specifications is through manual analysis of software documents, which requires a significant amount of engineering effort from experienced researchers and engineers. Meanwhile, system update further increases the human labor cost to maintain a corresponding formal specification, making the manual analysis approach a time-consuming and error-prone task. Recent advances in Large Language Models (LLMs) have demonstrated promising capabilities in natural language understanding. Yet, the feasibility of using LLMs to automate the extraction of formal specifications from software documents remains unexplored. We conduct an empirical study by constructing a comprehensive dataset comprising 603 specifications from 37 documents across three representative open-source software. We then evaluate the most recent LLMs' capabilities in extracting formal specifications from documents in an end-to-end fashion, including GPT-4o, Claude, and Llama. Our study demonstrates the application of LLMs in formal specification extraction tasks while identifying two major limitations: specification oversimplification and specification fabrication. We attribute these deficiencies to the LLMs' inherent limitations in processing and expressive capabilities, as well as their tendency to fabricate fictional information. Inspired by human cognitive processes, we propose a novel two-stage method, annotation-then-conversion, to address these challenges. Our method decomposes the task into sentence annotation and temporal logic conversion, reducing the demands on LLMs' processing and expressive capabilities for each subtask. Furthermore, by generating verifiable sentence-specification pairs, our method enables effective fact-checking, thereby mitigating hallucination effects. Our method demonstrates significant improvements over the end-to-end method, with a 29.2 % increase in the number of correctly extracted specifications and a 14.0 % improvement in average accuracy. In particular, our best-performing LLM achieves an accuracy of \mathbf7 1. 6 %.","2025-04","2025-11-25 22:38:56","2025-11-25 22:38:56","","1-12","","","","","","","","","","","","","","","","","","","","ISSN: 2643-7171","","","","Large language models; Test pattern generators; Manuals; Software engineering; Software reliability; Accuracy; Security; Fabrication; Text analysis; Formal specifications","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YTWJIR6E","journalArticle","2025","Ságodi, Zoltán; Kolláth, István; Hegedűs, Péter; Ferenc, Rudolf","A Program Synthesis Dataset for LLM Temperature Analysis","IEEE Access","","2169-3536","10.1109/ACCESS.2025.3625443","","Large Language Models (LLMs) play an increasingly critical role in software engineering research, aiding tasks such as program synthesis, automated program repair, and test case generation. While extensive evaluations of LLMs exist, those results are mostly based on well-known and frequently used benchmarks, therefore, the available generated texts are mostly the same. This means the published generated texts do not provide additional data to the researcher community. Generated texts on well-known and frequently used benchmarks also introduce the risk that models may be inadvertently or deliberately optimized for those specific benchmarks, thereby distorting the assessment of their true generalization capabilities. This work introduces a dataset containing 18,900 raw and 18,896 processed LLM-generated outputs from nine open-source models across three model families (Llama, Qwen, DeepSeek). These models were evaluated on seven programming tasks sourced from the Sapientia ECN competition, ensuring diversity in available LLM-generated content. To capture the stochastic variability of LLM generation, inference was conducted three times per model while systematically varying the temperature parameter across 100 settings per run. Using all model and temperature variations on every task for the evaluation, we created a JSON file that contains information about 29.882 test cases that can be used simply by loading the JSON file.","2025","2025-11-25 22:38:56","2025-11-25 22:38:56","","184022-184029","","","13","","","","","","","","","","","","","","","","","","","","","Analytical models; Large language models; Software engineering; Benchmark testing; Prompt engineering; evaluation; Training; Programming profession; Computational modeling; Maintenance engineering; Biological system modeling; data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VVGRDCJZ","conferencePaper","2025","Cunha, Felipe; Perkusich, Mirko; Albuquerque, Danyllo; Gorgônio, Kyller; Perkusich, Angelo","Llm-Codeval: a Framework for Verifying Implementations of Mathematical Functions Using Language Models","2025 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)","","","10.23919/SoftCOM66362.2025.11197356","","Mathematical functions are essential to software systems in areas like probabilistic modeling, simulation, and scientific computing. However, verifying their correctness becomes challenging when definitions come from proprietary tools or academic literature without reference code, leaving no clear ground truth. This paper proposes LLMCODEVAL, a lightweight and systematic framework for validating such implementations using Large Language Models (LLMs). The framework comprises three stages: (i) specification and implementation based on formal contracts; (ii) semantic validation via LLM-generated correctness proofs, followed by expert review; and (iii) empirical evaluation using statistical metrics. We applied LLM-CODEVAL to five ranked-node aggregation functions in Bayesian Networks (BNs) across 48 scenarios with varying weights, variances, and parent states. All implementations achieved Brier Scores below 0.0001, with LLM proofs aligning to specifications after at most two review iterations. The results demonstrate that LLM-CODEVAL enables explainable, auditable, and reproducible verification of mathematical functions. Its stage-based design makes it adaptable to domains where formal definitions exist but verified implementations are unavailable. These findings underscore both the potential and the current limitations of LLM-assisted verification: while LLMs excel at structuring logical arguments, they still require expert oversight to ensure precise mathematical semantics.","2025-09","2025-11-25 22:38:56","2025-11-25 22:38:56","","1-6","","","","","","","","","","","","","","","","","","","","ISSN: 1847-358X","","","","Large language models; Semantics; Software systems; Codes; Large Language Models; Software Engineering; Test Automation; Reviews; Systematics; Computational modeling; Telecommunications; Mathematical models; Bayes methods; Bayesian Networks; Code Validation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CMRGSFFB","conferencePaper","2024","Keluskar, Aryan; Bhattacharjee, Amrita; Liu, Huan","Do LLMs Understand Ambiguity in Text? A Case Study in Open-world Question Answering","2024 IEEE International Conference on Big Data (BigData)","","","10.1109/BigData62323.2024.10825265","","Ambiguity in natural language poses significant challenges to Large Language Models (LLMs) used for open-domain question answering. LLMs often struggle with the inherent uncertainties of human communication, leading to misinterpretations, miscommunications, hallucinations, and biased responses. This significantly weakens their ability to be used for tasks like fact-checking, question answering, feature extraction, and sentiment analysis. Using open-domain question answering as a test case, we compare off-the-shelf and few-shot LLM performance, focusing on measuring the impact of explicit disambiguation strategies. We demonstrate how simple, training-free, token-level disambiguation methods may be effectively used to improve LLM performance for ambiguous question answering tasks. We empirically show our findings and discuss best practices and broader impacts regarding ambiguity in LLMs.","2024-12","2025-11-25 22:38:56","2025-11-25 22:38:56","","7485-7490","","","","","","","","","","","","","","","","","","","","ISSN: 2573-2978","","","","large language model; Large language models; LLM; Sentiment analysis; Focusing; Big Data; ambiguity; Data models; Natural languages; Uncertainty; Question answering (information retrieval); Best practices; Feature extraction; question-answering; sensitivity","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YDZXABMT","conferencePaper","2024","Jin, Zhaoming; Shi, Zhixin","Boosting self-repair workflow with brainstorming for code generation","2024 IEEE 33rd Asian Test Symposium (ATS)","","","10.1109/ATS64447.2024.10915518","","Utilizing large language models (LLMs) for code generation has greatly enhanced software development. However, despite the advances in automated code generation frameworks that incorporate self-repair strategies, successful outcomes are not always guaranteed. This led us to explore a different approach to improving code quality. So we implemented a brainstorm-select-repair framework. For a natural language query, our framework first generates multiple foundational code snippets. Then these snippets are tested on test cases, and a text-similarity-based algorithm is used to identify the most accurate code. If none of the generated code snippets successfully fulfills the test cases, our proposed self-repair strategy is used to rectify any flawed code snippets until a code snippet that can pass the test case is identified and then provided to the user. Based on ChatGPT3, our framework reached 89% Pass@1 on HumanEval dataset (at least 3% higher than ChatGPT-4 the best model to date on this dataset). Our framework also surpasses state-of-the-art methods and delivers superior performance on the HumanEval-ET, MBPP, and MBPP-ET datasets. To further validate our design rationale, we conducted comprehensive experiments and analyzed the impact of each component.","2024-12","2025-11-25 22:38:56","2025-11-25 22:38:56","","1-6","","","","","","","","","","","","","","","","","","","","ISSN: 2377-5386","","","","large language model; Large language models; Software engineering; Software development management; Codes; code generation; Natural languages; Heuristic algorithms; Maintenance engineering; Problem-solving; AI for software engineering; Brain modeling; Particle swarm optimization; self repair","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"342NX6QH","journalArticle","2024","Black, Gavin; Mathew Vaidyan, Varghese; Comert, Gurcan","Evaluating Large Language Models for Enhanced Fuzzing: An Analysis Framework for LLM-Driven Seed Generation","IEEE Access","","2169-3536","10.1109/ACCESS.2024.3484947","","Fuzzing is a crucial technique for detecting software defects by dynamically generating and testing program inputs. This study introduces a framework designed to assess the application of Large Language Models (LLMs) to automate the generation of effective seed inputs for fuzzing, particularly in the Python programming environment where traditional approaches are less effective. Utilizing the Atheris fuzzing framework, we created over 38,000 seed inputs from LLMs targeted at 50 Python functions from widely-used libraries. Our findings underscore the critical role of LLM selection in seed effectiveness. In certain cases, seeds generated by LLMs rivaled or surpassed traditional fuzzing campaigns, with a corpus of fewer than 100 LLM-generated entries outperforming over 100,000 conventionally produced inputs. These seeds significantly improved code coverage and instruction count during fuzzing sessions, illustrating the efficacy of our framework in facilitating an automated, scalable approach to evaluating LLM effectiveness. The results, validated through linear regression analysis, demonstrate that selecting the appropriate LLM based on its training and capabilities is essential for optimizing fuzzing efficiency and facilitates the testing of future LLM versions.","2024","2025-11-25 22:38:57","2025-11-25 22:38:57","","156065-156081","","","12","","","","","","","","","","","","","","","","","","","","","Large language models; Machine learning; Codes; Measurement; Testing; large language models; python; Fuzzing; machine learning; Security; Python; Training; Protocols","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L2RUZE7G","conferencePaper","2025","Domanski, Peter; Sahoo, Deepesh; Ortega, Eduardo; Firouzi, Farshad; Chakrabarty, Krishnendu","LLM-Aided In-Field Workload Generation for Detecting Silent Data Corruptions at Scale","2025 IEEE International Test Conference (ITC)","","","10.1109/ITC58126.2025.00018","","Computational integrity is crucial in large-scale data centers where Silent Data Corruptions (SDCs) pose a growing reliability challenge. SDCs can lead to incorrect computation results not captured by traditional error detection mechanisms, making their detection and mitigation essential. However, existing post-manufacturing and in-field testing methods, such as opportunistic and ripple testing, face significant scalability challenges due to high computational costs and test times. We propose an LLM-aided approach for generating targeted test cases to detect SDCs. As a case study, we focus on the functional blocks of a RISC-V CV32E40P processor core. Our method generates targeted test cases that maximize voltage droops in given hardware modules, such as functional units, increasing the likelihood of triggering SDCs in-field. Additionally, our approach is architecture-aware and layout-aware, enhancing fault activation and enabling automated optimization of generated test cases. Experimental evaluations demonstrate that the proposed method significantly improves SDC detection efficiency by reducing the number of required test cases while preserving high test coverage. Compared to commonly used test cases, the proposed approach increases average voltage droops by up to 38%. By integrating LLM-aided test case generation, the proposed approach achieves voltage droops of up to 9% relative to the supply voltage, improving the effectiveness of in-field SDC detection and mitigation strategies.","2025-09","2025-11-25 22:38:57","2025-11-25 22:38:57","","121-130","","","","","","","","","","","","","","","","","","","","ISSN: 2378-2250","","","","Optimization; Scalability; Hardware; Reliability; Real-time systems; Prevention and mitigation; Runtime; System-on-chip; Telemetry; Voltage","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WJYFP8K7","conferencePaper","2023","Li, Ziyu; Li, Zhendu; Xiao, Kaiming; Li, Xuan","Evaluating LLM’s Code Reading Abilities in Big Data Contexts using Metamorphic Testing","2023 9th International Conference on Big Data and Information Analytics (BigDIA)","","","10.1109/BigDIA60676.2023.10429345","","With the explosive growth of Big Data, understanding complex data-centering algorithms and software has become essential. Large Language Models (LLMs) especially ChatGPT models have been increasingly deployed in Big Data environments to improve workflow in various tasks, including code reading. The current testing method on LLMs’ code reading ability focuses more on code structural understanding and sentiment understanding, all tests are conducted on different prompts with different assumptions. This paper analyzes current LLM code-reading testing methods and presents an innovative evaluation of Metamorphic Testing to evaluate LLMs’ code-reading abilities. We proposed two new metamorphic relations specified f or code reading challenges and evaluated the ChatGPT-3.5 on its code understanding capabilities. Our study offers insights into LLMs’ capabilities to correctly understand diverse code bases and maintain validity.","2023-12","2025-11-25 22:38:57","2025-11-25 22:38:57","","232-239","","","","","","","","","","","","","","","","","","","","ISSN: 2771-6902","","","","Codes; Testing; Big Data; Data models; Training data; Trajectory; Numerical models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FA8B4DQ2","conferencePaper","2024","Rahman, Shanto; Baz, Abdelrahman; Misailovic, Sasa; Shi, August","Quantizing Large-Language Models for Predicting Flaky Tests","2024 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST60714.2024.00018","","A major challenge in regression testing practice is the presence of flaky tests, which non-deterministically pass or fail when run on the same code. Previous research identified multiple categories of flaky tests. Prior research has also de-veloped techniques for automatically detecting which tests are flaky or categorizing flaky tests, but these techniques generally involve repeatedly rerunning tests in various ways, making them costly to use. Although several recent approaches have utilized large-language models (LLMs) to predict which tests are flaky or predict flaky-test categories without needing to rerun tests, they are costly to use due to relying on a large neural network to perform feature extraction and prediction. We propose FlakyQ to improve the effectiveness of LLM-based flaky-test prediction by quantizing LLM's weights. The quantized LLM can extract features from test code more efficiently. To make up for loss in prediction performance due to quantization, we further train a traditional ML classifier (e.g., a random forest) to learn from the quantized LLM-extracted features and do the same prediction. The final model has similar prediction performance while running faster than the non-quantized LLM. Our evaluation finds that FlakyQ classifiers consistently improves prediction time over the non-quantized LLM classifier, saving 25.4% in prediction time over all tests, along with a 48.4 % reduction in memory usage. Furthermore, prediction performance is equal or better than the non-quantized LLM classifier.","2024-05","2025-11-25 22:38:57","2025-11-25 22:38:57","","93-104","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Software testing; Codes; Source coding; Training; Computational modeling; Artificial neural networks; Flaky Test Categorization; Large-Language Models; Quantization; Quantization (signal)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IBP67JD3","conferencePaper","2024","Dahiya, Mahima; Gill, Rashminder; Niu, Nan; Gudaparthi, Hemanth; Peng, Zedong","Leveraging ChatGPT to Predict Requirements Testability with Differential In-Context Learning","2024 IEEE International Conference on Information Reuse and Integration for Data Science (IRI)","","","10.1109/IRI62200.2024.00044","","Testability is a desired property of requirements, indicating how easy or difficult a requirements artifact supports its own testing. Prior work predicts natural language (NL) requirements’ testability by training a decision tree (DT) via some readability and word measures. To explore better ways of predicting requirements testability, we examine in this paper large language models-ChatGPT in particular. Our experiments on a total of 1,181 requirements from six software systems show that ChatGPT’s zero-shot learning performs worse than the DT. A main reason is due to the lack of context specific to the testability prediction task. However, applying ChatGPT’s incontext learning (ICL) reveals a limitation of skewed examples caused by the imbalanced data. Thus, we propose a novel approach, called differential ICL, to address the challenges by exploiting the DT and show quantitatively the higher accuracy achieved by differential ICL.","2024-08","2025-11-25 22:38:57","2025-11-25 22:38:57","","170-175","","","","","","","","","","","","","","","","","","","","ISSN: 2835-5776","","","","software testing; Software systems; Testing; large language models; machine learning; Training; Natural languages; Predictive models; Rough surfaces; Instruments; Decision trees; Particle measurements; requirements testability; Zero shot learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UUNWPBBU","conferencePaper","2025","Xu, Zhuolin; Li, Qiushi; Tant, Shin Hwei","Understanding and Enhancing Attribute Prioritization in Fixing Web UI Tests with LLMs","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST62969.2025.10989008","","The rapid evolution of Web UI incurs time and effort in UI test maintenance. Prior techniques in Web UI test repair focus on locating the target elements on the new Webpage that match the old ones so that the corresponding broken statements can be repaired. These techniques usually rely on prioritizing certain attributes (e.g., XPath) during matching where the similarity of certain attributes is ranked before other attributes, indicating that there may be bias towards certain attributes during matching. To mitigate the bias, we present the first study that investigates the feasibility of using prior Web UI repair techniques for initial matching and then using ChatGPT to perform subsequent matching. Our key insight is that given a list of elements matched by prior techniques, ChatGPT can leverage language understanding to perform subsequent matching and use its code generation model for fixing the broken statements. To mitigate hallucination in ChatGPT, we design an explanation validator that checks if the provided explanation for the matching results is consistent, and provides hints to ChatGPT via a self-correction prompt to further improve its results. Our evaluation on a widely used dataset shows that the ChatGPT-enhanced techniques improve the effectiveness of existing Web test repair techniques. Our study also shares several important insights in improving future Web UI test repair techniques.","2025-03","2025-11-25 22:38:57","2025-11-25 22:38:57","","326-337","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Software testing; Chatbots; Maintenance; Codes; Maintenance engineering; Test Maintenance; UI Element Matching; Web UI Test Repair","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RXRHN398","conferencePaper","2024","Kande, Rahul; Gohil, Vasudev; DeLorenzo, Matthew; Chen, Chen; Rajendran, Jeyavijayan","LLMs for Hardware Security: Boon or Bane?","2024 IEEE 42nd VLSI Test Symposium (VTS)","","","10.1109/VTS60656.2024.10538871","","Large language models (LLMs) have emerged as transformative tools within the hardware design and verification lifecycle, offering numerous capabilities in accelerating design processes. Recent research has showcased the efficacy of LLMs in translating design specifications into source code through hardware description languages. Researchers are also using LLMs to generate test cases and write assertion rules to bolster the detection of hardware vulnerabilities. Thus, the semiconductor industry is swiftly integrating LLMs into its design workflows. However, this adoption is not without its challenges.While LLMs offer remarkable benefits, they concurrently introduce security concerns that demand a thorough examination. These concerns manifest as potential vulnerabilities indirectly introduced into the designs while generating the design code, or by directly equipping the attackers with novel avenues for exploitation. In this paper, we discuss the emerging security implications due to the capabilities introduced by LLMs in the context of hardware design verification, evaluate the capabilities of existing security detection and mitigation techniques, and highlight the possible future security attacks that use LLMs.","2024-04","2025-11-25 22:38:57","2025-11-25 22:38:57","","1-4","","","","","","","","","","","","","","","","","","","","ISSN: 2375-1053","","","","LLM; Industries; Codes; Source coding; verification; security; Very large scale integration; Design methodology; Electronics industry; hardware; Hardware security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3NU22TII","conferencePaper","2025","Hazott, Christoph; Große, Daniel","LLM-assisted Metamorphic Testing of Embedded Graphics Libraries","2025 Forum on Specification & Design Languages (FDL)","","","10.1109/FDL68117.2025.11165405","","Modern applications increasingly rely on embedded systems that incorporate visual interfaces developed utilizing so-called embedded graphics libraries. Verifying these embedded graphics libraries is challenging due to hardware dependencies and the lack of reference outputs. The lack of reference outputs is tackled in Metamorphic Testing (MT) by constructing two Firmware (FW) versions with distinct implementations that maintain the same input-output relationships. These relations are known as Metamorphic Relations (MRs). However, the development of these MRs remains a tedious and challenging task.In this paper, we present a novel approach for generating MRs for MT of embedded graphics libraries using Large Language Models (LLMs). Because directly creating MRs with simple prompts is too complex for the LLM, we employ proven prompting strategies to develop our LLM-assisted MR pipeline. Strategies include role prompting, least-to-most prompting, zero-shot prompting, constraint-based prompting, and style prompting. In our experiments, we verify a widely used embedded graphics library. We compare our results with an existing manual approach and demonstrate that LLM-assisted MRs nearly doubles coverage and identifies additional bugs.","2025-09","2025-11-25 22:38:57","2025-11-25 22:38:57","","1-10","","","","","","","","","","","","","","","","","","","","ISSN: 1636-9874","","","","Large language models; Manuals; Testing; Visualization; Computer bugs; Hardware; Libraries; Pipelines; Microprogramming; System analysis and design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C6BLS3DP","conferencePaper","2025","Babikian, Aren A.; Chen, Boqi; Mussbacher, Gunter","Exploring Large Language Models for Requirements on String Values","2025 IEEE/ACM Workshop on Multi-disciplinary, Open, and RElevant Requirements Engineering (MO2RE)","","","10.1109/MO2RE66661.2025.00009","","Behavior-driven development (BDD) enables collaboration among different stakeholders by employing a natural-language representation of system requirements and of test scenarios. These scenarios often involve constraints over string values, e.g. for the validity of email addresses, which are challenging to test comprehensively. Traditional methods like SMT solvers (e.g. Z3, Ostrich) handle constraints efficiently but produce unrealistic strings and require formal specifications that are often unavailable and expensive to compute. This paper explores the potential of large language models (LLMs) in generating realistic, constraint-satisfying strings for BDD. We propose an evaluation framework to assess LLMs’ ability to (1) generate consistent string values and (2) detect constraint inconsistencies. In our experiments, three LLMs are compared to state-of-the-art solvers using constraints from a software engineering course project. Results show that while solvers dominate in precision and recall, LLMs derive realistic strings more suitable for a requirements engineering context. With these trade-offs, we believe that, when formal constraints are available, a combined LLM-solver approach could offer a more effective solution.","2025-04","2025-11-25 22:38:57","2025-11-25 22:38:57","","17-23","","","","","","","","","","","","","","","","","","","","","","","","large language model; Large language models; Software engineering; Testing; Hybrid power systems; constraint solving; Natural languages; Requirements engineering; Stakeholders; Electronic mail; Formal specifications; Filtering; specification-based testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VIEGMSEL","conferencePaper","2025","Jain, Kush; Kate, Kiran; Tsay, Jason; Le Goues, Claire; Hirzel, Martin","Improving Examples in Web API Specifications using Iterated-Calls In-Context Learning","2025 IEEE/ACM International Conference on Automation of Software Test (AST)","","","10.1109/AST66626.2025.00016","","Examples in web API specifications can be essential for API testing, API understanding, and even building chat-bots for APIs. Unfortunately, most API specifications lack human-written examples. This paper introduces a novel technique for generating examples for web API specifications. We start from in-context learning (Icl): given an API parameter, use a prompt context containing a few examples from other similar API parameters to call a model to generate new examples. However, while ICL tends to generate correct examples, those lack diversity, which is also important for most downstream tasks. Therefore, we extend the technique to iterated-calls ICL (IcIcl): use a few different prompt contexts, each containing a few examples, to iteratively call the model with each context. Our intrinsic evaluation demonstrates that IcIcl improves both correctness and diversity of generated examples. More importantly, our extrinsic evaluation demonstrates that those generated examples significantly improve the performance of downstream tasks of testing, understanding, and chat-bots for APIs.","2025-04","2025-11-25 22:38:57","2025-11-25 22:38:57","","91-102","","","","","","","","","","","","","","","","","","","","ISSN: 2833-9061","","","","Software testing; Software; Automation; Buildings; llm; Filling; Context modeling; Intent recognition; openapi; rest","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZMYBY5CM","conferencePaper","2024","Dasgupta, Dipankar; Roy, Arunava","Issues with Generic Large Language Models (GLLMs)","2024 Artificial Intelligence for Business (AIxB)","","","10.1109/AIxB62249.2024.00015","","Generic Large Language Models (GLLMs) are continuously being released with enhanced size and capabilities, promoting the abilities of these tools for different use. GLLMs excel in text, image, and video generation (assembling, summarizing, translating) with proper queries and prompts. However, the reliability of GLLMs’ responses is questionable in critical applications due to factual inaccuracies, and inappropriate or unrelated responses. Also there remain many open questions on the data collection-privacy, legal and ethical issues. This short report emphasizes the reliability and security aspects of GLLMs while recognizing significant benefits in a wide variety of applications.","2024-12","2025-11-25 22:38:57","2025-11-25 22:38:57","","47-50","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Generative AI; Business; Large Language Models (LLMs); Reliability; Security; Ethics; Data models; Law; Generative Pre-Trained Models (GPTs); Small Parameterized Data Models (SPDM)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BL6U5IUQ","conferencePaper","2025","Honarvar, Shahin; van der Wilk, Mark; Donaldson, Alastair F.","Turbulence: Systematically and Automatically Testing Instruction-Tuned Large Language Models for Code","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST62969.2025.10989005","","We present a method for systematically evaluating the correctness and robustness of instruction-tuned large language models (LLMs) for code generation via a new benchmark, Turbulence. Turbulence consists of a large set of natural language question templates, each of which is a programming problem, parameterised so that it can be asked in many different forms. Each question template has an associated test oracle that judges whether a code solution returned by an LLM is correct. Thus, from a single question template, it is possible to ask an LLM a neighbourhood of very similar programming questions, and assess the correctness of the result returned for each question. This allows gaps in an LLM's code generation abilities to be identified, including anomalies where the LLM correctly solves almost all questions in a neighbourhood but fails for particular parameter instantiations. We present experiments against five LLMs from OpenAI, Cohere and Meta, each at two temperature configurations. Our findings show that, across the board, Turbulence is able to reveal gaps in LLM reasoning ability. This goes beyond merely highlighting that LLMs sometimes produce wrong code (which is no surprise): by systematically identifying cases where LLMs are able to solve some problems in a neighbourhood but do not manage to generalise to solve the whole neighbourhood, our method is effective at highlighting robustness issues. We present data and examples that shed light on the kinds of mistakes that LLMs make when they return incorrect code results.","2025-03","2025-11-25 22:38:57","2025-11-25 22:38:57","","80-91","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Software testing; Codes; Benchmark testing; Cognition; code generation; Programming; Robustness; Natural languages; robustness; AI evaluation; correctness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7WMV5ECX","conferencePaper","2024","Ramírez-Rueda, Rolando; Benítez-Guerrero, Edgard; Mezura-Godoy, Carmen; Bárcenas, Everardo","Transforming Software Development: A Study on the Integration of Multi-Agent Systems and Large Language Models for Automatic Code Generation","2024 12th International Conference in Software Engineering Research and Innovation (CONISOFT)","","","10.1109/CONISOFT63288.2024.00013","","This paper explores the integration of Multi-Agent Systems (MAS) and Large Language Models (LLMs) for auto-matic code generation, addressing the limitations of traditional manual coding. By conducting a comprehensive review of existing literature and analyzing a practical case study, we demonstrate how MAS and LLMs can collaboratively enhance software development processes. The research focuses on the technical and theoretical challenges of this integration, highlighting the potential for improved productivity, adaptability, and quality in code generation. The findings contribute to AI-based software engineering by revealing new research directions in collective intelligence and automated programming.","2024-10","2025-11-25 22:38:57","2025-11-25 22:38:57","","11-20","","","","","","","","","","","","","","","","","","","","","","","","Large language models; LLM; Software; Software engineering; Generative AI; Software development management; Codes; Technological innovation; Productivity; Software Development; Standards; Multi-agent systems; Collective intelligence; Multi-Agent","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8TUNTQDM","conferencePaper","2025","Zhou, Shiyao; Wang, Jincheng; Ye, He; Zhou, Hao; Goues, Claire Le; Luo, Xiapu","LWDIFF: an LLM-Assisted Differential Testing Framework for Webassembly Runtimes","2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)","","","10.1109/ICSE55347.2025.00233","","WebAssembly (Wasm) runtimes execute Wasm programs, a popular low-level language for efficiently executing high-level languages in browsers, with broad applications across diverse domains. The correctness of those runtimes is critical for both functionality and security of Wasm execution, motivating testing approaches that target Wasm runtimes specifically. However, existing Wasm testing frameworks fail to generate test cases that effectively test all three phases of runtime, i.e., decoding, validation, and execution. To address this research gap, we propose a new differential testing framework for Wasm runtimes, which leverages knowledge from the Wasm language specification that prior techniques overlooked, enhancing comprehensive testing of runtime functionality. Specifically, we first use a large language model to extract that knowledge from the specification. We use that knowledge in the context of multiple novel mutation operators that generate test cases with diverse features to test all three runtime phases. We evaluate LWDIFF by applying it to eight Wasm runtimes. Compared with the state-of-the-art Wasm testers, LWDIFF achieves the highest branch coverage and identifies the largest number of bugs. In total, LWDIFF discovers 31 bugs across eight runtimes, all of which are confirmed, with 25 of them previously undiscovered.","2025-04","2025-11-25 22:38:57","2025-11-25 22:38:57","","153-164","","","","","","","","","","","","","","","","","","","","ISSN: 1558-1225","","","","Large language models; Codes; Testing; Computer bugs; Security; Runtime; Feature extraction; Browsers; Decoding; High level languages","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J7WBSPBN","conferencePaper","2024","Wei, Bingyang","Requirements are All You Need: From Requirements to Code with LLMs","2024 IEEE 32nd International Requirements Engineering Conference (RE)","","","10.1109/RE59067.2024.00049","","The pervasive use of textual formats in the documentation of software requirements presents a great opportunity for applying large language models (LLMs) to software engineering tasks. High-quality software requirements not only enhance the manual software development process but also position organizations to fully harness the potential of the emerging LLMs technology. This paper introduces a tailored LLM for automating the generation of code snippets from well-structured requirements documents. This LLM is augmented with knowledge, heuristics, and instructions that are pertinent to the software development process, requirements analysis, object-oriented design, and test-driven development, effectively emulating the expertise of a seasoned software engineer. We introduce a “Progressive Prompting” method that allows software engineers to engage with this LLM in a stepwise manner. Through this approach, the LLM incrementally tackles software development tasks by interpreting the provided requirements to extract functional requirements, using these to create object-oriented models, and subsequently generating unit tests and code based on the object-oriented designs. We demonstrate the LLM's proficiency in comprehending intricate user requirements and producing robust design and code solutions through a case study focused on the development of a web project. This study underscores the potential of integrating LLMs into the software development workflow to significantly enhance both efficiency and quality. The tailored LLM is available at https://chat.openai.com/g/g-bahoiKzkB-software-engineer-gpt.","2024-06","2025-11-25 22:39:08","2025-11-25 22:39:08","","416-422","","","","","","","","","","","","","","","","","","","","ISSN: 2332-6441","","","","Software; ChatGPT; Codes; Knowledge engineering; Requirements Engineering; Large Language Models (LLMs); Code Generation; Refining; Requirements engineering; Software design; Object oriented modeling; Automated Software Engineering; Software Specification; Use Cases","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YQ8EMJN6","conferencePaper","2024","Cao, Duy; Nguyen, Phu; Le, Vy; Nguyen, Long; Nguyen, Vu","Try-Then-Eval: Equipping an LLM-based Agent with a Two-Phase Mechanism to Solve Computer Tasks","2024 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","","","10.1109/SMC54092.2024.10831260","","Building an autonomous intelligent agent capable of carrying out web automation tasks from descriptions in natural language offers a wide range of applications, including software testing, virtual assistants, and task automation in general. However, recent studies addressing this problem often require manually constructing of prior human demonstrations. In this paper, we approach the problem by leveraging the idea of reinforcement learning (RL) with the two-phase mechanism to form an agent using LLMs for automating computer tasks without relying on human demonstrations. We evaluate our LLM-based agent using the MiniWob++ dataset of web-based application tasks, showing that our approach achieves 85% success rate without prior demonstrations. The results also demonstrate the agent's capability of self-improvement through training.","2024-10","2025-11-25 22:39:08","2025-11-25 22:39:08","","1224-1229","","","","","","","","","","","","","","","","","","","","","","","","Software testing; Automation; Planning; Training; Natural languages; Reinforcement learning; Intelligent agents; Cybernetics; Supervised learning; Virtual assistants","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SV55SIHY","conferencePaper","2024","Paul, Debalina Ghosh; Zhu, Hong; Bayley, Ian","ScenEval: A Benchmark for Scenario-Based Evaluation of Code Generation","2024 IEEE International Conference on Artificial Intelligence Testing (AITest)","","","10.1109/AITest62860.2024.00015","","In the scenario-based evaluation of machine learning models, a key problem is how to construct test datasets that represent various scenarios. The methodology proposed in this paper is to construct a benchmark and attach metadata to each test case. Then a test system can be constructed with test morphisms that filter the test cases based on metadata to form a dataset. The paper demonstrates this methodology with large language models for code generation. A benchmark called ScenEval is constructed from problems in textbooks, an online tutorial website and Stack Overflow. Filtering by scenario is demonstrated and the test sets are used to evaluate ChatGPT for Java code generation. Our experiments found that the performance of ChatGPT decreases with the complexity of the coding task. It is weakest for advanced topics like multi-threading, data structure algorithms and recursive methods. The Java code generated by ChatGPT tends to be much shorter than reference solution in terms of number of lines, while it is more likely to be more complex in both cyclomatic and cognitive complexity metrics, if the generated code is correct. However, the generated code is more likely to be less complex than the reference solution if the code is incorrect.","2024-07","2025-11-25 22:39:08","2025-11-25 22:39:08","","55-63","","","","","","","","","","","","","","","","","","","","ISSN: 2835-3560","","","","Large language models; Performance evaluation; Java; Manuals; Machine learning; ChatGPT; Metadata; Codes; Measurement; Benchmark testing; Code Generation; Benchmark; Scenario-based testing; Tutorials","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F2RHGFMA","conferencePaper","2025","He, Zilong; Pan, Yijun; Li, Hebei; Ma, Feipeng; Peng, Yansong; Wu, Siying; Sun, Xiaoyan","Enhancing Visual Question Answering Via Clustered In-Context Sequence Configuration","2025 IEEE International Conference on Image Processing (ICIP)","","","10.1109/ICIP55913.2025.11084650","","Recent advances in Multimodal In-Context Learning (M-ICL) for Multimodal Large Language Models (MLLMs) have attracted considerable attention. These developments primarily focus on configuring an in-context sequence for a given test case based on instance-level semantic similarity. However, high similarity among demonstrations in the sequence introduces inductive biases, which may mislead MLLMs and ultimately degrade their overall performance. To address this, we propose a novel cluster-based in-context configuration method that adaptively groups candidate data and selects demonstrations from each cluster. This method enhances the diversity within the sequence while preserving semantic consistency, enabling MLLMs to focus on the main intent of the demonstrations. The experimental results on four Visual Question Answering (VQA) benchmarks, including OK-VQA, VQAv2, VizWiz, and TextVQA, demonstrate the effectiveness of our proposed method.","2025-09","2025-11-25 22:39:08","2025-11-25 22:39:08","","935-940","","","","","","","","","","","","","","","","","","","","ISSN: 2381-8549","","","","Large language models; Semantics; Visualization; Benchmark testing; Focusing; In-Context Learning; Question answering (information retrieval); Clustering Algorithm; Clustering algorithms; Diversity methods; Image processing; Inference algorithms; Multimodal Large Language Model; Similarity Retrieval","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F6UTZXIA","conferencePaper","2024","Guo, An; Zhou, Yuan; Tian, Haoxiang; Fang, Chunrong; Sun, Yunjian; Sun, Weisong; Gao, Xinyu; Luu, Anh Tuan; Liu, Yang; Chen, Zhenyu","SoVAR: Building Generalizable Scenarios from Accident Reports for Autonomous Driving Testing","2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE)","","","","","Autonomous driving systems (ADSs) have undergone remarkable development and are increasingly employed in safety-critical applications. However, recently reported data on fatal accidents involving ADSs suggests that the desired level of safety has not yet been fully achieved. Consequently, there is a growing need for more comprehensive and targeted testing approaches to ensure safe driving. Scenarios from real-world accident reports provide valuable resources for ADS testing, including critical scenarios and high-quality seeds. However, existing scenario reconstruction methods from accident reports often exhibit limited accuracy in information extraction. Moreover, due to the diversity and complexity of road environments, matching current accident information with the simulation map data for reconstruction poses significant challenges.In this paper, we design and implement SoVAR, a tool for automatically generating road-generalizable scenarios from accident reports. SoVAR utilizes well-designed prompts with linguistic patterns to guide the large language model (LLM) in extracting accident information from textual data. Subsequently, it formulates and solves accident-related constraints in conjunction with the extracted accident information to generate accident trajectories. Finally, SoVAR reconstructs accident scenarios on various map structures and converts them into test scenarios to evaluate its capability to detect defects in industrial ADSs. We experiment with SoVAR, using the accident reports from the National Highway Traffic Safety Administration’s (NHTSA) database to generate test scenarios for the industrial-grade ADS Apollo. The experimental findings demonstrate that SoVAR can effectively generate generalized accident scenarios across different road structures. Furthermore, the results confirm that SoVAR identified 5 distinct safety violation types that contributed to the crash of Baidu Apollo.CCS Concepts• Software and its engineering → Software testing and debugging.","2024-10","2025-11-25 22:39:09","2025-11-25 22:39:09","","268-280","","","","","","","","","","","","","","","","","","","","ISSN: 2643-1572","","","","Software testing; Software; Roads; Data mining; Safety; Computer crashes; Autonomous vehicles; Trajectory; Accidents; Automatic test generation; Autonomous driving system; Constraint solving; Linguistics; Web and internet services","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GU4X94HS","conferencePaper","2025","Rezazadeh, Farhad; Gargari, Amir Ashtari; Lagén, Sandra; Song, Houbing; Niyato, Dusit; Liu, Lingjia","Toward Generative 6G Simulation: An Experimental Multi-Agent LLM and ns-3 Integration","2025 IEEE International Mediterranean Conference on Communications and Networking (MeditCom)","","","10.1109/MeditCom64437.2025.11104374","","The move toward open Sixth-Generation (6G) networks necessitates a novel approach to full-stack simulation environments for evaluating complex technology developments before prototyping and real-world implementation. This paper introduces an innovative approach11 A lightweight, mock version of the code is available on GitHub at https://github.com/frezazadeh/LangChain-RAG-Technology that combines a multi-agent framework with the Network Simulator 3 (ns-3) to automate and optimize the generation, debugging, execution, and analysis of complex Fifth-Generation (5G) network scenarios. Our framework orchestrates a suite of specialized agents—namely, the Simulation Generation Agent, Test Designer Agent, Test Executor Agent, and Result Interpretation Agent—using advanced LangChain coordination. The Simulation Generation Agent employs a structured chain-of-thought (CoT) reasoning process, leveraging large language models (LLMs) and retrieval-augmented generation (RAG) to translate natural language simulation specifications into precise ns-3 scripts. Concurrently, the Test Designer Agent generates comprehensive automated test suites by integrating knowledge retrieval techniques with dynamic test case synthesis. The Test Executor Agent dynamically deploys and runs simulations, managing dependencies and parsing detailed performance metrics. At the same time, the Result Interpretation Agent utilizes LLM-driven analysis to extract actionable insights from the simulation outputs. By integrating external resources such as library documentation and ns-3 testing frameworks, our experimental approach can enhance simulation accuracy and adaptability, reducing reliance on extensive programming expertise. A detailed case study using the ns-3 5G-LENA module validates the effectiveness of the proposed approach. The code generation process converges in an average of 1.8 iterations, has a syntax error rate of 17.0%, a mean response time of 7.3 seconds, and receives a human evaluation score of 7.5.","2025-07","2025-11-25 22:39:09","2025-11-25 22:39:09","","1-6","","","","","","","","","","","","","","","","","","","","","","","","Syntactics; Software development management; Codes; Standards; RAG; Time factors; Translation; Retrieval augmented generation; Transforms; 5G mobile communication; 5G/6G; 6G mobile communication; chain-of-thought; generative simulation; multi-agent LLM; ns-3","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9UGLKQYK","conferencePaper","2025","Jalil, Sajed","The Transformative Influence of LLMs on Software Development & Developer Productivity","2025 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)","","","10.1109/ACDSA65407.2025.11166110","","The increasing adoption and commercialization of generalized Large Language Models (LLMs) have profoundly impacted various aspects of our daily lives. Initially embraced by the computer science community, the versatility of LLMs has found its way into diverse domains. In particular, the software engineering realm has seen the most transformative changes. With LLMs increasingly serving as AI Pair Programming Assistants spurred the development of specialized models aimed at aiding software engineers. Although this new paradigm offers numerous advantages, it presents critical challenges and open problems.To identify the potential and prevailing obstacles, we systematically reviewed contemporary scholarly publications, emphasizing the perspectives of software developers and usability concerns. Preliminary findings underscore pressing concerns about data privacy, bias, and misinformation. Additionally, we identified several usability challenges, including prompt engineering, increased cognitive demands, and mistrust. Finally, we introduce 12 open problems identified through our survey, covering these domains.","2025-08","2025-11-25 22:39:09","2025-11-25 22:39:09","","1-10","","","","","","","","","","","","","","","","","","","","","","","","large language model; Large language models; Software; Software engineering; Software development management; Prompt engineering; Technological innovation; Surveys; survey; software engineering; Productivity; Programming; developer productivity; Usability; ai pair programming; usability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z77BJDX9","journalArticle","2024","Ahmad, Baleegh; Thakur, Shailja; Tan, Benjamin; Karri, Ramesh; Pearce, Hammond","On Hardware Security Bug Code Fixes by Prompting Large Language Models","IEEE Transactions on Information Forensics and Security","","1556-6021","10.1109/TIFS.2024.3374558","","Novel AI-based code-writing Large Language Models (LLMs) such as OpenAI’s Codex have demonstrated capabilities in many coding-adjacent domains. In this work, we consider how LLMs may be leveraged to automatically repair identified security-relevant bugs present in hardware designs by generating replacement code. We focus on bug repair in code written in Verilog. For this study, we curate a corpus of domain-representative hardware security bugs. We then design and implement a framework to quantitatively evaluate the performance of any LLM tasked with fixing the specified bugs. The framework supports design space exploration of prompts (i.e., prompt engineering) and identifying the best parameters for the LLM. We show that an ensemble of LLMs can repair all fifteen of our benchmarks. This ensemble outperforms a state-of-the-art automated hardware bug repair tool on its own suite of bugs. These results show that LLMs have the ability to repair hardware security bugs and the framework is an important step towards the ultimate goal of an automated end-to-end bug repair tool.","2024","2025-11-25 22:39:09","2025-11-25 22:39:09","","4043-4057","","","19","","","","","","","","","","","","","","","","","","","","","Software; Codes; Computer bugs; large language models; Hardware; Security; Maintenance engineering; Registers; Hardware security; bug repair","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CIZFF53J","conferencePaper","2025","Zhang, Jiaming; Sun, Chang-ai; Liu, Huai; Dong, Sijin","Can Large Language Models Discover Metamorphic Relations? A Large-Scale Empirical Study","2025 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)","","","10.1109/SANER64311.2025.00011","","Software testing is a mainstream approach for software quality assurance. One fundamental challenge for testing is that in many practical situations, it is very difficult to verify the correctness of test results given inputs for Software Under Test (SUT), which is known as the oracle problem. Metamorphic Testing (MT) is a software testing technique that can effectively alleviate the oracle problem. The core component of MT is a set of Metamorphic Relations (MRs), which are basically the necessary properties of SUT, represented in the form of relationship among multiple inputs and their corresponding expected outputs. Different methods have been proposed to support the systematic MR identification. However, most of them still rely heavily on test engineers' understanding of the SUT and involve massive manual work. Although a few preliminary studies have shown LLMs' viability in generating MRs, there does not exist a thorough and in-depth investigation on their capability in MR identification. We are thus motivated to conduct a comprehensive and large-scale empirical study to systematically evaluate the performance of LLMs in identifying appropriate MRs for a wide variety of software systems. This study makes use of 37 SUTs collected from previous MT studies. Prompts are constructed for two LLMs, gpt-3.5-turbo-1106 and gpt-4-1106-preview, to perform the MR identification for each SUT. The empirical results demonstrate that both LLMs can generate a large amount of MR candidates (MRCs). Among them, 29.86% and 43.79% of all MRCs are identified as the MRs valid for the corresponding SUT, respectively. In addition, 24.59% and 38.63% of all MRCs are MRs that had never been identified in previous studies. Our study not only reinforces LLM-based MR identification as a promising research direction for MT, but also provides some practical guidelines for how to further improve LLMs' performance in generating good MRs.","2025-03","2025-11-25 22:39:09","2025-11-25 22:39:09","","24-35","","","","","","","","","","","","","","","","","","","","ISSN: 2640-7574","","","","Large language models; Software testing; Manuals; Software quality; Software systems; large language models; empirical study; metamorphic relation; Metamorphic testing; Guidelines; oracle problem; Systematics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H3YDKE2X","conferencePaper","2023","Tsigkanos, Christos; Rani, Pooja; Müller, Sebastian; Kehrer, Timo","Large Language Models: The Next Frontier for Variable Discovery within Metamorphic Testing?","2023 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)","","","10.1109/SANER56733.2023.00070","","Metamorphic testing involves reasoning on necessary properties that a program under test should exhibit regarding multiple input and output variables. A general approach consists of extracting metamorphic relations from auxiliary artifacts such as user manuals or documentation, a strategy particularly fitting to testing scientific software. However, such software typically has large input-output spaces, and the fundamental prerequisite – extracting variables of interest – is an arduous and non-scalable process when performed manually. To this end, we devise a workflow around an autoregressive transformer-based Large Language Model (LLM) towards the extraction of variables from user manuals of scientific software. Our end-to-end approach, besides a prompt specification consisting of few-shot examples by a human user, is fully automated, in contrast to current practice requiring human intervention. We showcase our LLM workflow over a real case, and compare variables extracted to ground truth manually labelled by experts. Our preliminary results show that our LLM-based workflow achieves an accuracy of 0.87, while successfully deriving 61.8% of variables as partial matches and 34.7% as exact matches.","2023-03","2025-11-25 22:39:09","2025-11-25 22:39:09","","678-682","","","","","","","","","","","","","","","","","","","","ISSN: 2640-7574","","","","Analytical models; Manuals; Software; Natural Language Processing; Large Language Models; Cognition; Transformers; Metamorphic Testing; Documentation; Fitting; Scientific Software","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"24FPK6IL","conferencePaper","2025","Li, Jiageng; Dong, Zhen; Wang, Chong; You, Haozhen; Zhang, Cen; Liu, Yang; Peng, Xin","LLM Based Input Space Partitioning Testing for Library APIs","2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)","","","10.1109/ICSE55347.2025.00153","","Automated library APIs testing is difficult as it requires exploring a vast space of parameter inputs that may involve objects with complex data types. Existing search based approaches, with limited knowledge of relations between object states and program branches, often suffer from the low efficiency issue, i.e., tending to generate invalid inputs. Symbolic execution based approaches can effectively identify such relations, but fail to scale to large programs. In this work, we present an LLM-based input space partitioning testing approach, LISP, for library APIs. The approach lever-ages LLMs to understand the code of a library API under test and perform input space partitioning based on its understanding and rich common knowledge. Specifically, we provide the signature and code of the API under test to LLMs, with the expectation of obtaining a text description of each input space partition of the API under test. Then, we generate inputs through employing the generated text description to sample inputs from each partition, ultimately resulting in test suites that systematically explore the program behavior of the API. We evaluate LISP on more than 2,205 library API meth-ods taken from 10 popular open-source Java libraries (e.g., apache/commons-lang with 2.6k stars, guava with 48.8k stars on GitHub). Our experiment results show that LISP is effective in library API testing. It significantly outperforms state-of-the-art tool EvoSuite in terms of edge coverage. On average, LISP achieves 67.82 % branch coverage, surpassing EvoSuite by 1.21 times. In total, LISP triggers 404 exceptions or errors in the experiments, and discovers 13 previously unknown vulnerabilities during evaluation, which have been assigned CVE IDs.","2025-04","2025-11-25 22:39:09","2025-11-25 22:39:09","","1436-1448","","","","","","","","","","","","","","","","","","","","ISSN: 1558-1225","","","","Large language models; Java; Software engineering; Software development management; Codes; Testing; Large Language Models; Symbolic Execution; API testing; Libraries; Search problems; Space exploration; Input Space Partitioning Testing; Stars","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AH4LUW7W","conferencePaper","2025","Wang, Haibo; Xu, Zhuolin; Tan, Shin Hwei","Testing Refactoring Engine via Historical Bug Report driven LLM","2025 IEEE/ACM Second International Conference on AI Foundation Models and Software Engineering (Forge)","","","10.1109/Forge66646.2025.00020","","Refactoring is the process of restructuring existing code without changing its external behavior while improving its internal structure. Refactoring engines are integral components of modern Integrated Development Environments (IDEs) and can automate or semi-automate this process to enhance code readability, reduce complexity, and improve the maintainability of software products. Similar to traditional software systems such as compilers, refactoring engines may also contain bugs that can lead to unexpected behaviors. In this paper, we propose a novel approach called RETester, a LLM-based framework for automated refactoring engine testing. Specifically, by using input program structure templates extracted from historical bug reports and input program characteristics that are error-prone, we design chain-of-thought (CoT) prompts to perform refactoring-preserving transformations. The generated variants are then tested on the latest version of refactoring engines using differential testing. We evaluate RETester on two most popular modern refactoring engines (i.e., Eclipse, and IntelliJ IDEA). It successfully revealed 18 previously unknown bugs in the latest version of those refactoring engines, seven of them have been confirmed by their developers, and three have been fixed.","2025-04","2025-11-25 22:39:09","2025-11-25 22:39:09","","113-124","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Test pattern generators; Software engineering; Software systems; Codes; Testing; Computer bugs; Test generation; Refactoring; Complexity theory; Engines; Foundation models; Bug detection; Refactoring engine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NAU8ZYYE","conferencePaper","2024","Widyasari, Ratnadira; Ang, Jia Wei; Nguyen, Truong Giang; Sharma, Neil; Lo, David","Demystifying Faulty Code: Step-by-Step Reasoning for Explainable Fault Localization","2024 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)","","","10.1109/SANER60148.2024.00064","","Fault localization is a critical process that involves identifying specific program elements responsible for program failures. Manually pinpointing these elements, such as classes, methods, or statements, which are associated with a fault is laborious and time-consuming. To overcome this challenge, various fault localization tools have been developed. These tools typically generate a ranked list of suspicious program elements. However, this information alone is insufficient. A prior study emphasized that automated fault localization should offer a rationale. In this study, we investigate the step-by-step reasoning for explainable fault localization. We explore the potential of Large Language Models (LLM) in assisting developers in reasoning about code. We proposed FuseFL that utilizes several combinations of information to enhance the LLM results which are spectrum-based fault localization results, test case execution outcomes, and code description (i.e., explanation of what the given code is intended to do). We conducted our investigation using faulty code from Refactory dataset. First, we evaluate the performance of the automated fault localization. Our results demonstrate a 32.3 % increase in the number of successfully localized faults at Top-1 compared to the baseline. To evaluate the explanations generated by FuseFL, we create a dataset of human explanations that provide step-by-step reasoning as to why specific lines of code are considered faulty. This dataset consists of 324 faulty code files, along with explanations for 600 faulty lines. Furthermore, we also conducted human studies to evaluate the explanations. We found that for 22 out of the 30 randomly sampled cases, FuseFL generated correct explanations.","2024-03","2025-11-25 22:39:09","2025-11-25 22:39:09","","568-579","","","","","","","","","","","","","","","","","","","","ISSN: 2640-7574","","","","Large language models; LLM; Software; ChatGPT; Codes; Location awareness; Fault diagnosis; Cognition; dataset; fault localization; explanation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VNB4B6GI","journalArticle","2024","Zhang, Qiang; Shen, Yuheng; Liu, Jianzhong; Xu, Yiru; Shi, Heyuan; Jiang, Yu; Chang, Wanli","ECG: Augmenting Embedded Operating System Fuzzing via LLM-Based Corpus Generation","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","1937-4151","10.1109/TCAD.2024.3447220","","Embedded operating systems (Embedded OSs) power much of our critical infrastructure but are, in general, much less tested for bugs than general-purpose operating systems. Fuzzing Embedded OSs encounter significant roadblocks due to much less documented specifications, an inherent ineffectiveness in generating high-quality payloads. In this article, we propose ECG, an Embedded OS fuzzer empowered by large language models (LLMs) to sufficiently mitigate the aforementioned issues. ECG approaches fuzzing Embedded OS by automatically generating input specifications based on readily available source code and documentation, instrumenting and intercepting execution behavior for directional guidance information, and generating inputs with payloads according to the pregenerated input specifications and directional hints provided from previous runs. These methods are empowered by using an interactive refinement method to extract the most from LLMs while using established parsing checkers to validate the outputs. Our evaluation results demonstrate that ECG uncovered 32 new vulnerabilities across three popular open-source Embedded OS (RT-Linux, RaspiOS, and OpenWrt) and detected ten bugs in a commercial Embedded OS running on an actual device. Moreover, compared to Syzkaller, Moonshine, KernelGPT, Rtkaller, and DRLF, ECG has achieved additional kernel code coverage improvements of 23.20%, 19.46%, 10.96%, 15.47%, and 11.05%, respectively, with an overall average improvement of 16.02%. These results underscore ECG’s enhanced capability in uncovering vulnerabilities, thus contributing to the overall robustness and security of the Embedded OS.","2024-11","2025-11-25 22:39:09","2025-11-25 22:39:09","","4238-4249","","11","43","","","","","","","","","","","","","","","","","","","","","Large language models; Source coding; Computer bugs; Fuzzing; Security; vulnerability detection; Robustness; Payloads; Electrocardiography; Embedded operating system (Embedded OS); fuzz testing; Integrated circuits; Kernel","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GYJXV3L5","conferencePaper","2025","P, Kumar; A, Sandeep Abinash; Dineshkumar, Mithun","LLM-Based Autonomous Automotive Testing Simulation with Claude 3.5 Sonnet","2025 11th International Conference on Communication and Signal Processing (ICCSP)","","","10.1109/ICCSP64183.2025.11088758","","The development of autonomous driving technologies has experienced rapid growth over the last few years, promising to transform the transportation industry. Since these cars are expected to drive in dynamic and complex conditions, rigorous verification and validation of the underlying algorithms are required to ensure their safe and reliable operation, particularly in mission-critical applications where human lives can be at risk or development expenses are too high. Traditional testing methods, for example, on-road testing, have proven to be time-consuming and costly, prompting researchers to look for alternative solutions, for example, scenario-based testing. A promising alternative is the application of simulation frameworks, which enable the development and testing of a large range of testing conditions without the risks and limitations of real-world testing. To this end, the CARLA simulator, coupled with artificial intelligence-based code generation tools, presents a safe solution for autonomous vehicle testing, enabling the simulation of complex driving scenarios and vehicle performance testing with a stochastic driver model. The approach utilizes test cases that are generated using an NLP based algorithm using autonomous vehicle disengagement reports as datasets and uses an AI-based code generate coded models that can be run using the CARLA Simulator. This paper explores the effectiveness of this approach in testing lane departure correction systems, thus presenting a model for the extensive testing and validation of autonomous driving technologies.","2025-06","2025-11-25 22:39:09","2025-11-25 22:39:09","","25-30","","","","","","","","","","","","","","","","","","","","ISSN: 2836-1873","","","","Large language models; Natural language processing; Codes; Testing; Safety; Heuristic algorithms; Autonomous vehicles; Artificial Intelligence based code generator; Automobiles; Autonomous Vehicle Disengagement Report; Autonomous Vehicle System; CARLA Simulator; Sensor Data; Vehicle dynamics; Vehicles","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2UTAADDJ","conferencePaper","2023","Shah, Hardik; Kamuni, Navin","DesignSystemsJS - Building a Design Systems API for aiding standardization and AI integration","2023 International Conference on Computing, Networking, Telecommunications & Engineering Sciences Applications (CoNTESA)","","","10.1109/CoNTESA61248.2023.10384889","","In the rapidly evolving landscape of software development, Design Systems have emerged as crucial frameworks that enforce consistency and efficiency in user interface (UI) and user experience (UX) design. However, the implementation of these systems varies significantly across teams, leading to discrepancies in understanding and applying design requirements. This inconsistency often results in divergent costs, efforts, and timelines in software projects. Our research introduces “DesignSystemsJS,” a novel JavaScript library, to address these challenges. Leveraging the widespread popularity and flexibility of JavaScript, widely recognized as a dominant programming language in both client and server-side web applications, DesignSystemsJS aims to standardize Design Systems requirements. It does so by enforcing a JSON format specification for defining the Design System, based on a meticulously researched checklist. This standardization is crucial in a domain where software reuse and the evolution of JavaScript applications often pose challenges related to third-party dependencies. Furthermore, DesignSystemsJS aims to support Artificial Intelligence (AI) integration, reflecting the ongoing paradigm shift in the software development industry towards generative AI assistants. AI integration in software testing and development has seen significant advancements, offering various approaches and tools to enhance efficiency and effectiveness. Our library taps into these advancements, utilizing AI techniques like data mining and machine learning to promote automated software reuse, a key aspect of modern software development. DesignSystemsJS’s architecture and API facilitate a more unified and streamlined approach to designing and implementing Design Systems, potentially revolutionizing how design consistency is maintained across different platforms and tools. This research paper details the methodology behind DesignSystemsJS, its features, and the potential impact it holds for the future of Design Systems development and AI integration.","2023-12","2025-11-25 22:39:09","2025-11-25 22:39:09","","83-89","","","","","","","","","","","","","","","","","","","","","","","","Software testing; Industries; Machine learning; Libraries; User interfaces; User experience; Design Systems; Design Systems AI integration; Design Systems API; DesignSystemsJS; Standardization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C462AWBJ","conferencePaper","2025","Velasco, Alejandro; Garryyeva, Aya; Palacio, David N.; Mastropaolo, Antonio; Poshyvanyk, Denys","Toward Neurosymbolic Program Comprehension","2025 IEEE/ACM 33rd International Conference on Program Comprehension (ICPC)","","","10.1109/ICPC66645.2025.00047","","Recent advancements in Large Language Models (LLMs) have paved the way for Large Code Models (LCMs), enabling automation in complex software engineering tasks, such as code generation, software testing, and program comprehension, among others. Tools like GitHub Copilot and ChatGPT have shown substantial benefits in supporting developers across various practices. However, the ambition to scale these models to trillion-parameter sizes, exemplified by GPT-4, poses significant challenges that limit the usage of Artificial Intelligence (AI)-based systems powered by large Deep Learning (DL) models. These include rising computational demands for training and deployment and issues related to trustworthiness, bias, and interpretability. Such factors can make managing these models impractical for many organizations, while their “black-box” nature undermines key aspects, including transparency and accountability. In this paper, we question the prevailing assumption that increasing model parameters is always the optimal path forward, provided there is sufficient new data to learn additional patterns. In particular, we advocate for a Neurosymbolic research direction that combines the strengths of existing DL techniques (e.g., LLMs) with traditional symbolic methods-renowned for their reliability, speed, and determinism. To this end, we outline the core features and present preliminary results for our envisioned approach, aimed at establishing the first Neurosymbolic Program Comprehension (NsPC) framework to aid in identifying defective code components.","2025-04","2025-11-25 22:39:09","2025-11-25 22:39:09","","377-381","","","","","","","","","","","","","","","","","","","","ISSN: 2643-7171","","","","Large language models; Software testing; Software engineering; Software development management; Codes; Reliability; Vulnerability Detection; Program Comprehension; Deep learning; Training; Computational modeling; Interpretability; Organizations; Neuro-Symbolic AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A4AWRZS6","conferencePaper","2024","Jahić, Jasmin; Sami, Ashkan","State of Practice: LLMs in Software Engineering and Software Architecture","2024 IEEE 21st International Conference on Software Architecture Companion (ICSA-C)","","","10.1109/ICSA-C63560.2024.00059","","Large Language Models (LLMs) are finding their way into Software Engineering by assisting with tasks such as code generation. Furthermore, LLMs might have a potential to perform even more complex tasks, such as suggesting architectural design. However, there is a lack of empirical surveys on how software engineering companies use (and plan to use) LLMs and if LLMs truly can provide benefits to software architects. To understand the state of practice considering adoption of LLMs in software engineering, existing challenges, and future trends, we have surveyed 15 different software engineering companies. To understand the ability of LLMs to perform more complex tasks, we report on our experiments with LLM-assisted architectural design. We applied ChatGPT on 5 software projects and in total performed 50 different experiments. Our results capture the state of the practice of LLMs in software engineering and demonstrate how LLMs perform when assisting with (more complex task such as) architectural design. Engineers, architects, and project managers should profit from these results to guide their decision towards targeted adoption of LLMs in their business and engineering domains.","2024-06","2025-11-25 22:39:09","2025-11-25 22:39:09","","311-318","","","","","","","","","","","","","","","","","","","","ISSN: 2768-4288","","","","Large language models; Chatbots; ChatGPT; Market research; Codes; AI; Companies; Surveys; Architecture; Design Space Exploration; Software architecture","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5I3HI6A5","conferencePaper","2025","Bradbury, Jeremy S.; More, Riddhi","Addressing Data Leakage in HumanEval Using Combinatorial Test Design","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST62969.2025.10989022","","The use of large language models (LLMs) is widespread across many domains, including Software Engineering, where they have been used to automate tasks such as program generation and test classification. As LLM-based methods continue to evolve, it is important that we define clear and robust methods that fairly evaluate performance. Benchmarks are a common approach to assess LLMs with respect to their ability to solve problem-specific tasks as well as assess different versions of an LLM to solve tasks over time. For example, the HumanEval benchmark is composed of 164 hand-crafted tasks and has become an important tool in assessing LLM-based program generation. However, a major barrier to a fair evaluation of LLMs using benchmarks like HumanEval is data contamination resulting from data leakage of benchmark tasks and solutions into the training data set. This barrier is compounded by the black-box nature of LLM training data which makes it difficult to even know if data leakage has occurred. To address the data leakage problem, we propose a new benchmark construction method where a benchmark is composed of template tasks that can be instantiated into new concrete tasks using combinatorial test design. Concrete tasks for the same template task must be different enough that data leakage has minimal impact and similar enough that the tasks are interchangeable with respect to performance evaluation. To assess our benchmark construction method, we propose HumanEval_T, an alternative benchmark to HumanEval that was constructed using template tasks and combinatorial test design.","2025-03","2025-11-25 22:39:09","2025-11-25 22:39:09","","587-591","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Performance evaluation; Software engineering; Benchmark testing; software engineering; Large Language Models (LLMs); benchmark; program generation; evaluation; fairness; Training data; Combinatorial testing; Closed box; combinatorial testing; Contamination; template","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NTAXC5HF","conferencePaper","2025","Rantanen, P.; Saari, M.; Virta, U-T.; Abrahamsson, P.","Toward AI Evaluation of Student Essays","2025 MIPRO 48th ICT and Electronics Convention","","","10.1109/MIPRO65660.2025.11132037","","The integration of artificial intelligence (AI) into teaching practices has expanded rapidly, and there is increasing discussion about computer-assisted assessment. This study investigates the application of AI for evaluating student essays, focusing on a Software Testing course at Tampere University. Traditionally, on this course essays are assessed manually by teaching assistants using predefined evaluation criteria. This research explores the feasibility of automating the evaluation process by comparing assessments generated by AI models—OpenAI's large language models (LLM) API—with human evaluations. The study focuses on the research question: Can automated assessment replace human evaluation? The key stages in this study include selecting a suitable AI model, its implementation and configuration, and designing an automated assessment workflow that aligns with human evaluation. The results indicate that, while AI models can evaluate at a level comparable to humans, current language models have limitations, such as consistency issues and an inability to interpret context. The findings of this study suggest that AI can be used as complementary tools rather than fully replacing humans. This work highlights the potential of AI in educational assessment while acknowledging the need for further refinement in model performance. Future directions include exploring open-source models and addressing the technical challenges related to resource requirements.","2025-06","2025-11-25 22:39:27","2025-11-25 22:39:27","","729-734","","","","","","","","","","","","","","","","","","","","ISSN: 1847-3938","","","","Large language models; Software testing; Education; Artificial intelligence; Focusing; AI; automatic evaluation; Uncertainty; Context modeling; student essays","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DWKJZLHG","conferencePaper","2025","Hayawi, Kadhim; Shahriar, Sakib","Dear LLM, Why Are They Complaining? On Root Cause Analysis (RCA) In Customer Service Automation Using Large Language Models (LLMS)","2025 IEEE 5th International Conference on Human-Machine Systems (ICHMS)","","","10.1109/ICHMS65439.2025.11154153","","The goal of Root Cause Analysis (RCA) is to determine the underlying concern from a host of concerns. In customer service, RCA can help organizations identify fundamental issues in customer complaints and dissatisfaction. Traditional RCA methods relied on human analysis, often prone to subjectivity and bias. Moreover, processing a large volume of complaint narratives demands significant resources. With datadriven approaches like machine learning, the efficiency of RCA has improved. However, these approaches require domain expertise and training data for accurate performance. Large Language Models (LLMs) can mitigate this challenge by learning from limited examples through few-shot learning or without any examples through zero-shot learning. This research introduces a dataset for RCA detection collected from realworld customer complaint narratives across diverse issues. Six state-of-the-art LLMs, including OpenAI's o1, GPT-4o, Sonnet, and Gemini, were evaluated on their performance in detecting RCA. Llama performed the best in open-ended RCA (55%) whereas o1 performed the best in multiclass RCA (68%). Our findings highlight the potential of LLMs in automating RCA. The limitations of current approaches, like model interpretability and domain-specific generalization, were discussed along with future research directions to enhance RCA capabilities further using LLMs.","2025-05","2025-11-25 22:39:27","2025-11-25 22:39:27","","338-343","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Automation; Accuracy; Large Language Models; Root Cause Analysis; Training data; Few shot learning; Customer services; Organizations; Root cause analysis; Zero shot learning; Contextual Learning; Customer Service Automation; Few-shot Learning; Human-machine systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8VQ5YPCB","conferencePaper","2025","Sivasothy, Shangeetha; Barnett, Scott; Kurniawan, Stefanus; Rasool, Zafaryab; Vasa, Rajesh","RAGProbe: Breaking RAG Pipelines with Evaluation Scenarios","2025 IEEE/ACM 4th International Conference on AI Engineering – Software Engineering for AI (CAIN)","","","10.1109/CAIN66642.2025.00015","","Retrieval Augmented Generation (RAG) is increasingly employed in building Generative AI applications, yet their evaluation often relies on manual, trial-and-error processes. Automating this evaluation process involves generating test data to trigger failures involving context comprehension, data formatting, specificity, and content completeness. Random question-answer generation is insufficient. However, prior works rely on standard QA datasets, benchmarks and tactics that are not tailored to the specific domain requirements. Hence, current approaches and datasets do not trigger sufficiently broad and context-specific failures. In this paper, we introduce evaluation scenarios that describe the process of generating question-answer pairs from content indexed by RAG pipelines, and they are designed to trigger a wider range of failures and to simplify automation. This enables developers to identify and address weaknesses more effectively. We validate our approach on five open-source RAG pipelines using three datasets. Our approach triggers high failure rates, by generating prompts that combine multiple questions (up to 91% failure rate) highlighting the need for developers to prioritize handling such queries. We generated failure rates of 60% in an academic domain dataset and 53% and 64% in open-domain datasets. Compared to existing state-of-the-art methods, our approach triggers 77% more failures on average per RAG pipeline and 53% more failures on average per dataset, offering a mechanism to support developers to improve the RAG pipeline quality.","2025-04","2025-11-25 22:39:27","2025-11-25 22:39:27","","60-71","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Manuals; Software; Software engineering; Generative AI; Large Language Models; Benchmark testing; Buildings; Retrieval Augmented Generation; Standards; Pipelines; Retrieval augmented generation; Software Evaluation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WF3NUC4J","conferencePaper","2025","Khare, Avishree; Dutta, Saikat; Li, Ziyang; Solko-Breslin, Alaia; Alur, Rajeev; Naik, Mayur","Understanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST62969.2025.10988968","","Security vulnerabilities in modern software are prevalent and harmful. While automated vulnerability detection techniques have made promising progress, their scalability and applicability remain challenging. The remarkable performance of Large Language Models (LLMs), such as GPT-4 and CodeLlama, on code-related tasks has prompted recent works to explore if LLMs can be used to detect security vulnerabilities. In this paper, we perform a more comprehensive study by examining a larger and more diverse set of datasets, languages, and LLMs, and qualitatively evaluating detection performance across prompts and vulnerability classes. Concretely, we evaluate the effectiveness of 16 pre-trained LLMs on 5,000 code samples-1,000 randomly selected each from five diverse security datasets. These balanced datasets encompass synthetic and real-world projects in Java and C/C++ and cover 25 distinct vulnerability classes. Our results show that LLMs across all scales and families show modest effectiveness in end-to-end reasoning about vul-nerabilities, obtaining an average accuracy of 62.8% and F1 score of 0.71 across all datasets. LLMs are significantly better at detecting vulnerabilities that typically only need intra-procedural reasoning, such as OS Command Injection and NULL Pointer Dereference. Moreover, LLMs report higher accuracies on these vulnerabilities than popular static analysis tools, such as CodeQL. We find that advanced prompting strategies that involve step-by-step analysis significantly improve performance of LLMs on real-world datasets in terms of F1 score (by up to 0.18 on average). Interestingly, we observe that LLMs show promising abilities at performing parts of the analysis correctly, such as identifying vulnerability-related specifications (e.g., sources and sinks) and leveraging natural language information to understand code behavior (e.g., to check if code is sanitized). We believe our insights can motivate future work on LLM-augmented vulnerability detection systems.","2025-03","2025-11-25 22:39:27","2025-11-25 22:39:27","","103-114","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Software testing; Java; Software; Static analysis; Accuracy; Codes; Scalability; Cognition; Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9ADFKRWJ","conferencePaper","2023","Elvira, Timothy; Procko, Tyler Thomas; Couder, Juan Ortiz; Ochoa, Omar","Digital Rubber Duck: Leveraging Large Language Models for Extreme Programming","2023 Congress in Computer Science, Computer Engineering, & Applied Computing (CSCE)","","","10.1109/CSCE60160.2023.00051","","The recent prevalence of Large Language models (LLMs), e.g., GPT-3.5 and GPT-4, has brought about a new age of man-computer symbiosis, where LLMs are employed for a litany of creative, constructive, scientific, or otherwise content-generative tasks, e.g., as general chatbot assistants, writing editors, digital subject matter experts, programming consultants, and so on. Of interest to software engineers is the concept of “rubber duck debugging”, which is the act of expressing code, line-by-line, in natural language, to an inanimate object, e.g., a rubber duck, for the purpose of elucidating potential issues that can then be corrected. In this paper, we detail a workflow process that leverages the concept of rubber duck debugging, replacing the duck with a capable LLM, e.g., GPT-4. We call it Digital Rubber Duck Programming. Furthermore, the Extreme Programming (XP) method, an implementation of the Agile paradigm, is considered as easily integrated with the proposed workflow, as XP is performed in pairs (much like the modern software engineer works in pairwise fashion with an LLM) and because XP places emphasis on performing extensive code reviews and unit testing all code, which capable LLMs like GPT-4 can facilitate.","2023-07","2025-11-25 22:39:27","2025-11-25 22:39:27","","295-304","","","","","","","","","","","","","","","","","","","","","","","","Software; Chatbots; Codes; Debugging; Software Engineering; GPT-4; Programming; Natural languages; Process control; ChatGPT Paper Type - “Regular research Paper”; Code Refactoring; Extreme Programmaing; Walkthroughs","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6YJU8Y3T","conferencePaper","2025","Takerngsaksiri, Wannita; Pasuksmit, Jirat; Thongtanunam, Patanamon; Tantithamthavorn, Chakkrit; Zhang, Ruixiong; Jiang, Fan; Li, Jing; Cook, Evan; Chen, Kun; Wu, Ming","Human-In-The-Loop Software Development Agents","2025 IEEE/ACM 47th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)","","","10.1109/ICSE-SEIP66354.2025.00036","","Recently, Large Language Models (LLMs)-based multi-agent paradigms for software engineering are introduced to automatically resolve software development tasks (e.g., from a given issue to source code). However, existing work is evaluated based on historical benchmark datasets, rarely considers human feedback at each stage of the automated software development process, and has not been deployed in practice. In this paper, we introduce a Human-in-the-loop LLM-based Agents framework (HULA) for software development that allows software engineers to refine and guide LLMs when generating coding plans and source code for a given task. We design, implement, and deploy the HULA framework into Atlassian JIRA for internal uses. Through a multi-stage evaluation of the HULA framework, Atlassian software engineers perceive that HULA can minimize the overall development time and effort, especially in initiating a coding plan and writing code for straightforward tasks. On the other hand, challenges around code quality remain a concern in some cases. We draw lessons learned and discuss opportunities for future work, which will pave the way for the advancement of LLM-based agents in software development.","2025-04","2025-11-25 22:39:27","2025-11-25 22:39:27","","342-352","","","","","","","","","","","","","","","","","","","","ISSN: 2832-7659","","","","Large language models; Software; Software engineering; Writing; Software development management; Codes; Source coding; Large Language Models; Surveys; Software Development; User interfaces; Human in the loop; Human-in-the-loop; LLM-based Agents","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RFQBPA4G","conferencePaper","2025","Wang, Jing; Zhang, Weixi; Wang, Weiwei; Zhao, Ruilian; Shang, Ying","Predicting the Root Cause of Flaky Tests Based on Test Smells","2025 IEEE/ACM 22nd International Conference on Software and Systems Reuse (ICSR)","","","10.1109/ICSR66718.2025.00015","","Flaky tests refer to test cases that exhibit inconsistent behaviors across multiple executions, potentially passing or failing unpredictably. They are frequently associated with suboptimal design practices that testers may utilize when crafting test cases, which undermine the quality of software testing. So, identifying the root causes of flaky tests is crucial for fixing them. Currently, inspired by the success of the Large Language Models (LLMs), researchers leverage the pre-trained language model to embed flaky test code as vectors and predict its root cause category based on vector similarity measures. However, such code embeddings generated by LLM mainly focus on capturing general semantic features but lack sufficient comprehension of the behavioral patterns involved in test scenarios, resulting in poor root cause identification. Test smells, which reflect poor coding practices or habits when writing test cases, provide complementary information in the root cause identification of test flakiness. Therefore, this paper proposes a root cause identification method for flaky tests based on test smells. Test smells are used to abstract and express behavioral patterns of test codes, and general semantic features extracted by vector embeddings to enhance the feature representation of flaky tests. Furthermore, to capture the complex nonlinear relationships between test smell features and code embeddings, a Feedforward Neural Network is constructed to categorize the root cause of test flakiness. To validate the effectiveness of our method, we performed evaluations on a dataset consisting of 451 Java flaky test cases. The experimental results indicate that our method achieves an F1-score of 80%, which is 7% higher than that of the baseline model that does not incorporate test smells.","2025-04","2025-11-25 22:39:27","2025-11-25 22:39:27","","84-94","","","","","","","","","","","","","","","","","","","","","","","","Semantics; software testing; Software testing; Software; Writing; Codes; LLMs; test smells; Predictive models; Vectors; Feature extraction; CodeBERT; Eigenvalues and eigenfunctions; Feedforward neural networks; flaky tests root causes","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4R5LCPFE","conferencePaper","2025","Reddy, Harishwar; Srinivasan, Madhusudan; Kanewala, Upulee","Metamorphic Testing for Fairness Evaluation in Large Language Models: Identifying Intersectional Bias in LLaMA and GPT","2025 IEEE/ACIS 23rd International Conference on Software Engineering Research, Management and Applications (SERA)","","","10.1109/SERA65747.2025.11154488","","Large Language Models (LLMs) have made significant strides in Natural Language Processing but remain vulnerable to fairness-related issues, often reflecting biases inherent in their training data. These biases pose risks, particularly when LLMs are deployed in sensitive areas such as healthcare, finance, and law. This paper introduces a metamorphic testing approach to systematically identify fairness bugs in LLMs. We define and apply a set of fairness-oriented metamorphic relations (MRs) to assess the LLaMA and GPT model, a state-of-the-art LLM, across diverse demographic inputs. Our methodology includes generating source and follow-up test cases for each MR and analyzing model responses for fairness violations. The results demonstrate the effectiveness of MT in exposing bias patterns, especially in relation to tone and sentiment, and highlight specific intersections of sensitive attributes that frequently reveal fairness faults. This research improves fairness testing in LLMs, providing a structured approach to detect and mitigate biases and improve model robustness in fairness-sensitive applications.","2025-05","2025-11-25 22:39:27","2025-11-25 22:39:27","","239-246","","","","","","","","","","","","","","","","","","","","ISSN: 2770-8209","","","","Large language models; Software engineering; Natural language processing; Measurement; Testing; Fault diagnosis; Metamorphic Testing; Robustness; Finance; Training data; Medical services; Fairness Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X3BED7GK","journalArticle","2025","Black, Gavin; Yocam, Eric; Mathew Vaidyan, Varghese; Comert, Gurcan; Wang, Yong","From LLMs to Randomness: Analyzing Program Input Efficacy With Resource and Language Metrics","IEEE Access","","2169-3536","10.1109/ACCESS.2025.3571205","","Security-focused program testing typically focuses on crash detection and code coverage while overlooking additional system behaviors that can impact program confidentiality and availability. To address this gap, we propose a statistical framework that combines embedding-based anomaly detection, resource usage metrics, and resource-state distance measures to systematically profile software behaviors beyond traditional coverage-based methods. Leveraging over 5 million labeled samples from 50 Python programs, we evaluate how these independent scoring terms distinguish among different sources of input, including Large Language Model (LLM)-generated inputs, and demonstrate how standard statistical tests (e.g., Kolmogorov—Smirnov and Kendall’s \tau ) confirm their effectiveness. Our findings show that LLM-generated samples can trigger diverse behaviors but are often less effective at exploring resource usage dynamics (CPU, memory) compared with conventional fuzzing. However, combining LLM outputs with existing techniques broadens behavior coverage and reveals commonalities between commercial LLM outputs. We provide open-source tools for this evaluation framework, demonstrating the potential to refine software testing by integrating behavior metrics into security-testing workflows.","2025","2025-11-25 22:39:27","2025-11-25 22:39:27","","87928-87940","","","13","","","","","","","","","","","","","","","","","","","","","Software; Codes; Measurement; Testing; large language models; Fuzzing; Python; Statistical analysis; Computer crashes; anomaly detection; Anomaly detection; fuzzing techniques; Memory management; program behavior analysis; resource usage metrics; Software profiling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QDNSHR43","journalArticle","2024","Kuhail, Mohammad Amin; Berengueres, Jose; Taher, Fatma; Khan, Sana Zeb; Siddiqui, Ansah","Designing a Haptic Boot for Space With Prompt Engineering: Process, Insights, and Implications","IEEE Access","","2169-3536","10.1109/ACCESS.2024.3449396","","Recent literature indicates the potential of applying Artificial Intelligence (AI) tools to enhance ideation outcomes and optimize functionality across various engineering disciplines. However, a comprehensive understanding of applying AI in the design process is lacking, particularly regarding projects involving innovative design. Here, we address the integration of AI in a case study project. The project goal is to design a haptic boot to be used on Mars. We apply a popular AI tool, ChatGPT-3.5, to each design step, from the requirement gathering phase to the prototyping and testing phase. To assess the merit of the AI contributions, we asked eight domain experts to give qualitative feedback. The results indicate that current AI tools can provide a valuable starting point in the requirements and design phases. However, we noted instances of hallucinations and poor traceability. Finally, the experts’ evaluation points out that the AI-proposed requirements and design are missing key elements expected as an outcome in an engineering design process. This study offers insight into the practical application of AI through a specific engineering design process.","2024","2025-11-25 22:39:27","2025-11-25 22:39:27","","134235-134255","","","12","","","","","","","","","","","","","","","","","","","","","Software engineering; Industries; Chatbots; ChatGPT; Education; Artificial intelligence; Prompt engineering; AI; design; engineering; Haptic interfaces; haptics; space","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LT6FSDBG","conferencePaper","2025","Feng, Sidong; Du, Changhao; Liu, Huaxiao; Wang, Qingnan; Lv, Zhengwei; Huo, Gang; Yang, Xu; Chen, Chunyang","Agent for User: Testing Multi - User Interactive Features in TikTok","2025 IEEE/ACM 47th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)","","","10.1109/ICSE-SEIP66354.2025.00011","","TikTok, a widely-used social media app boasting over a billion monthly active users, requires effective app quality assurance for its intricate features. Feature testing is crucial in achieving this goal. However, the multi-user interactive features within the app, such as live streaming, voice calls, etc., pose significant challenges for developers, who must handle simultaneous device management and user interaction coordination. To address this, we introduce a novel multi-agent approach, powered by the Large Language Models (LLMs), to automate the testing of multi-user interactive app features. In detail, we build a virtual device farm that allocates the necessary number of devices for a given multi-user interactive task. For each device, we deploy an LLM-based agent that simulates a user, thereby mimicking user interactions to collaboratively automate the testing process. The evaluations on 24 multi-user interactive tasks within the TikTok app, showcase its capability to cover 75% of tasks with 85.9 % action similarity and offer 87 % time savings for developers. Additionally, we have also integrated our approach into the real-world TikTok testing platform, aiding in the detection of 26 multi-user interactive bugs.","2025-04","2025-11-25 22:39:27","2025-11-25 22:39:27","","57-68","","","","","","","","","","","","","","","","","","","","ISSN: 2832-7659","","","","Software testing; Software; Software engineering; Testing; Computer bugs; Time factors; Social networking (online); Servers; android app testing; multi-agent LLMs; multi-user interactive feature; Video on demand; Web sites","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YLGKZRKS","conferencePaper","2023","Petersone, Ilona; Dovgaluka, Elina; Gudzuka, Justine; Kartenko, Roberts; Romanovs, Andrejs","An Analysis of IT Outsourcing Risks in Post-COVID World","2023 IEEE 64th International Scientific Conference on Information Technology and Management Science of Riga Technical University (ITMS)","","","10.1109/ITMS59786.2023.10317676","","Outsourcing IT services has become a common practice for many enterprises, as it can provide financial benefits and improve competitive advantage. This paper outlines the reasons underlying corporate decision to opt for IT service outsourcing (ITO) instead of managing IT internally within a company and what are the major risks caused by ITO and how to mitigate those risks by applying artificial intelligence tools, such as ChatGPT, Github Copilot or similar.","2023-10","2025-11-25 22:39:27","2025-11-25 22:39:27","","1-9","","","","","","","","","","","","","","","","","","","","ISSN: 2771-6937","","","","Writing; Chatbots; ChatGPT; Artificial intelligence; Information technology; Companies; Indium tin oxide; Intellectual property; IT outsourcing; Optimized production technology; Outsourcing; Risk mitigation; Risks","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MM2UG2QP","journalArticle","2025","Zhang, Quanjun; Fang, Chunrong; Zheng, Yi; Qian, Ruixiang; Yu, Shengcheng; Zhao, Yuan; Zhou, Jianyi; Yang, Yun; Zheng, Tao; Chen, Zhenyu","Improving Retrieval-Augmented Deep Assertion Generation via Joint Training","IEEE Transactions on Software Engineering","","1939-3520","10.1109/TSE.2025.3545970","","Unit testing attempts to validate the correctness of basic units of the software system under test and has a crucial role in software development and testing. However, testing experts have to spend a huge amount of effort to write unit test cases manually. Very recent work proposes a retrieve-and-edit approach to automatically generate unit test oracles, i.e., assertions. Despite being promising, it is still far from perfect due to some limitations, such as splitting assertion retrieval and generation into two separate components without benefiting each other. In this paper, we propose AG-RAG, a retrieval-augmented automated assertion generation (AG) approach that leverages external codebases and joint training to address various technical limitations of prior work. Inspired by the plastic surgery hypothesis, AG-RAG attempts to combine relevant unit tests and advanced pre-trained language models (PLMs) with retrieval-augmented fine-tuning. The key insight of AG-RAG is to simultaneously optimize the retriever and the generator as a whole pipeline with a joint training strategy, enabling them to learn from each other. Particularly, AG-RAG builds a dense retriever to search for relevant test-assert pairs (TAPs) with semantic matching and a retrieval-augmented generator to synthesize accurate assertions with the focal-test and retrieved TAPs as input. Besides, AG-RAG leverages a code-aware language model CodeT5 as the cornerstone to facilitate both assertion retrieval and generation tasks. Furthermore, AG-RAG designs a joint training strategy that allows the retriever to learn from the feedback provided by the generator. This unified design fully adapts both components specifically for retrieving more useful TAPs, thereby generating accurate assertions. AG-RAG is a generic framework that can be adapted to various off-the-shelf PLMs. We extensively evaluate AG-RAG against six state-of-the-art AG approaches on two benchmarks and three metrics. Experimental results show that AG-RAG significantly outperforms previous AG approaches on all benchmarks and metrics, e.g., improving the most recent baseline EditAS by 20.82% and 26.98% in terms of accuracy. AG-RAG also correctly generates 1739 and 2866 unique assertions that all baselines fail to generate, 3.45X and 9.20X more than EditAS. We further demonstrate the positive contribution of our joint training strategy, e.g., AG-RAG improving a variant without the retriever by an average accuracy of 14.11%. Besides, adopting other PLMs can provide substantial advancement, e.g., AG-RAG with four different PLMs improving EditAS by an average accuracy of 9.02%, highlighting the generalizability of our framework. Overall, our work demonstrates the promising potential of jointly fine-tuning the PLM-based retriever and generator to predict accurate assertions by incorporating external knowledge sources, thereby reducing the manual efforts of unit testing experts in practical scenarios.","2025-04","2025-11-25 22:39:28","2025-11-25 22:39:28","","1232-1247","","4","51","","","","","","","","","","","","","","","","","","","","","Semantics; Test pattern generators; Accuracy; Codes; Testing; Benchmark testing; Unit testing; AI4SE; Training; Pipelines; Generators; assertion generation; pre-trained language models; Surgery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AEEX9TE3","conferencePaper","2024","Gupta, Danika; Burke, Paul; Phalke, Vidya; Khanduja, Nimisha","Enhancing Educational Websites With AI Chatbots: Design Considerations for Safety","2024 Conference on AI, Science, Engineering, and Technology (AIxSET)","","","10.1109/AIxSET62544.2024.00058","","Generative AI and Large Language Models provide an immense opportunity to add value to educational organizations and websites. However, a software engineering challenge exists to integrate GenAI while factoring in the requirements of educational sites. We describe a pipeline for testing generative AI chatbots for an academic site [1]. We describe our requirements, which we believe will be represented in other educational sites, and our approach to test case generation, prompt engineering, and response assessments across various Large Language Models. We also present case study results and pipeline code in an open-source repository [14].","2024-09","2025-11-25 22:39:28","2025-11-25 22:39:28","","327-329","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Software engineering; Chatbots; Education; Generative AI; Codes; Testing; Large Language Models; Prompt engineering; Software Engineering; Safety; Pipelines; Organizations","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"39PSKX8A","conferencePaper","2024","Yang, Sichao; Yang, Ye","FormalEval: A Method for Automatic Evaluation of Code Generation via Large Language Models","2024 2nd International Symposium of Electronics Design Automation (ISEDA)","","","10.1109/ISEDA62518.2024.10617643","","One of the promising applications of Large Language Models (LLMs) is code generation. However, evaluating the quality of the generated code poses a significant challenge. Existing evaluation methods such as Rouge or HumanEval have limitations in terms of accuracy or efficiency. In this paper, we propose a formal evaluation method called FormalEval, which automates the process of checking generated code without the need for manual test case curation. We evaluated our method on common tasks related to Register-Transistor-Level (RTL) Verilog and SystemVerilog Assertions (SVA) generation in the field of Electronic Design Automation (EDA). Our method not only identifies 23 % of evaluation error in existing RTL benchmarking dataset, but also fixes the error via test case augmentation. We show FormalEval can help to identify better LLM prompting techniques on SVA generation task. Our method demonstrates state-of-the-art accuracy on the testing dataset.","2024-05","2025-11-25 22:39:28","2025-11-25 22:39:28","","660-665","","","","","","","","","","","","","","","","","","","","","","","","Large language models; LLM; Manuals; Accuracy; Codes; Scalability; Code Generation; Training; Design automation; Evaluation Metric; Formal; RTL; SVA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NIUPX4HL","conferencePaper","2025","Van Hooren, Colin; Ricós, Fernando Pastor; Bromuri, Stefano; Vos, Tanja E. J.; Marín, Beatriz","LLM-Empowered Scriptless Functional Testing","2025 25th International Conference on Software Quality, Reliability and Security (QRS)","","","10.1109/QRS65678.2025.00012","","Scriptless testing generates test sequences dynamically by automatically exploring the Graphical User Interface (GUI). Instead of relying on predefined scripts-which have proven expensive to maintain-scriptless tools detect available widgets, derive possible actions, and select actions on the fly using exploratory techniques such as random selection, model-based inference, or reinforcement learning. While scriptless testing is a valuable complement to scripted approaches, current techniques lack the intelligence needed to strategically select and execute GUI actions that fulfill specific functional testing goals-such as those derived from requirements, use cases, or user stories. Unsurprisingly, this leads companies to question the viability of scriptless testing tools and continue relying on scripts for test automation. This paper reports on the integration of Large Language Models (LLMs) into a scriptless GUI testing tool for action selection, aiming to determine whether it can generate effective action sequences to test specific functional requirements. Our results demonstrate that a multi-objective test goal structure, combined with historical and feedback context, enables LLM-empowered scriptless testing to automate functional testing. Although further research is needed to tackle challenges in complex test scenarios, our findings offer promising results that LLM-empowered scriptless testing can reduce reliance on the maintenance-heavy aspects of traditional scripted testing.","2025-07","2025-11-25 22:39:28","2025-11-25 22:39:28","","1-12","","","","","","","","","","","","","","","","","","","","ISSN: 2693-9177","","","","Large language models; Software reliability; Software quality; Testing; large language models; Scalability; Security; Robustness; Training data; Production; Graphical user interfaces; scriptless testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"43UVE29F","journalArticle","2025","Lin, Bo; Wang, Shangwen; Qin, Yihao; Chen, Liqian; Mao, Xiaoguang","Large Language Models-Aided Program Debloating","IEEE Transactions on Software Engineering","","1939-3520","10.1109/TSE.2025.3594673","","As software grows in complexity to accommodate diverse features and platforms, software bloating has emerged as a significant challenge, adversely affecting performance and security. However, existing approaches inadequately address the dual objectives of debloating: maintaining functionality by preserving essential features and enhancing security by reducing security issues. Specifically, current software debloating techniques often rely on input-based analysis, using user inputs as proxies for the specifications of desired features. However, these approaches frequently overfit provided inputs, leading to functionality loss and potential security vulnerabilities. To address these limitations, we propose LEADER, a program debloating framework enhanced by Large Language Models (LLMs), which leverages their semantic understanding, generative capabilities, and decision-making strengths. LEADER mainly consists of two modules: (1) a documentation-guided test augmentation module designed to preserve functionality, which leverages LLMs to comprehend program documentation and generates sufficient tests to cover the desired features comprehensively, and (2) a multi-advisor-aided program debloating module that employs a neuro-symbolic pipeline to ensure that the security of the software can be perceived during debloating. This module combines debloating and security advisors for analysis and employs an LLM as a decision-maker to eliminate undesired code securely. Extensive evaluations on widely used benchmarks demonstrate the efficacy of LEADER. It achieves a 95.5% test case pass rate and reduces program size by 42.5%. Notably, it reduces the introduction of vulnerabilities during debloating by 79.1% and decreases pre-existing vulnerabilities by 16.5% more than CovA. These results demonstrate that LEADER surpasses the state-of-the-art tool CovA in functionality and security. These results underscore the potential of LEADER to set a new standard in program debloating by effectively balancing functionality and security.","2025-09","2025-11-25 22:39:28","2025-11-25 22:39:28","","2651-2670","","9","51","","","","","","","","","","","","","","","","","","","","","large language model; Large language models; Semantics; Software; Codes; Benchmark testing; Decision making; Fuzzing; Security; program reduction; Training; Documentation; Software debloating","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DXH8BJ4X","conferencePaper","2025","Xu, Kangwei; Li, Bing; Zhang, Grace Li; Schlichtmann, Ulf","HLSTester: Efficient Testing of Behavioral Discrepancies with LLMs for High-Level Synthesis","2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)","","","10.1109/ICCAD66269.2025.11240891","","In high-level synthesis (HLS), C/C++ programs with synthesis directives are used to generate circuits for FPGA implementations. However, hardware-specific and platform-dependent characteristics in circuit implementations can introduce behavioral discrepancies between the original C/C++ programs and the circuits after high-level synthesis. Existing methods for testing behavioral discrepancies in HLS are still immature, and the testing workflow requires significant human efforts. To address this challenge, we propose HLSTester, a large language model (LLM) aided testing framework that efficiently detects behavioral discrepancies in HLS. To mitigate hallucinations in LLMs and enhance prompt quality, existing C/C++ testbenches are used to guide the LLM to generate HLS-compatible versions, effectively eliminating certain traditional C/C++ syntax that are incompatible with HLS tools. Key variables are pinpointed through a backward slicing technique in both C/C++ and HLS programs to monitor their runtime spectra, enabling an in-depth analysis of the discrepancy symptoms. Then, a testing input generation mechanism is introduced to integrate dynamic mutation with insights from an LLM-based progressive reasoning chain. In addition, repetitive hardware testing is skipped by a redundancy-aware technique for the generated test inputs. Experimental results demonstrate that the proposed LLM-aided testing framework significantly accelerates the testing workflow while achieving higher testbench simulation pass rates compared with the traditional method and the direct use of LLMs on the same HLS programs.","2025-10","2025-11-25 22:39:28","2025-11-25 22:39:28","","1-9","","","","","","","","","","","","","","","","","","","","ISSN: 1558-2434","","","","Large language models; Syntactics; Life estimation; Testing; Hardware; Cognition; Runtime; Monitoring; Circuits; Field programmable gate arrays","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7LDRZWP9","conferencePaper","2025","Ranjan, Atul; Malhotra, Ruchika","Lang: A Programming Language and Framework for Test-Driven Development and Enhanced Error Interpretation with LLM Integration","2025 International Conference on Engineering Innovations and Technologies (ICoEIT)","","","10.1109/ICoEIT63558.2025.11211825","","The lack of interpretability in runtime and logical errors often hinders efficient software development, leading to increased debugging time and reduced developer productivity. Despite advancements in Large Language Models (LLMs) for code generation and debugging, current implementations fail to provide actionable insights into error causes, particularly for complex logic flows and nested execution contexts. Prompt engineering has shown potential in leveraging LLMs for debugging, but its integration into programming workflows remains limited in scope and effectiveness. To address these challenges, this research introduces Lang, a novel programming language and framework designed to enhance test-driven development and error interpretability. Lang integrates LLM-powered call stack analysis to provide comprehensive information about the state and execution flow during runtime. The framework enforces a modular structure where each file encapsulates a single function, divided into task, explanation, code, and test blocks. This structured approach facilitates precise debugging, improves logical error interpretability, and ensures comprehensive test coverage. By combining test-driven language design with LLM integration, Lang aims to reduce development time while enhancing code reliability. This solution represents a significant step forward in addressing the inefficiencies of traditional debugging and test-generation methodologies, providing a robust framework for modern software engineering practices.","2025-07","2025-11-25 22:39:28","2025-11-25 22:39:28","","244-254","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Software reliability; Syntactics; Software development management; Codes; Large Language Models; Debugging; Technological innovation; Software Engineering; Logical Errors; Computer languages; Runtime; Reliability engineering; Abstract Syntax Tree; Call Stack Analysis; Error Interpretability; Programming Language Design; Runtime Errors; Test-Driven Development","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QXRYISVW","conferencePaper","2025","Peng, Yun; Gotmare, Akhilesh Deepak; Lyu, Michael R.; Xiong, Caiming; Savarese, Silvio; Sahoo, Doyen","PerfCodeGen: Improving Performance of LLM Generated Code with Execution Feedback","2025 IEEE/ACM Second International Conference on AI Foundation Models and Software Engineering (Forge)","","","10.1109/Forge66646.2025.00008","","Large Language Models (LLMs) are widely adopted for assisting in software development tasks, yet their performance evaluations have narrowly focused on the functional correctness of generated code. Human programmers, however, expect AI assistants to generate not only correct but also optimally efficient code. We propose PerfCodeGen, a training-free framework that enhances the performance of LLM-generated code by incorporating feedback based on runtime during test case execution into the self-refinement iterations. With PerfCodeGen, we achieve speedups for a significantly higher proportion of problems compared to using the base LLM with sophisticated prompting techniques. Applied to open-weight language models like Phi-3-mini, PerfCodeGen achieves code optimization rates comparable to naive prompting of powerful closed models like GPT-4. We achieve state-of-the-art code optimization on benchmarks such as HumanEval, MBPP, and APPS, frequently surpassing the ground truth reference solutions with PerfCodeGen using GPT-3.5 and GPT-4. Additionally, we demonstrate the effectiveness of our approach in enhancing code quality across a range of open-weight LLMs of varying sizes including Phi-3-mini (3.8B), Llama 3 8B, Mixtral 8x7B (13B active), Command R (35B), and Llama 3 70B. PerfCodeGen’s effectiveness at generating performant code underscores the importance of integrating execution feedback into the code generation process, highlighting a path forward for more robust and reliable AI-driven software development.","2025-04","2025-11-25 22:39:28","2025-11-25 22:39:28","","1-13","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Optimization; Software reliability; Software development management; Codes; Large Language Models; Benchmark testing; Code Generation; Programming; Python; Code Optimization; Runtime; Reliability engineering; Efficient Code; Runtime Efficiency","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D8ZC546L","conferencePaper","2025","Masserini, Elena; Clerissi, Diego; Micucci, Daniela; Campos, João R.; Mariani, Leonardo","Towards the Assessment of Task-based Chatbots: From the TOFU-R Snapshot to the BRASATO Curated Dataset","2025 IEEE 36th International Symposium on Software Reliability Engineering (ISSRE)","","","10.1109/ISSRE66568.2025.00060","","Task-based chatbots are increasingly being used to deliver real services, yet assessing their reliability, security, and robustness remains underexplored, also due to the lack of large-scale, high-quality datasets. The emerging automated quality assessment techniques targeting chatbots often rely on limited pools of subjects, such as custom-made toy examples, or outdated, no longer available, or scarcely popular agents, complicating the evaluation of such techniques. In this paper, we present two datasets and the tool support necessary to create and maintain these datasets. The first dataset is RASA TASK-BASED CHATBOTS FROM GITHUB (TOFU-R), which is a snapshot of the Rasa chatbots available on GitHub, representing the state of the practice in open-source chatbot development with Rasa. The second dataset is BOT RASA COLLECTION (BRASATO), a curated selection of the most relevant chatbots for dialogue complexity, functional complexity, and utility, whose goal is to ease reproducibility and facilitate research on chatbot reliability.","2025-10","2025-11-25 22:39:28","2025-11-25 22:39:28","","73-82","","","","","","","","","","","","","","","","","","","","ISSN: 2332-6549","","","","Quality assessment; Software reliability; Chatbots; ChatGPT; Software development management; Dataset; GitHub; Security; Robustness; Rasa; Complexity theory; Reproducibility of results; Chatbot; Quality control; Toy manufacturing industry","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8V8A9QEJ","conferencePaper","2025","Ugarte, Miriam; Valle, Pablo; Antonio, Jose Parejo; Segura, Sergio; Arrieta, Aitor","ASTRAL: Automated Safety Testing of Large Language Models","2025 IEEE/ACM International Conference on Automation of Software Test (AST)","","","10.1109/AST66626.2025.00018","","Large Language Models (LLMs) have recently gained significant attention due to their ability to understand and generate sophisticated human-like content. However, ensuring their safety is paramount as they might provide harmful and unsafe responses. Existing LLM testing frameworks address various safety-related concerns (e.g., drugs, terrorism, animal abuse) but often face challenges due to unbalanced and obsolete datasets. In this paper, we present ASTRAL, a tool that automates the generation and execution of test cases (i.e., prompts) for testing the safety of LLMs. First, we introduce a novel black-box coverage criterion to generate balanced and diverse unsafe test inputs across a diverse set of safety categories as well as linguistic writing characteristics (i.e., different style and persuasive writing techniques). Second, we propose an LLM-based approach that leverages Retrieval Augmented Generation (RAG), few-shot prompting strategies and web browsing to generate up-to-date test inputs. Lastly, similar to current LLM test automation techniques, we leverage LLMs as test oracles to distinguish between safe and unsafe test outputs, allowing a fully automated testing approach. We conduct an extensive evaluation on well-known LLMs, revealing the following key findings: i) GPT3.5 outperforms other LLMs when acting as the test oracle, accurately detecting unsafe responses, and even surpassing more recent LLMs (e.g., GPT-4), as well as LLMs that are specifically tailored to detect unsafe LLM outputs (e.g., LlamaGuard); ii) the results confirm that our approach can uncover nearly twice as many unsafe LLM behaviors with the same number of test inputs compared to currently used static datasets; and iii) our black-box coverage criterion combined with web browsing can effectively guide the LLM on generating up-to-date unsafe test inputs, significantly increasing the number of unsafe LLM behaviors.","2025-04","2025-11-25 22:39:28","2025-11-25 22:39:28","","114-124","","","","","","","","","","","","","","","","","","","","ISSN: 2833-9061","","","","Large language models; Automation; Writing; Software development management; Testing; generative ai; testing; llms; safety; Safety; Systematics; Closed box; Retrieval augmented generation; Terrorism","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9GDLDXKG","conferencePaper","2025","Baresi, Luciano; Hu, Davide Yi Xian; Mas’udi, Muhammad Irfan; Quattrocchi, Giovanni","DILLEMA: Diffusion and Large Language Models for Multi-Modal Augmentation","2025 IEEE/ACM International Workshop on Deep Learning for Testing and Testing for Deep Learning (DeepTest)","","","10.1109/DeepTest66595.2025.00010","","Ensuring the robustness of deep learning models requires comprehensive and diverse testing. Existing approaches, often based on simple data augmentation techniques or generative adversarial networks, are limited in producing realistic and varied test cases. To address these limitations, we present a novel framework for testing vision neural networks that leverages Large Language Models and control-conditioned Diffusion Models to generate synthetic, high-fidelity test cases. Our approach begins by translating images into detailed textual descriptions using a captioning model, allowing the language model to identify modifiable aspects of the image and generate counterfactual descriptions. These descriptions are then used to produce new test images through a text-to-image diffusion process that preserves spatial consistency and maintains the critical elements of the scene. We demonstrate the effectiveness of our method using two datasets: ImageNet1K for image classification and SHIFT for semantic segmentation in autonomous driving. The results show that our approach can generate significant test cases that reveal weaknesses and improve the robustness of the model through targeted retraining. We conducted a human assessment using Mechanical Turk to validate the generated images. The responses from the participants confirmed, with high agreement among the voters, that our approach produces valid and realistic images.","2025-05","2025-11-25 22:39:39","2025-11-25 22:39:39","","29-36","","","","","","","","","","","","","","","","","","","","","","","","Large language models; generative AI; Testing; large language models; deep learning testing; Robustness; Deep learning; Neural networks; autonomous driving systems; Diffusion models; Translation; Autonomous vehicles; diffusion models; Semantic segmentation; Text to image","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BH886HNB","journalArticle","2025","Shahriar, Asif; Hisham, Syed Jarullah; Rahman, K. M. Asifur; Islam, Ruhan; Hossain, Md. Shohrab; Hwang, Ren-Hung; Lin, Ying-Dar","5GPT: 5G Vulnerability Detection by Combining Zero-Shot Capabilities of GPT-4 With Domain Aware Strategies Through Prompt Engineering","IEEE Transactions on Information Forensics and Security","","1556-6021","10.1109/TIFS.2025.3586480","","Identifying vulnerabilities in complex 5G network protocols is a challenging task. Manual analysis is time-consuming and often inadequate. Modern ML and NLP methods, though effective, are resource-intensive and struggle to find implicit vulnerabilities. In this research, we utilize GPT-4’s advanced language understanding to detect vulnerabilities directly from 5G specifications. To assess GPT-4’s fundamental capabilities in this domain, we first adopt a zero-shot approach that relies solely on the specification text without external guidance. For detecting more sophisticated vulnerabilities that require deep contextual understanding, we introduce a novel domain-aware strategy, where we explicitly teach GPT-4 about security properties and hazard indicators from related works using few-shot learning. We further employ chain-of-thought prompting to guide the model through structured reasoning steps to identify violations or exploitations that may lead to vulnerabilities. A two-tier filtering process ensures that only promising test-cases are retained. Our method has identified 47 potential vulnerabilities in 5G mobility management procedures, including 27 previously unreported issues, and generated corresponding test-cases. Simulating 14 of them, we have found 9 vulnerabilities, five of which are new. The zero-shot approach is effective in detecting procedural and validation flaws, while the domain-aware method excels in finding protocol violations and advanced attack scenarios. These findings validate our methodology and demonstrate its strength in discovering both known and novel vulnerabilities in 5G protocols.","2025","2025-11-25 22:39:39","2025-11-25 22:39:39","","7045-7060","","","20","","","","","","","","","","","","","","","","","","","","","LLM; Testing; Adaptation models; prompt engineering; Prompt engineering; Cognition; Fuzzing; Security; vulnerability detection; few-shot learning; Training; Protocols; network security; 5G mobile communication; 5G; AI for security; Authentication","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ADRZTAKP","conferencePaper","2025","Mascia, Cristian; Guerriero, Antonio; Giamattei, Luca; Pietrantuono, Roberto; Russo, Stefano","Microservices Performance Testing with Causality-enhanced Large Language Models","2025 IEEE/ACM Second International Conference on AI Foundation Models and Software Engineering (Forge)","","","10.1109/Forge66646.2025.00022","","Efficient performance testing of microservices is essential for engineers to ensure that deviations of performance/resource usage metrics from expectations are promptly identified within their rapid release cycle. To this aim, engineers would need to explore the space of possible workload configurations and focus only on the critical ones, e.g., low-load configurations that unexpectedly cause performance issues. This requires a great effort, and can be infeasible in short release cycles.We present CALLMIT, a framework using Large Language Models (LLM) enhanced by causal reasoning to automatically generate critical workloads for microservices performance testing. Engineers query CALLMIT to generate workload configurations expected to expose deviations from performance requirements, so as to actually run only tests that trigger critical configurations. We present the experimental evaluation on three subjects, with comparison to a conventional Retrieval-Augmented Generation technique. The results show that causal models improve the correct identification by LLM of performance-critical workload configurations.","2025-04","2025-11-25 22:39:39","2025-11-25 22:39:39","","136-140","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Manuals; Software engineering; Measurement; Testing; Large Language Models; Cognition; Retrieval-augmented generation; Microservice architectures; Retrieval augmented generation; Space exploration; Foundation models; Causal reasoning; Microservices; Performance testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PX7XR66L","conferencePaper","2023","Wolfschwenger, Patrick; Sabitzer, Barbara; Lavicza, Zsolt","Integrating Cloud-Based AI in Software Engineers' Professional Training and Development","2023 IEEE Frontiers in Education Conference (FIE)","","","10.1109/FIE58773.2023.10343391","","Artificial Intelligence (AI) has recently gained immense popularity. With impressive capabilities and versatility, large language models have quickly become a valuable tool for a wide range of applications, from chatbots and language translation to content creation and research. Generative AI can aid in the creation of computer code and provide information on a wide range of technical topics. This work-in-progress brings AI to vocational training by incorporating Cloud Computing (CC) services into a professional training and development program for software engineers, concentrating on practical skills development, hands-on experience and job-specific competencies. The approach is evaluated in the context of action research, with an emphasis on the potential benefits and challenges of code generation.","2023-10","2025-11-25 22:39:39","2025-11-25 22:39:39","","1-5","","","","","","","","","","","","","","","","","","","","ISSN: 2377-634X","","","","Software; Chatbots; Codes; Artificial Intelligence; Computational modeling; Cloud computing; Learning (artificial intelligence); Cloud Computing; Lifelong Learning; Professional Training and Development; Vocational training","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IRHPZFAV","conferencePaper","2023","Kale, Sumedh S.; Andreopoulos, William B.","Job Tailored Resume Content Generation","2023 IEEE Ninth International Conference on Big Data Computing Service and Applications (BigDataService)","","","10.1109/BigDataService58306.2023.00012","","Generally candidates apply to multiple jobs with a single resume and do not tend to customize their resume to match the job description. This hampers their chances of getting a resume shortlisted for the job. The project aims to help such candidates build job tailored resumes that help them create a customized and targeted resume for a specific job. The tool specifically targets candidates’ employment work history for resume content generation. We create a synthetic dataset built from candidates’ employment history and online job descriptions. We use natural language processing (NLP) techniques to extract and organize the dataset, experiment with multiple dataset variations and cite ways to effectively build the dataset for the proposed task. We then use natural language generation by fine tuning GPT-2 for the task of resume content generation. Finally we evaluate the fine tuned model on various metrics and report our findings.","2023-07","2025-11-25 22:39:40","2025-11-25 22:39:40","","40-47","","","","","","","","","","","","","","","","","","","","","","","","Natural language processing; generative AI; Measurement; natural language processing; Transformers; transformers; Computer architecture; content generation; Employment; History; natural language generation; Resumes","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8KIX23K6","conferencePaper","2024","Vierhauser, Michael; Groher, Iris; Antensteiner, Tobias; Sauerwein, Clemens","Towards Integrating Emerging AI Applications in SE Education","2024 36th International Conference on Software Engineering Education and Training (CSEE&T)","","","10.1109/CSEET62301.2024.10663045","","Artificial Intelligence (AI) approaches have been incorporated into modern learning environments and software engineering (SE) courses and curricula for several years. However, with the significant rise in popularity of large language models (LLMs) in general, and OpenAI's LLM-powered chatbot ChatGPT in particular in the last year, educators are faced with rapidly changing classroom environments and disrupted teaching principles. Examples range from programming assignment solutions that are fully generated via ChatGPT, to various forms of cheating during exams. However, despite these negative aspects and emerging challenges, AI tools in general, and LLM applications in particular, can also provide significant opportunities in a wide variety of SE courses, supporting both students and educators in meaningful ways. In this early research paper, we present preliminary results of a systematic analysis of current trends in the area of AI, and how they can be integrated into university-level SE curricula, guidelines, and approaches to support both instructors and learners. We collected both teaching and research papers and analyzed their potential usage in SE education, using the ACM Computer Science Curriculum Guidelines CS2023. As an initial outcome, we discuss a series of opportunities for AI applications and further research areas.","2024-07","2025-11-25 22:39:40","2025-11-25 22:39:40","","1-5","","","","","","","","","","","","","","","","","","","","ISSN: 2377-570X","","","","Large language models; Chatbots; Market research; Artificial intelligence; AI; Software Engineering Education; Systematics; Learning (artificial intelligence); Engineering education; Roadmap","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B8EQ6Z4I","conferencePaper","2025","Liu, Yong; Wang, Xin; Liu, Hengyuan; Huang, Ruishi; Wu, Yonghao","Empirical Evaluation of LLMs for Automated Program Fault Localisation","2025 25th International Conference on Software Quality, Reliability, and Security Companion (QRS-C)","","","10.1109/QRS-C65679.2025.00063","","Many recent service interruptions caused by software faults have shown that fault localisation is crucial for automated debugging and repair. In this context, Large Language Models (LLMs) have emerged as a promising tool that has demonstrated strong capabilities in a variety of software engineering tasks such as code generation, program repair, summarisation and test generation. While existing research has either leveraged LLMs for function-level fault localisation or fine-tuned them using exclusively faulty code snippets, the efficacy of LLM-based statement-level fault localisation with dynamic execution information remains inadequately investigated. Therefore, to address this research gap, we present a comprehensive empirical evaluation of statement-level fault localisation capabilities across multiple LLM architectures, including the ChatGPT series, the DeepSeek series, and small-scale, locally deployed open-source language models. Utilising the extensively validated Defects4J v1.5.0 benchmark, we systematically compare these models against existing statement-level fault localisation techniques. We also investigate the consistency of LLMs in fault localisation, as well as how prompt engineering affects the fault localisation effectiveness. Our empirical findings reveal that within a function-level context, DeepSeek-R1 surpasses all baselines in statement-level fault localisation, with the integration of error logs enhancing accuracy by 72.6% over SmartFL in terms of TOP-1 metric. Moreover, our ablation experiments demonstrate that removing test cases and error logs reduces the LLM’s TOP-1 accuracy by 4.7% and 8.6% on average, respectively, while explicit instructions to rank faulty statements improves accuracy by 6.9%. These observations suggest that LLMs offer promising capabilities for automated fault localisation. Furthermore, utilising dynamic execution information and providing instructions to rank faulty statements can significantly enhance the fault localisation capabilities of LLMs.","2025-07","2025-11-25 22:39:40","2025-11-25 22:39:40","","454-463","","","","","","","","","","","","","","","","","","","","ISSN: 2693-9371","","","","Large language models; Test pattern generators; Software engineering; Software reliability; Software quality; Accuracy; Chatbots; ChatGPT; DeepSeek; Codes; Large Language Models; Empirical Study; Security; Maintenance engineering; Fault Localisation; Open-Source Language Models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A7T64RA4","conferencePaper","2025","Fregnan, Enrico; Göttel, Christian; Maag, Balz; Dawoud, Abdallah; Nakas, Georgios","Leveraging LLMs Towards Assistant-based Support for Industrial Threat Models","2025 IEEE 30th International Conference on Emerging Technologies and Factory Automation (ETFA)","","","10.1109/ETFA65518.2025.11205762","","Threat models contribute to strengthen the security of an enterprise system by listing the cybersecurity threats that might affect it as well as possible mitigations to such threats. Given the importance of these models, different tools have been devised to support creating or updating threat models. However, these tools focus mainly on the needs of cybersecurity experts, often overlooking non-expert users.To address this gap, in the first part of this paper we present a chat-based LLM assistant to answer both expert and non-expert queries on threat models. The assistant is based on a microservice architecture and uses Retrieval-Augmented Generation (RAG) to extract relevant information from an industrial threat model.Guaranteeing the reliability of an LLM’s answers in sensitive fields such as cybersecurity is paramount. For this reason, in the second part of this paper, we evaluate the LLM assistant using (i) Bleu and Rouge score metrics, (ii) human evaluation, and (iii) an automatic LLM-as-a-judge approach.The evaluation, conducted on 275 test cases, confirms the accuracy of the answers provided by our LLM-based threat model assistant. Additionally, the results of the human and LLM-as-a-judge evaluations are consistent. This corroborates the effectiveness of automatic LLM-as-a-judge methods in assessing the performance of LLM-based industrial solutions.","2025-09","2025-11-25 22:39:40","2025-11-25 22:39:40","","1-8","","","","","","","","","","","","","","","","","","","","ISSN: 1946-0759","","","","Accuracy; Measurement; Data mining; GenAI; Reliability; RAG; Microservice architectures; Prevention and mitigation; Retrieval augmented generation; Manufacturing automation; Computer crime; Cyber Security; Threat Model; Threat modeling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LS9I6765","conferencePaper","2024","Cotroneo, Domenico; Liguori, Pietro","Neural Fault Injection: Generating Software Faults from Natural Language","2024 54th Annual IEEE/IFIP International Conference on Dependable Systems and Networks - Supplemental Volume (DSN-S)","","","10.1109/DSN-S60304.2024.00016","","Traditional software fault injection methods, while foundational, face limitations in adequately representing real-world faults, offering customization, and requiring significant manual effort and expertise. This paper introduces a novel methodology that harnesses the capabilities of Large Language Models (LLMs) augmented with Reinforcement Learning from Human Feedback (RLHF) to overcome these challenges. The usage of RLHF emphasizes an iterative refinement process, allowing testers to provide feedback on generated faults, which is then used to enhance the LLM’s fault generation capabilities, ensuring the generation of fault scenarios that closely mirror actual operational risks. This innovative methodology aims to significantly reduce the manual effort involved in crafting fault scenarios as it allows testers to focus on higher-level testing strategies, hence paving the way to new possibilities for enhancing the dependability of software systems.","2024-06","2025-11-25 22:39:40","2025-11-25 22:39:40","","23-27","","","","","","","","","","","","","","","","","","","","ISSN: 2833-292X","","","","Large language models; Manuals; Natural language processing; Natural Language Processing; Software systems; Large Language Models; Reinforcement Learning from Human Feedback; Iterative methods; Reinforcement learning; Mirrors; Software Fault Injection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H7H7678T","conferencePaper","2025","Garg, Spandan; Moghaddam, Roshanak Zilouchian; Sundaresan, Neel","RAPGen: An Approach for Fixing Code Inefficiencies in Zero-Shot","2025 IEEE/ACM 47th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)","","","10.1109/ICSE-SEIP66354.2025.00017","","Performance bugs are non-functional bugs that can even manifest in well-tested commercial products. Fixing these performance bugs is an important yet challenging problem. In this work, we address this challenge and present a new approach called Retrieval-Augmented Prompt Generation (RAPGen). Given a code snippet with a performance issue, RAPGen first retrieves a prompt instruction from a pre-constructed knowledge-base of previous performance bug fixes and then generates a prompt using the retrieved instruction. It then uses this prompt on a Large Language Model in zero-shot to generate a fix. We compare our approach with the various prompt variations and state of the art methods in the task of performance bug fixing. Our empirical evaluation shows that RAPGen can generate performance improvement suggestions equivalent or better than a developer in \sim 60 % of the cases, getting \sim 42 % of them verbatim, in an expert-verified dataset of past performance changes made by C# developers. Furthermore, we conduct an in-the-wild evaluation to verify the model's effectiveness in practice by suggesting fixes to developers in a large software company. So far, we have shared performance fixes on 10 codebases that represent production services running in the cloud and 7 of the fixes have been accepted by the developers and integrated into the code.","2025-04","2025-11-25 22:39:40","2025-11-25 22:39:40","","124-135","","","","","","","","","","","","","","","","","","","","ISSN: 2832-7659","","","","Large language models; Software engineering; Software performance; Codes; Computer bugs; Large Language Models; Prompt engineering; AI for SE; Production; Encoding; Maintenance engineering; Knowledge based systems; Bug Repair; Software Performance","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EAEHUH36","conferencePaper","2025","Corazza, Jan; Gavran, Ivan; Moreira, Gabriela; Neider, Daniel","Accessible Smart Contracts Verification: Synthesizing Formal Models with Tamed LLMs","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","","","10.1109/ICST62969.2025.10989026","","When blockchain systems are said to be trustless, what this really means is that all the trust is put into software. Thus, there are strong incentives to ensure blockchain software is correct–vulnerabilities here cost millions and break businesses. One of the most powerful ways of establishing software correctness is by using formal methods. Approaches based on formal methods, however, induce a significant overhead in terms of time and expertise required to successfully employ them. Our work addresses this critical disadvantage by automating the creation of a formal model–a mathematical abstraction of the software system–which is often a core task when employing formal methods. We perform model synthesis in three phases: we first transpile the code into model stubs; then we “fill in the blanks” using a large language model (LLM); finally, we iteratively repair the generated model, on both syntactical and semantical level. In this way, we significantly reduce the amount of time necessary to create formal models and increase accessibility of valuable software verification methods that rely on them. The practical context of our work was reducing the time-to-value of using formal models for correctness audits of smart contracts.","2025-03","2025-11-25 22:39:40","2025-11-25 22:39:40","","542-552","","","","","","","","","","","","","","","","","","","","ISSN: 2159-4848","","","","Large language models; Software; Syntactics; Codes; Large Language Models; Blockchains; Smart contracts; Code Generation; Smart Contracts; Maintenance engineering; Mathematical models; User experience; Formal Methods; Model Synthesis; Model-Based Techniques; Software Auditing; Trustless services","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q6JP5SLZ","conferencePaper","2024","Schwachhofer, Denis; Domanski, Peter; Becker, Steffen; Wagner, Stefan; Sauer, Matthias; Pflüger, Dirk; Polian, Ilia","Training Large Language Models for System-Level Test Program Generation Targeting Non-functional Properties","2024 IEEE European Test Symposium (ETS)","","","10.1109/ETS61313.2024.10567741","","System-Level Test (SLT) has been an integral part of integrated circuit test flows for over a decade and continues to be significant. Nevertheless, there is a lack of systematic approaches for generating test programs, specifically focusing on the non-functional aspects of the Device under Test (DUT). Currently, test engineers manually create test suites using commercially available software to simulate the end-user environment of the DUT. This process is challenging and laborious and does not assure adequate control over non-functional properties. This paper proposes to use Large Language Models (LLMs) for SLT program generation. We use a pre-trained LLM and fine-tune it to generate test programs that optimize non-functional properties of the DUT, e.g., instructions per cycle. Therefore, we use Gem5, a microarchitectural simulator, in conjunction with Reinforcement Learning-based training. Finally, we write a prompt to generate C code snippets that maximize the instructions per cycle of the given architecture. In addition, we apply hyperparameter optimization to achieve the best possible results in inference.","2024-05","2025-11-25 22:39:40","2025-11-25 22:39:40","","1-4","","","","","","","","","","","","","","","","","","","","ISSN: 1558-1780","","","","Optimization; Codes; Large Language Models; Test Generation; Knowledge engineering; Training; Systematics; Process control; Functional Test; System-Level Test; Hyperparameter optimization; Microarchitecture","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XN6SUH36","journalArticle","2025","Altin, Mahsun; Mutlu, Behcet; Kilinc, Deniz; Cakir, Altan","Automated Testing for Service-Oriented Architecture: Leveraging Large Language Models for Enhanced Service Composition","IEEE Access","","2169-3536","10.1109/ACCESS.2025.3571994","","This article explores the application of Large Language Models (LLMs), including proprietary models such as OpenAI’s ChatGPT 4o and ChatGPT 4o-mini, Anthropic’s Claude 3.5 Sonnet and Claude 3.7 Sonnet, and Google’s Gemini 1.5 Pro, Gemini 2.0 Flash, and Gemini 2.0 Flash-Lite, as well as open-source alternatives including Qwen2.5-14B-Instruct-1M, and commercially accessed models such as DeepSeek R1 and DeepSeek V3, which were tested via APIs despite having open-source variants, to automate validation and verification in Application Programming Interface (API) testing within a Service-Oriented Architecture (SOA). Our system compares internal responses from the Enuygun Web Server against third-party API outputs in both JSON and XML formats, validating critical parameters such as flight prices, baggage allowances, and seat availability. We generated 100 diverse test scenarios across varying complexities (1-4 flight results) by randomly altering request and response parameters. Experimental results show that Google Gemini 2.0 Flash achieved high accuracy (up to 99.98%) with the lowest completion time (85.34 seconds), while Qwen2.5-14B-Instruct-1M exhibited limited capability in processing complex formats. Models such as OpenAI’s ChatGPT and Anthropic’s Claude Sonnet models also demonstrated strong performance in single-flight validation scenarios, making them suitable for low-latency, high-precision tasks. Our findings indicate that some open-source models can offer promising cost-effective alternatives, though performance significantly varies. This integration of LLMs reduced manual workload, improved test scalability, and enabled real-time validation across large-scale datasets. As LLM technologies mature, we anticipate further advances in automation, accuracy, and efficiency in software validation systems.","2025","2025-11-25 22:39:40","2025-11-25 22:39:40","","89627-89640","","","13","","","","","","","","","","","","","","","","","","","","","Software testing; Software reliability; Software systems; Accuracy; Software development management; large language models; Scalability; Complexity theory; Real-time systems; AI-driven automation; Indexing; scalability in API testing; service-oriented architecture; Service-oriented architecture; software validation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9ELRNGWI","conferencePaper","2025","Hou, Chenxi; Chong, Chun Jie; Yao, Zhihao; Peng, Hui","Catamaran: User Privacy Violation Detection in Mobile Logging","2025 IEEE Secure Development Conference (SecDev)","","","10.1109/SecDev66745.2025.00013","","Logs are widely used in mobile apps for debugging and diagnosis. Unfortunately, they frequently expose personally identifiable information (PII) due to inattentive logging practices, leading to privacy hazards, which has been categorized as a Common Weakness Enumeration, CWE-532. Existing manual redaction and anonymization techniques are often incomplete and ineffective, as evidenced by the rising number of CWE-532 vulnerabilities. To address this, we introduce Catamaran, a framework to proactively detect PII violations in Android app logs. Catamaran takes a comprehensive approach combining dynamic and static analyses: dynamic analysis captures PII leaks directly at runtime in logcat and other log files, while static analysis expands detection to untriggered code paths by reconstructing contextualized log statements. The static analysis leverages call graph analysis, constant propagation, and Large Language Model (LLM) to intelligently identify potential PII issues. Our extensive evaluation on 3,885 Android apps uncovered runtime PII leakage in 233 apps (approximately 6%), comprising a total of 7,485 violations. The static analysis of \mathbf3 0 0, 0 7 3 log statements across \mathbf3, 9 9 4 apps identified 1,788 potential log-based PII leaks in 932 apps and 97 distinct libraries. We have responsibly disclosed our findings, leading to the confirmation of nine vulnerabilities and four patches in the Android Open Source Project (AOSP).","2025-10","2025-11-25 22:39:40","2025-11-25 22:39:40","","16-28","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Manuals; Static analysis; Program Analysis; Libraries; Mobile applications; Privacy; Runtime; Data privacy; Identification of persons; Logging; Mobile Privacy; Operating systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZI8HS3EZ","journalArticle","2024","Kang, Sungmin; Yoon, Juyeon; Askarbekkyzy, Nargiz; Yoo, Shin","Evaluating Diverse Large Language Models for Automatic and General Bug Reproduction","IEEE Transactions on Software Engineering","","1939-3520","10.1109/TSE.2024.3450837","","Bug reproduction is a critical developer activity that is also challenging to automate, as bug reports are often in natural language and thus can be difficult to transform to test cases consistently. As a result, existing techniques mostly focused on crash bugs, which are easier to automatically detect and verify. In this work, we overcome this limitation by using large language models (LLMs), which have been demonstrated to be adept at natural language processing and code generation. By prompting LLMs to generate bug-reproducing tests, and via a post-processing pipeline to automatically identify promising generated tests, our proposed technique Libro could successfully reproduce about one-third of all bugs in the widely used Defects4J benchmark. Furthermore, our extensive evaluation on 15 LLMs, including 11 open-source LLMs, suggests that open-source LLMs also demonstrate substantial potential, with the StarCoder LLM achieving 70% of the reproduction performance of the closed-source OpenAI LLM code-davinci-002 on the large Defects4J benchmark, and 90% of performance on a held-out bug dataset likely not part of any LLM's training data. In addition, our experiments on LLMs of different sizes show that bug reproduction using Libro improves as LLM size increases, providing information as to which LLMs can be used with the Libro pipeline.","2024-10","2025-11-25 22:39:40","2025-11-25 22:39:40","","2677-2694","","10","50","","","","","","","","","","","","","","","","","","","","","Large language models; Java; Codes; Computer bugs; Debugging; natural language processing; Test generation; software engineering; Pipelines; Computational modeling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DIIZAD6Q","journalArticle","2024","Fatima, Sakina; Hemmati, Hadi; C. Briand, Lionel","FlakyFix: Using Large Language Models for Predicting Flaky Test Fix Categories and Test Code Repair","IEEE Transactions on Software Engineering","","1939-3520","10.1109/TSE.2024.3472476","","Flaky tests are problematic because they non-deterministically pass or fail for the same software version under test, causing confusion and wasting development effort. While machine learning models have been used to predict flakiness and its root causes, there is much less work on providing support to fix the problem. To address this gap, in this paper, we focus on predicting the type of fix that is required to remove flakiness and then repair the test code on that basis. We do this for a subset of flaky tests where the root cause of flakiness is in the test itself and not in the production code. One key idea is to guide the repair process with additional knowledge about the test's flakiness in the form of its predicted fix category. Thus, we first propose a framework that automatically generates labeled datasets for 13 fix categories and trains models to predict the fix category of a flaky test by analyzing the test code only. Our experimental results using code models and few-shot learning show that we can correctly predict most of the fix categories. To show the usefulness of such fix category labels for automatically repairing flakiness, we augment the prompts of GPT 3.5 Turbo, a Large Language Model (LLM), with such extra knowledge to request repair suggestions. The results show that our suggested fix category labels, complemented with in-context learning, significantly enhance the capability of GPT 3.5 Turbo in generating fixes for flaky tests. Based on the execution and analysis of a sample of GPT-repaired flaky tests, we estimate that a large percentage of such repairs, (roughly between 51% and 83%) can be expected to pass. For the failing repaired tests, on average, 16% of the test code needs to be further changed for them to pass.","2024-12","2025-11-25 22:39:40","2025-11-25 22:39:40","","3146-3171","","12","50","","","","","","","","","","","","","","","","","","","","","Analytical models; Large language models; software testing; Java; Manuals; Codes; large language models; Python; code models; Predictive models; Production; Few shot learning; Maintenance engineering; few shot learning; fix category; Flaky tests; test repair","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9JS4HLXK","conferencePaper","2024","Liu, Jun; Yan, Jiwei; Xie, Yuanyuan; Yan, Jun; Zhang, Jian","Fix the Tests: Augmenting LLMs to Repair Test Cases with Static Collector and Neural Reranker","2024 IEEE 35th International Symposium on Software Reliability Engineering (ISSRE)","","","10.1109/ISSRE62328.2024.00043","","During software evolution, it is advocated that test code should co-evolve with production code. In real development scenarios, test updating may lag behind production code changing, which may cause compilation failure or bring other troubles. Existing techniques based on pre-trained language models can be directly adopted to repair obsolete tests caused by such unsynchronized code changes, especially syntactic-related ones. However, the lack of task-oriented contextual information affects the repair accuracy on large-scale projects. Starting from an obsolete test, the key challenging task is precisely identifying and constructing Test-Repair-Oriented Contexts (TROCtxs) from the whole repository within a limited token size.In this paper, we propose Synter (SYNtactic-breaking- changes-induced TEst Repair), a novel approach based on LLMs to automatically repair obsolete test cases via precise and concise TROCtxs construction. Inspired by developers’ programming practices, we design three types of TROCtx: class context, usage context, and environment context. Given an obsolete test case to repair, Synter firstly collects the related code information for each type of TROCtx through static analysis techniques automatically. Then, it generates reranking queries to identify the most relevant TROCtxs, which will be taken as the repair-required key contexts and be input to the large language model for the final test repair.To evaluate the effectiveness of Synter, we construct a benchmark dataset that contains a set of obsolete tests caused by syntactic breaking changes. The experimental results show that Synter outperforms baseline approaches both on textual- and intent-matching metrics. With the augmentation of constructed TROCtxs, hallucinations are reduced by 57.1%.","2024-10","2025-11-25 22:39:40","2025-11-25 22:39:40","","367-378","","","","","","","","","","","","","","","","","","","","ISSN: 2332-6549","","","","Large language models; LLM; Software; Software reliability; Static analysis; Syntactics; Codes; Measurement; Static Analysis; Programming; Software Evolution; Production; Maintenance engineering; Obsolete Test Repair","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SB79IHVJ","journalArticle","2025","Yan, Ming; Chen, Junjie; Jiang, Tianjie; Jiang, Jiajun; Wang, Zan","Evaluating Spectrum-Based Fault Localization on Deep Learning Libraries","IEEE Transactions on Software Engineering","","1939-3520","10.1109/TSE.2025.3552622","","Deep learning (DL) libraries have become increasingly popular and their quality assurance is also gaining significant attention. Although many fault detection techniques have been proposed, effective fault localization techniques tailored to DL libraries are scarce. Due to the unique characteristics of DL libraries (e.g., complicated code architecture supporting DL model training and inference with extensive multidimensional tensor calculations), the effectiveness of existing fault localization techniques for traditional software is also unknown on DL library faults. To bridge this gap, we conducted the first empirical study to investigate the effectiveness of fault localization on DL libraries. Specifically, we evaluated spectrum-based fault localization (SBFL) due to its high generalizability and affordable overhead on such complicated libraries. Based on the key aspects in SBFL, our study investigated the effectiveness of SBFL with different sources of passing test cases (including human-written, fuzzer-generated, and mutation-based test cases) and various suspicious value calculation methods. In particular, mutation-based test cases are produced by our designed rule-based mutation technique and LLM-based mutation technique tailored to DL library faults. To enable our extensive study, we built the first benchmark (Defects4DLL), which contains 120 real-world faults in PyTorch and TensorFlow with easy-to-use experimental environments. Our study delivered a series of useful findings. For example, the rule-based approach is effective in localizing crash faults in DL libraries, successfully localizing 44.44% of crash faults within Top-10 functions and 74.07% of crash faults within Top-10 files, while the passing test cases from DL library fuzzers perform poorly on this task. Furthermore, based on our findings on the complementarity of different sources, we designed a hybrid technique by effectively integrating human-written, LLM-mutated, rule-based mutated test cases, which further achieves 31.48%\boldsymbol\sim∼61.36% improvements over each single source in terms of the number of detected faults within Top-5 files.","2025-05","2025-11-25 22:39:40","2025-11-25 22:39:40","","1399-1414","","5","51","","","","","","","","","","","","","","","","","","","","","Codes; Location awareness; Benchmark testing; empirical study; Libraries; Deep learning; Fault localization; Training; Computer crashes; deep learning library; Fault location; Runtime environment; Tensors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZWTZH5HF","conferencePaper","2025","Abtahi, Seyed Moein; Azim, Akramul","Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements","2025 IEEE/ACM Second International Conference on AI Foundation Models and Software Engineering (Forge)","","","10.1109/Forge66646.2025.00017","","This study examined code issue detection and revision automation by integrating Large Language Models (LLMs) such as OpenAI’s GPT-3.5 Turbo and GPT-4o into software development workflows. A static code analysis framework detects issues such as bugs, vulnerabilities, and code smells within a large-scale software project. Detailed information on each issue was extracted and organized to facilitate automated code revision using LLMs. An iterative prompt engineering process is applied to ensure that prompts are structured to produce accurate and organized outputs aligned with the project requirements. Retrieval-augmented generation (RAG) is implemented to enhance the relevance and precision of the revisions, enabling LLM to access and integrate real-time external knowledge. The issue of LLM hallucinations—where the model generates plausible but incorrect outputs—is addressed by a custom-built ""Code Comparison App,"" which identifies and corrects erroneous changes before applying them to the codebase. Subsequent scans using the static code analysis framework revealed a significant reduction in code issues, demonstrating the effectiveness of combining LLMs, static analysis, and RAG to improve code quality, streamline the software development process, and reduce time and resource expenditure.","2025-04","2025-11-25 22:39:40","2025-11-25 22:39:40","","82-92","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Static analysis; Automation; Software quality; Software development management; Codes; Prompt engineering; Prompt Engineering; LLMs; GPT-4o; RAG; Real-time systems; Costs; Retrieval augmented generation; Code Comparison; GPT-3.5 Turbo; Issue Detection; Software Development Automation; Static Code Analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9MBM3R66","conferencePaper","2025","Zhu, Shengkai; Gan, Shuitao; Wang, Xiaofeng; Nan, Yan","GLMFuzz: Vulnerability Knowledge Guided Prompting for Efficient Network Protocol Fuzzing","2025 25th International Conference on Software Quality, Reliability and Security (QRS)","","","10.1109/QRS65678.2025.00062","","Security vulnerabilities in network protocol implementations have increased rapidly, posing serious threats to network infrastructure. Fuzzing has emerged as a primary method for detecting these vulnerabilities. However, due to the stateful nature of protocol implementations, existing mutationbased protocol fuzzing methods face challenges in generating high-quality initial seeds and developing effective mutation strategies. In this paper, we propose GLMFuzz, which is a vulnerability-knowledge-guided fuzzing method for protocol implementations based on large language models(LLMs). By analyzing historical vulnerability sequences, which contain special states and field information that can trigger boundary conditions and exceptional behaviors in protocol implementations, GLMFuzz constructs a vulnerability knowledge vector database and utilizes LLM to generate diverse initial seeds. During the fuzzing process, GLMFuzz uses vulnerability sequences to guide the LLM in performing directed mutations on the message, producing test cases that cover unusual behaviors. In addition, we design an automatic prompt engineering algorithm to dynamically adjust prompt templates based on fuzzing results. To validate the performance of GLMFuzz, we conduct a comparative evaluation with two state-of-the-art (SOTA) tools, AFLNET and CHATAFL. The experimental results demonstrate that GLMFuzz achieves average improvements of 54.18 % and 16.48 % in state transitions, 14.57 % and 4.97 % in state coverage, 10.61 % and 7.01 % in code coverage compared to AFLNET and CHATAFL, respectively, and identifies a previously unknown vulnerability in protocol implementations, which has been assigned CVE identifier.","2025-07","2025-11-25 22:39:40","2025-11-25 22:39:40","","564-575","","","","","","","","","","","","","","","","","","","","ISSN: 2693-9177","","","","Software reliability; Codes; large language models; Prompt engineering; Fuzzing; fuzzing; Security; Databases; Protocols; Vectors; Heuristic algorithms; automatic prompt engineering; Boundary conditions; protocol implementations","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FREUYXV4","conferencePaper","2024","Huang, Yuchao; Wang, Junjie; Liu, Zhe; Wang, Yawen; Wang, Song; Chen, Chunyang; Hu, Yuanzhe; Wang, Qing","CrashTranslator: Automatically Reproducing Mobile Application Crashes Directly from Stack Trace","2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE)","","","10.1145/3597503.3623298","","Crash reports are vital for software maintenance since they allow the developers to be informed of the problems encountered in the mobile application. Before fixing, developers need to reproduce the crash, which is an extremely time-consuming and tedious task. Existing studies conducted the automatic crash reproduction with the natural language described reproducing steps. Yet we find a non-neglectable portion of crash reports only contain the stack trace when the crash occurs. Such stack-trace-only crashes merely reveal the last GUI page when the crash occurs, and lack step-by-step guidance. Developers tend to spend more effort in understanding the problem and reproducing the crash, and existing techniques cannot work on this, thus calling for a greater need for automatic support. This paper proposes an approach named CrashTranslator to automatically reproduce mobile application crashes directly from the stack trace. It accomplishes this by leveraging a pre-trained Large Language Model to predict the exploration steps for triggering the crash, and designing a reinforcement learning based technique to mitigate the inaccurate prediction and guide the search holistically. We evaluate CrashTranslator on 75 crash reports involving 58 popular Android apps, and it successfully reproduces 61.3% of the crashes, outperforming the state-of-the-art baselines by 109% to 206%. Besides, the average reproducing time is 68.7 seconds, out-performing the baselines by 302% to 1611%. We also evaluate the usefulness of CrashTranslator with promising results.","2024-04","2025-11-25 22:39:40","2025-11-25 22:39:40","","190-202","","","","","","","","","","","","","","","","","","","","ISSN: 1558-1225","","","","Optimization; Computer bugs; Mobile applications; Reinforcement learning; Task analysis; Computer crashes; Software maintenance; Bug reproduction; Mobile application testing; Stack trace","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KQMTHUUQ","journalArticle","2024","Zheng, Tao; Shao, Jiang; Dai, Jinqiao; Jiang, Shuyu; Chen, Xingshu; Shen, Changxiang","RESTLess: Enhancing State-of-the-Art REST API Fuzzing With LLMs in Cloud Service Computing","IEEE Transactions on Services Computing","","1939-1374","10.1109/TSC.2024.3489441","","REST API Fuzzing is an emerging approach for automated vulnerability detection in cloud services. However, existing SOTA fuzzers face challenges in generating lengthy sequences comprising high-semantic requests, so that they may hardly trigger hard-to-reach states within a cloud service. To overcome this problem, we propose RESTLess, a flexible and efficient approach with hybrid optimization strategies for REST API fuzzing enhancement. Specifically, to pass the cloud gateway syntax semantic checking, we construct a dataset of valid parameters of REST API with Large Language Model named RTSet, then utilize it to develop an efficient REST API specification semantic enhancement approach. To detect vulnerability hidden under complex API operations, we design a flexible parameter rendering order optimization algorithm to increase the length and type of request sequences. Evaluation results highlight that RESTLess manifests noteworthy enhancements in the semantic quality of generated sequences in comparison to existing tools, thereby augmenting their capabilities in detecting vulnerabilities effectively. We also apply RESTLess to nine real-world cloud service such as Microsoft Azure, Amazon Web Services, Google Cloud, etc., and detecte 38 vulnerabilities, of which 16 have been confirmed and fixed by the relevant vendors.","2024-11","2025-11-25 22:39:40","2025-11-25 22:39:40","","4225-4238","","6","17","","","","","","","","","","","","","","","","","","","","","Optimization; Semantics; Codes; Testing; Computer bugs; Fuzzing; Vulnerability detection; Dictionaries; cloud services; efficiency improving; HTTP; Logic gates; Rendering (computer graphics); REST API fuzzing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YXUCNIUL","conferencePaper","2024","Vishwakarma, Sanjay; Harkins, Francis; Golecha, Siddharth; Bajpe, Vishal Sharathchandra; Dupuis, Nicolas; Buratti, Luca; Kremer, David; Faro, Ismael; Puri, Ruchir; Cruz-Benito, Juan","Qiskit HumanEval: An Evaluation Benchmark for Quantum Code Generative Models","2024 IEEE International Conference on Quantum Computing and Engineering (QCE)","","","10.1109/QCE60285.2024.00137","","Quantum programs are typically developed using quantum Software Development Kits (SDKs). The rapid advancement of quantum computing necessitates new tools to streamline this development process, and one such tool could be Generative Artificial intelligence (GenAI). In this study, we introduce and use the Qiskit HumanEval dataset, a hand-curated collection of tasks designed to benchmark the ability of Large Language Models (LLMs) to produce quantum code using Qiskit – a quantum SDK. This dataset consists of more than 100 quantum computing tasks, each accompanied by a prompt, a canonical solution, a comprehensive test case, and a difficulty scale to evaluate the correctness of the generated solutions. We systematically assess the performance of a set of LLMs against the Qiskit HumanEval dataset's tasks and focus on the models ability in producing executable quantum code. Our findings not only demonstrate the feasibility of using LLMs for generating quantum code but also establish a new benchmark for ongoing advancements in the field and encourage further exploration and development of GenAI-driven tools for quantum code generation.","2024-09","2025-11-25 22:40:41","2025-11-25 22:40:41","","1169-1176","","","01","","","","","","","","","","","","","","","","","","","","","Large language models; Generative AI; Software development management; Codes; Large Language Models; Benchmark testing; Quantum computing; Computational modeling; HumanEval; Evaluation benchmarks; Qiskit; Qiskit HumanEval","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BSK4AK37","conferencePaper","2024","Minani, Jean Baptiste; Fellah, Yahia El; Ahmed, Sanam; Sabir, Fatima; Moha, Naouel; Guéhéneuc, Yann-Gaël","An Exploratory Study on Code Quality, Testing, Data Accuracy, and Practical Use Cases of IoT Wearables","2024 7th Conference on Cloud and Internet of Things (CIoT)","","","10.1109/CIoT63799.2024.10756966","","The growth of the Internet of Things (IoT), particularly in wearable devices like Fitbits, has raised challenges related to source code quality, testing, data accuracy, and practical applications. This paper investigates issues in Fitbit apps by (1) analyzing GitHub repositories of Fitbit projects to identify code quality issues, (2) using Large Language Models (LLMs) to automate testing, (3) comparing data variations across different Fitbit models, and (4) experimenting with real-world use cases for Fitbit devices. Our analysis of \mathbf1 6 GitHub repositories revealed code quality issues in Fitbit apps, highlighting the need for better practices. Using LLMs like ChatGPT-4, we generated unit tests with 100 % coverage. Data comparisons across Fitbit Versa models showed consistent accuracy. Finally, we showed the potential of wearable devices in the real-world with two practical use cases: health monitoring with robotic assistance and location-based tracking. These findings open new avenues for research in wearables.","2024-10","2025-11-25 22:40:41","2025-11-25 22:40:41","","1-5","","","","","","","","","","","","","","","","","","","","ISSN: 2159-6972","","","","Accuracy; Software development management; Codes; Testing; Source coding; Data models; Internet of Things; Object recognition; IoT Wearables; Quality Issues; Robots; Testing Fitbit; Wearable Health Monitoring Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KTJR3DJN","conferencePaper","2025","Mani, Nariman; Attaranasl, Salma","Enhancing Adaptive Test Healing with Graph Neural Networks for Dependency-Aware Decision Making","2025 IEEE International Conference on Artificial Intelligence Testing (AITest)","","","10.1109/AITest66680.2025.00023","","Flaky tests are a major obstacle in modern CI/CD pipelines, leading to unreliable feedback, increased reruns, and developer frustration. Our previously published adaptive healing framework combined Large Language Models (LLMs) and Reinforcement Learning (RL) to automate flaky test recovery, but it assumed test independence and failed to account for structural dependencies between tests. In this paper, we introduce a significant extension to that baseline: a Graph Neural Network (GNN)-based Test Dependency Mapping layer that models intertest relationships. By integrating GNN embeddings with LLM-classified failures, the RL agent becomes dependency-aware, enabling more precise and efficient healing decisions. We evaluate the enhanced framework on a real-world industrial platform, a social lifestyle application actively used by thousands of users for health, nutrition, and coaching. Results show a 90% reduction in flaky test-related costs and faster, autonomous resolution of dependency-induced failures.","2025-07","2025-11-25 22:40:41","2025-11-25 22:40:41","","126-133","","","","","","","","","","","","","","","","","","","","ISSN: 2835-3560","","","","Large language models; Testing; Decision making; GPT; Transformers; Large Language Models (LLMs); Pipelines; Reinforcement learning; Real-time systems; Costs; Adaptive Test Healing; Continuous Integration (CI); Flaky Tests; Reinforcement Learning (RL); Self-Healing Test Automation; Adaptive systems; Graph neural networks; Graph Neural Networks (GNN)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BKPZQNTT","conferencePaper","2024","Fu, Jingzhou; Liang, Jie; Wu, Zhiyong; Jiang, Yu","Sedar: Obtaining High-Quality Seeds for DBMS Fuzzing via Cross-DBMS SQL Transfer","2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE)","","","10.1145/3597503.3639210","","Effective DBMS fuzzing relies on high-quality initial seeds, which serve as the starting point for mutation. These initial seeds should incorporate various DBMS features to explore the state space thoroughly. While built-in test cases are typically used as initial seeds, many DBMSs lack comprehensive test cases, making it difficult to apply state-of-the-art fuzzing techniques directly. To address this, we propose Sedar which produces initial seeds for a target DBMS by transferring test cases from other DBMSs. The underlying insight is that many DBMSs share similar functionalities, allowing seeds that cover deep execution paths in one DBMS to be adapted for other DBMSs. The challenge lies in converting these seeds to a format supported by the grammar of the target database. Sedar follows a three-step process to generate seeds. First, it executes existing SQL test cases within the DBMS they were designed for and captures the schema information during execution. Second, it utilizes large language models (LLMs) along with the captured schema information to guide the generation of new test cases based on the responses from the LLM. Lastly, to ensure that the test cases can be properly parsed and mutated by fuzzers, Sedar temporarily comments out unparsable sections for the fuzzers and uncomments them after mutation. We integrate Sedar into the DBMS fuzzers SQUIRREL and Griffin, targeting DBMSs such as Virtuoso, Mon-etDB, DuckDB, and ClickHouse. Evaluation results demonstrate significant improvements in both fuzzers. Specifically, compared to SQUIRREL and Griffin with non-transferred seeds, Sedar enhances code coverage by 72.46%-214.84% and 21.40%-194.46%; compared to SQUIRREL and Griffin with native test cases of these DBMSs as initial seeds, incorporating the transferred seeds of Sedar results in an improvement in code coverage by 4.90%-16.20% and 9.73%-28.41 %. Moreover, Sedar discovered 70 new vulnerabilities, with 60 out of them being uniquely found by Sedar with transferred seeds, and 19 of them have been assigned with CVEs.","2024-04","2025-11-25 22:40:41","2025-11-25 22:40:41","","1799-1810","","","","","","","","","","","","","","","","","","","","ISSN: 1558-1225","","","","Codes; Computer bugs; Fuzzing; Vulnerability Detection; Databases; Space exploration; Structured Query Language; Grammar; DBMS Fuzzing; Initial Seeds","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BWV3SAIZ","conferencePaper","2024","Tao, Qingxiao; Yu, Tingrui; Gu, Xiaodong; Shen, Beijun","Unraveling the Potential of Large Language Models in Code Translation: How Far are We?","2024 31st Asia-Pacific Software Engineering Conference (APSEC)","","","10.1109/APSEC65559.2024.00046","","While large language models (LLMs) exhibit state-of-the-art performance in various tasks, recent studies have revealed their struggle for code translation. This is because they haven't been extensively pre-trained with parallel multilingual code, which code translation heavily depends on. Moreover, existing benchmarks only cover a limited subset of common programming languages, and thus cannot reflect the full potential of LLMs in code translation. In this paper, we conduct a large-scale empirical study to exploit the capabilities and incapabilities of LLMs in code translation tasks. We first craft a novel benchmark called PolyHumanEval by extending HumanEval to a multilingual benchmark of 14 languages. With PolyHumanEval, we then perform over 110,000 translations with bleeding-edge code LLMs. The result shows LLMs' suboptimal performance on Python to other languages and the negligible impact of widely adopted LLM optimization techniques such as conventional pre-training and instruction tuning on code translation. To further uncover the potential of LLMs in code translation, we propose two methods: (1) intermediary translation which selects an intermediary language between the source and target ones; and (2) self-training which fine-tunes LLMs on self-generated parallel data. Evaluated with CodeLlama-13B, our approach yields an average improvement of 11.7% computation accuracy on Python-to-other translations. Notably, we interestingly find that Go can serve as a lingua franca for translating between any two studied languages.","2024-12","2025-11-25 22:40:41","2025-11-25 22:40:41","","353-362","","","","","","","","","","","","","","","","","","","","ISSN: 2640-0715","","","","Large language models; Test pattern generators; Software engineering; Codes; Source coding; large language models; Benchmark testing; Python; Code translation; Tuning; Translation; intermediary translation; Multilingual; self-training","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A3R8IN46","conferencePaper","2025","Souza, Beatriz; Pradel, Michael","Treefix: Enabling Execution with a Tree of Prefixes","2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)","","","10.1109/ICSE55347.2025.00215","","The ability to execute code is a prerequisite for various dynamic program analyses. Learning-guided execution has been proposed as an approach to enable the execution of arbitrary code snippets by letting a neural model predict likely values for any missing variables. Although state-of-the-art learning-guided execution approaches, such as LExecutor, can enable the execution of a relative high amount of code, they are limited to predicting a restricted set of possible values and do not use any feedback from previous executions to execute even more code. This paper presents Treefix, a novel learning-guided execution approach that leverages LLMs to iteratively create code prefixes that enable the execution of a given code snippet. The approach addresses the problem in a multi-step fashion, where each step uses feedback about the code snippet and its execution to instruct an LLM to improve a previously generated prefix. This process iteratively creates a tree of prefixes, a subset of which is returned to the user as prefixes that maximize the number of executed lines in the code snippet. In our experiments with two datasets of Python code snippets, Treefix achieves 25% and 7% more coverage relative to the current state of the art in learning-guided execution, covering a total of 84% and 82% of all lines in the code snippets.","2025-04","2025-11-25 22:40:41","2025-11-25 22:40:41","","2676-2688","","","","","","","","","","","","","","","","","","","","ISSN: 1558-1225","","","","Software engineering; Codes; Python; Predictive models; Performance analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DSDI7UIF","conferencePaper","2025","Wang, Chaofan; Qiu, Guanjie; Gu, Xiaodong; Shen, Beijun","ApiRAT: Integrating Multi-source API Knowledge for Enhanced Code Translation with LLMs","2025 IEEE 49th Annual Computers, Software, and Applications Conference (COMPSAC)","","","10.1109/COMPSAC65507.2025.00176","","Code translation is an essential task in software migration, multilingual development, and system refactoring. Recent advancements in large language models (LLMs) have demonstrated significant potential in this task. However, prior studies have highlighted that LLMs often struggle with domain-specific code, particularly in resolving cross-lingual API mappings. To tackle this challenge, we propose ApiRAT, a novel code translation method that integrates multi-source API knowledge. ApiRAT employs three API knowledge augmentation techniques, including API sequence retrieval, API sequence back-translation, and API mapping, to guide LLMs to translating code, ensuring both the correct structure of API sequences and the accurate usage of individual APIs. Extensive experiments on two public datasets, CodeNet and AVATAR, indicate that ApiRAT significantly surpasses existing LLM-based methods, achieving improvements in computational accuracy ranging from 4% to 15.1%. Additionally, our evaluation across different LLMs showcases the generalizability of ApiRAT. An ablation study further confirms the individual contributions of each API knowledge component, underscoring the effectiveness of our approach.","2025-07","2025-11-25 22:40:41","2025-11-25 22:40:41","","1400-1405","","","","","","","","","","","","","","","","","","","","ISSN: 2836-3795","","","","Large language models; Software; Accuracy; Codes; Logic; code translation; retrieval augmented generation; Translation; Retrieval augmented generation; Distance measurement; Multilingual; API knowledge retrieval; API mistranslation; Robust stability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SWC337PT","conferencePaper","2024","Chen, Mouxiang; Liu, Zhongxin; Tao, He; Hong, Yusu; Lo, David; Xia, Xin; Sun, Jianling","ℬ4: Towards Optimal Assessment of Plausible Code Solutions with Plausible Tests","2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE)","","","","","Selecting the best code solution from multiple generated ones is an essential task in code generation, which can be achieved by using some reliable validators (e.g., developer-written test cases) for assistance. Since reliable test cases are not always available and can be expensive to build in practice, researchers propose to automatically generate test cases to assess code solutions. However, when both code solutions and test cases are plausible and not reliable, selecting the best solution becomes challenging. Although some heuristic strategies have been proposed to tackle this problem, they lack a strong theoretical guarantee and it is still an open question whether an optimal selection strategy exists. Our work contributes in two ways. First, we show that within a Bayesian framework, the optimal selection strategy can be defined based on the posterior probability of the observed passing states between solutions and tests. The problem of identifying the best solution is then framed as an integer programming problem. Second, we propose an efficient approach for approximating this optimal (yet uncomputable) strategy, where the approximation error is bounded by the correctness of prior knowledge. We then incorporate effective prior knowledge to tailor code generation tasks. Both theoretical and empirical studies confirm that existing heuristics are limited in selecting the best solutions with plausible test cases. Our proposed approximated optimal strategy ℬ4 significantly surpasses existing heuristics in selecting code solutions generated by large language models (LLMs) with LLM-generated tests, achieving a relative performance improvement by up to 50% over the strongest heuristic and 246% over the random selection in the most challenging scenarios. Our code is publicly available at https://github.com/ZJU-CTAG/B4.CCS CONCEPTS• Computing methodologies → Artificial intelligence; • Software and its engineering → Software design engineering.","2024-10","2025-11-25 22:40:42","2025-11-25 22:40:42","","1693-1705","","","","","","","","","","","","","","","","","","","","ISSN: 2643-1572","","","","Large language models; Software; Software engineering; Codes; Large Language Models; Software Engineering; Code Generation; Computational modeling; Software design; Bayes methods; Integer programming; Posterior probability; Reliability theory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EFHZBMS3","conferencePaper","2024","Wu, Jiahui; Lu, Chengjie; Arrieta, Aitor; Yue, Tao; Ali, Shaukat","Reality Bites: Assessing the Realism of Driving Scenarios with Large Language Models","2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering (Forge) Conference Acronym:","","","10.1145/3650105.3652296","","Large Language Models (LLMs) are demonstrating outstanding potential for tasks such as text generation, summarization, and classification. Given that such models are trained on a humongous amount of online knowledge, we hypothesize that LLMs can as-sess whether driving scenarios generated by autonomous driving testing techniques are realistic, i.e., being aligned with real-world driving conditions. To test this hypothesis, we conducted an empir-ical evaluation to assess whether LLMs are effective and robust in performing the task. This reality check is an important step towards devising LLM-based autonomous driving testing techniques. For our empirical evaluation, we selected 64 realistic scenarios from DeepScenario-an open driving scenario dataset. Next, by introducing minor changes to them, we created 512 additional realistic sce-narios, to form an overall dataset of 576 scenarios. With this dataset, we evaluated three LLMs (GPT-3.5, Llama2-13B, and Mistral-7B) to assess their robustness in assessing the realism of driving scenarios. Our results show that: (1) Overall, GPT- 3.5 achieved the highest robustness compared to Llama2-13B and Mistral-7B, consistently throughout almost all scenarios, roads, and weather conditions; (2) Mistral-7B performed the worst consistently; (3) Llama2-13B achieved good results under certain conditions; and (4) roads and weather conditions do influence the robustness of the LLMs.","2024-04","2025-11-25 22:40:42","2025-11-25 22:40:42","","40-51","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Testing; Large Language Models; Roads; Robustness; Task analysis; Autonomous vehicles; Terminology; Realistic Driving Scenarios","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TYE68T7Z","journalArticle","2025","Khojah, Ranim; de Oliveira Neto, Francisco Gomes; Mohamad, Mazen; Leitner, Philipp","The Impact of Prompt Programming on Function-Level Code Generation","IEEE Transactions on Software Engineering","","1939-3520","10.1109/TSE.2025.3587794","","Large Language Models (LLMs) are increasingly used by software engineers for code generation. However, limitations of LLMs such as irrelevant or incorrect code have highlighted the need for prompt programming (or prompt engineering) where engineers apply specific prompt techniques (e.g., chain-of-thought or input-output examples) to improve the generated code. While some prompt techniques have been studied, the impact of different techniques — and their interactions — on code generation is still not fully understood. In this study, we introduce CodePromptEval, a dataset of 7072 prompts designed to evaluate five prompt techniques (few-shot, persona, chain-of-thought, function signature, list of packages) and their effect on the correctness, similarity, and quality of complete functions generated by three LLMs (GPT-4o, Llama3, and Mistral). Our findings show that while certain prompt techniques significantly influence the generated code, combining multiple techniques does not necessarily improve the outcome. Additionally, we observed a trade-off between correctness and quality when using prompt techniques. Our dataset and replication package enable future research on improving LLM-generated code and evaluating new prompt techniques.","2025-08","2025-11-25 22:40:42","2025-11-25 22:40:42","","2381-2395","","8","51","","","","","","","","","","","","","","","","","","","","","Large language models; Software; Software engineering; Accuracy; Codes; Benchmark testing; Prompt engineering; code generation; Programming; Training; Encoding; Few shot learning; prompt programming","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7NYN3XC2","conferencePaper","2024","Ramasamy, Vijayalakshmi; Ramamoorthy, Suganya; Walia, Gursimran Singh; Kulpinski, Eli; Antreassian, Aaron","Enhancing User Story Generation in Agile Software Development Through Open AI and Prompt Engineering","2024 IEEE Frontiers in Education Conference (FIE)","","","10.1109/FIE61694.2024.10893343","","This innovative practice full paper explores the use of AI technologies in user story generation. With the emergence of agile software development, generating comprehensive user stories that capture all necessary functionalities and perspectives has become crucial for software development. Every computing program in the United States requires a semester-or year-long senior capstone project, which requires student teams to gather and document technical requirements. Effective user story generation is crucial for successfully implementing software projects. However, user stories written in natural language can be prone to inherent defects such as incompleteness and incorrectness, which may creep in during the downstream development activities like software designs, construction, and testing. One of the challenges faced by software engineering educators is to teach students how to elicit and document requirements, which serve as a blueprint for software development. Advanced AI technologies have increased the popularity of large language models (LLMs) trained on large multimodal datasets. Therefore, utilizing LLM-based techniques can assist educators in helping students discover aspects of user stories that may have been overlooked or missed during the manual analysis of requirements from various stakeholders. The main goal of this research study is to investigate the potential application of OpenAI techniques in software development courses at two academic institutions to enhance software design and development processes, aiming to improve innovation and efficiency in team project-based educational settings. The data used for the study constitute student teams generating user stories by traditional methods (control) vs. student teams using OpenAI agents (treatment) such as gpt-4-turbo for generating user stories. The overarching research questions include: RQ-l) What aspects of user stories generated using OpenAI prompt engineering differ significantly from those generated using the traditional method? RQ-2) Can the prompt engineering data provide insights into the efficacy of the questions/prompts that affect the quality and comprehensiveness of user stories created by software development teams? Industry experts evaluated the user stories created and analyzed how prompt engineering affects the overall effectiveness and innovation of user story creation, which provided guidelines for incorporating AI-driven approaches into software development practices. Overall, this research seeks to contribute to the growing body of knowledge on the application of AI in software engineering education, specifically in user story generation. Investigating the use of AI technologies in user story generation could further enhance the usability of prompt engineering in agile software development environments. We plan to expand the study to investigate the long-term effects of prompt engineering on all phases of software development.","2024-10","2025-11-25 22:40:42","2025-11-25 22:40:42","","1-8","","","","","","","","","","","","","","","","","","","","ISSN: 2377-634X","","","","Software; Software engineering; Testing; Prompt engineering; Technological innovation; Collaboration; Stakeholders; Agile software development; Usability; Collaboration network; complex network analysis; structured collaboration network; Technical requirements","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RQSUIRDS","conferencePaper","2025","More, Priyanka; Kothawade, Nishica; More, Anushka","Leveraging Continuous Integration and Deployment to Operationalize a Fine-Tuned LLaMA Healthcare Chatbot","2025 2nd International Conference on New Frontiers in Communication, Automation, Management and Security (ICCAMS)","","","10.1109/ICCAMS65118.2025.11233960","","The study demonstrates the application of Llama 3.1 paradigm and Groq API to build a full CI/CD pipeline that deploys Flask-based applications. A CI/CD orchestration function belongs to Jenkins, while Docker performs containerization and DockerHub registers images and Kubernetes deploys applications, and GitHub manages source code. The fully automated solution allows minimal human involvement, which enables complete automation of machine learning application testing and development, and deployment thus shortening the time to release and reducing errors. This implementation demonstrates the effectiveness of contemporary DevOps practices when delivering big language models. The research examines LLM-based service containerization challenges alongside the advantages of AI application automated deployment before presenting planned enhancements in monitoring frameworks, together with multi-environment deployment methods.","2025-07","2025-11-25 22:40:42","2025-11-25 22:40:42","","1-6","","","","","","","","","","","","","","","","","","","","","","","","Automation; Testing; Source coding; Scalability; Large Language Models (LLMs); Security; DevOps; Pipelines; Medical services; Monitoring; Biomedical imaging; CI/CD Pipeline; Docker Containerization; Jenkins Automation; Kubernetes Orchestration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y72QBBF4","conferencePaper","2024","Crandall, Aaron S.; Fischer, Bryan J.; Crandall, Johannah L.","WIP: ARTful Insights from a Pilot Study on GPT-Based Automatic Code Reviews in Undergraduate Computer Science Programs","2024 IEEE Frontiers in Education Conference (FIE)","","","10.1109/FIE61694.2024.10893407","","This work in progress research paper describes a pilot study using a Large Language Model (LLM) Generative Pre-Trained Transformer-based (GPT) system that generates industry-style code reviews for student feedback on software development projects in Computer Science 2nd, 3rd, and 4th+ semester classes (CS2, CS3, CS4+) at an ABET accredited baccalaureate institution. Code reviews are a valuable, but work-intensive, component of the software engineering process and provide important training to undergraduate students in the form of mentor-peer knowledge transfer. Participants in this study engaged in iterative experiential learning using the Automatic Review Tool (ART), an artificial intelligence tool to support software engineering as an Automatic Static Analysis Tool in the Continuous Integration pipeline alongside software testing harnesses and code style checkers. This pilot study was based on earlier results from a full computer science second semester (CS2) class (\mathrmn=74) to develop an ART-generated code review intervention pilot study with a small group of students in CS2 / 3 and CS4. The project underway uses an experiential learning and iterative feedback process to answer research questions including “Does ART provide accurate and actionable code reviews for students” and “Which levels of students are best prepared to receive and use ART-based code reviews?” During this pilot study, the project used a mixed methods research approach with a series of surveys, code review interventions, and numerical analysis of the code reviews' accuracy. Results showed a reasonable degree of code review accuracy by ART and the students learned code review skills from interaction with the ART-based reviews they received. Ongoing work includes increasing the scale of data collection, using this work to refine and focus the ART-based reviews onto the categories of feedback that students find the most valuable, and building out a more modular tool for wider release in the academic community.","2024-10","2025-11-25 22:40:42","2025-11-25 22:40:42","","1-5","","","","","","","","","","","","","","","","","","","","ISSN: 2377-634X","","","","Software engineering; Accuracy; Codes; Surveys; Transformers; Software Engineering Education; Training; Reviews; Iterative methods; Subspace constraints; Computer science; Adaptive computer learning; Code Reviews; Mixed methods research; Qualitative","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IG4SD6F4","conferencePaper","2025","Domanski, Peter; Faridi, Mukarram Ali; Kaunang, Gabriel; Pradeep, Wilson; Singh, Adit; Amrizal, Muhammad Alfian; Li, Yanjing; Firouzi, Farshad; Chakrabarty, Krishnendu","Silent Data Corruption: Advancing Detection, Diagnosis, and Mitigation Strategies","2025 IEEE 43rd VLSI Test Symposium (VTS)","","","10.1109/VTS65138.2025.11022867","","Silent Data Corruptions (SDCs) pose a critical challenge to computer system reliability, arising from vulnerabilities across different layers of the computing stack. This paper addresses this challenge through three complementary contributions that systematically target SDCs from hardware manufacturing to application-level resilience. First, we analyze timing failures caused by random process variations in advanced technology nodes, revealing that extreme slow paths at lower voltages are dominated by single weak transistors—insights crucial for manufacturing and in-field testing. Second, we introduce an LLM-driven framework that generates targeted functional test programs to induce SDCs, demonstrating its effectiveness in stressing hardware, uncovering latent vulnerabilities, and increasing energy consumption in a given device under test (DUT), making it a valuable tool for in-field testing. Third, as machine learning continues to drive advancements across critical domains such as healthcare, finance, and autonomous systems, ensuring its reliability is paramount. However, the susceptibility of these applications to SDCs threatens their reliability and robustness. To address this, we propose Fidelity-Q, a novel fault injection methodology to evaluate the impact of SDCs on Quantized Neural Networks (QNNs), showing that lower-bit quantization increases error susceptibility. Collectively, these contributions provide a comprehensive approach to identifying, analyzing, and mitigating SDCs across the computing stack, from hardware testing to machine learning applications.","2025-04","2025-11-25 22:40:42","2025-11-25 22:40:42","","1-11","","","","","","","","","","","","","","","","","","","","ISSN: 2375-1053","","","","Test pattern generators; Machine learning; Testing; Hardware; Test Generation; Reliability; Prevention and mitigation; Manufacturing; Very large scale integration; Computer network reliability; Error Detection; Fault Injection; Hardware Reliability; Resilience; Silent Data Corruption","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"33ZMMJRR","conferencePaper","2025","Ouédraogo, Wendkûuni C.; Li, Yinghua; Dang, Xueqi; Zhou, Xin; Koyuncu, Anil; Klein, Jacques; Lo, David; Bissyandé, Tegawendé F.","Rethinking Cognitive Complexity for Unit Tests: Toward a Readability-Aware Metric Grounded in Developer Perception","2025 IEEE International Conference on Software Maintenance and Evolution (ICSME)","","","10.1109/ICSME64153.2025.00082","","Automatically generated unit tests-from searchbased tools like EvoSuite or LLMs-vary significantly in structure and readability. Yet most evaluations rely on metrics like Cyclomatic Complexity and Cognitive Complexity, designed for functional code rather than test code. Recent studies have shown that SonarSource's Cognitive Complexity metric assigns nearzero scores to LLM-generated tests, yet its behavior on EvoSuitegenerated tests and its applicability to test-specific code structures remain unexplored. We introduce CCTR, a Test-Aware Cognitive Complexity metric tailored for unit tests. CCTR integrates structural and semantic features like assertion density, annotation roles, and test composition patterns-dimensions ignored by traditional complexity models but critical for understanding test code. We evaluate \mathbf1 5, 7 5 0 test suites generated by EvoSuite, GPT4o, and Mistral Large-1024 across 350 classes from Defects4J and SF110. Results show CCTR effectively discriminates between structured and fragmented test suites, producing interpretable scores that better reflect developer-perceived effort. By bridging structural analysis and test readability, CCTR provides a foundation for more reliable evaluation and improvement of generated tests. We publicly release all data, prompts, and evaluation scripts to support replication.","2025-09","2025-11-25 22:40:42","2025-11-25 22:40:42","","797-802","","","","","","","","","","","","","","","","","","","","ISSN: 2576-3148","","","","Large language models; Semantics; Test pattern generators; Software engineering; Software reliability; Software quality; Codes; Measurement; Large Language Models; Automatic Test Generation; Software Quality; Empirical Software Engineering; Complexity theory; Software maintenance; Cognitive complexity; Metrics; Test Code Understandability; Unit test","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I32PX4WZ","journalArticle","2024","Çetiner, Gökhan; Yayan, Uğur; Yazici, Ahmet","Mutation-Based White Box Testing of Deep Neural Networks","IEEE Access","","2169-3536","10.1109/ACCESS.2024.3482114","","Deep Neural Networks (DNNs) are used in many critical areas, such as autonomous vehicles, generative AI systems, etc. Therefore, testing DNNs is vital, especially for models used in critical areas. Mutation-based testing is a very successful technique for testing DNNs by mutating their complex structures. Deep Mutation Module was developed to address mutation-based testing and the robustness challenges of DNNs. It analyses the structures of DNNs in detail. It tests models by applying mutation to parameters and structures using its fault library. Testing DNN structures and detecting faults is a highly complex and open-ended challenge. The method proposed in this study applies mutations to DNN parameters to expose faults and weaknesses in the models, thereby testing their robustness. The paper focuses on mutation-based tests of an Reinforce Learning (RL) model developed for electric vehicle routing, a Long Short-Term Memory (LSTM) model developed for prognostic predictions, and a Transformer-based neural network model for electric vehicle routing tasks. The best mutation scores for the LSTM model were measured as 96%, 91.02%, 71.19%, and 68.77%. The test results for the RL model resulted in mutation scores of 93.20%, 72.13%, 77.47%, 79.28%, and 55.74%. The mutation scores of the Transformer model were 75.87%, 76.36%, and 74.93%. These results show that the module can successfully test the targeted models and generate mutants classified as “survived mutants” that outperform the original models. In this way, it provides critical information to researchers to improve the overall performance of the models. Conducting these tests before using them in real-world applications minimizes faults and maximizes model success.","2024","2025-11-25 22:40:42","2025-11-25 22:40:42","","160156-160174","","","12","","","","","","","","","","","","","","","","","","","","","Software testing; Accuracy; Testing; Transformers; machine learning; reinforcement learning; Libraries; Robustness; transformers; deep neural networks; Predictive models; Reinforcement learning; Artificial neural networks; Convolutional neural network; Convolutional neural networks; Long short term memory; long short-term memory; mutation-based testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CMI6R9YE","conferencePaper","2025","Wang, Yuchen; Guo, Shangxin; Tan, Chee Wei","Contextual Augmented Multi-Model Programming (CAMP): A Local-Cloud Copilot Solution","2025 IEEE Conference on Artificial Intelligence (CAI)","","","10.1109/CAI64502.2025.00122","","The rapid advancement of cloud-based Large Language Models (LLMs) has revolutionized AI-assisted programming, but their integration into local development environments faces trade-offs between performance and cost. Cloud LLMs deliver superior generative power but incur high computational costs and latency, whereas local models offer faster, context-aware retrieval but are limited in scope. To address this, we propose Camp,a multi-model copilot solution that leverages context-based Retrieval Augmented Generation (RAG) to enhance LLM performance through dynamic context retrieval from local codebases which optimizes context-aware prompt construction. Experimental results show Campachieves a 12.5% improvement over context-less generation and 6.3% over the basic RAG approach. We demonstrate the methodology through the development of “Copilot for Xcode,” which supports generative programming tasks including code completion, error detection, and documentation. The tool gained widespread adoption and was subsequently integrated into GitHub Copilot, highlighting CAMP's impact on AI-assisted programming and its potential to transform future software development workflows.","2025-05","2025-11-25 22:40:42","2025-11-25 22:40:42","","675-681","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Software testing; Test pattern generators; Software reliability; Software development management; Codes; Large Language Models; Software Engineering; Programming; Retrieval Augmented Generation; AI-Assisted Programming; Retrieval augmented generation; Transforms; Context modeling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2HJY6KIU","conferencePaper","2025","Panagopoulou, Artemis; Zhou, Honglu; Savarese, Silvio; Xiong, Caiming; Callison-Burch, Chris; Yatskar, Mark; Niebles, Juan Carlos","ViUniT: Visual Unit Tests for More Robust Visual Programming","2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","10.1109/CVPR52734.2025.02295","","Programming based approaches to reasoning tasks have substantially expanded the types of questions models can answer about visual scenes. Yet on benchmark visual reasoning data, when models answer correctly, they produce incorrect programs 33% of the time. These models are often right for the wrong reasons and risk unexpected failures on new data. Unit tests play a foundational role in ensuring code correctness and could be used to repair such failures. We propose Visual Unit Testing (ViUniT), a framework to improve the reliability of visual programs by automatically generating unit tests. In our framework, a unit test is represented as a novel image and answer pair meant to verify the logical correctness of a program produced for a given query. Our method leverages a language model to create unit tests in the form of image descriptions and expected answers, followed by image synthesis to produce corresponding images. We conduct a comprehensive analysis of what constitutes an effective visual unit test suite, exploring unit test generation, sampling strategies, image generation methods, and varying the number of programs and unit tests. Additionally, we introduce four applications of visual unit tests: best program selection, answer refusal, re-prompting, and unsupervised reward formulations for reinforcement learning. Experiments with two models across three datasets in visual question answering and image-text matching demonstrate that ViUniT improves model performance by 11.4 points in accuracy. Notably, it enables 7B open-source language models to outperform gpt-4o-mini in visual program generation by an average of 7.7 points and reduces the occurrence of programs that are correct for the wrong reasons by 40%.","2025-06","2025-11-25 22:40:42","2025-11-25 22:40:42","","24646-24656","","","","","","","","","","","","","","","","","","","","ISSN: 2575-7075","","","","Test pattern generators; Visualization; Cognition; llm; unit tests; Programming; Reliability; Data models; Reinforcement learning; Question answering (information retrieval); diffusion models; compositional reasoning; Image synthesis; multimodal; Pattern recognition; visual programing; visual question answering; vlm; vqa","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JZJT2ZUF","conferencePaper","2024","Esfahani, Ali Mohammadi; Kahani, Nafiseh; Ajila, Samuel A.","Understanding Defects in Generated Codes by Language Models","2024 34th International Conference on Collaborative Advances in Software and COmputiNg (CASCON)","","","10.1109/CASCON62161.2024.10837857","","This study investigates the reliability of code generation by Large Language Models (LLMs), focusing on identifying and analyzing defects in the generated code. Despite the advanced capabilities of LLMs in automating code generation, ensuring the accuracy and functionality of the output remains a significant challenge. By using a structured defect classification method to understand their nature and origins this study categorizes and analyzes 367 identified defects from code snippets generated by LLMs, with a significant proportion being functionality and algorithm errors. These error categories indicate key areas where LLMs frequently fail, underscoring the need for targeted improvements. To enhance the accuracy of code generation, this paper implemented five prompt engineering techniques, including Scratchpad Prompting, Program of Thoughts Prompting, Chain-of-Thought Prompting, Chain of Code Prompting, and Structured Chain-of-Thought Prompting. These techniques were applied to refine the input prompts, aiming to reduce ambiguities and improve the models' accuracy rate. The research findings suggest that precise and structured prompting significantly miti-gates common defects, thereby increasing the reliability of LLM-aenerated code.","2024-11","2025-11-25 22:40:42","2025-11-25 22:40:42","","1-10","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Software; Software reliability; Accuracy; Codes; Prompt engineering; Focusing; Software Testing; Prompt Engineering; Large Language Models (LLMs); Code Generation; Collaboration; Encoding; Reliability engineering; Defect Classification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S6HVAEHD","conferencePaper","2025","Abdelraheem, Ahmed; Elbanna, Malak; Elnaggar, Mohamed; Shawky, Doaa","Defining a New Metric for Detecting Bias in Software Systems: Towards Ethical Software Engineering","2025 15th International Conference on Electrical Engineering (ICEENG)","","","10.1109/ICEENG64546.2025.11031392","","Bias in software systems poses ethical concerns that may lead to unintended discrimination, particularly when sensitive variables (e.g., gender, ethnicity) influence decision-making processes. While bias detection in machine learning models has been extensively studied, traditional software systems remain largely unexplored. However, implicit bias can manifest in conditional logic, user role definitions, or static decision trees, which directly influences user experience and access equity in real-world applications (e.g., government services or healthcare platforms). This paper presents a novel static analysis methodology for detecting and quantifying bias in general-purpose software systems. The proposed approach leverages static backward slicing to isolate relevant code, builds a Control Flow Graph (CFG) to trace sensitive variables in conditional branches, and utilizes a Control Dependency Graph (CDG) to assess bias propagation in a weighted-analysis format. Two bias metrics are inferred from the process: Bias Impact Score (BIS), which quantifies how the detected bias influences code execution, and the Bias Severity Score (BSS), which measures the broader implications of the impact. A final composite metric is introduced combining static code structure and ML-based sensitivity analysis. The proposed methodology is evaluated using an LLM-generated dataset of 3920 code snippets from prior research, covering different demographic bias directions such as ethnicity, gender, religion, and occupation. Results, achieving 94.6% accuracy in bias detection, show that the proposed methodology effectively identifies and quantifies bias, allowing developers to mitigate ethical risks early in the software development lifecycle. This research paper provides a foundation for ethical software engineering by offering a systematic and scalable approach to bias detection not only limited to AI-driven models. The methodology currently focuses on Python code and may require adaptation for multi-file projects, reflecting scalability trade-offs. This opens future work opportunities including advanced contextual and implicit bias detection in several frameworks.","2025-05","2025-11-25 22:40:42","2025-11-25 22:40:42","","1-6","","","","","","","","","","","","","","","","","","","","","","","","software testing; Software engineering; Static analysis; Machine learning; Software systems; Codes; Measurement; Ethics; Systematics; User experience; and program slicing; bias detection; control flow analysis; ethical software systems; Ethnicity; machine learning (ML)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UP8H9RZI","conferencePaper","2025","Huang, Jiaying","Research on Multi-Model Fusion Machine Learning Demand Intelligent Forecasting System in Cloud Computing Environment","2025 2nd International Conference on Intelligent Algorithms for Computational Intelligence Systems (IACIS)","","","10.1109/IACIS65746.2025.11210946","","Background: Large language models (LLMs) are increasingly being used for automated unit test generation, but reported performance varies across tasks and datasets, and key aspects such as assertion plausibility and target coverage are not well understood. Methods: We perform a structured evaluation of LLM-based test generation on recent benchmarks and settings, summarizing research results in hinting, static analysis guidance, multi-agent work frameworks, and oracle generation; we compare LLMs with traditional tools such as EvoSuite, and analyze factors that affect coverage, fault detection, and maintainability. Results: When LLMs are used in conjunction with static analysis or method slicing, competitive and improved coverage can be achieved; achieving target line/branch/path coverage and obtaining robust oracles remain challenging; using multi-stage hints and tools (e.g., interpreters/RAGs) can enhance the correctness of results, and manually verified benchmarks can effectively improve the reliability of evaluations. The tests generated by LLM are promising but not uniformly so: future extensions depend on better oracle reasoning, task-appropriate prompts and scaffolding, and rigorous contamination-aware evaluation with statistical tests and repeatable properties.","2025-08","2025-11-25 22:40:42","2025-11-25 22:40:42","","1-7","","","","","","","","","","","","","","","","","","","","","","","","Test pattern generators; Static analysis; Benchmark testing; machine learning; Reliability; Predictive models; Computational modeling; Cloud computing; Machine learning algorithms; cloud computing; Demand forecasting; ensemble learning; resource demand forecasting; Stacking","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F6RVYDTX","journalArticle","2025","Lee, Dong-Kyu; Joe, Inwhee","A GPT-Based Code Review System With Accurate Feedback for Programming Education","IEEE Access","","2169-3536","10.1109/ACCESS.2025.3581139","","The increasing demand for programming education and growing class sizes require immediate and personalized feedback. However, integrating Large Language Models (LLMs) like ChatGPT in introductory programming courses raises concerns about AI-assisted cheating. In large-scale settings, faulty code submissions may lead LLMs to overanalyze, causing unnecessary token consumption. This paper proposes a GPT-4o-based code review system that provides accurate feedback while reducing token usage and preventing AI-assisted cheating. Unlike general-purpose LLM tools for professionals, the system is pedagogically designed for primary and secondary students by focusing on review necessity and learner-friendly feedback. The system features a Code Review Module (CRM) that reduces token usage via a Review Necessity Chain (RNC), and Code Correctness Check Module (CCM) combining test case validation with LLM-based assessment. To prevent AI-assisted cheating, the system provides automated feedback on submitted code without prompting and revealing correct answers, which are accessed only through the “Ask Code Tutor” button. In usability test, the system detected up to 42.86% more errors than a conventional online judge. BERTScore analysis showed that over 80% of the system-generated reviews were semantically aligned with human feedback. A performance comparison with state-of-the-art systems demonstrated a blocking success rate of 86%, with a comparable review omission rate. These results indicate that the system provides more accurate feedback than conventional automated code reviews, while achieving token efficiency and supporting self-directed learning through educational feedback. Thus, it can serve as a practical solution for scalable programming education in primary and secondary classes.","2025","2025-11-25 22:40:42","2025-11-25 22:40:42","","105724-105737","","","13","","","","","","","","","","","","","","","","","","","","","Automation; Accuracy; Chatbots; Education; Codes; Proposals; GPT-4o; programming education; Large language models (LLMs); Training; Reviews; Programming profession; Usability; LangChain; learner-friendly code reviews","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E9SBTBC5","conferencePaper","2025","Mulakala, Benarji; Saini, Madan Lal; Bhukya, Vamsi; Siddhartha; Chand, Neela Prem","Evaluation of LLMs’ Reasoning for Retrieval Augmented Generation (RAG) Systems","2025 World Skills Conference on Universal Data Analytics and Sciences (WorldSUAS)","","","10.1109/WorldSUAS66815.2025.11199211","","Large Language Models (LLMs) have shown remarkable capabilities in various Natural Language Processing Tasks, but are usually suffered by hallucinations which refers to generating factually incorrect content. Retrieval-Augmented Generation (RAG) has shown itself to be a promising approach for minimizing such hallucinations in by accessing external data sources. However, the impact of LLM’s reasoning capabilities such as Chain-of-thought (CoT) prompting in RAG systems is unclear. Three different LLMs, o1-mini, o3-mini, and gpt-4o, with the same RAG pipeline are evaluated on the same PubMed dataset across three different performance metrics Mean Reciprocal Rank (MRR), Faithfulness, and Semantic Answer Similarity (SAS). All three models scored a perfect MRR score, meaning good document retrieval but the reasoning models performed better than non-reasoning baseline in faithfulness and SAS. O1-mini model recorded the optimal somatic quality versus roundedness tradeoff. The outcome indicates that it is possible to improve the quality and credibility of produced answers by using the reasoning-capable LLMs for test generation in RAG systems.","2025-08","2025-11-25 22:40:42","2025-11-25 22:40:42","","1-5","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Semantics; Test pattern generators; Large Language Models; Cognition; Retrieval-Augmented Generation; Hallucination; Pipelines; Question answering (information retrieval); Retrieval augmented generation; Faithfulness; Performance metrics; Reasoning; Semantic Answer Similarity; Soft sensors; Synthetic aperture sonar","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E6Q6ECLB","journalArticle","2025","Nadǎş, Mihai; Dioşan, Laura; Tomescu, Andreea","Synthetic Data Generation Using Large Language Models: Advances in Text and Code","IEEE Access","","2169-3536","10.1109/ACCESS.2025.3589503","","This survey reviews how large language models (LLMs) are transforming synthetic training data generation in both natural language and code domains. By producing artificial but task-relevant examples, these models can significantly augment or even substitute for real-world datasets, particularly in scenarios where labeled data is scarce, expensive, or sensitive. This paper surveys recent advances in leveraging LLMs to create synthetic text and code, highlighting key techniques such as prompt-based generation, retrieval-augmented pipelines, and iterative self-refinement. We examine how these methods can enrich low-resource tasks (e.g. classification, question answering) and facilitate code-centric applications (e.g. instruction tuning, code translation, bug repair) through automated verification of functional correctness. Alongside potential benefits—cost-effectiveness, broad coverage, and controllable diversity—we discuss the accompanying challenges, including factual inaccuracies in generated text, insufficient stylistic or distributional realism, and risks of bias amplification. Proposed mitigation strategies range from filtering and weighting synthetic outputs to reinforcement learning with execution feedback in code domains. We conclude by outlining open research directions, such as automated prompt engineering, cross-modal data synthesis, and robust evaluation frameworks, underscoring the growing importance of LLM-generated synthetic data in accelerating AI development while emphasizing ethical and quality safeguards.","2025","2025-11-25 22:41:16","2025-11-25 22:41:16","","134615-134633","","","13","","","","","","","","","","","","","","","","","","","","","Large language models; Natural language processing; Codes; prompt engineering; Surveys; code generation; large language models (LLMs); Data models; Training; Tuning; Reviews; natural language processing (NLP); Translation; Synthetic data; automated data annotation; bias and fairness in synthetic data; code data synthesis; evaluation of synthetic data; instruction tuning; machine learning training data; model collapse in LLMs; reinforcement learning for code; retrieval-augmented generation (RAG); Synthetic data generation; text data augmentation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"USL4GFP4","conferencePaper","2025","Eggenkemper, Florian; Rehers, Teresa; Swerew, Jana; Mertens, Robert","SPOC: A Scale for Potential Operation Consequences of UI Interactions","2025 Conference on Artificial Intelligence x Multimedia (AIxMM)","","","10.1109/AIxMM62960.2025.00011","","Many approaches rely on a semantic understanding of screen contents to perform further actions on this information. This is relevant for test automation, robotic process automation, or context based assistive tasks (digital agents). Most of these approaches just analyze basic information about user interface elements but fall short in predicting the consequences of single user interface actions. Possible consequences of UI actions include (permanent) data loss, effects on the reputation of people (e.g. posting on social media), or other irreversible consequences. This makes it necessary to identify and analyze the potential consequences of operation on user interfaces to prevent automatic systems from making possibly fatal mistakes with a high impact on the human user. This paper presents a Scale for Potential Operation Consequences (SPOC) and demonstrates an automatic rating system based on Large Language Models (LLMs), that can be performed locally or in cloud implementations. The LLM-based ratings were tested against a human control group based on a questionnaire (n=35), and proved to perform as well as their human counterparts, making SPOC a robust and automation-friendly rating system for user interface element actions.","2025-02","2025-11-25 22:41:16","2025-11-25 22:41:16","","28-34","","","","","","","","","","","","","","","","","","","","","","","","Large language models; Semantics; User interfaces; scale; Social networking (online); hci; Intelligent automation; user interface","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"96WQ75MS","conferencePaper","2025","Firouzi, Farshad; Pan, David Z.; Gu, Jiaqi; Farahani, Bahar; Chaudhuri, Jayeeta; Yin, Ziang; Ma, Pingchuan; Domanski, Peter; Chakrabarty, Krishnendu","ChipMnd: LLMs for Agile Chip Design","2025 IEEE 43rd VLSI Test Symposium (VTS)","","","10.1109/VTS65138.2025.11022936","","The increasing complexity of semiconductor design, along with stringent performance, power, and time-to-market requirements, has outpaced the capabilities of traditional Electronic Design Automation (EDA) methodologies. Conventional design workflows rely on manual intervention for critical tasks such as hardware description, synthesis optimization, and verification, leading to inefficiencies and scalability limitations. Large Language Models (LLMs) present a transformative approach by automating key stages of the design pipeline, enabling intelligent synthesis tuning, test generation, and security analysis. This paper introduces ChipMind, an LLM-driven framework comprising specialized agents and modules for digital and analog chip design. ChipMind integrates AI-driven methodologies to enhance design efficiency, accelerate prototyping, and optimize key design trade-offs, thereby addressing fundamental challenges in modern semiconductor development.","2025-04","2025-11-25 22:41:16","2025-11-25 22:41:16","","1-10","","","","","","","","","","","","","","","","","","","","ISSN: 2375-1053","","","","Large language models; Optimization; Test pattern generators; Scalability; Large Language Models (LLMs); Complexity theory; Very large scale integration; Design methodology; Hardware security; Design automation; AI for chip design; Chip scale packaging; Electronic Design Automation (EDA); hardware security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LPIGIV4T","conferencePaper","2024","Xia, Xin; Jin, Zhi; Aiello, Marco; Zhang, Dongmei; Liang, Guangtai; Hu, Xing","Software Service Engineering in the Era of Large Language Models","2024 IEEE International Conference on Software Services Engineering (SSE)","","","10.1109/SSE62657.2024.00026","","Large Language Models (LLMs) such as GPT-4, trained on massive amounts of natural language and source code data, have exhibited remarkable proficiency in automating many aspects of software development and maintenance. As a result, these models have been extensively applied to various Software Service Engineering (SSE) tasks, including software requirement analysis, software coding, software testing, and Artificial Intelligence for IT Operations (AIOps). Despite their widespread adoption, numerous challenges persist in fully utilizing LLMs for SSE, such as the need for integrating domain-specific knowledge to generate project-level code or patches effectively. Furthermore, there remains a lack of clarity on how traditional SSE practices can adapt to support the full lifecycle of LLMs, from the initial training and fine-tuning with domain-specific data to the ongoing inference, application, and maintenance (i.e., LLMOps). Effective LLMOps require new methodologies and tools to manage the unique demands of LLMs, including data handling, model updates, performance monitoring, and scalability. These challenges underscore the need for innovative approaches to manage the integration of LLM capabilities within established SSE frameworks.","2024-07","2025-11-25 22:41:16","2025-11-25 22:41:16","","xxiii-xxiii","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""